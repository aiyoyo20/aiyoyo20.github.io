<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <!-- Set the viewport width to device width for mobile -->
    <meta name="viewport" content="width=device-width" />

    <title>第五章_爬虫</title>

    <link rel="stylesheet" href="/theme/css/normalize.css" />
    <link rel="stylesheet" href="/theme/css/foundation.min.css" />
    <link rel="stylesheet" href="/theme/css/style.css" />
    <link rel="stylesheet" href="/theme/css/pygments.css" />	
    <script src="/theme/js/custom.modernizr.js"></script>

    <!-- So Firefox can bookmark->"abo this site" -->

</head>

<body>

<!-- Nav Bar -->
<nav>
<div class="top-bar">
<div class="row">
    <div class="large-9 large-centered columns">
	    <h1><a href="">gogogo</a></h1>
    </div>
</div>
</div>

<!-- Show menu items and pages -->
<div class="row">
<div class="large-9 columns">
    <ul class="button-group navigation">

    </ul>
</div>
</div>
</nav>
<!-- End Nav -->


<!-- Main Page Content and Sidebar -->
<div class="row">

    <!-- Main Blog Content -->
    <div class="large-9 columns">
<article>
    <header>
        <h3 class="article-title"><a href="/di-wu-zhang-_pa-chong.html" rel="bookmark"
        title="Permalink to 第五章_爬虫">第五章_爬虫</a></h3>
    </header>

<h6 class="subheader" title="2022-05-12T18:44:45.262486+02:00">Thu 12 May 2022
</h6>


    <h2>第五章 爬虫</h2>
<h3>一、常用库与模块</h3>
<h4>1. 试列出至少三种目前流行的大型数据库的名称:</h4>
<blockquote>
<p><code>____</code>、<code>____</code>、<code>____</code>, 其中您最熟悉的是 <code>____</code>, 从 <code>____</code> 年开始使用（考察对数据可的熟悉程度，同时考察你的工作年限注意和自己简历一致）。
Oracle，Mysql，SQLServer、MongoDB 根据自己情况（推荐 Mysql 、MongoDB）。</p>
</blockquote>
<h4>2. 列举您使用过的 Python 网络爬虫所用到的网络数据包？</h4>
<blockquote>
<p><code>requests</code>、<code>urllib</code>、<code>urllib2</code>、<code>httplib2</code>。</p>
</blockquote>
<h4>3. 列举您使用过的 Python 网络爬虫所用到的解析数据包</h4>
<blockquote>
<p><code>BeautifulSoup</code>、<code>pyquery</code>、<code>Xpath</code>、<code>lxml</code>。</p>
</blockquote>
<h4>4. 爬取数据后使用哪个数据库存储数据的，为什么？</h4>
<blockquote>
<p><code>MongoDB</code> 是使用比较多的数据库，这里以 MongoDB 为例，大家需要结合自己真实开发环境回答。
<code>原因</code>：
1）与关系型数据库相比，MongoDB 的优点如下。
①弱一致性（最终一致），更能保证用户的访问速度.</p>
</blockquote>
<div class="highlight"><pre><span></span><code>举例来说，在传统的关系型数据库中，一个 COUNT 类型的操作会锁定数据集，这样可以保证得到 “当前” 情况下的较精确值。

这在某些情况下，例如通过 ATM 查看账户信息的时候很重要，但对于 Wordnik 来说，数据是不断更新和增长的，这种 “较精确” 的保证几乎没有任何意义，反而会产生很大的延迟。

他们需要的是一个 “大约” 的数字以及更快的处理速度。

但某些情况下 MongoDB 会锁住数据库。如果此时正有数百个请求，则它们会堆积起来，造成许多问题。我们使用了下面的优化方式来避免锁定。

每次更新前，我们会先查询记录。查询操作会将对象放入内存，于是更新则会尽可能的迅速。在主 / 从部署方案中，从节点可以使用 “-pretouch” 参数运行，这也可以得到相同的效果。

使用多个 mongod 进程。我们根据访问模式将数据库拆分成多个进程。
</code></pre></div>

<p>②文档结构的存储方式，能够更便捷的获取数据。</p>
<div class="highlight"><pre><span></span><code>对于一个层级式的数据结构来说，如果要将这样的数据使用扁平式的，表状的结构来保存数据，
这无论是在查询还是获取数据时都十分困难。
</code></pre></div>

<p>③内置 GridFS，支持大容量的存储。</p>
<div class="highlight"><pre><span></span><code>GridFS 是一个出色的分布式文件系统，可以支持海量的数据存储。
内置了 GridFS 了 MongoDB，能够满足对大数据集的快速范围查询。
</code></pre></div>

<p>④内置 Sharding。</p>
<div class="highlight"><pre><span></span><code>提供基于 Range 的 Auto Sharding 机制：
一个 collection 可按照记录的范围，分成若干个段，切分到不同的 Shard 上。
Shards 可以和复制结合，配合 Replica sets 能够实现 Sharding+fail-over，不同的 Shard 之间可以负载均衡。
查询是对客户端是透明的。
客户端执行查询，统计，MapReduce 等操作，这些会被 MongoDB 自动路由到后端的数据节点。
这让我们关注于自己的业务，适当的时候可以无痛的升级。
MongoDB 的 Sharding 设计能力较大可支持约 20 petabytes，足以支撑一般应用。
这可以保证 MongoDB 运行在便宜的 PC 服务器集群上。
PC 集群扩充起来非常方便并且成本很低，避免了 “sharding” 操作的复杂性和成本。
</code></pre></div>

<p>⑤第三方支持丰富。(这是与其他的 NoSQL 相比，MongoDB 也具有的优势)</p>
<div class="highlight"><pre><span></span><code>现在网络上的很多 NoSQL 开源数据库完全属于社区型的，没有官方支持，给使用者带来了很大的风险。
而开源文档数据库 MongoDB 背后有商业公司 10gen 为其提供供商业培训和支持。
而且 MongoDB 社区非常活跃，很多开发框架都迅速提供了对 MongDB 的支持。
不少知名大公司和网站也在生产环境中使用 MongoDB，越来越多的创新型企业转而使用 MongoDB 作为和 Django，RoR 来搭配的技术方案。
</code></pre></div>

<p>⑥性能优越</p>
<div class="highlight"><pre><span></span><code>在使用场合下，千万级别的文档对象，近 10G 的数据，对有索引的 ID 的查询不会比 mysql 慢，而对非索引字段的查询，则是全面胜出。
mysql 实际无法胜任大数据量下任意字段的查询，而 mongodb 的查询性能实在让我惊讶。
写入性能同样很令人满意，同样写入百万级别的数据，mongodb 比我以前试用过的 couchdb 要快得多，基本 10 分钟以下可以解决。
补上一句，观察过程中 mongodb 都远算不上是 CPU 杀手。
</code></pre></div>

<h4>2) Mongodb 与 redis 相比较</h4>
<p>①mongodb 文件存储是 BSON 格式类似 JSON，或自定义的二进制格式。</p>
<div class="highlight"><pre><span></span><code>mongodb 与 redis 性能都很依赖内存的大小，mongodb 有丰富的数据表达、索引；最类似于关
系数据库，支持丰富的查询语言，redis 数据丰富，较少的 IO ，这方面 mongodb 优势明显。
</code></pre></div>

<p>②mongodb 不支持事物，靠客户端自身保证，redis 支持事务，比较弱，仅能保证事物中的操作按顺序执行，这方面 redis 优于 mongodb。</p>
<p>③mongodb 对海量数据的访问效率提升，redis 较小数据量的性能及运算，这方面 mongodb 性能优于 redis .monbgodb 有 mapredurce 功能，提供数据分析，redis 没有，这方面 mongodb 优于 redis 。</p>
<h4>5. 你用过的爬虫框架或者模块有哪些？谈谈他们的区别或者优缺点？</h4>
<table>
<thead>
<tr>
<th>爬虫</th>
<th>模块</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python 自带</td>
<td>urllib、urllib2</td>
</tr>
<tr>
<td>第三方</td>
<td>requests</td>
</tr>
<tr>
<td>框架</td>
<td>Scrapy</td>
</tr>
</tbody>
</table>
<blockquote>
<p>urllib 和 urllib2 模块都做与请求 URL 相关的操作，但他们提供不同的功能。
urllib2：urllib2.urlopen 可以接受一个 Request 对象或者 url，（在接受 Request 对象时候，并以此可以来设置一个 URL 的 headers），urllib.urlopen 只接收一个 url。
urllib 有 urlencode,urllib2 没有，因此总是 urllib，urllib2 常会一起使用的原因。<br>
scrapy 是封装起来的框架，它包含了下载器，解析器，日志及异常处理，基于多线程，twisted 的方式处理，对于固定单个网站的爬取开发，有优势，但是对于多网站爬取，并发及分布式处理方面，不够灵活，不便调整与括展。
request 是一个 HTTP 库， 它只是用来，进行请求，对于 HTTP 请求，他是一个强大的库，下载，解析全部自己处理，灵活性更高，高并发与分布式部署也非常灵活，对于功能可以更好实现</p>
</blockquote>
<p>Scrapy 优点：</p>
<blockquote>
<p>scrapy 是异步的；<br>
采取可读性更强的 xpath 代替正则；<br>
强大的统计和 log 系统；<br>
同时在不同的 url 上爬行；<br>
支持 shell 方式，方便独立调试；<br>
写 middleware, 方便写一些统一的过滤器；<br>
通过管道的方式存入数据库；</p>
</blockquote>
<p>Scrapy 缺点：</p>
<blockquote>
<p>基于 python 的爬虫框架，扩展性比较差；<br>
基于 twisted 框架，运行中的 exception 是不会干掉 reactor，并且异步框架出错后是不会停掉  其他任务的，数据出错后难以察觉。</p>
</blockquote>
<h4>6. 写爬虫是用多进程好？还是多线程好？ 为什么？</h4>
<blockquote>
<p>IO 密集型代码 (文件处理、网络爬虫等)，多线程能够有效提升效率 (单线程下有 IO 操作会进行 IO 等<br>
待，造成不必要的时间浪费，而开启多线程能在线程 A 等待时，自动切换到线程 B，可以不浪费 CPU<br>
的资源，从而能提升程序执行效率)。在实际的数据采集过程中，既考虑网速和响应的问题，也需要考虑自<br>
身机器的硬件情况，来设置多进程或多线程。</p>
</blockquote>
<h4>7. 常见的反爬虫和应对方法？</h4>
<blockquote>
<p>通过 Headers 反爬虫：<br>
从用户请求的 Headers 反爬虫是最常见的反爬虫策略。很多网站都会对 Headers 的 User-Agent 进行检测，还有一部分网站会对 Referer 进行检测（一些资源网站的防盗链就是检测 Referer）。如果  遇到了这类反爬虫机制，可以直接在爬虫中添加 Headers，将浏览器的 User-Agent 复制到爬虫的 Headers 中；或者将 Referer 值修改为目标网站域名。对于检测 Headers 的反爬虫，在爬虫中修改或者添加 Headers 就能很好的绕过。  </p>
<p>基于用户行为反爬虫：<br>
还有一部分网站是通过检测用户行为，例如同一 IP 短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。
多数网站都是前一种情况，对于这种情况，使用 IP 代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理 ip，检测后全部保存起来。这样的代理 ip 爬虫经常会用到，最好自己准备一个。有了大量代理 ip 后可以每请求几次更换一个 ip，这在 requests 或者 urllib2 中很容易做到，这样就能很容  易的绕过第一种反爬虫。
对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限  制。</p>
<p>动态页面的反爬虫：<br>
上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过 ajax 请求得到，或者通过 JavaScript 生成的。首先用 Fiddler 对网络请求进行分析。如果能够找到 ajax 请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用 requests 或者 urllib2  模拟 ajax 请求，对响应的 json 进行分析得到需要的数据。<br>
能够直接模拟 ajax 请求获取数据固然是极好的，但是有些网站把 ajax 请求的所有参数全部加密了。<br>
我们根本没办法构造自己所需要的数据的请求。这种情况下就用 selenium+phantomJS，调用浏览器  内核，并利用 phantomJS 执行 js 来模拟人为操作以及触发页面中的 js 脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程  模拟一遍。<br>
用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加 Headers 一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS 就是一个没有界面的  浏览器，只是操控这个浏览器的不是人。利 selenium+phantomJS 能干很多事情，例如识别点触式 （12306）或者滑动式的验证码，对页面表单进行暴力破解等。</p>
</blockquote>
<h4>8. 解析网页的解析器使用最多的是哪几个？</h4>
<blockquote>
<p>lxml，html5lib，html.parser,lxml-xml，正则表达式。</p>
</blockquote>
<h4>9. 需要登录的网页，如何解决同时限制 ip，cookie,session（其中有一些是动态生成的）在不使用动态爬取的情况下？</h4>
<p>解决限制 IP 可以使用代理 IP 地址池、服务器；<br>
不适用动态爬取的情况下可以使用反编译 JS 文件获取相应的文件，或者换用其他平台（比如手机端）<br>
看看是否可以获取相应的 json 文件。</p>
<h4>10. 验证码的解决？&gt; 图形验证码：干扰、杂色不是特别多的图片可以使用开源库 Tesseract 进行识别，太过复杂的需要</h4>
<p>借助第三方打码平台。<br>
点击和拖动滑块验证码可以借助 selenium、无图形界面浏览器（chromedirver 或者 phantomjs）和 pillow 包来模拟人的点击和滑动操作，pillow 可以根据色差识别需要滑动的位置。</p>
<h4>11. 使用最多的数据库（Mysql，Mongodb，redis 等），对他们的理解？</h4>
<blockquote>
<p>MySQL 数据库：开源免费的关系型数据库，需要实现创建数据库、数据表和表的字段，表与表之<br>
间可以进行关联（一对多、多对多），是持久化存储。<br>
Mongodb 数据库：是非关系型数据库，数据库的三元素是，数据库、集合、文档，可以进行持久化存储，也可作为内存数据库，存储数据不需要事先设定格式，数据以键值对的形式存储。
redis 数据库：非关系型数据库，使用前可以不用设置格式，以键值对的方式保存，文件格式相对自由，主要用与缓存数据库，也可以进行持久化存储。</p>
</blockquote>
<h4>12. 字符集和字符编码</h4>
<blockquote>
<p>字符是各种文字和符号的总称，包括各个国家文字、标点符号、图形符号、数字等。<br>
字符集是多个字符的集合，字符集种类较多，每个字符集包含的字符个数不同，常见字符集有：<br>
ASCII 字符集、ISO 8859 字符集、GB2312 字符集、BIG5 字符集、GB18030 字符集、Unicode 字符集等。<br>
字符编码就是以二进制的数字来对应字符集的字符。<br>
常见的编码字符集（简称字符集）如下所示：<br>
Unicode：也叫统一字符集，它包含了几乎世界上所有的已经发现且需要使用的字符（如中文、日文、英文、德文等）。 
ASCII：ASCII 既是编码字符集，又是字符编码。早期的计算机系统只能处理英文，所以 ASCII 也就 成为了计算机的缺省字符集，包含了英文所需要的所有字符。 
GB2312：中文字符集，包含 ASCII 字符集。ASCII 部分用单字节表示，剩余部分用双字节表示。 
GBK：GB2312 的扩展，但完整包含了 GB2312 的所有内容。 
GB18030：GBK 字符集的超集，常叫大汉字字符集，也叫 CJK（Chinese，Japanese，Korea）字符集，包含了中、日、韩三国语。 
注意：Unicode 字符集有多种编码方式，如 UTF-8、UTF-16 等；ASCII 只有一种；大多数 MBCS（包括 GB2312）也只有一种。</p>
</blockquote>
<h4>13. 写一个邮箱地址的正则表达式？</h4>
<blockquote>
<p>[A-Za-z0-9_-]+@[a-zA-Z0-9_-]+(.[a-zA-Z0-9_-]+)+$</p>
</blockquote>
<h4>14. 编写过哪些爬虫中间件？</h4>
<blockquote>
<p>user-agent、代理池等。</p>
</blockquote>
<h4>15. 极验滑动验证码如何破解？</h4>
<blockquote>
<p>1.selenium 控制鼠标实现，速度太机械化，成功率比较低<br>
2. 计算缺口的偏移量（ 推荐博客：http://blog.csdn.net/paololiu/article/details/52514504?%3E<br>
3. 极验滑动验证码需要具体网站具体分析，一般牵扯算法乃至深度学习相关知识。</p>
</blockquote>
<h4>16. 爬的那些内容数据量有多大，多久爬一次，爬下来的数据是怎么存储？</h4>
<blockquote>
<p>京东整站的数据大约在 1 亿左右，爬下来的数据存入数据库，mysql 数据库中如果有重复的 url 建议去重存入数据库，可以考虑引用外键。评分，评论如果做增量，Redis 中 url 去重，评分和评论建议建立一张新表用 id 做关联。<br>
多久爬一次这个问题要根据公司的要求去处理，不一定是每天都爬。<br>
Mongo 建立唯一索引键（id）可以做数据重复前提是数据量不大 2 台电脑几百万的情况数<br>
据库需要做分片（数据库要设计合理）。<br>
例：租房的网站数据量每天大概是几十万条，每周固定爬取。</p>
</blockquote>
<h4>17. cookie 过期的处理问题？</h4>
<blockquote>
<p>因为 cookie 存在过期的现象，一个很好的处理方法就是做一个异常类，如果有异常的话 cookie 抛出异常类在执行程序。</p>
</blockquote>
<h4>18. 动态加载又对及时性要求很高怎么处理？</h4>
<blockquote>
<p>Selenium+Phantomjs<br>
尽量不使用 sleep 而使用 WebDriverWait<br>
关于 HTTP/HTTPS 的区别，分别应该在什么场合下。</p>
</blockquote>
<h4>19. HTTPS 有什么优点和缺点</h4>
<blockquote>
<p>优点：
1、使用 HTTPS 协议可认证用户和服务器，确保数据发送到正确的客户机和服务器；
2、HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、身份认证的网络协议，要比 http<br>
协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。<br>
3、HTTPS 是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本<br>
缺点：<br>
1.HTTPS 协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用<br>
2.HTTPS 协议还会影响缓存，增加数据开销和功耗，甚至已有安全措施也会受到影响也会因此而受到影响。<br>
3.SSL 证书需要钱。功能越强大的证书费用越高。个人网站、小网站没有必要一般不会用。<br>
4.HTTPS 连接服务器端资源占用高很多，握手阶段比较费时对网站的相应速度有负面影响。<br>
5.HTTPS 连接缓存不如 HTTP 高效。</p>
</blockquote>
<h4>20. HTTPS 是如何实现安全传输数据的。(2018-4-20-xhq)</h4>
<blockquote>
<p>HTTPS 其实就是在 HTTP 跟 TCP 中间加多了一层加密层 TLS/SSL。SSL 是个加密套件，负责对 HTTP 的数据进行加密。TLS 是 SSL 的升级版。现在提到 HTTPS，加密套件基本指的是 TLS。原先是应用层将数据直接给到 TCP 进行传输，现在改成应用层将数据给到 TLS/SSL，将数据加密后，再给到 TCP 进行传输。</p>
</blockquote>
<h4>21. TTL，MSL，RTT？</h4>
<blockquote>
<p>MSL：报文最大生存时间，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。
TTL：TTL 是 time to live 的缩写，中文可以译为生存时间，这个生存时间是由源主机设置初始值但不是存的具体时间，而是存储了一个 ip 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。RFC 793 中规定 MSL 为 2 分钟，实际应用中常用的是 30 秒，1 分钟和 2 分钟等。TTL 与 MSL 是有关系的但不是简单的相等的关系，MSL 要大于等于 TTL。<br>
RTT： RTT 是客户到服务器往返所花时间（round-trip time，简称 RTT），TCP 含有动态估算 RTT 的算法。TCP 还持续估算一个给定连接的 RTT，这是因为 RTT 受网络传输拥塞程序的变化而变化。</p>
</blockquote>
<h4>22. 谈一谈你对 Selenium 和 PhantomJS 了解</h4>
<blockquote>
<p>Selenium 是一个 Web 的自动化测试工具，可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。Selenium 自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行， 所以我  们可以用一个叫 PhantomJS 的工具代替真实的浏览器。Selenium 库里有个叫 WebDriver 的 API。
WebDriver 有点儿像可以加载网站的浏览器，但是它也可以像 BeautifulSoup 或者其他 Selector 对象一样用来查找页面元素，与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫。<br>
PhantomJS 是一个基于 Webkit 的无界面 (headless) 浏览器，它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效。相比传统的 Chrome 或 Firefox 浏览器等，资源消耗会更少。<br>
如果我们把 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理 JavaScript、Cookie、headers，以及任何我们真实用户需要做的事情。</p>
<p>主程序退出后，selenium 不保证 phantomJS 也成功退出，最好手动关闭 phantomJS 进程。（有可能会导致多个 phantomJS 进程运行，占用内存）。<br>
WebDriverWait 虽然可能会减少延时，但是目前存在 bug（各种报错），这种情况可以采用 sleep。<br>
phantomJS 爬数据比较慢，可以选择多线程。如果运行的时候发现有的可以运行，有的不能，可以尝试将 phantomJS 改成 Chrome。</p>
</blockquote>
<h4>23. 代理 IP 里的透明匿名高匿分别是指？</h4>
<blockquote>
<p>透明代理的意思是客户端根本不需要知道有代理服务器的存在，但是它传送的仍然是真实的 IP。你  要想隐藏的话，不要用这个。<br>
普通匿名代理能隐藏客户机的真实 IP，但会改变我们的请求信息，服务器端有可能会认为我们使用了代理。不过使用此种代理时，虽然被访问的网站不能知道你的 ip 地址，但仍然可以知道你在使用代理，当然某些能够侦测 ip 的网页仍然可以查到你的 ip。<br>
高匿名代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真实 IP 是隐藏的，服务器端不会认为我们使用了代理。<br>
设置代理有以下两个好处：<br>
1，让服务器以为不是同一个客户端在请求<br>
2，防止我们的真实地址被泄露，防止被追究</p>
</blockquote>
<h4>24. requests 返回的 content 和 text 的区别？</h4>
<blockquote>
<p>a) response.text 返回的是 Unicode 型数据；<br>
a) response.content 返回的是 bytes 类型，也就是二进制数据；<br>
b) 获取文本使用，response.text；<br>
b) 获取图片，文件，使用 response.content；<br>
c) response.text<br>
类型：str<br>
解码类型： 根据 HTTP 头部对响应的编码作出有根据的推测，推测的文本编码<br>
如何修改编码方式：response.encoding=gbk<br>
c) response.content<br>
类型：bytes<br>
解码类型： 没有指定<br>
如何修改编码方式：response.content.decode (utf8)</p>
</blockquote>
<h4>25. robots 协议</h4>
<blockquote>
<p>Robots 协议：网站通过 Robots 协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。</p>
</blockquote>
<h4>26. 为什么 requests 请求需要带上 header？</h4>
<blockquote>
<p>原因是：模拟浏览器，欺骗服务器，获取和浏览器一致的内容<br>
header 的形式：字典<br>
headers = {User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36(KHTML, like Gecko)Chrome/54.0.2840.99 Safari/537.36}<br>
用法： requests.get (url,headers=headers)</p>
</blockquote>
<h4>27. dumps,loads 与 dump,load 的区别？</h4>
<p>json.dumps () 将 pyhton 的 dict 数据类型编码为 json 字符串；
json.loads () 将 json 字符串解码为 dict 的数据类型；
json.dump (x,y) x 是 json 对象，y 是文件对象，最终是将 json 对象写入到文件中；
json.load (y) 从文件对象 y 中读取 json 对象。</p>
<h4>28. 通用爬虫：通常指搜索引擎的爬虫</h4>
<blockquote>
<p>聚焦爬虫：针对特定网站的爬虫
(https://img.alicdn.com/imgextra/i2/296192965/O1CN01yOEDgw1Xm0yDJpLLK_!!296192965.png)</p>
<p><code>通用搜素引擎的局限性</code>：
通用搜索引擎所返回的网页里 90% 的内容无用。
图片、数据库、音频、视频多媒体的内容通用搜索引擎无能为力。不同用户搜索的目的不全相同，但是返回内容相同。</p>
</blockquote>
<h4>29. requests 使用小技巧</h4>
<blockquote>
<p>1、<code>reqeusts.util.dict_from_cookiejar</code> 把 cookie 对象转化为字典
<code>requests.get(url,cookies={})</code></p>
<p>2、设置请求不用 <code>SSL</code> 证书验证
<code>response = requests.get("https://www.123cn/mormhweb/ ", verify=False)</code></p>
<p>3、设置超时
<code>response = requests.get(url,timeout=10)</code></p>
<p>4、配合状态码判断是否请求成功
<code>assert response.status_code == 200</code></p>
</blockquote>
<h4>30. 平常怎么使用代理的？</h4>
<blockquote>
<p>1、自己维护代理池
2、付费购买（目前市场上有很多 ip 代理商，可自行百度了解，建议看看他们的接口文档（API&amp;SDK））</p>
</blockquote>
<h4>31. IP 存放在哪里？怎么维护 IP？对于封了多个 ip 的，怎么判定 IP 没被封？(2018-4-23-xhq)</h4>
<blockquote>
<p>存放在数据库 (redis、mysql 等)。
维护多个代理网站：
一般代理的存活时间往往在十几分钟左右，定时任务，加上代理 IP 去访问网页，验证其是否可用，如果返回状态为 200，表示这个代理是可以使用的。</p>
</blockquote>
<h4>32. 怎么获取加密的数据？</h4>
<p>1、 Web 端加密可尝试移动端（app）
2、 解析加密，看能否破解
3、 反爬手段层出不穷，js 加密较多，只能具体问题具体分析</p>
<h4>33. 假如每天爬取量在 5、6 万条数据，一般开几个线程，每个线程 ip 需要加锁限定吗？</h4>
<p>1、5、6 万条数据相对来说数据量比较小，线程数量不做强制要求 (做除法得一个合理值即可）
2、多线程使用代理，应保证不在同时一刻使用一个代理 IP</p>
<h4>34. 怎么监控爬虫的状态</h4>
<blockquote>
<p>1、使用 python 的 STMP 包将爬虫的状态信心发送到指定的邮箱
2、Scrapyd、pyspider
3、引入日志</p>
</blockquote>
<h3>二、Scrapy</h3>
<h4>1. 谈谈你对 Scrapy 的理解？</h4>
<blockquote>
<p>scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架，我们只需要实现少量代码，就能够快速的抓取到数据内容。Scrapy 使用了 Twisted 异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。<br>
scrapy 框架的工作流程：
1. 首先 Spiders（爬虫）将需要发送请求的 url (requests) 经 ScrapyEngine（引擎）交给 Scheduler（调度器）。
2.Scheduler（排序，入队）处理后，经 ScrapyEngine，DownloaderMiddlewares (可选，主要有 User_Agent， Proxy 代理) 交给 Downloader。
3.Downloader 向互联网发送请求，并接收下载响应（response）。将响应（response）经 ScrapyEngine，SpiderMiddlewares (可选) 交给 Spiders。
4.Spiders 处理 response，提取数据并将数据经 ScrapyEngine 交给 ItemPipeline 保存（可以是本地，可以是数据库）。提取 url 重新经 ScrapyEngine 交给 Scheduler 进行下一个循环。直到无 Url 请求程序停止结束。</p>
</blockquote>
<h4>2. 爬取下来的数据如何去重，说一下具体的算法依据</h4>
<blockquote>
<ol>
<li>通过 MD5 生成电子指纹来判断页面是否改变<br>
2.nutch 去重。nutch 中 digest 是对采集的每一个网页内容的 32 位哈希值，如果两个网页内容完全一样，它们的 digest 值肯定会一样。 数据量不大时，可以直接放在内存里面进行去重，python 可以使用 set () 进行去重。当去重数据需要持久化时可以使用 redis 的 set 数据结构。 当数据量再大一点时，可以用不同的加密算法先将长字符串压缩成 16/32/40 个字符，再使用上面两种方法去重。
当数据量达到亿（甚至十亿、百亿）数量级时，内存有限，必须用位来去重，才能够满足需求。Bloomfilter 就是将去重对象映射到几个内存位，通过几个位的 0/1 值来判断一个对象是否已经存在。<br>
然而 Bloomfilter 运行在一台机器的内存上，不方便持久化（机器 down 掉就什么都没啦），也不方便分布式爬虫的统一去重。如果可以在 Redis 上申请内存进行 Bloomfilter，以上两个问题就都能解决了。<br>
simhash 最牛逼的一点就是将一个文档，最后转换成一个 64 位的字节，暂且称之为特征字，然后判断重复只需要判断他们的特征字的距离是不是 &lt; n（根据经验这个 n 一般取值为 3），就可以判断两个文档是否相似。 可见 scrapy_redis 是利用 set 数据结构来去重的，去重的对象是 request 的 fingerprint（其实就是用 hashlib.sha1 () 对 request 对象的某些字段信息进行压缩）。其实 fp 就是 request 对象加密压缩后的一个字符串（40 个字符，0~f）。</li>
</ol>
</blockquote>
<h5>3. Scrapy 的优缺点？</h5>
<blockquote>
<p>优点
1）scrapy 是异步的<br>
2）采取可读性更强的 xpath 代替正则<br>
3）强大的统计和 log 系统<br>
4）同时在不同的 url 上爬行<br>
5）支持 shell 方式，方便独立调试<br>
5）写 middleware, 方便写一些统一的过滤器<br>
6）通过管道的方式存入数据库<br>
缺点：<br>
1）基于 python 的爬虫框架，扩展性比较差<br>
2）基于 twisted 框架，运行中的 exception 是不会干掉 reactor（反应器），并且异步框架出错后是不会停掉其他任务的，数据出错后难以察觉。</p>
</blockquote>
<h3>三、Scrapy-Redis</h3>
<h4>1. scrapy 和 scrapy-redis 有什么区别？为什么选择 redis 数据库？</h4>
<blockquote>
<p>scrapy 是一个 Python 爬虫框架，爬取效率极高，具有高度定制性，但是不支持分布式。而<br>
scrapy-redis 一套基于 redis 数据库、运行在 scrapy 框架之上的组件，可以让 scrapy 支持分布式策略，
Slaver 端共享 Master 端 redis 数据库里的 item 队列、请求队列和请求指纹集合。<br>
为什么选择 redis 数据库，因为 redis 支持主从同步，而且数据都是缓存在内存中的，所以基于 redis 的分布式爬虫，对请求和数据的高频读取效率非常高。</p>
</blockquote>
<h4>2. 分布式爬虫主要解决什么问题？</h4>
<blockquote>
<p>1.ip
2. 带宽
3.cpu
4.io</p>
</blockquote>
<h4>3. 什么是分布式存储？</h4>
<blockquote>
<p>传统定义：分布式存储系统是大量 PC 服务器通过 Internet 互联，对外提供一个整体的服务。
<code>分布式存储系统具有以下的几个特性</code>：
<code>可扩展</code>：分布式存储系统可以扩展到几百台甚至几千台这样的一个集群规模，系统的整体性能线性增长。
<code>低成本</code>：分布式存储系统的自动容错、自动负载均衡的特性，允许分布式存储系统可以构建在低成本的服务器上。另外，线性的扩展能力也使得增加、减少服务器的成本低，实现分布式存储系统的自动运维。
<code>高性能</code>：无论是针对单台服务器，还是针对整个分布式的存储集群，都要求分布式存储系统具备高性能。
<code>易用</code>：分布式存储系统需要对外提供方便易用的接口，另外，也需要具备完善的监控、运维工具，并且可以方便的与其他的系统进行集成。分布式存储系统的挑战主要在于数据和状态信息的持久化，要求在自动迁移、自动容错和并发读写的过程中，保证数据的一致性。
<code>容错</code>：可以快速检测到服务器故障，并自动的将在故障服务器上的数据进行迁移。
<code>负载均衡</code>：新增的服务器在集群中保障负载均衡？数据迁移过程中保障不影响现有的服务。
<code>事务与并发控制</code>：实现分布式事务。
<code>易用性</code>：设计对外接口，使得设计的系统易于使用。</p>
</blockquote>
<h4>4. 你所知道的分布式爬虫方案有哪些？</h4>
<h4>三种分布式爬虫策略：</h4>
<p>1.Slaver 端从 Master 端拿任务（Request/url/ID）进行数据抓取，在抓取数据的同时也生成新任务，并将任务抛给 Master。Master 端只有一个 Redis 数据库，负责对 Slaver 提交的任务进行去重、加入待爬队列。</p>
<div class="highlight"><pre><span></span><code>优点： scrapy-redis 默认使用的就是这种策略，我们实现起来很简单，因为任务调度等工作 scrapy-redis 都已经帮我们做好了，我们只需要继承 RedisSpider、指定 redis_key 就行了。
缺点： scrapy-redis 调度的任务是 Request 对象，里面信息量比较大（不仅包含 url，还有 callback 函数、headers 等信息），导致的结果就是会降低爬虫速度、而且会占用 Redis 大量的存储空间。当然我们可以重写方法实现调度 url 或者用户 ID。

2.Master 端跑一个程序去生成任务（Request/url/ID）。Master 端负责的是生产任务，并把任务去重、加入到待爬队列。Slaver 只管从 Master 端拿任务去爬。```
优点： 将生成任务和抓取数据分开，分工明确，减少了 Master 和 Slaver 之间的数据交流；Master 端生成任务还有一个好处就是：可以很方便地重写判重策略（当数据量大时优化判重的性能和速度还是很重要的）。
缺点： 像 QQ 或者新浪微博这种网站，发送一个请求，返回的内容里面可能包含几十个待爬的用户 ID，即几十个新爬虫任务。但有些网站一个请求只能得到一两个新任务，并且返回的内容里也包含爬虫要抓取的目标信息，如果将生成任务和抓取任务分开反而会降低爬虫抓取效率。毕竟带宽也是爬虫的一个瓶颈问题，我们要秉着发送尽量少的请求为原则，同时也是为了减轻网站服务器的压力，要做一只有道德的 Crawler。所以，视情况而定。

3.Master 中只有一个集合，它只有查询的作用。Slaver 在遇到新任务时询问 Master 此任务是否已爬，如果未爬则加入 Slaver 自己的待爬队列中，Master 把此任务记为已爬。它和策略一比较像，但明显比策略一简单。策略一的简单是因为有 scrapy-redis 实现了 scheduler 中间件，它并不适用于非 scrapy 框架的爬虫。```
优点： 实现简单，非 scrapy 框架的爬虫也适用。Master 端压力比较小，Master 与 Slaver 的数据交流也不大。
缺点：“健壮性” 不够，需要另外定时保存待爬队列以实现 “断点续爬” 功能。各 Slaver 的待爬任务不通用。

如果把 Slaver 比作工人，把 Master 比作工头。
策略一就是工人遇到新任务都上报给工头，需要干活的时候就去工头那里领任务；
策略二就是工头去找新任务，工人只管从工头那里领任务干活；
策略三就是工人遇到新任务时询问工头此任务是否有人做了，没有的话工人就将此任务加到自己的 “行程表”。
</code></pre></div>

<h4>5. 除了 scrapy-redis，有做过其他的分布式爬虫吗？</h4>
<blockquote>
<p>Celery、gearman 等，参考其他分布式爬虫策略。</p>
</blockquote>
<h4>6. 在爬取的时候遇到某些内容字段缺失怎么判断及处理？</h4>
<blockquote>
<p>判读字段缺失，做异常处理即可。</p>
</blockquote>
<h3>四、自定义框架</h3>
<p class="subheader">Category: <a href="/category/interview_questions.html">Interview_questions</a>

</p>




</article>
    </div>
    <!-- End Main Content -->

    <!-- Sidebar -->
    <aside class="large-3 columns">
        <h5 class="sidebar-title">Site</h5>
        <ul class="side-nav">
            <li><a href="/archives.html">Archives</a>
            <li><a href="/tags.html">Tags</a>


        </ul>

		
        <h5 class="sidebar-title">Categories</h5>
        <ul class="side-nav">
            <li><a href="/category/android.html">Android</a></li>
            <li><a href="/category/database.html">DataBase</a></li>
            <li><a href="/category/front-end.html">Front-end</a></li>
            <li><a href="/category/golang.html">Golang</a></li>
            <li><a href="/category/interview_questions.html">Interview_questions</a></li>
            <li><a href="/category/linux.html">Linux</a></li>
            <li><a href="/category/others.html">Others</a></li>
            <li><a href="/category/python.html">Python</a></li>
            <li><a href="/category/tools.html">Tools</a></li>
            <li><a href="/category/windows.html">Windows</a></li>
   
        </ul>

        <h5 class="sidebar-title">Links</h5>
        <ul class="side-nav">
            <li><a href="https://getpelican.com/">Pelican</a></li>
            <li><a href="https://www.python.org/">Python.org</a></li>
            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
            <li><a href="#">github</a></li>
        </ul>
		
        <h5 class="sidebar-title">Social</h5>
        <ul class="side-nav">
            <li><a href="#">You can add links in your config file</a></li>
            <li><a href="#">Another social link</a></li>
        </ul>

    </aside> <!-- End Sidebar -->

</div> <!-- End Main Content and Sidebar -->


<!-- Footer -->
<footer class="row">
    <div class="large-12 columns">
        <hr />
        <div class="row">
            <div class="large-6 columns">
                <p>gogogo by yoyo</p>
            </div>
            </div>
    </div>
</footer>