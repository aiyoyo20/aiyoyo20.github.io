[{"content":"陆陆续续使用了不少软件记录笔记，从最初的记事本，word 再到现在用过的各种 markdown 软件，各有特色，做个总结吧，希望对没有接触过的有个直观的了解。\n有的软件是早期的时候使用的，所以可能会出现上面写的一些问题在下载的最新版本中解决了优化了的情况。仅为自己当时的主观感受，不代表产品的实际，毕竟，每个人的喜好不一样。\nBasic Markdown Editor 专注于 Markdown 的编辑器，\nTypora 使用的第一款 markdown 编辑器，使用过程还是挺舒适的，在最开始不太熟悉 markdown 语法又想要 markdwon 格式的时候使用的，但也由于这样，在较长一段时间内才完全熟练 markdwon 的语法，可以说在一定程度上的依赖反而造成了拖累。\n使用感受：\n1. 主题多样可切换， 2. '所见即所得'，写作时直接预览，这个各有看法吧，有的人说是简单主义，是个优点，而我更喜欢从代码编写。 3. 只能创建 md 文件，主要阅读的是 md 文件和一些文本文件，其他的无法读取或打开为乱码。 4. 可以打开多级文件夹，但是无法在软件内创建文件夹。  总结：\n如果单单作为一个 markdown 编辑器是很优秀的，大多数的功能都有，但是如果想要当作一个笔记，管理一个系列的文件显然是不够的，而这也正是我放弃的原因，后期的东西原来越多，日记，笔记等等，同步性等等，typora 已不能满足这些。 2023.1.13 新增，之前的一些问题后期应该是解决了的，当时写这个评价的时候是 2019 年，这么多年了后续好像还发展成收费的了，实际的应用体验应该有提升。 全平台：Windows、Mac、Linux 语言：多语言，中英日等 官网：\u0026lt;https://typora.io/\u0026gt;  Marktext 有人将其称之为未来的 markdown 软件。\n总结：\n即可所见即所得，也可通过快捷键切换预览与编辑。 关键可在里面为其编辑与预览指定字体。 初使用是不错的，但是在 mac 和 windows 上没有测试，在现在的 linux 上会有卡顿的情况，快捷键的延迟很高，甚至于没有响应。只能期待于以后的优化了再使用。 全平台：Windows、Mac、Linux 语言：目前只有英语 官网：\u0026lt;https://marktext.app/\u0026gt; Github 项目地址：\u0026lt;https://github.com/marktext/marktext/releases\u0026gt;  Vnote 免费的开源笔记软件。VNote 专注于 Markdown 的编辑与阅读，以提供舒适的编辑体验为设计目标。\n老版本： 新版本： 总结：\n1. 可定制性较强，界面支持小部件拖拽移动，放在自己喜欢的地方，调整为自己喜欢、习惯的布局。 2. 支持图片预览，且插入图片的图片可以通过内定的方式设置大小。方式为在写入图片地址后添加一个空格后写上设置的大小，如（=900x），但是这种方式仅仅对于使用了相同内置环境的编辑器有效，为了更兼容，在后期将所有需要设置大小的图片都改为了使用`\u0026lt;img /\u0026gt;`标签去设置。但是需要注意这种方式不是它的默认检测图片的范围内，所以修改完之后提示有图片不再使用，是否放入回收站，而实际上是使用的。 3. 整个使用过程从 typora 迁移过来并没有很费劲，反而觉得在使用中由于是从代码编写，所以对 markdown 的语法有了更深刻的理解，也为了一些排版啥的去逐渐学习使用一些更为高级的语法使得自己的笔记更加美观与实用。 4. 如果有添加在目录下但是不使用的照片会提示是否删除，删除也不是直接删除，他自己会创建一个以日期为名的文件夹为回收站去另存，这个其实感觉可以定义为自己检查而不是自动检查，有时候由于编写打乱了图片链接的代码便会马上提示显得很麻烦，比较影响使用体验。 5. 在后期版本更新的时候对界面做了大改，精简很多，但是这时自己的笔记已经较多了，最最最明显的一个问题暴露出来，早期的版本不支持目录，当较多的时候需要分类就不够用了，后期的版本可以了，但是在导入其他的笔记时，没有办法导入目录中的目录。 6. 新版本修复了这个问题。所以又回来使用了。 7. 在 2021 年就停止了使用，有个个人觉得很不妥的问题，他的一些文件的配置文件、生成的项目的辅助文件（多是json类，感觉json），我的项目是有关联 git ,这些新生成的文件在提交的时候很麻烦，而且自己查看的时候也别扭。就没有继续使用了。 8. 这次添加新的编辑器的使用也去看了这个编辑器的更新，以及支持 PDF Viewer and MindMap Editor。 9. 还引入了语法片段，并且可自定义，这个我看有不少编辑器用作热点，通过比如 `dd` 快速插入时间日期。提升效率完全感受不到，编程中用倒是方便不少。 10. 标题前会自动生成、补全、修正序号，排版那种序号，比如三级标题 `1.1.2`。这个怎么说呢，个人不太喜欢这个功能，在大纲里面还行，实际嵌入源码中不习惯。不过好在可关闭。 11. 源码、预览的同步有但是有明显延迟。 12. 全局搜索有。 13. 目前的版本的暗示主题在 Gnome 环境的暗示环境下标题栏和菜单栏无效。 14. 会在原项目文件夹中生成文件的问题还在。 全平台：Windows、Mac、Linux 语言：中、英、日三种语言可选择 官网: \u0026lt;https://tamlok.github.io/vnote\u0026gt; Github 项目地址: \u0026lt;https://github.com/tamlok/vnote\u0026gt;  vim、sublime 等编辑器 vim、sublime 通过插件和设置能够实现较为完美的编辑、预览环境。主要是够轻量级。即开即关。IDE 切换项目麻烦且耗费时间。\nBetter Editor 介绍的这些编辑器都有向下兼容 markdown，此外对其可添加的内容进行了大幅度的拓展，有的是 Multimedia notes （多媒体笔记），能够保存更多格式的文件，有的支持 bi-directional link（双向链接）并且提供了 graph view（关系图谱）。\nJoplin  Joplin 是一款全平台的开源且完全免费的笔记应用，同时也是一个高效的 To-Do 待办事项工具和生产力工具。你可以用标签或笔记本进行分类整理，笔记支持 Markdown，可实现全文搜索。\n 总结：\n1. 多个主题可切换，支持多种云，保存更方便。 2. 可大纲、源码、效果预览三栏显示，源码和预览同步滚动。 3. 有个笔记附件功能不错，能够快速的管理文章内添加的图片等附件，如果文章内进行了删除，在软件内就可进行删除操作，无需进入文件管理。这个功能蛮实用的，自己的文章有时候很长，改动后有不用的附件忘记删除是常有的事，为此还自写了一个脚本用于查找项目内不再使用的附件及删除。 4. 有个切换外部编辑功能，个人感觉多余了。点击图标或者使用快捷键调用该功能会在保留当前窗口的情况下启用默认文本编辑器编辑。 5. 目前的版本的暗示主题在 Gnome 环境的暗示环境下标题栏和菜单栏无效。 6. 使用Chrome和Firefox上的web clipper扩展，可以保存网页或截图作为笔记。具体的使用情况也没有实际测试。 [插件](https://github.com/joplin/plugins/blob/master/README.md#plugins): 有不少的插件，不过没有尝试。 多平台：Windows、macOS、Linux、iOS、Android 等主流桌面平台和手机平台都提供有客户端，甚至还提供了命令行版本 语言：很多，包含中文 官网：\u0026lt;https://logseq.com/\u0026gt;  Logseq  A privacy-first, open-source platform for knowledge management and collaboration (一个隐私至上、开源的知识管理和协作平台)\n 1. 同样也是带有关系图谱功能的双向链编辑器。但是不是基于文本，但是可以从目录（文件）导入。但是并不是创建指向到文件夹（文件），而是读取然后单独保存，更像是数据库保存，并且原来的文件会被“智能拆分”成片段，之所以是引号，是因为效果并不理想。 2. 也同样支持插件，197 个插件 39 个主题可选择。 3. 可配置 Git 自动 commit，不过没测试。 4. 有日记和记忆卡片功能。 5. 目前的版本的暗示主题在 Gnome 环境的暗示环境下标题栏和菜单栏无效。 多平台：Windows、macOS、Linux、iOS、Android 等主流桌面平台和手机平台都提供有客户端，甚至还提供了命令行版本 语言：很多，包含中文 官网：\u0026lt;https://logseq.com/\u0026gt; Github 项目地址: \u0026lt;https://github.com/logseq/logseq/\u0026gt;  Obsidian  Obsidian 是一个功能强大的知识管理软件,是一款功能强大的带有关系图谱功能的双向链笔记。\n 总结：\n1. 可以安装一些插件，比如字数统计啥的。近期看到了很多文章对该编辑的推荐，去看了下，这段时间发展很快，已经积累了很多很多功能各异的插件。 2. 建立了社区进行交流，所以主题很多，各种使用也是满意的。 3. 关键在同时打开代码模式和预览模式的时候，滑动是同步的，需要修改的话很方便。 4. 之前在系统设置代码可视更友好的字体，但是软件内无效，也没有发现可以自定义的地方，就放弃了。现在的版本好像没有这个异常了。不过也不确定，这次是换了系统，这个系统自带的字体依旧足够友好所以就没有安装字体了。 5. 同样也可以实现源码、预览、大纲三栏同框，但是不同步。 6. 支持全局搜索。 插件网站:[plugins](https://obsidian.md/plugins),用 xpahth 工具提取了一下，已经有 814\t个了。应该是包含主题类插件，但是这么多我是没想到的。 插件的种类很丰富，太多了看不过来了。比如 markdown 文本的美化格式化，代码片段、计算器、进从图像 (OCR) 和 PDF 中提取文本、找到所有外部图片的链接，然后将图片下载并保存到本地、从图像中提取文本并使其可用于搜索等。 全平台：Windows、Mac、Linux 语言：语言挺多的，主要的语言都有 官网：\u0026lt;https://obsidian.md/\u0026gt; Github 项目地址：\u0026lt;https://github.com/obsidianmd/obsidian-releases/releases\u0026gt;  几种开源双向链编辑器对比    软件 包大小 内存占用 github star     Joplin 205M 233M 34k   Logseq 151M 219M 19.7k   Obsidian 94M 164M 3.9k    Trilium 超高自由度的个人知识库。\n1. 可加密，轻盈简洁，能让信息不受阻碍直接呈现在眼前，高自由度。 2. 并且有一个配套的插件可以在网页中截图保存、保存整个网页、即时创建文本写入想要记下的。 3. 另外有一个服务器端的，有需要的可以布置到自己的服务器或者自己额外的电脑设备上，就可以通过网页访问了，在保障同步性的情况下完全不用担忧隐私安全问题。 插件地址：\u0026lt;https://github.com/zadam/trilium-web-clipper/\u0026gt;  总结：\n全平台：Windows、Mac、Linux 语言：目前只有英文的，有个人用户对 win64 的做了简单的汉化并打包了的，可以自行搜索 Github 项目地址: \u0026lt;https://github.com/zadam/trilium\u0026gt;  Other Gridea  Gridea 是一个静态博客写作客户端，帮助你更容易地构建并管理博客或任何静态站点。 想要体验博客那种感觉？没有服务器？懒得在本地搭建环境？这个软件可以让你在客户端写作然后在本地网页可以预览效果。\n  Markdown：知道你钟爱 Markdown 写作，我们也是 快且安全：Gridea 所有文件都在你的本地，构建为更快更安全的静态网站，无需管理数据库，向 Wordpress 说拜拜 简而不凡：简单几步即可搭建网站。无论博客抑或企业站点，强大的自定义能力，轻松驾驭\n 总结： 简单使用了一下，官方介绍的 简单几步即可搭建网站 并没有去实践。\n全平台：Windows、Mac、Linux 语言：中英两种语言 Github项目地址: \u0026lt;https://github.com/getgridea/gridea/\u0026gt; 官网地址: \u0026lt;https://gridea.dev/docs/\u0026gt;  ","date":"2023-01-14","permalink":"/post/note_editor_compared/","tags":["Tools","Editor"],"title":"Note_Editor_Compared"},{"content":"框架及实现语言 静态网站的生成框架挺多，这里举几个流行的。\n   框架 实现语言     Jekyll Ruby   Hexo Node.js   Hugo Go   Pelican Python   Middleman Ruby   Metalsmith nodejs   Wintersmith Nodejs   Cactus Python+Django    之前使用过 Pelican，之所以使用它而不是其他的框架是考虑到自己使用的是 Python，如果有自定义的需求，想要去改动的话熟悉的语言更有优势，但是实际的应用中，自己的文章篇数大概有 100 来篇，不少文章的篇幅挺长，在编译成静态文件的时候还是挺耗费时间的，而且需要在文件中添加元信息，自己一篇篇去手动添加元信息不太可能，写脚本的去执行的话也觉得麻烦，当时着力于自己的博客系统，也就没怎么上心，统一添加了一些元信息就给部署上去了。\n后续也没有更新。直到后来自己拓展了 Go 语言了解到 Hugo，更快速简洁的静态页面框架。\nHugo 的使用 ","date":"2023-01-14","permalink":"/post/githubpage/","tags":["Tools","Editor"],"title":"githubPage"},{"content":"yield 关键字初识 先看一个简单例子：\ndef yie(): n = 1 yield n print(n) n += 1 yield n print(n) n += 1 yield n print(n) a = yie() one = next(a) print(f'one {one}') two = next(a) print(f'two {two}') thr = next(a) print(f'thr {thr}') # one 1 # 1 # two 2 # 2 # thr 3  当调用时，遇到 yield 关键字时函数和遇到 return 用类似的结果，返回一个值，但不同的是并没有退出函数，当下一次运行时，继续函数里后面的代码。 同时我们取值的方式比较特殊，通过 next() 去取对象的值，而这应该是属于迭代器的方法。\n生成器函数（generator function） 如果函数包含至少一个 yield语句 (它可能包含其他 yield 或 return 语句)，那么它将成为一个生成器函数。\n生成器函数与正常函数的差异下面列出的是生成器函数与正常函数的区别 ： 当被调用时，它返回一个对象(迭代器)，但不会立即开始执行。 iter() 和 next() 之类的方法将自动实现。所以可以使用 next() 迭代项目。 一旦函数遇到 yield，该函数将被暂停，并将该控制权交给调用者。局部变量及其状态在连续调用之间被记住。 最后，当函数终止时，StopIteration会在进一步的调用时自动引发。\n上面例子中定义的 yie() 函数便是一个生成器函数。\n生成器（generator） 而生成器即是生成器函数被调用后产生的对象。 如例子中的 a 便是一个生成器。 在每个调用之间函数会保持住变量n的值。与正常函数不同，当函数产生时，局部变量不会被销毁。 此外，生成器对象只能重复一次。 要重新启动该过程，需要使用类似于 a = yie() 的方法创建另一个生成器对象。\n并且支持 for 循环遍历。\n生成器推导式 使用生成器表达式，可以轻松创建简单的生成器。 它使构建生成器变得容易。 与 lambda 函数一样创建一个匿名函数，生成器表达式创建一个匿名生成函数。生成器表达式的语法与 Python 中的列表解析类似。 但方圆 [] 替换为圆括号 ()。 列表推导和生成器表达式之间的主要区别是：列表推导产生整个列表，生成器表达式一次生成一个项目。 它们是处理方式是懒惰的，只有在被要求时才能生产项目。 因此，生成器表达式的存储器效率高于等效列表的值。\nge = (i for i in range(10)) print(type(ge)) # \u0026lt;class 'generator'\u0026gt;  yield from 语法： yield from generator\nyield from 后面可以跟的可以是“ 生成器 、元组、 列表、range（）函数产生的序列等可迭代对象” 简单地说，yield from generator 。实际上就是返回另外一个生成器。而yield只是返回一个元素。从这个层面来说，有下面的等价关系：yield from iterable本质上等于 for item in iterable: yield item 。\nTODO 深入的看着有点晕，暂时放放吧，留两个链接 https://blog.csdn.net/qq_27825451/article/details/85244237 https://www.cnblogs.com/wongbingming/p/9085268.html\n生成器专属方法 send、throw、close send 方法详解 generator.send(value)\n作用：向生成器发送一个值，随后恢复执行。 value 参数是 send 方法向生成器发送的值，这个值会作为当前所在的 yield 表达式的结果。 随后生成器恢复执行，直到下一个 yield，把它后面的值作为 send 方法的结果返回。\n如果恢复执行后再也没有 yield 语句，生成器退出，并抛出 StopIteration 异常。\n如果一开始使用 send 启动生成器，必须使用 None 作为参数，因为一开始没有可以接收值的 yield 表达式。\ndef gen(): for i in range(2): x = yield i print('x:', x) a = gen() for _ in a: print(_) # 0 # x: None # 1 # x: None  不同于 return，yield 的值是可以进行赋值操作的，但是由于其特性，赋值在正常i情况下都为空。 如上面的例子，x = yield i 等式是从右向左的，当遇到 yield 时就抛出值，而下一次调用时才进行赋值，这时值已经不存在的，便为 None 了。\ndef gen(): for i in range(2): x = yield i print('x:', x) a = gen() print(a.send(None)) # a.send(None) 等同于 next(a) print(a.send(3)) # 0 # x: 3 # 1  这便是 send() 的用法，能够在使用生成器的使用根据需要传递值进去。\nthrow 方法详解 generator.throw(type[, value[, traceback]])\n作用：在生成器暂停的地方抛出类型为 type 的异常，并返回下一个 yield 的返回值。 如果生成器函数没有捕获并处理传入的异常，或者说抛出了另一个异常，那么该异常会被传递给调用方。 如果生成器退出时还没有 yield 新值，则会抛出 StopIteration 异常。\n（第一种情况：捕获并处理传入的异常，得到下一个 yield 的返回值。 def gen(): n = 0 while True: try: yield n n += 1 except ZeroDivisionError: print('捕获到了 ZeroDivisionError') print('此时的 n 为：%s' % n) g = gen() ret = next(g) print('第一次 yield 的返回值：%s' % ret) ret = g.throw(ZeroDivisionError) print(ret) # 第一次 yield 的返回值：0 # 捕获到了 ZeroDivisionError # 此时的 n 为：0 # 0  第一次调用时遇到 yield ,抛出值，第二次调用时接受到了异常，跳过了 n + 1，所以n保持为 0。 如果通过 throw 传入的异常被捕获的话，生成器能够恢复执行直到下一个 yield。\n（1 如果捕获不准确的话\ndef gen(): for i in range(5): try: yield i except Exception as e: print(e) a = gen() b = next(a) # a.send(None) 等同于 next(a) print(b) c = a.throw(ZeroDivisionError) print(c) d = next(a) # a.send(None) 等同于 next(a) print(d) # 0 # (这样是捕获不到异常的，所以为空) # 1 # 2  后面的可以继续执行，捕获不到异常应该是throw的异常没有继承 excaption。\n（2 甚至于不做捕获\ndef gen(): for i in range(5): try: print('try') yield i except: print('error') a = gen() b = next(a) # a.send(None) 等同于 next(a) print(b) c = a.throw(ZeroDivisionError) print(c) d = next(a) # a.send(None) 等同于 next(a) print(d) # try # 0 # error # try # 1 # try # 2 # error # try # Exception ignored in: \u0026lt;generator object gen at 0x7fb5827ec2e0\u0026gt; # RuntimeError: generator ignored GeneratorExit  后续的依旧能执行，但每次执行完如果后续没有继续取值会多执行一次抛出生成器退出异常。\n（第二种情况：没有捕获并处理 throw 传入的异常，异常会回传给调用方 import sys def gen(): n = 0 while True: yield n n += 1 g = gen() ret1 = next(g) print('第一次 yield 的返回值：%s' % ret1) try: ret2 = g.throw(ZeroDivisionError) # ret2 并没有收到任何值 except ZeroDivisionError: print('调用方捕获到 ZeroDivisionError 异常') print(sys.exc_info()) # 第一次 yield 的返回值：0 # 调用方捕获到 ZeroDivisionError 异常 # (\u0026lt;class 'ZeroDivisionError'\u0026gt;, ZeroDivisionError(), \u0026lt;traceback object at 0x7fd771bcf5c0\u0026gt;)  这个比较容易理解，类似于将 异常 yield 回来了 而对于已经通过抛出异常而退出的生成器再使用 next(g) 会持续抛出 StopIteration 异常。\n（第三种情况：生成器退出时没有 yield 新值，会抛出 StopIteration 异常。 def gen(): try: # 注意是在当前暂停的 yield 处抛出异常 # 所以要在这里捕获 yield 1 except Exception as e: print(f'在生成器内部捕获了异常{e.args}') # print(e.args) # yield 2 g = gen() print(next(g)) g.throw(TypeError, '类型错误') # 1 # 在生成器内部捕获了异常('类型错误',) # Traceback (most recent call last): # File \u0026quot;/home/fiki/Documents/PycharmProjects/thread_yy.py\u0026quot;, line 54, in \u0026lt;module\u0026gt; # g.throw(TypeError, '类型错误') # StopIteration  内部捕获到了异常，但是由于没有可迭代的了，又抛出 StopIteration 给显式覆盖了。\n生成器的 close 方法 generator.close()\n作用：在生成器函数暂停的地方抛出一个 GeneratorExit 异常。 这并不等价于 generator.throw(GeneratorExit)，后面会说原因。 如果生成器抛出 StopIteration 异常（不管是由于正常退出还是因为该生成器已经关闭），或者抛出 GeneratorExit 异常（不捕获该异常即可），close 方法不传递该异常，直接返回到调用方。而生成器抛出的其他异常会传递给调用方。 GeneratorExit 异常的产生意味着生成器对象的生命周期已经结束，因此生成器方法后续语句中不能再有 yield，否则会产生 RuntimeError。（而 throw 方法是期待一个 yield 返回值的，如果没有，则会抛出 StopIteration 异常。） 对于已经正常退出或者因为异常退出的生成器对象，close 方法不会进行任何操作\n（第一种情况： 不捕获 GeneratorExit 异常，close 方法返回调用方，不传递该异常。\ndef gen(): yield 1 yield 2 g = gen() print(next(g)) g.close() print(next(g)) # 1 # Traceback (most recent call last): # File \u0026quot;/home/fiki/Documents/PycharmProjects/thread_yy.py\u0026quot;, line 50, in \u0026lt;module\u0026gt; # print(next(g)) # StopIteration  注意：对已经关闭的生成器对象使用 next 会抛出 StopIteration 异常。\n（第二种情况： 生成器自然退出抛出 StopIteration 异常，该异常不会传递给调用方，close 方法正常返回。\ndef gen(): try: yield 1 except GeneratorExit: print('捕获到GeneratorExit') print('生成器函数结束了') g = gen() print(next(g)) g.close() \u0026gt; 1 \u0026gt; 捕获到GeneratorExit \u0026gt; 生成器函数结束了  （第三种情况： 在 GeneratorExit 抛出后还有 yield 语句，会产生 RuntimeError。另外生成器对象被垃圾回收时，解释器会自动调用该对象的 close 方法（PEP 342），这意味着最好不要在相应的 except 和 finally 中写 yield 语句，否则不知道什么时候就会抛出 RuntimeError 异常。\ndef gen(): try: yield 1 except GeneratorExit: print('捕获到 GeneratorExit') print('尝试在 GeneratorExit 产生后 yield 一个值') yield 2 print('生成器结束') g = gen() next(g) g.close() \u0026quot;\u0026quot;\u0026quot; 捕获到 GeneratorExit 尝试在 GeneratorExit 产生后 yield 一个值 Traceback (most recent call last): File \u0026quot;test.py\u0026quot;, line 14, in \u0026lt;module\u0026gt; g.close() RuntimeError: generator ignored GeneratorExit \u0026quot;\u0026quot;\u0026quot;  一种防止抛出 RuntimeError 的安全生成器写法：设置一个布尔标识。\ndef safegen(): yield 'so far so good' closed = False try: yield 'yay' except GeneratorExit: closed = True raise finally: if not closed: yield 'boo'  （第四种情况： 对已经关闭的生成器对象调用 close() 方法，不会进行任何操作。\ndef gen(): yield 1 print('我不会被执行') print('因为在 yield 1 就抛出了 GeneratorExit 异常') print('未经捕获的 GeneratorExit 异常不会传递') print('返回执行权给 close 的调用方') g = gen() g.close() g.close() g.close() # 多次调用 close，什么效果都没有  补充：GeneratorExit 异常只有在生成器对象被激活后，才有可能产生。\n生成器优点 1. 容易实现 与其迭代器类相比，发生器可以以清晰简洁的方式实现。 以下是使用迭代器类来实现2的幂次序的例子。\nclass PowTwo: def __init__(self, max = 0): self.max = max def __iter__(self): self.n = 0 return self def __next__(self): if self.n \u0026gt; self.max: raise StopIteration result = 2 ** self.n self.n += 1 return result  上面代码有点长，可以使用一个生成器函数实现同样的功能。\ndef PowTwoGen(max = 0): n = 0 while n \u0026lt; max: yield 2 ** n n += 1  因为，生成器自动跟踪的细节，它更简洁，更干净。\n2.内存高效 返回序列的正常函数将在返回结果之前会在内存中的创建整个序列。如果序列中的项目数量非常大，这可是要消耗内存的。 序列的生成器实现是内存友好的，并且是推荐使用的，因为它一次仅产生一个项目。\n3. 表示无限流 生成器是表示无限数据流的绝佳媒介。 无限流不能存储在内存中，由于生成器一次只能生成一个项目，因此可以表示无限数据流。 以下示例可以生成所有偶数(至少在理论上)。\ndef all_even(): n = 0 while True: yield n n += 2  4.管道生成器 生成器可用于管理一系列操作，下面使用一个例子说明。 假设我们有一个快餐连锁店的日志文件。 日志文件有一列(第4列)，用于跟踪每小时销售的比萨饼数量，我们想算出在5年内销售的总萨饼数量。 假设一切都是字符串，不可用的数字标记为“N / A”。 这样做的生成器实现可以如下。\nwith open('sells.log') as file: pizza_col = (line[3] for line in file) per_hour = (int(x) for x in pizza_col if x != 'N/A') print(\u0026quot;Total pizzas sold = \u0026quot;,sum(per_hour))  这种管道的方式是更高效和易于阅读的。\n判断生成器、生成器函数 我们可以用inspect类里的isgeneratorfunction类方法判断是否是一个生成器函数，以及使用 isgenerator类方法判断是否是一个生成器。\nfrom inspect import isgeneratorfunction, isgenerator print(f'fibonacci is a generator function: {isgeneratorfunction(fibonacci)}') print(f'fib is a generator: {isgenerator(fib)}')  应用生成器的场景与好处 生成器可用于产生数据流，而且并不立刻产生返回值，而是等到被需要的时候才会产生返回值，相当于一个主动拉取的过程(pull)，比如现在有一个日志文件，每行产生一条记录，对于每一条记录，不同部门的人可能处理方式不同，但是我们可以提供一个公用的、按需生成的数据流。 还有做爬虫的时候，爬取大量数据的时候如果使用生成器每次需要的时候执行输出也可以大大降低资源的消耗。 使用生成器的好处当然不仅限于此，让我们来看一下下面的例子，我们打算读取小说《三国演义》的所有文字内容，如果直接对文件对象调用 read() 方法，会导致不可预测的内存占用。好的方法是利用固定长度的缓冲区来不断读取文件内容。而且同时通过 yield来执行每次输出，就可以轻松实现文件读取。\nfrom pathlib import Path file = Path('三国演义.txt') def read_file(fpath): BLOCK_SIZE = 1024 with file.open(encoding='GB18030') as f: while True: block_content = f.read(BLOCK_SIZE) if block_content: yield block_content else: return for c in read_file(file): print(c)  yield 与递归产生的异常分析 最初的异常代码：\nfrom config import frontMatter, noteDir, ignoreFile import os def getFile(filePath): files = os.listdir(filePath) for fi in files: fi_d = os.path.join(filePath, fi) if os.path.isdir(fi_d): getFile(fi_d) else: fileName = os.path.join(filePath, fi_d) if \u0026quot;.md\u0026quot; in fileName: print(fileName) yield fileName  就是一个简单的递归输出文件的函数，如果把yield的语句注释，正常print是可以正常输出的，但是使用yield语句就只能得到一个输出。通过添加print的方式发现整个函数的流程和使用print是一样的，但就是没有输出。\n解释之前，先来看yield 的几个特性。\n yield可以看作加强版本的return，会抛出结果，但是不会退出函数。 函数中只要有yield关键字，这个函数就成了生成器函数，调用这个函数会得到一个生成器。并且这与yield语句是否被执行无关，如果是增加了判断语句，但是有yield的分支一直未被执行，这个函数也是生成器函数。 生成器需要使用next()函数来迭代，或者是通过循环语句来迭代。生成器函数被调用一次就是一个生成器，所以调用的是一次调用多次迭代，如果使用next(funcName())是创建了多个生成器，且只迭代了里面的第一个结果。 抛出结果后保留函数的状态，下一次使用next()函数调用时或循环迭代时从保留的状态恢复继续向后执行。  上面的代码将其用另一个类似变量、代码量简化的的替换。实际这个代码有问题，假如正常执行的话，多次之后传入的参数将为空列表，但是空列表取[1:]是可行的，最后会超出递归限制而报错。\ndef yieldNumberList(numberList): yield numberList[0] yieldNumberList(numberList[1:]) if __name__ == '__main__': numberList = [1,2,3,4] for number in yieldNumberList(numberList): print(number) # 1  代码的实际运行并没有像想象中那样逐个数字打印，只有第一个输出了。\n实际上后续的代码也完完整整的走过一遍了的，但是在递归的时候，yieldNumberList([1:])该语句并不是单单纯的函数了，而是一个生成器，这里创建了一个生成器，但是没有对其进行迭代，而后续的生成器中的生成器也是同理，也就是该生成器实际是一个多重生成器，但是只迭代出了第一重。\n改进\ndef yieldNumberList(numberList): yield numberList[0] yield from yieldNumberList(numberList[1:]) if __name__ == '__main__': numberList = [1,2,3,4] for number in yieldNumberList(numberList): print(number) # 1  参考 Python生成器\npython协程系列（三）——yield from原理详解\nPython 生成器与它的 send，throw，close 方法\n用yield关键字创建生成器\n","date":"2023-01-13","permalink":"/post/generator/","tags":["Python"],"title":"generator"},{"content":"需求：有一个文件在两个月之前删除了，但是现在想看看里面的内容 ","date":"2023-01-13","permalink":"/post/git_examples/","tags":["Tools","Git"],"title":"Git_examples"},{"content":"插件 插件管理器vim-plug curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n其他插件及配置 配置文件vimrc里面有详细的备注，就不一个个展开了。\n快捷键设置 模式映射 使用h key-notation命令查看帮助信息,查看键盘符号的详细说明\n   notation meaning equivalent decimal value(s)~     \u0026lt;Nul\u0026gt; zero CTRL-@ 0 (stored as 10)*\u0026lt;Nul\u0026gt;   \u0026lt;BS\u0026gt; backspace CTRL-H 8 *backspace*   \u0026lt;Tab\u0026gt; tab CTRL-I 9 *tab* *Tab*     *linefeed*     \u0026lt;NL\u0026gt; linefeed CTRL-J 10 (used for \u0026lt;Nul\u0026gt;)   \u0026lt;CR\u0026gt; carriage return CTRL-M 13 *carriage-return*   \u0026lt;Return\u0026gt; same as    *\u0026lt;Return\u0026gt;*   \u0026lt;Enter\u0026gt; same as    *\u0026lt;Enter\u0026gt;*   \u0026lt;Esc\u0026gt; escape CTRL-[ 27 *escape* *\u0026lt;Esc\u0026gt;*   \u0026lt;Space\u0026gt; space  32 *space*   \u0026lt;lt\u0026gt; less-than \u0026lt; 60 **   \u0026lt;Bslash\u0026gt; backslash \\ 92 *backslash* *\u0026lt;Bslash\u0026gt;*   \u0026lt;Bar\u0026gt; vertical bar \\ 124 *\u0026lt;Bar\u0026gt;*   \u0026lt;Del\u0026gt; delete  127     使用 :map 命令，可以列出所有键盘映射,使用:map!命令，则只列出插入和命令行模式的映射。而:imap，:vmap，:omap，:nmap命令则只是列出相应模式下的映射\nmarkdown-preview.nvim 问题 原来的 markdown-preview.vim 插件在文件较大时会导致 vim 卡死。 需要yarn 环境对包进行编译安装。成功后才能正常使用。如果安装后执行预览命令无反映，可以检查环境后自行cd ~/.vim/plugged/markdown-preview.nvim/app \u0026amp; yarn install 编译安装试试。\n内容显示异常 vim 打开文件有时内容排序混乱，行内容中部有横穿的直线， 可视化目录及标题栏异常等问题可以先尝试退出重新载入，有时候时插件未加载完全或者配置内容加载错误导致的。 在文本中有一个键\u0026quot;\u0026lt;Del\u0026gt;\u0026quot;,被错误的识别造成后面的内容全被增加了删除线。\nindentLine 造成的 markdown 问题 输入*号看不见，其他编辑器打开能看见 indentLine 插件的配置造成的，要么根据官方文档调整配置，要么使用其他的类似插件。\npython 的 if __name__ == \u0026quot;__main__\u0026quot; 变成 if name == \u0026quot;main\u0026quot;，光标移动到该行变正常 包括上面的问题，设置项有个特殊参let g:markdown_syntax_conceal=0,加上就好了。\nlsp 的 bash 问题 出现\u0026quot;SHELLCHECK_PATH\u0026quot; 或 \u0026ldquo;shellchcek\u0026rdquo;，原因是 shell 文本中没有shebang（hashbang）头导致的。\n插件 fcitx.vim 与 python 虚拟环境 与 vim 的使用 fcitx.vim 使用了 python3 和自带的库dbus-python，具体的功能没有细看，但是这个库是在主环境中的，如果使用虚拟环境，而又没有安装这个库，使用虚拟环境的目的就是为了得到纯净环境，好调试，所以也不可能创建一个虚拟环境便去安装一次这个库。而且试着安装了一下，没成功，看错误日志好像并不是纯 python 包，下载下来后有编译或是其他操作，所以基本不用考虑自己安装了。\n没有安装这个库倒是不会影响该 vim 插件的使用，但是在启用的时候加载插件会调动这个库，没有 python 就会抛出异常。\n尝试使用vim-virtualenv，在 vim 中切换环境后，使用!python %运行输出版本的脚本版本改变了，但是在 vim 中直接:python3 import sys; print(sys.version)，版本还是基本环境的版本，而且这种方法对使用vim-lsp补全不起效果，无法使用虚拟环境的库进行补全。\n这个问题最后还是在官方插件文档找到了解决方法：插件默认会使用 Python 3 并通过 D-Bus 来切换输入法状态。\n如果是使用 fcitx5 输入法，可以通过 fcitx5-remote 来切换。\n设置参数后不使用 dbus 就不会使用 python 基本就不会有这个问题。\nPlug 'lilydjwg/fcitx.vim' let g:fcitx5_remote = '/usr/bin/fcitx5-remote' \u0026quot; 命令的地址 let g:fcitx5_rime = 1 \u0026quot; 使用 rime 输入法需设置  ","date":"2023-01-11","permalink":"/post/vim/","tags":["Tools","Editor","Vim"],"title":"vim"},{"content":"查找并过滤出自己 markdown 笔记项目下的所有图片名 find . -name images | xargs ls -l | grep -E \u0026quot;rw\u0026quot; | tr -s \u0026quot; \u0026quot; | cut -d \u0026quot; \u0026quot; -f 9\n过滤出文件中所有使用的图片名 find . -name \u0026quot;*md\u0026quot; | xargs cat | sed -n 's/.*images\\/\\(.*png\\).*/\\1/p'\n这里有个陷阱，find 命令查找的内容名并不会自动转义，比如空格或其他特殊字符，当用其他命令接收时会出错。\n例子：\n文件名：test .txt test\\.txt\nfind 结果： find . -name \u0026quot;*test*\u0026quot; \u0026raquo; ./test .txt ./test\\.txt 可以看到输出的内容即没有进行转义，也没有用引号包裹内容，这时我们使用联合命令find . -name \u0026quot;*test*\u0026quot; | xargs cat ,\n结果是\ncat: ./test: No such file or directory cat: .txt: No such file or directory cat: ./test.txt: No such file or directory  对于文件test .txt ,由于查找出来的结果传递给下一个命令时没有使用括号包裹，中间有空格，空格又是默认分隔符，预想的cat \u0026quot;test .txt\u0026quot;就变成了cat test .txt，被当成了两个文件\n对于文件test\\.txt ,\\在这里起到了转义作用，预想的cat \u0026quot;test\\.txt\u0026quot;变成了cat test.txt\n可以使用自带的参数-exce执行命令：find . -name \u0026quot;*test*txt\u0026quot; -exec cat {} \\;\n也可以使用 sed 替换，这里自己被绕了一下，想找能直接格式化输出的命令，但是没有合适的，也想到sed，不过忘了可以搭配正则集，认为要多次替换会麻烦不如使用其他语言。\n实现：find . -name \u0026quot;*test*txt\u0026quot; | sed 's/\\([[\\*^$ ]\\)/\\\\\\1/g' | xargs cat，[]填写需要转义的字符，空格就直接空格[ ]，但是需要注意的是 sed 是行处理的，直接 cat 得到不结果，需要 xargs 逐参运行。\n","date":"2023-01-09","permalink":"/post/myshell/","tags":["Linux"],"title":"myShell"},{"content":"第五章 爬虫  一、常用库与模块 1. 试列出至少三种目前流行的大型数据库的名称:  ____、____、____, 其中您最熟悉的是 ____, 从 ____ 年开始使用（考察对数据可的熟悉程度，同时考察你的工作年限注意和自己简历一致）。 Oracle，Mysql，SQLServer、MongoDB 根据自己情况（推荐 Mysql 、MongoDB）。\n 2. 列举您使用过的 Python 网络爬虫所用到的网络数据包？  requests、urllib、urllib2、httplib2。\n 3. 列举您使用过的 Python 网络爬虫所用到的解析数据包  BeautifulSoup、pyquery、Xpath、lxml。\n 4. 爬取数据后使用哪个数据库存储数据的，为什么？  MongoDB 是使用比较多的数据库，这里以 MongoDB 为例，大家需要结合自己真实开发环境回答。 原因： 1）与关系型数据库相比，MongoDB 的优点如下。 ①弱一致性（最终一致），更能保证用户的访问速度.\n 举例来说，在传统的关系型数据库中，一个 COUNT 类型的操作会锁定数据集，这样可以保证得到 “当前” 情况下的较精确值。 这在某些情况下，例如通过 ATM 查看账户信息的时候很重要，但对于 Wordnik 来说，数据是不断更新和增长的，这种 “较精确” 的保证几乎没有任何意义，反而会产生很大的延迟。 他们需要的是一个 “大约” 的数字以及更快的处理速度。 但某些情况下 MongoDB 会锁住数据库。如果此时正有数百个请求，则它们会堆积起来，造成许多问题。我们使用了下面的优化方式来避免锁定。 每次更新前，我们会先查询记录。查询操作会将对象放入内存，于是更新则会尽可能的迅速。在主 / 从部署方案中，从节点可以使用 “-pretouch” 参数运行，这也可以得到相同的效果。 使用多个 mongod 进程。我们根据访问模式将数据库拆分成多个进程。  ②文档结构的存储方式，能够更便捷的获取数据。\n对于一个层级式的数据结构来说，如果要将这样的数据使用扁平式的，表状的结构来保存数据， 这无论是在查询还是获取数据时都十分困难。  ③内置 GridFS，支持大容量的存储。\nGridFS 是一个出色的分布式文件系统，可以支持海量的数据存储。 内置了 GridFS 了 MongoDB，能够满足对大数据集的快速范围查询。  ④内置 Sharding。\n提供基于 Range 的 Auto Sharding 机制： 一个 collection 可按照记录的范围，分成若干个段，切分到不同的 Shard 上。 Shards 可以和复制结合，配合 Replica sets 能够实现 Sharding+fail-over，不同的 Shard 之间可以负载均衡。 查询是对客户端是透明的。 客户端执行查询，统计，MapReduce 等操作，这些会被 MongoDB 自动路由到后端的数据节点。 这让我们关注于自己的业务，适当的时候可以无痛的升级。 MongoDB 的 Sharding 设计能力较大可支持约 20 petabytes，足以支撑一般应用。 这可以保证 MongoDB 运行在便宜的 PC 服务器集群上。 PC 集群扩充起来非常方便并且成本很低，避免了 “sharding” 操作的复杂性和成本。  ⑤第三方支持丰富。(这是与其他的 NoSQL 相比，MongoDB 也具有的优势)\n现在网络上的很多 NoSQL 开源数据库完全属于社区型的，没有官方支持，给使用者带来了很大的风险。 而开源文档数据库 MongoDB 背后有商业公司 10gen 为其提供供商业培训和支持。 而且 MongoDB 社区非常活跃，很多开发框架都迅速提供了对 MongDB 的支持。 不少知名大公司和网站也在生产环境中使用 MongoDB，越来越多的创新型企业转而使用 MongoDB 作为和 Django，RoR 来搭配的技术方案。  ⑥性能优越\n在使用场合下，千万级别的文档对象，近 10G 的数据，对有索引的 ID 的查询不会比 mysql 慢，而对非索引字段的查询，则是全面胜出。 mysql 实际无法胜任大数据量下任意字段的查询，而 mongodb 的查询性能实在让我惊讶。 写入性能同样很令人满意，同样写入百万级别的数据，mongodb 比我以前试用过的 couchdb 要快得多，基本 10 分钟以下可以解决。 补上一句，观察过程中 mongodb 都远算不上是 CPU 杀手。  2) Mongodb 与 redis 相比较 ①mongodb 文件存储是 BSON 格式类似 JSON，或自定义的二进制格式。\nmongodb 与 redis 性能都很依赖内存的大小，mongodb 有丰富的数据表达、索引；最类似于关 系数据库，支持丰富的查询语言，redis 数据丰富，较少的 IO ，这方面 mongodb 优势明显。  ②mongodb 不支持事物，靠客户端自身保证，redis 支持事务，比较弱，仅能保证事物中的操作按顺序执行，这方面 redis 优于 mongodb。\n③mongodb 对海量数据的访问效率提升，redis 较小数据量的性能及运算，这方面 mongodb 性能优于 redis .monbgodb 有 mapredurce 功能，提供数据分析，redis 没有，这方面 mongodb 优于 redis 。\n5. 你用过的爬虫框架或者模块有哪些？谈谈他们的区别或者优缺点？    爬虫 模块     Python 自带 urllib、urllib2   第三方 requests   框架 Scrapy     urllib 和 urllib2 模块都做与请求 URL 相关的操作，但他们提供不同的功能。 urllib2：urllib2.urlopen 可以接受一个 Request 对象或者 url，（在接受 Request 对象时候，并以此可以来设置一个 URL 的 headers），urllib.urlopen 只接收一个 url。 urllib 有 urlencode,urllib2 没有，因此总是 urllib，urllib2 常会一起使用的原因。 scrapy 是封装起来的框架，它包含了下载器，解析器，日志及异常处理，基于多线程，twisted 的方式处理，对于固定单个网站的爬取开发，有优势，但是对于多网站爬取，并发及分布式处理方面，不够灵活，不便调整与括展。 request 是一个 HTTP 库， 它只是用来，进行请求，对于 HTTP 请求，他是一个强大的库，下载，解析全部自己处理，灵活性更高，高并发与分布式部署也非常灵活，对于功能可以更好实现\n Scrapy 优点：\n scrapy 是异步的； 采取可读性更强的 xpath 代替正则； 强大的统计和 log 系统； 同时在不同的 url 上爬行； 支持 shell 方式，方便独立调试； 写 middleware, 方便写一些统一的过滤器； 通过管道的方式存入数据库；\n Scrapy 缺点：\n 基于 python 的爬虫框架，扩展性比较差； 基于 twisted 框架，运行中的 exception 是不会干掉 reactor，并且异步框架出错后是不会停掉 其他任务的，数据出错后难以察觉。\n 6. 写爬虫是用多进程好？还是多线程好？ 为什么？  IO 密集型代码 (文件处理、网络爬虫等)，多线程能够有效提升效率 (单线程下有 IO 操作会进行 IO 等 待，造成不必要的时间浪费，而开启多线程能在线程 A 等待时，自动切换到线程 B，可以不浪费 CPU 的资源，从而能提升程序执行效率)。在实际的数据采集过程中，既考虑网速和响应的问题，也需要考虑自 身机器的硬件情况，来设置多进程或多线程。\n 7. 常见的反爬虫和应对方法？  通过 Headers 反爬虫： 从用户请求的 Headers 反爬虫是最常见的反爬虫策略。很多网站都会对 Headers 的 User-Agent 进行检测，还有一部分网站会对 Referer 进行检测（一些资源网站的防盗链就是检测 Referer）。如果 遇到了这类反爬虫机制，可以直接在爬虫中添加 Headers，将浏览器的 User-Agent 复制到爬虫的 Headers 中；或者将 Referer 值修改为目标网站域名。对于检测 Headers 的反爬虫，在爬虫中修改或者添加 Headers 就能很好的绕过。\n  基于用户行为反爬虫： 还有一部分网站是通过检测用户行为，例如同一 IP 短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。 多数网站都是前一种情况，对于这种情况，使用 IP 代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理 ip，检测后全部保存起来。这样的代理 ip 爬虫经常会用到，最好自己准备一个。有了大量代理 ip 后可以每请求几次更换一个 ip，这在 requests 或者 urllib2 中很容易做到，这样就能很容 易的绕过第一种反爬虫。 对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限 制。\n  动态页面的反爬虫： 上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过 ajax 请求得到，或者通过 JavaScript 生成的。首先用 Fiddler 对网络请求进行分析。如果能够找到 ajax 请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用 requests 或者 urllib2 模拟 ajax 请求，对响应的 json 进行分析得到需要的数据。 能够直接模拟 ajax 请求获取数据固然是极好的，但是有些网站把 ajax 请求的所有参数全部加密了。 我们根本没办法构造自己所需要的数据的请求。这种情况下就用 selenium+phantomJS，调用浏览器 内核，并利用 phantomJS 执行 js 来模拟人为操作以及触发页面中的 js 脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程 模拟一遍。 用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加 Headers 一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS 就是一个没有界面的 浏览器，只是操控这个浏览器的不是人。利 selenium+phantomJS 能干很多事情，例如识别点触式 （12306）或者滑动式的验证码，对页面表单进行暴力破解等。\n 8. 解析网页的解析器使用最多的是哪几个？  lxml，html5lib，html.parser,lxml-xml，正则表达式。\n 9. 需要登录的网页，如何解决同时限制 ip，cookie,session（其中有一些是动态生成的）在不使用动态爬取的情况下？ 解决限制 IP 可以使用代理 IP 地址池、服务器； 不适用动态爬取的情况下可以使用反编译 JS 文件获取相应的文件，或者换用其他平台（比如手机端） 看看是否可以获取相应的 json 文件。\n10. 验证码的解决？\u0026gt; 图形验证码：干扰、杂色不是特别多的图片可以使用开源库 Tesseract 进行识别，太过复杂的需要 借助第三方打码平台。 点击和拖动滑块验证码可以借助 selenium、无图形界面浏览器（chromedirver 或者 phantomjs）和 pillow 包来模拟人的点击和滑动操作，pillow 可以根据色差识别需要滑动的位置。\n11. 使用最多的数据库（Mysql，Mongodb，redis 等），对他们的理解？  MySQL 数据库：开源免费的关系型数据库，需要实现创建数据库、数据表和表的字段，表与表之 间可以进行关联（一对多、多对多），是持久化存储。 Mongodb 数据库：是非关系型数据库，数据库的三元素是，数据库、集合、文档，可以进行持久化存储，也可作为内存数据库，存储数据不需要事先设定格式，数据以键值对的形式存储。 redis 数据库：非关系型数据库，使用前可以不用设置格式，以键值对的方式保存，文件格式相对自由，主要用与缓存数据库，也可以进行持久化存储。\n 12. 字符集和字符编码  字符是各种文字和符号的总称，包括各个国家文字、标点符号、图形符号、数字等。 字符集是多个字符的集合，字符集种类较多，每个字符集包含的字符个数不同，常见字符集有： ASCII 字符集、ISO 8859 字符集、GB2312 字符集、BIG5 字符集、GB18030 字符集、Unicode 字符集等。 字符编码就是以二进制的数字来对应字符集的字符。 常见的编码字符集（简称字符集）如下所示： Unicode：也叫统一字符集，它包含了几乎世界上所有的已经发现且需要使用的字符（如中文、日文、英文、德文等）。 ASCII：ASCII 既是编码字符集，又是字符编码。早期的计算机系统只能处理英文，所以 ASCII 也就 成为了计算机的缺省字符集，包含了英文所需要的所有字符。 GB2312：中文字符集，包含 ASCII 字符集。ASCII 部分用单字节表示，剩余部分用双字节表示。 GBK：GB2312 的扩展，但完整包含了 GB2312 的所有内容。 GB18030：GBK 字符集的超集，常叫大汉字字符集，也叫 CJK（Chinese，Japanese，Korea）字符集，包含了中、日、韩三国语。 注意：Unicode 字符集有多种编码方式，如 UTF-8、UTF-16 等；ASCII 只有一种；大多数 MBCS（包括 GB2312）也只有一种。\n 13. 写一个邮箱地址的正则表达式？  [A-Za-z0-9_-]+@[a-zA-Z0-9_-]+(.[a-zA-Z0-9_-]+)+$\n 14. 编写过哪些爬虫中间件？  user-agent、代理池等。\n 15. 极验滑动验证码如何破解？  1.selenium 控制鼠标实现，速度太机械化，成功率比较低\n 计算缺口的偏移量（ 推荐博客：http://blog.csdn.net/paololiu/article/details/52514504?%3E 极验滑动验证码需要具体网站具体分析，一般牵扯算法乃至深度学习相关知识。  16. 爬的那些内容数据量有多大，多久爬一次，爬下来的数据是怎么存储？  京东整站的数据大约在 1 亿左右，爬下来的数据存入数据库，mysql 数据库中如果有重复的 url 建议去重存入数据库，可以考虑引用外键。评分，评论如果做增量，Redis 中 url 去重，评分和评论建议建立一张新表用 id 做关联。 多久爬一次这个问题要根据公司的要求去处理，不一定是每天都爬。 Mongo 建立唯一索引键（id）可以做数据重复前提是数据量不大 2 台电脑几百万的情况数 据库需要做分片（数据库要设计合理）。 例：租房的网站数据量每天大概是几十万条，每周固定爬取。\n 17. cookie 过期的处理问题？  因为 cookie 存在过期的现象，一个很好的处理方法就是做一个异常类，如果有异常的话 cookie 抛出异常类在执行程序。\n 18. 动态加载又对及时性要求很高怎么处理？  Selenium+Phantomjs 尽量不使用 sleep 而使用 WebDriverWait 关于 HTTP/HTTPS 的区别，分别应该在什么场合下。\n 19. HTTPS 有什么优点和缺点  优点： 1、使用 HTTPS 协议可认证用户和服务器，确保数据发送到正确的客户机和服务器； 2、HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、身份认证的网络协议，要比 http 协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。 3、HTTPS 是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本 缺点： 1.HTTPS 协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用 2.HTTPS 协议还会影响缓存，增加数据开销和功耗，甚至已有安全措施也会受到影响也会因此而受到影响。 3.SSL 证书需要钱。功能越强大的证书费用越高。个人网站、小网站没有必要一般不会用。 4.HTTPS 连接服务器端资源占用高很多，握手阶段比较费时对网站的相应速度有负面影响。 5.HTTPS 连接缓存不如 HTTP 高效。\n 20. HTTPS 是如何实现安全传输数据的。(2018-4-20-xhq)  HTTPS 其实就是在 HTTP 跟 TCP 中间加多了一层加密层 TLS/SSL。SSL 是个加密套件，负责对 HTTP 的数据进行加密。TLS 是 SSL 的升级版。现在提到 HTTPS，加密套件基本指的是 TLS。原先是应用层将数据直接给到 TCP 进行传输，现在改成应用层将数据给到 TLS/SSL，将数据加密后，再给到 TCP 进行传输。\n 21. TTL，MSL，RTT？  MSL：报文最大生存时间，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。 TTL：TTL 是 time to live 的缩写，中文可以译为生存时间，这个生存时间是由源主机设置初始值但不是存的具体时间，而是存储了一个 ip 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。RFC 793 中规定 MSL 为 2 分钟，实际应用中常用的是 30 秒，1 分钟和 2 分钟等。TTL 与 MSL 是有关系的但不是简单的相等的关系，MSL 要大于等于 TTL。 RTT： RTT 是客户到服务器往返所花时间（round-trip time，简称 RTT），TCP 含有动态估算 RTT 的算法。TCP 还持续估算一个给定连接的 RTT，这是因为 RTT 受网络传输拥塞程序的变化而变化。\n 22. 谈一谈你对 Selenium 和 PhantomJS 了解  Selenium 是一个 Web 的自动化测试工具，可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。Selenium 自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行， 所以我 们可以用一个叫 PhantomJS 的工具代替真实的浏览器。Selenium 库里有个叫 WebDriver 的 API。 WebDriver 有点儿像可以加载网站的浏览器，但是它也可以像 BeautifulSoup 或者其他 Selector 对象一样用来查找页面元素，与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫。 PhantomJS 是一个基于 Webkit 的无界面 (headless) 浏览器，它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效。相比传统的 Chrome 或 Firefox 浏览器等，资源消耗会更少。 如果我们把 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理 JavaScript、Cookie、headers，以及任何我们真实用户需要做的事情。\n  主程序退出后，selenium 不保证 phantomJS 也成功退出，最好手动关闭 phantomJS 进程。（有可能会导致多个 phantomJS 进程运行，占用内存）。 WebDriverWait 虽然可能会减少延时，但是目前存在 bug（各种报错），这种情况可以采用 sleep。 phantomJS 爬数据比较慢，可以选择多线程。如果运行的时候发现有的可以运行，有的不能，可以尝试将 phantomJS 改成 Chrome。\n 23. 代理 IP 里的透明匿名高匿分别是指？  透明代理的意思是客户端根本不需要知道有代理服务器的存在，但是它传送的仍然是真实的 IP。你 要想隐藏的话，不要用这个。 普通匿名代理能隐藏客户机的真实 IP，但会改变我们的请求信息，服务器端有可能会认为我们使用了代理。不过使用此种代理时，虽然被访问的网站不能知道你的 ip 地址，但仍然可以知道你在使用代理，当然某些能够侦测 ip 的网页仍然可以查到你的 ip。 高匿名代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真实 IP 是隐藏的，服务器端不会认为我们使用了代理。 设置代理有以下两个好处： 1，让服务器以为不是同一个客户端在请求 2，防止我们的真实地址被泄露，防止被追究\n 24. requests 返回的 content 和 text 的区别？  a) response.text 返回的是 Unicode 型数据； a) response.content 返回的是 bytes 类型，也就是二进制数据； b) 获取文本使用，response.text； b) 获取图片，文件，使用 response.content； c) response.text 类型：str 解码类型： 根据 HTTP 头部对响应的编码作出有根据的推测，推测的文本编码 如何修改编码方式：response.encoding=gbk c) response.content 类型：bytes 解码类型： 没有指定 如何修改编码方式：response.content.decode (utf8)\n 25. robots 协议  Robots 协议：网站通过 Robots 协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。\n 26. 为什么 requests 请求需要带上 header？  原因是：模拟浏览器，欺骗服务器，获取和浏览器一致的内容 header 的形式：字典 headers = {User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36(KHTML, like Gecko)Chrome/54.0.2840.99 Safari/537.36} 用法： requests.get (url,headers=headers)\n 27. dumps,loads 与 dump,load 的区别？ json.dumps () 将 pyhton 的 dict 数据类型编码为 json 字符串； json.loads () 将 json 字符串解码为 dict 的数据类型； json.dump (x,y) x 是 json 对象，y 是文件对象，最终是将 json 对象写入到文件中； json.load (y) 从文件对象 y 中读取 json 对象。\n28. 通用爬虫：通常指搜索引擎的爬虫  聚焦爬虫：针对特定网站的爬虫 (https://img.alicdn.com/imgextra/i2/296192965/O1CN01yOEDgw1Xm0yDJpLLK_!!296192965.png)\n  通用搜素引擎的局限性： 通用搜索引擎所返回的网页里 90% 的内容无用。 图片、数据库、音频、视频多媒体的内容通用搜索引擎无能为力。不同用户搜索的目的不全相同，但是返回内容相同。\n 29. requests 使用小技巧  1、reqeusts.util.dict_from_cookiejar 把 cookie 对象转化为字典 requests.get(url,cookies={})\n  2、设置请求不用 SSL 证书验证 response = requests.get(\u0026quot;https://www.123cn/mormhweb/ \u0026quot;, verify=False)\n  3、设置超时 response = requests.get(url,timeout=10)\n  4、配合状态码判断是否请求成功 assert response.status_code == 200\n 30. 平常怎么使用代理的？  1、自己维护代理池 2、付费购买（目前市场上有很多 ip 代理商，可自行百度了解，建议看看他们的接口文档（API\u0026amp;SDK））\n 31. IP 存放在哪里？怎么维护 IP？对于封了多个 ip 的，怎么判定 IP 没被封？(2018-4-23-xhq)  存放在数据库 (redis、mysql 等)。 维护多个代理网站： 一般代理的存活时间往往在十几分钟左右，定时任务，加上代理 IP 去访问网页，验证其是否可用，如果返回状态为 200，表示这个代理是可以使用的。\n 32. 怎么获取加密的数据？ 1、 Web 端加密可尝试移动端（app） 2、 解析加密，看能否破解 3、 反爬手段层出不穷，js 加密较多，只能具体问题具体分析\n33. 假如每天爬取量在 5、6 万条数据，一般开几个线程，每个线程 ip 需要加锁限定吗？ 1、5、6 万条数据相对来说数据量比较小，线程数量不做强制要求 (做除法得一个合理值即可） 2、多线程使用代理，应保证不在同时一刻使用一个代理 IP\n34. 怎么监控爬虫的状态  1、使用 python 的 STMP 包将爬虫的状态信心发送到指定的邮箱 2、Scrapyd、pyspider 3、引入日志\n 二、Scrapy 1. 谈谈你对 Scrapy 的理解？  scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架，我们只需要实现少量代码，就能够快速的抓取到数据内容。Scrapy 使用了 Twisted 异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。 scrapy 框架的工作流程：\n 首先 Spiders（爬虫）将需要发送请求的 url (requests) 经 ScrapyEngine（引擎）交给 Scheduler（调度器）。 2.Scheduler（排序，入队）处理后，经 ScrapyEngine，DownloaderMiddlewares (可选，主要有 User_Agent， Proxy 代理) 交给 Downloader。 3.Downloader 向互联网发送请求，并接收下载响应（response）。将响应（response）经 ScrapyEngine，SpiderMiddlewares (可选) 交给 Spiders。 4.Spiders 处理 response，提取数据并将数据经 ScrapyEngine 交给 ItemPipeline 保存（可以是本地，可以是数据库）。提取 url 重新经 ScrapyEngine 交给 Scheduler 进行下一个循环。直到无 Url 请求程序停止结束。   2. 爬取下来的数据如何去重，说一下具体的算法依据   通过 MD5 生成电子指纹来判断页面是否改变 2.nutch 去重。nutch 中 digest 是对采集的每一个网页内容的 32 位哈希值，如果两个网页内容完全一样，它们的 digest 值肯定会一样。 数据量不大时，可以直接放在内存里面进行去重，python 可以使用 set () 进行去重。当去重数据需要持久化时可以使用 redis 的 set 数据结构。 当数据量再大一点时，可以用不同的加密算法先将长字符串压缩成 16/32/40 个字符，再使用上面两种方法去重。 当数据量达到亿（甚至十亿、百亿）数量级时，内存有限，必须用位来去重，才能够满足需求。Bloomfilter 就是将去重对象映射到几个内存位，通过几个位的 0/1 值来判断一个对象是否已经存在。 然而 Bloomfilter 运行在一台机器的内存上，不方便持久化（机器 down 掉就什么都没啦），也不方便分布式爬虫的统一去重。如果可以在 Redis 上申请内存进行 Bloomfilter，以上两个问题就都能解决了。 simhash 最牛逼的一点就是将一个文档，最后转换成一个 64 位的字节，暂且称之为特征字，然后判断重复只需要判断他们的特征字的距离是不是 \u0026lt; n（根据经验这个 n 一般取值为 3），就可以判断两个文档是否相似。 可见 scrapy_redis 是利用 set 数据结构来去重的，去重的对象是 request 的 fingerprint（其实就是用 hashlib.sha1 () 对 request 对象的某些字段信息进行压缩）。其实 fp 就是 request 对象加密压缩后的一个字符串（40 个字符，0~f）。   3. Scrapy 的优缺点？  优点 1）scrapy 是异步的 2）采取可读性更强的 xpath 代替正则 3）强大的统计和 log 系统 4）同时在不同的 url 上爬行 5）支持 shell 方式，方便独立调试 5）写 middleware, 方便写一些统一的过滤器 6）通过管道的方式存入数据库 缺点： 1）基于 python 的爬虫框架，扩展性比较差 2）基于 twisted 框架，运行中的 exception 是不会干掉 reactor（反应器），并且异步框架出错后是不会停掉其他任务的，数据出错后难以察觉。\n 三、Scrapy-Redis 1. scrapy 和 scrapy-redis 有什么区别？为什么选择 redis 数据库？  scrapy 是一个 Python 爬虫框架，爬取效率极高，具有高度定制性，但是不支持分布式。而 scrapy-redis 一套基于 redis 数据库、运行在 scrapy 框架之上的组件，可以让 scrapy 支持分布式策略， Slaver 端共享 Master 端 redis 数据库里的 item 队列、请求队列和请求指纹集合。 为什么选择 redis 数据库，因为 redis 支持主从同步，而且数据都是缓存在内存中的，所以基于 redis 的分布式爬虫，对请求和数据的高频读取效率非常高。\n 2. 分布式爬虫主要解决什么问题？  1.ip 2. 带宽 3.cpu 4.io\n 3. 什么是分布式存储？  传统定义：分布式存储系统是大量 PC 服务器通过 Internet 互联，对外提供一个整体的服务。 分布式存储系统具有以下的几个特性： 可扩展：分布式存储系统可以扩展到几百台甚至几千台这样的一个集群规模，系统的整体性能线性增长。 低成本：分布式存储系统的自动容错、自动负载均衡的特性，允许分布式存储系统可以构建在低成本的服务器上。另外，线性的扩展能力也使得增加、减少服务器的成本低，实现分布式存储系统的自动运维。 高性能：无论是针对单台服务器，还是针对整个分布式的存储集群，都要求分布式存储系统具备高性能。 易用：分布式存储系统需要对外提供方便易用的接口，另外，也需要具备完善的监控、运维工具，并且可以方便的与其他的系统进行集成。分布式存储系统的挑战主要在于数据和状态信息的持久化，要求在自动迁移、自动容错和并发读写的过程中，保证数据的一致性。 容错：可以快速检测到服务器故障，并自动的将在故障服务器上的数据进行迁移。 负载均衡：新增的服务器在集群中保障负载均衡？数据迁移过程中保障不影响现有的服务。 事务与并发控制：实现分布式事务。 易用性：设计对外接口，使得设计的系统易于使用。\n 4. 你所知道的分布式爬虫方案有哪些？ 三种分布式爬虫策略： 1.Slaver 端从 Master 端拿任务（Request/url/ID）进行数据抓取，在抓取数据的同时也生成新任务，并将任务抛给 Master。Master 端只有一个 Redis 数据库，负责对 Slaver 提交的任务进行去重、加入待爬队列。\n优点： scrapy-redis 默认使用的就是这种策略，我们实现起来很简单，因为任务调度等工作 scrapy-redis 都已经帮我们做好了，我们只需要继承 RedisSpider、指定 redis_key 就行了。 缺点： scrapy-redis 调度的任务是 Request 对象，里面信息量比较大（不仅包含 url，还有 callback 函数、headers 等信息），导致的结果就是会降低爬虫速度、而且会占用 Redis 大量的存储空间。当然我们可以重写方法实现调度 url 或者用户 ID。 2.Master 端跑一个程序去生成任务（Request/url/ID）。Master 端负责的是生产任务，并把任务去重、加入到待爬队列。Slaver 只管从 Master 端拿任务去爬。``` 优点： 将生成任务和抓取数据分开，分工明确，减少了 Master 和 Slaver 之间的数据交流；Master 端生成任务还有一个好处就是：可以很方便地重写判重策略（当数据量大时优化判重的性能和速度还是很重要的）。 缺点： 像 QQ 或者新浪微博这种网站，发送一个请求，返回的内容里面可能包含几十个待爬的用户 ID，即几十个新爬虫任务。但有些网站一个请求只能得到一两个新任务，并且返回的内容里也包含爬虫要抓取的目标信息，如果将生成任务和抓取任务分开反而会降低爬虫抓取效率。毕竟带宽也是爬虫的一个瓶颈问题，我们要秉着发送尽量少的请求为原则，同时也是为了减轻网站服务器的压力，要做一只有道德的 Crawler。所以，视情况而定。 3.Master 中只有一个集合，它只有查询的作用。Slaver 在遇到新任务时询问 Master 此任务是否已爬，如果未爬则加入 Slaver 自己的待爬队列中，Master 把此任务记为已爬。它和策略一比较像，但明显比策略一简单。策略一的简单是因为有 scrapy-redis 实现了 scheduler 中间件，它并不适用于非 scrapy 框架的爬虫。``` 优点： 实现简单，非 scrapy 框架的爬虫也适用。Master 端压力比较小，Master 与 Slaver 的数据交流也不大。 缺点：“健壮性” 不够，需要另外定时保存待爬队列以实现 “断点续爬” 功能。各 Slaver 的待爬任务不通用。 如果把 Slaver 比作工人，把 Master 比作工头。 策略一就是工人遇到新任务都上报给工头，需要干活的时候就去工头那里领任务； 策略二就是工头去找新任务，工人只管从工头那里领任务干活； 策略三就是工人遇到新任务时询问工头此任务是否有人做了，没有的话工人就将此任务加到自己的 “行程表”。  5. 除了 scrapy-redis，有做过其他的分布式爬虫吗？  Celery、gearman 等，参考其他分布式爬虫策略。\n 6. 在爬取的时候遇到某些内容字段缺失怎么判断及处理？  判读字段缺失，做异常处理即可。\n 四、自定义框架 ","date":"2023-01-09","permalink":"/post/spider/","tags":["InterviewQuestions","Python"],"title":"spider"},{"content":"一、元类 1.Python 中类方法、类实例方法、静态方法有何区别？ 类方法：是类对象的方法，在定义时需要在上方使用 @classmethod 进行装饰，形参为 cls，表示类对象，类对象和实例对象都可调用； 类实例方法：是类实例化对象的方法，只有实例对象可以调用，形参为 self，指代对象本身； 静态方法：是一个任意函数，在其上方使用 @staticmethod 进行装饰，可以用对象直接调用，静态方法实际上跟该类没有太大关系。  2.Python 中如何动态获取和设置对象的属性？ if hasattr(Parent, 'x'): print(getattr(Parent, 'x')) setattr(Parent, 'x', 3) print(getattr(Parent, 'x'))  二、内存管理与垃圾回收机制 1. Python 的内存管理机制及调优手段？ 内存管理机制：引用计数、垃圾回收、内存池。\n引用计数： 引用计数是一种非常高效的内存管理手段， 当一个 Python 对象被引用时其引用计数增加 1， 当其不再被一个变量引用时则计数减 1. 当引用计数等于 0 时对象被删除。\n垃圾回收： 1. 引用计数 引用计数也是一种垃圾收集机制，而且也是一种最直观，最简单的垃圾收集技术。当 Python 的某个对象的引用计数降为 0 时，说明没有任何引用指向该对象，该对象就成为要被回收的垃圾了。比如某个新建对象，它被分配给某个引用，对象的引用计数变为 1。如果引用被删除，对象的引用计数为 0，那么该对象就可以被垃圾回收。不过如果出现循环引用的话，引用计数机制就不再起有效的作用了 2. 标记清除 如果两个对象的引用计数都为 1，但是仅仅存在他们之间的循环引用，那么这两个对象都是需要被回收的，也就是说，它们的引用计数虽然表现为非 0，但实际上有效的引用计数为 0。所以先将循环引用摘掉，就会得出这两个对象的有效计数。 3. 分代回收 从前面标记 - 清除这样的垃圾收集机制来看，这种垃圾收集机制所带来的额外操作实际上与系统中总的内存块的数量是相关的，当需要回收的内存块越多时，垃圾检测带来的额外操作就越多，而垃圾回收带来的额外操作就越少；反之，当需回收的内存块越少时，垃圾检测就将比垃圾回收带来更少的额外操作。  举个例子：\n当某些内存块 M 经过了 3 次垃圾收集的清洗之后还存活时，我们就将内存块 M 划到一个集合 A 中去，而新分配的内存都划分到集合 B 中去。 当垃圾收集开始工作时，大多数情况都只对集合 B 进行垃圾回收，而对集合 A 进行垃圾回收要隔相当长一段时间后才进行，这就使得垃圾收集机制需要处理的内存少了，效率自然就提高了。 在这个过程中，集合 B 中的某些内存块由于存活时间长而会被转移到集合 A 中，当然，集合 A 中实际上也存在一些垃圾，这些垃圾的回收会因为这种分代的机制而被延迟。  内存池： 1.Python 的内存机制呈现金字塔形状，-1，-2 层主要有操作系统进行操作； 2.第 0 层是 C 中的 malloc，free 等内存分配和释放函数进行操作； 3.第 1 层和第 2 层是内存池，有 Python 的接口函数 PyMem_Malloc 函数实现，当对象小于 256K 时有该层直接分配内存； 4.第 3 层是最上层，也就是我们对 Python 对象的直接操作；  Python 在运行期间会大量地执行 malloc 和 free 的操作，频繁地在用户态和核心态之间进行切换，这将严重影响 Python 的执行效率。为了加速 Python 的执行效率，Python 引入了一个内存池机制，用于管理对小块内存的申请和释放。\nPython 内部默认的小块内存与大块内存的分界点定在 256 个字节，当申请的内存小于 256 字节时，PyObject_Malloc 会在内存池中申请内存；当申请的内存大于 256 字节时，PyObject_Malloc 的行为将蜕化为 malloc 的行为。当然，通过修改 Python 源代码，我们可以改变这个默认值，从而改变 Python 的默认内存管理行为。\n调优手段（了解） 1. 手动垃圾回收 2. 调高垃圾回收阈值 3. 避免循环引用（手动解循环引用和使用弱引用）  2. 内存泄露是什么？如何避免？ 指由于疏忽或错误造成程序未能释放已经不再使用的内存的情况。内存泄漏并非指内存在物理上的消失，而是应用程序分配某段内存后，由于设计错误，失去了对该段内存的控制，因而造成了内存的浪费。\n导致程序运行速度减慢甚至系统崩溃等严重后果。\n有 del() 函数的对象间的循环引用是导致内存泄漏的主凶。\u0026gt;不使用一个对象时使用:del object 来删除一个对象的引用计数就可以有效防止内存泄漏问题。\n通过 Python 扩展模块 gc 来查看不能回收的对象的详细信息。\n可以通过 sys.getrefcount(obj) 来获取对象的引用计数，并根据返回值是否为 0 来判断是否内存泄漏。\n三、函数 1. 函数参数 1.0 谈谈你对闭包的理解？ 闭包(closure) 是函数式编程的重要的语法结构。闭包也是一种组织代码的结构，它同样提高了代码的可重复使用性。 当一个内嵌函数引用其外部作作用域的变量，我们就会得到一个闭包。总结一下，创建一个闭包必须满足以下几点: 必须有一个内嵌函数 内嵌函数必须引用外部函数中的变量 外部函数的返回值必须是内嵌函数 重点是函数运行后并不会被撤销，就像 16 题的 instance 字典一样，当函数运行完后，instance 并不被销毁，而是继续留在内存空间里。这个功能类似类里的类变量，只不过迁移到了函数上. 闭包就像个空心球一样，你知道外面和里面，但你不知道中间是什么样.  1.1 Python 函数调用的时候参数的传递方式是值传递还是引用传递？ Python 的参数传递有：位置参数、默认参数、可变参数、关键字参数。\n函数的传值到底是值传递还是引用传递，要分情况：\n`不可变参数用值传递`： 像整数和字符串这样的不可变对象，是通过拷贝进行传递的，因为你无论如何都不可能在原处改变不可变对象 `可变参数是引用传递的`： 比如像列表，字典这样的对象是通过引用传递、和 C 语言里面的用指针传递数组很相似，可变对象能在函数内部改变。  1.2 对缺省参数的理解？ 缺省参数指在调用函数的时候没有传入参数的情况下，调用默认的参数，在调用函数的同时赋值时，所传入的参数会替代默认参数。\n`*args` 是不定长参数，他可以表示输入参数是不确定的，可以是任意多个。 `__kwargs` 是关键字参数，赋值的时候是以键 = 值的方式，参数是可以任意多对在定义函数的时候  不确定会有多少参数会传入时，就可以使用这两个参数。\n1.3 为什么函数名字可以当做参数用？ Python 中一切皆对象，函数名是函数在内存中的空间，也是一个对象。\n1.4 Python 中 pass 语句的作用是什么？ 在编写代码时只写框架思路，具体实现还未编写就可以用 pass 进行占位，使程序不报错，不会进行任何操作。\n1.5 有这样一段代码，print(c) 会输出什么，为什么？ a = 10 b = 20 c = [a] a = 15  答：10 对于字符串、数字，传递是相应的值。\n1.6 交换两个变量的值？ a,b = b,a\n2. 内建函数 2.1 map 函数和 reduce 函数？ `①从参数方面来讲`： `map()` 包含两个参数，第一个参数是一个函数，第二个是序列（列表或元组）。其中，函数（即 map 的第一个参数位置的函数）可以接收一个或多个参数。 `reduce()` 第一个参数是函数，第二个是序列（列表或元组）。但是，其函数必须接收两个参数。 `②从对传进去的数值作用来讲`： `map()` 是将传入的函数依次作用到序列的每个元素，每个元素都是独自被函数作用一次。 `reduce()` 是将传人的函数作用在序列的第一个元素得到结果后，把这个结果继续与下一个元素作用（累积计算）。  2.2 递归函数停止的条件？ 递归的终止条件一般定义在递归函数内部，在递归调用前要做一个条件判断，根据判断的结果选择是继续调用自身，还是 return; 返回终止递归。\n终止的条件：\n1.判断递归的次数是否达到某一限定值 2.判断运算的结果是否达到某个范围等，根据设计的目的来选择  2.3 回调函数，如何通信的？ 回调函数是把函数的指针(地址) 作为参数传递给另一个函数，将整个函数当作一个对象，赋值给调用的函数。\n2.4 Python 主要的内置数据类型都有哪些？ print(dir(\u0026lsquo;a\u0026rsquo;)) 的输出？ 内建类型：布尔类型、数字、字符串、列表、元组、字典、集合； 输出字符串 \u0026lsquo;a\u0026rsquo; 的内建方法；\n2.5 print(list(map(lambda x: x * x, [y for y in range(3)]))) 的输出？ [0， 1， 4]\n2.6 hasattr()、getattr()、setattr() 函数使用详解？ hasattr(object, name) 函数：\n判断一个对象里面是否有 name 属性或者 name 方法，返回 bool 值，有 name 属性(方法) 返回 True，否则返回 False。 注意：name 要使用引号括起来。\nclass function_demo(object): name = 'demo' def run(self): return \u0026quot;hello function\u0026quot; functiondemo = function_demo() res = hasattr(functiondemo, 'name') # 判断对象是否有 name 属性，True res = hasattr(functiondemo, \u0026quot;run\u0026quot;) # 判断对象是否有 run 方法，True res = hasattr(functiondemo, \u0026quot;age\u0026quot;) # 判断对象是否有 age 属性，Falsw print(res)  getattr(object, name [,default]) 函数：\n获取对象 object 的属性或者方法，如果存在则打印出来，如果不存在，打印默认值，默认值可选。\n注意：如果返回的是对象的方法，则打印结果是：方法的内存地址，如果需要运行这个方法，可以在后面添加括号 ()。\nfunctiondemo = function_demo() getattr(functiondemo, 'name') # 获取 name 属性，存在就打印出来 --- demo getattr(functiondemo,\u0026quot;run\u0026quot;) # 获取 run 方法，存在打印出方法的内存地址 ---\u0026lt;bound method function_demo.run of\u0026lt; main .function_demo object at 0x10244f320\u0026gt;\u0026gt; getattr(functiondemo, \u0026quot;age\u0026quot;) # 获取不存在的属性，报错如下： # Traceback(most recent call last): # File \u0026quot;/Users/liuhuiling/Desktop/MT_code/OpAPIDemo/conf/OPCommUtil.py\u0026quot;, line 39, in \u0026lt;module\u0026gt; # res = getattr(functiondemo, \u0026quot;age\u0026quot;) # AttributeError: 'function_demo' object has no attribute 'age' getattr(functiondemo, \u0026quot;age\u0026quot;, 18) # 获取不存在的属性，返回一个默认值  setattr(object,name,values) 函数：给对象的属性赋值，若属性不存在，先创建再赋值\nclass function_demo(object): name = 'demo' def run(self): return \u0026quot;hello function\u0026quot; functiondemo = function_demo() res = hasattr(functiondemo, 'age') # 判断 age 属性是否存在，False print(res) setattr(functiondemo, 'age', 18) # 对 age 属性进行赋值，无返回值 res1 = hasattr(functiondemo, 'age') # 再次判断属性是否存在，True  综合使用：\nclass function_demo(object): name = 'demo' def run(self): return \u0026quot;hello function\u0026quot; functiondemo = function_demo() res = hasattr(functiondemo, 'addr') # 先判断是否存在 if res: addr = getattr(functiondemo, 'addr') print(addr) else: addr = getattr(functiondemo, 'addr', setattr(functiondemo, 'addr', ' 北京首都 ')) # addr = getattr(functiondemo, 'addr', ' 美国纽约 ') print(addr) # 北京首都  2.7 一句话解决阶乘函数？ 在 Python2 中：\nreduce(lambda x,y: x*y, range(1,n+1))  Python3 中:\nfrom functools import reduce print(reduce(lambda x,y: x*y, range(1,n+1)))  3. Lambda 3.1 什么是 lambda 函数？ 有什么好处？ lambda 函数是一个可以接收任意多个参数(包括可选参数) 并且返回单个表达式值的函数\n1、lambda 函数比较轻便，即用即仍，很适合需要完成一项功能，但是此功能只在此一处使用，连名字都很随意的情况下； 2、匿名函数，一般用来给 filter， map 这样的函数式编程服务； 3、作为回调函数，传递给某些应用，比如消息处理  3.2 下面这段代码的输出结果将是什么？请解释。 def multipliers(): return [lambda x: i*x for i in range(4)] print([m(2) for m in multipliers()]) # [6, 6, 6, 6]  上面代码输出的结果是[6， 6， 6， 6](不是我们想的[0， 2， 4， 6])。\n你如何修改上面的 multipliers 的定义产生想要的结果？\n上述问题产生的原因是 Python 闭包的延迟绑定。\n这意味着内部函数被调用时，参数的值在闭包内进行查找。\n因此，当任何由 multipliers() 返回的函数被调用时，i 的值将在附近的范围进行查找。\u0026gt;那时，不管返回的函数是否被调用，for 循环已经完成，i 被赋予了最终的值 3。\n因此，每次返回的函数乘以传递过来的值 3，因为上段代码传过来的值是 2，它们最终返回的都是 6 。 (3*2) 碰巧的是，《The Hitchhiker\u0026rsquo;s Guide to Python》也指出，在与 lambdas 函数相关也有一个被广泛被误解的知识点，不过跟这个 case 不一样。由 lambda 表达式创造的函数没有什么特殊的地方，它其实是和 def 创造的函数式一样的。\n  下面是解决这一问题的一些方法。\n一种解决方法就是用Python 生成器。\ndef multipliers(): for i in range(4): yield lambda x : i * x print([m(2) for m in multipliers()]) # [0, 2, 4, 6]  另外一个解决方案就是创造一个 闭包，利用默认函数立即绑定 。\ndef multipliers(): return [lambda x, i=i: i * x for i in range(4)] print([m(2) for m in multipliers()]) # [0, 2, 4, 6]  3.3 什么是 lambda 函数？它有什么好处？写一个匿名函数求两个数的和？ lambda 函数是匿名函数；使用 lambda 函数能创建小型匿名函数。这种函数得名于省略了用 def 声明函数的标准步骤；\nf = lambda x, y: x + y print(f(2017, 2018)) # 4035  关于lambda 相关信息，参考 https://www.python87.com/pythonbasic/chapter14/lambda.html\n四、设计模式 ###1. 单例\n1.1 请手写一个单例 class A(object): __instance = None def __new__(cls, *args, __kwargs): if cls.instance is None: cls.instance = object.__new__(cls) return cls.instance else: return cls.instance  1.2 单例模式的应用场景有哪些？ 单例模式应用的场景一般发现在以下条件下：\n（1） 资源共享的情况下，避免由于资源操作时导致的性能或损耗等。如日志文件，应用配置。 （2） 控制资源的情况下，方便资源之间的互相通信。如线程池等。 1. 网站的计数器 2. 应用配置 3. 多线程池 4. 数据库配置，数据库连接池 5. 应用程序的日志应用….  2. 工厂 3. 装饰器 3.1 对装饰器的理解，并写出一个计时器记录方法执行性能的装饰器？ 装饰器本质上是一个 Python 函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。\nimport time def timeit(func): def wrapper(*args,__kwargs): start = time.time() func() end = time.time() print('used:', end - start) return wrapper @timeit def foo(): print('in foo()') foo() # in foo() # used: 2.8848648071289062e-05  3.2 解释一下什么是闭包？ 在函数内部再定义一个函数，并且这个函数用到了外边函数的变量，那么将这个函数以及用到的一些变量称之为闭包。\n3.3 函数装饰器有什么作用？ 装饰器本质上是一个 Python 函数，它可以在让其他函数在不需要做任何代码的变动的前提下增加额外的功能。 装饰器的返回值也是一个函数的对象，它经常用于有切面需求的场景。比如：插入日志、性能测试、事务处理、缓存、权限的校验等场景有了装饰器就可以抽离出大量的与函数功能本身无关的雷同代码并发并继续使用。\n4. 生成器 4.1 生成器、迭代器的区别？ 迭代器 是一个更抽象的概念，任何对象，如果它的类有 next 方法和 iter 方法返回自己本身，对于 string、list、dict、tuple 等这类容器对象，使用 for 循环遍历是很方便的。\n在后台 for 语句对容器对象调用 iter() 函数，iter() 是 python 的内置函数。\niter() 会返回一个定义了 next() 方法的迭代器对象，它在容器中逐个访问容器内元素，next() 也是 python 的内置函数。在没有后续元素时，next() 会抛出一个 StopIteration 异常。\n生成器（Generator） 是创建迭代器的简单而强大的工具。它们写起来就像是正规的函数，只是在需要返回数据的时候使用 yield 语句。每次 next() 被调用时，生成器会返回它脱离的位置（它记忆语句最后一次执行的位置和所有的数据值）\n区别：生成器能做到迭代器能做的所有事，而且因为自动创建了 iter() 和 next() 方法，生成器显得特别简洁，而且生成器也是高效的，使用生成器表达式取代列表解析可以同时节省内存。\n除了创建和保存程序状态的自动方法，当发生器终结时，还会自动抛出 StopIteration 异常。\n生成器 相关信息，参考 https://www.python87.com/pythonbasic/chapter13/\n4.2 X 是什么类型？ X = (i for i in range(10)) # \u0026lt;generator object \u0026lt;genexprat 0x7fd1a2e84270\u0026gt;  4.3 请尝试用一行代码实现将 1-N 的整数列表以 3 为单位分组，比如 1-100 分组后为？ print([[x for x in range(1,100)][i:i+3] for i in range(0,len(list_a),3)])  4.4 Python 中 yield 的用法？ yield 就是保存当前程序执行状态。你用 for 循环的时候，每次取一个元素的时候就会计算一次。用 yield 的函数叫 generator，和 iterator 一样，它的好处是不用一次计算所有元素，而是用一次算一次，可以节省很多空间。generator 每次计算需要上一次计算结果，所以用 yield，否则一 return，上次计算结果就没了。\ndef createGenerator(): mylist = range(3) for i in mylist: yield i * i mygenerator = createGenerator() # create a generator print(mygenerator) # mygenerator is an object! # \u0026lt;generator object createGenerator at 0x000001FBAA4996C8\u0026gt; for i in mygenerator: print(i)  yield 相关参考：https://www.python87.com/pythonbasic/chapter12/\n5. 试题 1 一行代码实现 1–100 之和 sum(range(0, 101))\n2 如何在一个函数内部修改全局变量 利用 global 修改全局变量\na = 20 def foo(): global a a = 47 foo() print(a) # 4  3 列出 5 个 python 标准库 os：提供了不少与操作系统相关联的函数 sys: 通常用于命令行参数 re: 正则匹配 math: 数学运算 datetime: 处理日期时间  4 字典如何删除键和合并两个字典 dic = {'name': 'kermit', 'age': 18} del dic ['name'] # 删除键 print(dic) # {'age': 18} dic2 = {'name': 'jack'} dic.update(dic2) print(dic) # {'age': 18, 'name': 'jack'}  5、谈下 python 的 GIL GIL 是 python 的全局解释器锁，同一进程中假如有多个线程运行，一个线程在运行 python 程序的时候会霸占 python 解释器（加了一把锁即 GIL）， 使该进程内的其他线程无法运行，等该线程运行完后其他线程才能运行。如果线程运行过程中遇到耗时操作，则解释器锁解开，使其他线程运行。 所以在多线程中，线程的运行仍是有先后顺序的，并不是同时进行。 多进程中因为每个进程都能被系统分配资源，相当于每个进程有了一个 python 解释器，所以多进程可以实现多个进程的同时运行，缺点是进程系统 资源开销大  五、面向对象 1. 类 2. 对象 2.1 Python 中的可变对象和不可变对象？ 不可变对象，该对象所指向的内存中的值不能被改变。当改变某个变量时候，由于其所指的值不能被改变，相当于把原来的值复制一份后再改变，这会开辟一个新的地址，变量再指向这个新的地址。\n可变对象，该对象所指向的内存中的值可以被改变。变量（准确的说是引用）改变后，实际上是其所指的值直接发生改变，并没有发生复制行为，也没有开辟新的出地址，通俗点说就是原地改变。\nPython 中，数值类型（int 和 float）、字符串 str、元组 tuple 都是不可变类型。而列表 list、字典 dict、集合 set 是可变类型。\n2.2 Python 中 is 和 == 的区别？ is 判断的是 a 对象是否就是 b 对象，是通过 id 来判断的。 == 判断的是 a 对象的值是否和 b 对象的值相等，是通过 value 来判断的。\n2.3 Python 的魔法方法 魔法方法就是可以给你的类增加魔力的特殊方法，如果你的对象实现（重载）了这些方法中的某一个，那么这个方法就会在特殊的情况下被 Python 所调用，你可以定义自己想要的行为，而这一切都是自动发生的。\n它们经常是两个下划线包围来命名的（比如 init ， lt ），Python 的魔法方法是非常强大的，所以了解其使用方法也变得尤为重要！\n   方法 说明     __init__ 构造器，当一个实例被创建的时候初始化的方法。但是它并不是实例化调用的第一个方法。   __new__ 才是实例化对象调用的第一个方法，它只取下 cls 参数，并把其他参数传给 __init__ 。__new__ 很少使用，但是也有它适合的场景，尤其是当类继承自一个像元组或者字符串这样不经常改变的类型的时候。   __call__ 允许一个类的实例像函数一样被调用。   __getitem__ 定义获取容器中指定元素的行为，相当于 self [key] 。   __getattr__ 定义当用户试图访问一个不存在属性的时候的行为。   __setattr__ 定义当一个属性被设置的时候的行为。   __getattribute__ 定义当一个属性被访问的时候的行为。    2.4 面向对象中怎么实现只读属性？ 1、将对象私有化，通过共有方法提供一个读取数据的接口。\nclass person: def __init__(self, x): self._age = 10 def age(self): return self._age t = person(22) # t._age = 100 print(t._age) # 10 class person: def __init__(self, x): self._age = 10 def age(self): return self._age t = person(22) print(t.age())  2、最好的方法\nclass MyCls(object): __weight = 50 @property # 以访问属性的方式来访问 weight 方法 def weight(self): return self.__weight if __name__ == '__main__': obj = MyCls() print(obj.weight) obj.weight = 12  2.5 谈谈你对面向对象的理解？ 面向对象是相对于面向过程而言的。面向过程语言是一种基于功能分析的、以算法为中心的程序设计方法；而面向对象是一种基于结构分析的、以数据为中心的程序设计思想。\n在面向对象语言中有一个有很重要东西，叫做 类。\n面向对象有三大特性：封装、继承、多态。\n六、正则表达式 1. Python 里 match 与 search 的区别？ `match()` 函数只检测 RE 是不是在 string 的开始位置匹配， `search()` 会扫描整个 string 查找匹配； 也就是说 match() 只有在 0 位置匹配成功的话才有返回，如果不是开始位置匹配成功的话，match() 就返回 none。  2. Python 字符串查找和替换？ re.findall(r'目的字符串'，'原有字符串') # 查询 re.findall(r'cast'，'itcast.cn')[0] re.sub(r‘要替换原字符'，'要替换新字符'，'原始字符串') re.sub(r'cast'，'heima'，'itcast.cn')  3. 用 Python 匹配 HTML g tag 的时候，\u0026lt;.\u0026gt;和 \u0026lt;.?\u0026gt;有什么区别？ \u0026lt;.*\u0026gt; 是 贪婪匹配，会从第一个 \u0026lt;开始匹配，直到最后一个\u0026gt;中间所有的字符都会匹配到，中间可能会包含 \u0026lt;\u0026gt;。\n\u0026lt;.*?\u0026gt; 是 非贪婪匹配，从第一个 \u0026lt;开始往后，遇到第一个\u0026gt;结束匹配，这中间的字符串都会匹配到，但是 不 会有 \u0026lt;\u0026gt;。\n4. 请写出下列正则关键字的含义？    语法说明 表达式 实例 完整匹配的字符串     字符      一般字符 匹配自身 abc abc   . 匹配任意除换行符”n” 外的字符。在 DOTALL 模式中也能匹配换行符。 a.c abc    转义字符，使后一个字符改变原来的意思。如果字符串中有字符需要匹配，可以使用 或者字符集 [*] a.c ac a.c ac   […] 字符集(字符类)。对应的位置可以是字符集中任意字符。字符集中的字符可以逐个列出，也可以给出范围，如 [abc] 或 [a-c]。第一个字符如果是 ^ 则表示取反，如 [^abc] 表示不是 abc 的其他字符。所有的特殊字符在字符集中都失去其原有的特殊含义。在字符集中如果要使用]、- 或 ^, 可以在前面加上反斜杠，或把]、- 放在第一个字符，把 ^ 放在非第 — 个字符。 a [bcd] e abe ace ade   预定义字符集(可以写在字符集 […] 中)      d 数字:[0-9] adc a1c   D 非数字:[^d] aDc abc   s 空白字符:[\u0026lt;空格trnfv] asc ac   S 非空白字符:[^s] aSc abc   w 单词字符:[A-Za-z0-9_] awc abc   W 非单词字符:[^W] aWc ac   数量词(用在字符或(…) 之后)      * 匹配前一个字符 0 或无限次 abc* ab abccc   + 匹配前一个字符 1 次或无限次 abc+ abc abccc   ? 匹配前一个字符 0 次或 1 次 abc? Ab abc   {m} 匹配前一个字符 m 次 ab {2} c abbc    七、系统编程 1. 进程总结 `进程`：程序运行在操作系统上的一个实例，就称之为进程。 进程需要相应的系统资源：`内存`、`时间片`、`pid`。 `创建进程`： 1. 首先要导入 multiprocessing 中的 Process； 2. 创建一个 Process 对象； 3. 创建 Process 对象时，可以传递参数； 4. 使用 start() 启动进程； 5. 结束进程。 #### Process 语法结构: p = Process(target=XXX, args=(元组，) , kwargs={key:value}) # target = XXX 指定的任务函数，不用加() # args=(元组，) , kwargs={key:value} 给任务函数传递的参数 Process([group [, target [, name [, args [, kwargs]]]]])     参数 解释     target 如果传递了函数的引用，可以让这个子进程就执行函数中的代码   args 给 target 指定的函数传递的参数，以元组的形式进行传递   kwargs 给 target 指定的函数传递参数，以字典的形式进行传递   name 给进程设定一个名字，可以省略   group 指定进程组，大多数情况下用不到    Process 创建的实例对象的常用方法有：    常用方法 解释     start() 启动子进程实例(创建子进程)   is_alive() 判断进程子进程是否还在活着   join(timeout) 是否等待子进程执行结束，或者等待多少秒   terminate() 不管任务是否完成，立即终止子进程    Process 创建的实例对象的常用属性：    常用属性 解释     name 当前进程的别名，默认为 Process-N,N 为从 1 开始递增的整数   pid 当前进程的 pid(进程号)    给子进程指定函数传递参数 Demo： import os from multiprocessing import Process import time def pro_func(name, age, __kwargs): for i in range(5): print(\u0026quot;子进程正在运行中，name=% s, age=% d, pid=% d\u0026quot; % (name, age, os.getpid())) print(kwargs) time.sleep(2) if __name__ == '__main__': # 创建 Process 对象 p = Process(target=pro_func, args=(' 小明 ', 18), kwargs={'m': 20}) # 启动进程 p.start() time.sleep(1) # 1 秒钟之后，立刻结束子进程 p.terminate() p.join() # 运行结果： # 子进程正在运行中，name = 小明，age=18, pid=22096 # {'m': 20} # 子进程正在运行中，name = 小明，age=18, pid=22096 # {'m': 20} # 子进程正在运行中，name = 小明，age=18, pid=22096 # {'m': 20} # 子进程正在运行中，name = 小明，age=18, pid=22096 # {'m': 20}  注意：进程间不共享全局变量。\n进程之间的通信 - Queue 在初始化 Queue() 对象时，(例如 q=Queue()，若在括号中没有指定最大可接受的消息数量，或数 量为负值时，那么就代表可接受的消息数量没有上限 - 直到内存的尽头)\nQueue.qsize()：返回当前队列包含的消息数量。\nQueue.empty()：如果队列为空，返回 True, 反之 False。\nQueue.full()：如果队列满了，返回 True，反之 False。\nQueue.get([block [,timeout]])：获取队列中的一条消息，然后将其从队列中移除，block 默认值为 True。 如果 block 使用默认值，且没有设置 timeout（单位秒），消息列队如果为空，此时程序将被阻塞（停在读取状态），直到从消息列队读到消息为止，如果设置了 timeout，则会等待 timeout 秒，若还没读取到任何消息，则抛出 Queue.Empty 异常； 如果 block 值为 False，消息列队如果为空，则会立刻抛出 Queue.Empty 异常；\nQueue.get_nowait()：相当 Queue.get(False)；\nQueue.put(item,[block [, timeout]])：将 item 消息写入队列，block 默认值为 True； 如果 block 使用默认值，且没有设置 timeout（单位秒），消息列队如果已经没有空间可写入，此时程序将被阻塞（停在写入状态），直到从消息列队腾出空间为止，如果设置了 timeout，则会等待 timeout 秒，若还没空间，则抛出 Queue.Full 异常； 如果 block 值为 False，消息列队如果没有空间可写入，则会立刻抛出 Queue.Full 异常；\nQueue.put_nowait(item)：相当 Queue.put(item, False)；\n进程间通信 Demo： from multiprocessing import Process, Queue import os, time, random # 写数据进程执行的代码:def write(q): for value in ['A', 'B', 'C']: print('Put % s to queue...' % value) q.put(value) time.sleep(random.random()) # 读数据进程执行的代码:def read(q): while True: if not q.empty(): value = q.get(True) print('Get % s from queue.' % value) time.sleep(random.random()) else: break if __name__ == '__main__': # 父进程创建 Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程 pw，写入: pw.start() # 等待 pw 结束: pw.join() # 启动子进程 pr，读取: pr.start() pr.join() # pr 进程里是死循环，无法等待其结束，只能强行终止: print('') print(' 所有数据都写入并且读完 ')  进程池 Pool from multiprocessing import Pool import os, time, random def worker(msg): t_start = time.time() print(\u0026quot;% s 开始执行，进程号为 % d\u0026quot; % (msg,os.getpid())) # random.random() 随机生成 0~1 之间的浮点数 time.sleep(random.random()*2) t_stop = time.time() print(msg,\u0026quot;执行完毕，耗时 %2f\u0026quot; % (t_stop-t_start)) po = Pool(3) # 定义一个进程池，最大进程数 3 for i in range(0,10): # Pool().apply_async(要调用的目标，(传递给目标的参数元祖，)) # 每次循环将会用空闲出来的子进程去调用目标 po.apply_async(worker,(i,)) print(\u0026quot;----start-----\u0026quot;) po.close() # 关闭进程池，关闭后 po 不再接收新的请求 po.join() # 等待 po 中所有子进程执行完成，必须放在 close 语句之后 print(\u0026quot;-----end---------------------------\u0026quot;)  multiprocessing.Pool 常用函数解析： `apply_async(func [, args [, kwds]])`：使用非阻塞方式调用 func（并行执行，堵塞方式必须等待上一个进程退出才能执行下一个进程），args 为传递给 func 的参数列表，kwds 为传递给 func 的关键字参数列表； `close()`：关闭 Pool，使其不再接受新的任务； `terminate()`：不管任务是否完成，立即终止； `join()`：主进程阻塞，等待子进程的退出， 必须在 close 或 terminate 之后使用；  进程池中使用 Queue 如果要使用 Pool 创建进程，就需要使用 multiprocessing.Manager() 中的 Queue()，而不是multiprocessing.Queue()，否则会得到一条如下的错误信息：\nRuntimeError: Queue objects should only be shared between processes through inheritance.\nfrom multiprocessing import Manager,Pool import os,time,random def reader(q): print(\u0026quot;reader 启动(% s), 父进程为(% s)\u0026quot; % (os.getpid(), os.getppid())) for i in range(q.qsize()): print(\u0026quot;reader 从 Queue 获取到消息：% s\u0026quot; % q.get(True)) def writer(q): print(\u0026quot;writer 启动(% s), 父进程为(% s)\u0026quot; % (os.getpid(), os.getppid())) for i in \u0026quot;itcast\u0026quot;: q.put(i) if __name__ == '__main__': print(\u0026quot;(% s) start\u0026quot; % os.getpid()) q = Manager().Queue() # 使用 Manager 中的 Queue po = Pool() po.apply_async(writer, (q,)) time.sleep(1) # 先让上面的任务向 Queue 存入数据，然后再让下面的任务开始从中取数据 po.apply_async(reader, (q,)) po.close() po.join() print(\u0026quot;(% s) End\u0026quot; % os.getpid())  2. 谈谈你对多进程，多线程，以及协程的理解，项目是否用？ 进程：一个运行的程序（代码）就是一个进程，没有运行的代码叫程序，进程是系统资源分配的最小单位，进程拥有自己独立的内存空间，所以进程间数据不共享，开销大。\n线程： 调度执行的最小单位，也叫执行路径，不能独立存在，依赖进程存在一个进程至少有一个线程，叫主线程，而多个线程共享内存(数据共享，共享全局变量)，从而极大地提高了程序的运行效率。\n协程：是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。\n3. 什么是多线程竞争？ 线程是非独立的，同一个进程里线程是数据共享的，当各个线程访问数据资源时会出现竞争状态即：数据几乎同步会被多个线程占用，造成数据混乱，即所谓的线程不安全\n那么怎么解决多线程竞争问题？ 锁。\n锁的好处： 确保了某段关键代码(共享数据资源) 只能由一个线程从头到尾完整地执行能解决多线程资源竞争下的原子操作问题。\n锁的坏处： 阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了 锁的致命问题：死锁。\n4. 解释一下什么是锁，有哪几种锁？ 锁(Lock) 是 Python 提供的对线程控制的对象。有互斥锁、可重入锁、死锁。\n5. 什么是死锁呢？ 若干子线程在系统资源竞争时，都在等待对方对某部分资源解除占用状态，结果是谁也不愿先解锁，互相干等着，程序无法执行下去，这就是死锁。\nGIL 锁（有时候，面试官不问，你自己要主动说，增加 b 格，尽量别一问一答的尬聊，不然最后等到的一句话就是：你还有什么想问的么？）\nGIL 锁全局解释器锁（只在 cpython 里才有）\n作用：限制多线程同时执行，保证同一时间只有一个线程执行，所以 cpython 里的多线程其实是伪多线程！\u0026gt;所以 Python 里常常使用协程技术来代替多线程，协程是一种更轻量级的线程，进程和线程的切换时由系统决定，而协程由我们程序员自己决定，而模块 gevent 下切换是遇到了耗时操作才会切换。\n三者的关系：进程里有线程，线程里有协程。\n6. 什么是线程安全，什么是互斥锁？ 每个对象都对应于一个可称为 互斥锁 的标记，这个标记用来保证在任一时刻，只能有一个线程访问该对象。\n同一个进程中的多线程之间是共享系统资源的，多个线程同时对一个对象进行操作，一个线程操作尚未结束，另一个线程已经对其进行操作，导致最终结果出现错误，此时需要对被操作对象添加互斥锁， 保证每个线程对该对象的操作都得到正确的结果。\n7. 说说下面几个概念：同步，异步，阻塞，非阻塞？ `同步`：多个任务之间有先后顺序执行，一个执行完下个才能执行。 `异步`：多个任务之间没有先后顺序，可以同时执行有时候一个任务可能要在必要的时候获取另一个同时执行的任务的结果，这个就叫回调！ `阻塞`：如果卡住了调用者，调用者不能继续往下执行，就是说调用者阻塞了。 `非阻塞`：如果不会卡住，可以继续执行，就是说非阻塞的。  同步异步相对于多任务而言，阻塞非阻塞相对于代码执行而言。\n8. 什么是僵尸进程和孤儿进程？怎么避免僵尸进程？ `孤儿进程`：父进程退出，子进程还在运行的这些子进程都是孤儿进程，孤儿进程将被 init 进程(进程号为 1) 所收养，并由 init 进程对它们完成状态收集工作。 `僵尸进程`：进程使用 fork 创建子进程，如果子进程退出，而父进程并没有调用 wait 或 waitpid 获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中的这些进程是僵尸进程。  避免僵尸进程的方法：\n1.fork 两次用孙子进程去完成子进程的任务； 2. 用 wait() 函数使父进程阻塞； 3. 使用信号量，在 signal handler 中调用 waitpid，这样父进程不用阻塞。  9. Python 中的进程与线程的使用场景？ 多进程适合在 CPU 密集型操作(cpu 操作指令比较多，如位数多的浮点运算)。 多线程适合在 IO 密集型操作(读写数据操作较多的，比如爬虫)。\n10. 线程是并发还是并行，进程是并发还是并行？ 线程是并发，进程是并行； 进程之间相互独立，是系统分配资源的最小单位，同一个线程中的所有线程共享资源。\n11. 并行（parallel）和并发（concurrency）？ 并行：同一时刻多个任务同时在运行。 并发：在同一时间间隔内多个任务都在运行，但是并不会在同一时刻同时运行，存在交替执行的情况。 实现并行的库有：`multiprocessing` 实现并发的库有：`threading`  程序需要执行较多的读写、请求和回复任务的需要大量的 IO 操作，IO 密集型操作使用并发更好。 CPU 运算量大的程序程序，使用并行会更好。\n12.IO 密集型和 CPU 密集型区别？ IO 密集型：系统运作，大部分的状况是 CPU 在等 I/O(硬盘 / 内存) 的读 / 写。 CPU 密集型：大部份时间用来做计算、逻辑判断等 CPU 动作的程序称之 CPU 密集型。  八、网络编程 1.UDP 总结 使用 udp 发送 / 接收数据步骤：  创建客户端套接字 发送 / 接收数据 关闭套接字  import socket def main(): # 1、创建 udp 套接字 # socket.AF_INET 表示 IPv4 协议 AF_INET6 表示 IPv6 协议 # socket.SOCK_DGRAM 数据报套接字，只要用于 udp 协议 udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # 2、准备接收方的地址 # 元组类型 ip 是字符串类型 端口号是整型 dest_addr = ('111111', 8888) # 要发送的数据 send_data = \u0026quot;我是要发送的数据\u0026quot; # 3、发送数据 udp_socket.sendto(send_data.encode(\u0026quot;utf-8\u0026quot;), dest_addr) # 4、等待接收方发送的数据 如果没有收到数据则会阻塞等待，直到收到数据 # 接收到的数据是一个元组 (接收到的数据，发送方的 ip 和端口) # 1024 表示本次接收的最大字节数 recv_data, addr = udp_socket.recvfrom(1024) # 5、关闭套接字 udp_socket.close() if __name__ == '__main__': 22． main() # 编码的转换: # str --\u0026gt;bytes: encode 编码 # bytes--\u0026gt;str: decode() 解码  UDP 绑定端口号： 1. 创建 socket 套接字 2. 绑定端口号 3. 接收 / 发送数据 4. 关闭套接字  import socket def main(): # 1、创建 udp 套接字 # socket.AF_INET 表示 IPv4 协议 AF_INET6 表示 IPv6 协议 # socket.SOCK_DGRAM 数据报套接字，只要用于 udp 协议 udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # 2、绑定端口 # 元组类型 ip 一般不写 表示本机的任何的一个 ip local_addr = ('', 7777) udp_socket.bind(local_addr) # 3、准备接收方的地址 # 元组类型 ip 是字符串类型 端口号是整型 dest_addr = ('111111', 8888) # 要发送的数据 send_data = \u0026quot;我是要发送的数据\u0026quot; # 4、发送数据 udp_socket.sendto(send_data.encode(\u0026quot;utf-8\u0026quot;), dest_addr) # 5、等待接收方发送的数据 如果没有收到数据则会阻塞等待，直到收到数据 # 接收到的数据是一个元组 (接收到的数据，发送方的 ip 和端口) # 1024 表示本次接收的最大字节数 recv_data, addr = udp_socket.recvfrom(1024) # 6、关闭套接字 udp_socket.close() if __name__ == '__main__': main() 注意点：绑定端口要在发送数据之前进行绑定。  2. TCP 总结 TCP 客户端的创建流程： 1. 创建 TCP 的 socket 套接字 2. 连接服务器 3. 发送数据给服务器端 4. 接收服务器端发送来的消息 5. 关闭套接字  import socket def main(): # 1、创建客户端的 socket # socket.AF_INET 表示 IPv4 协议 AF_INET6 表示 IPv6 协议 # socket.SOCK_STREAM 流式套接字，只要用于 TCP 协议 client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 2、构建目标地址 server_ip = input(\u0026quot;请输入服务器端的 IP 地址：\u0026quot;) server_port = int(input(\u0026quot;请输入服务器端的端口号：\u0026quot;)) # 3、连接服务器 # 参数：元组类型 ip 是字符串类型 端口号是整型 client_socket.connect((server_ip, server_port)) # 要发送给服务器端的数据 send_data = \u0026quot;我是要发送给服务器端的数据\u0026quot; # 4、发送数据 client_socket.send(send_data.encode(\u0026quot;gbk\u0026quot;)) # 5、接收服务器端恢复的消息， 没有消息会阻塞 # 1024 表示接收的最大字节数 recv_date= client_socket.recv(1024) print(\u0026quot;接收到的数据是：\u0026quot;, recv_date.decode('gbk')) # 6、关闭套接字 client_socket.close() if __name__ == '__main__': main()  TCP 服务器端的创建流程   创建 TCP 服务端的 socket 2.bing 绑定 ip 地址和端口号 3.listen 使套接字变为被动套接字 4.accept 取出一个客户端连接，用于服务 5.recv/send 接收和发送消息\n  关闭套接字\n  import socket def main(): # 1、创建 tcp 服务端的 socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 2、绑定 server_socket.bind(('', 8888)) # 3、listen 使套接字变为被动套接字 server_socket.listen(128) # 4、如果有新的客户端来链接服务器，那么就产生一个新的套接字专门为这个客户端服务 # client_socket 用来为这个客户端服务 # tcp_server_socket 就可以省下来专门等待其他新客户端的链接 client_socket, client_addr = server_socket.accept() # 5、接收客户端发来的消息 recv_data = client_socket.recv(1024) print(\u0026quot;接收到客户端 % s 的数据：% s\u0026quot; % (str(client_addr), recv_data.decode('gbk'))) # 6、回复数据给客户端 client_socket.send(\u0026quot;收到消息\u0026quot;.encode('gbk')) # 7、关闭套接字 client_socket.close() server_socket.close() if __name__ == '__main__': main()  4. TCP 是面向连接的通讯协议，通过三次握手建立连接，通讯完成时四次挥手 优点：TCP 在数据传递时，有确认、窗口、重传、阻塞等控制机制，能保证数据正确性，较为可靠。 缺点：TCP 相对于 UDP 速度慢一点，要求系统资源较多。  5. 简述浏览器通过 WSGI 请求动态资源的过程？ 发送 http 请求动态资源给 web 服务器 web 服务器收到请求后通过 WSGI 调用一个属性给应用程序框架 应用程序框架通过引用 WSGI 调用 web 服务器的方法，设置返回的状态和头信息。 调用后返回，此时 web 服务器保存了刚刚设置的信息 应用程序框架查询数据库，生成动态页面的 body 的信息 把生成的 body 信息返回给 web 服务器 web 服务器吧数据返回给浏览器  6. 描述用浏览器访问 www.baidu.com 的过程 先要解析出 baidu.com 对应的 ip 地址 * 要先使用 arp 获取默认网关的 mac 地址 * 组织数据发送给默认网关(ip 还是 dns 服务器的 ip，但是 mac 地址是默认网关的 mac 地址) * 默认网关拥有转发数据的能力，把数据转发给路由器 * 路由器根据自己的路由协议，来选择一个合适的较快的路径转发数据给目的网关 * 目的网关(dns 服务器所在的网关)，把数据转发给 dns 服务器 * dns 服务器查询解析出 baidu.com 对应的 ip 地址，并原路返回请求这个域名的 client 得到了 baidu.com 对应的 ip 地址之后，会发送 tcp 的 3 次握手，进行连接 使用 http 协议发送请求数据给 web 服务器 * web 服务器收到数据请求之后，通过查询自己的服务器得到相应的结果，原路返回给浏览器。 * 浏览器接收到数据之后通过浏览器自己的渲染功能来显示这个网页。 * 浏览器关闭 tcp 连接，即 4 次挥手结束，完成整个访问过程  7. Post 和 Get 请求的区别？ GET 请求：\n请求的数据会附加在 URL 之后，以？分割 URL 和传输数据，多个参数用 \u0026amp; 连接。URL 的 编码格式采用的是 ASCII 编码，而不是 uniclde，即是说所有的非 ASCII 字符都要编码之后再传输。  POST 请求：\nPOST 请求会把请求的数据放置在 HTTP 请求包的包体中。上面的 item=bandsaw 就是实际的传输数据。因此，GET 请求的数据会暴露在地址栏中，而 POST 请求则不会。 传输数据的大小： * 在 HTTP 规范中，没有对 URL 的长度和传输的数据大小进行限制。但是在实际开发过程中，对于 GET，特定的浏览器和服务器对 URL 的长度有限制。因此，在使用 GET 请求时，传输数据会受到 URL 长度的限制。 * 对于 POST，由于不是 URL 传值，理论上是不会受限制的，但是实际上各个服务器会规定对 POST提交数据大小进行限制，Apache、IIS 都有各自的配置。 安全性： * POST 的安全性比 GET 的高。这里的安全是指真正的安全，而不同于上面 GET 提到的安全方法中的安全，上面提到的安全仅仅是不修改服务器的数据。比如，在进行登录操作，通过 GET 请求，用户名和密码都会暴露再 URL 上，因为登录页面有可能被浏览器缓存以及其他人查看浏览器的历史记录的原因，此时的用户名和密码就很容易被他人拿到了。除此之外，GET 请求提交的数据还可能会造成 Cross-site request frogery 攻击。  效率：GET 比 POST 效率高。\nPOST 请求的过程： 浏览器请求 tcp 连接（第一次握手） 服务器答应进行 tcp 连接（第二次握手） 浏览器确认，并发送 post 请求头（第三次握手，这个报文比较小，所以 http 会在此时进行第一次数据发送） 服务器返回 100 continue 响应 浏览器开始发送数据 服务器返回 200 ok 响应 GET 请求的过程： 浏览器请求 tcp 连接（第一次握手） 服务器答应进行 tcp 连接（第二次握手） 浏览器确认，并发送 get 请求头和数据（第三次握手，这个报文比较小，所以 http 会在此时进行第一次数据发送） 服务器返回 200 OK 响应  8. cookie 和 session 的区别？ 1、cookie 数据存放在客户的浏览器上，session 数据放在服务器上。 2、cookie 不是很安全，别人可以分析存放在本地的 cookie 并进行 cookie 欺骗考虑到安全应当使用 session。 3、session 会在一定时间内保存在服务器上。当访问增多，会比较占用服务器的性能考虑到减轻服务器性能方面，应当使用 cookie。 4、单个 cookie 保存的数据不能超过 4K，很多浏览器都限制一个站点最多保存 20 个 cookie。 5、建议： 将登陆信息等重要信息存放为 SESSION 其他信息如果需要保留，可以放在 cookie 中  9. HTTP 协议状态码有什么用，列出你知道的 HTTP 协议的状态码，然后讲出他们都表示什么意思？ 通过状态码告诉客户端服务器的执行状态，以判断下一步该执行什么操作。 常见的状态机器码有： 100-199：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。 200-299：表示服务器成功接收请求并已完成处理过程，常用 200（OK 请求成功）。 300-399：为完成请求，客户需要进一步细化请求。302（所有请求页面已经临时转移到新的 url）。304、307（使用缓存资源）。 400-499：客户端请求有错误，常用 404（服务器无法找到被请求页面），403（服务器拒绝访问，权限不够）。 500-599：服务器端出现错误，常用 500（请求未完成，服务器遇到不可预知的情况）。  10. 请简单说一下三次握手和四次挥手？ 三次握手过程：\n1 首先客户端向服务端发送一个带有 SYN 标志，以及随机生成的序号 100(0 字节) 的报文 2 服务端收到报文后返回一个报文(SYN200(0 字节)，ACk1001(字节 + 1)) 给客户端 3 客户端再次发送带有 ACk 标志 201(字节 +) 序号的报文给服务端至此三次握手过程结束，客户端开始向服务端发送数据。 1 客户端向服务端发起请求：我想给你通信，你准备好了么？ 2 服务端收到请求后回应客户端：I'ok，你准备好了么 3 客户端礼貌的再次回一下客户端：准备就绪，咱们开始通信吧！ 整个过程跟打电话的过程一模一样：1 喂，你在吗 2 在，我说的你听得到不 3 恩，听得到(接下来请开始你的表演) 补充：SYN：请求询问，ACk：回复，回应。  四次挥手过程：\n由于 TCP 连接是可以双向通信的（全双工），因此每个方向都必须单独进行关闭（这句话才是精辟，后面四个挥手过程都是其具体实现的语言描述）四次挥手过程，客户端和服务端都可以先开始断开连接 1 客户端发送带有 fin 标识的报文给服务端，请求通信关闭 2 服务端收到信息后，回复 ACK 答应关闭客户端通信(连接) 请求 3 服务端发送带有 fin 标识的报文给客户端，也请求关闭通信 4 客户端回应 ack 给服务端，答应关闭服务端的通信(连接) 请求  11. 说一下什么是 tcp 的 2MSL？ 主动发送 fin 关闭的一方，在 4 次挥手最后一次要等待一段时间我们称这段时间为 2MSL TIME_WAIT 状态的存在有两个理由： 让 4 次挥手关闭流程更加可靠 防止丢包后对后续新建的正常连接的传输造成破坏。  12. 为什么客户端在 TIME-WAIT 状态必须等待 2MSL 的时间？ 1、为了保证客户端发送的最后一个 ACK 报文段能够达到服务器。 这个 ACK 报文段可能丢失，因而使处在 LAST-ACK 状态的服务器收不到确认。服务器会超时重传 FIN+ACK 报文段，客户端就能在 2MSL 时间内收到这个重传的 FIN+ACK 报文段，接着客户端重传一次确认，重启计时器。最好，客户端和服务器都正常进入到 CLOSED 状态。如果客户端在 TIME-WAIT 状态不等待一段时间，而是再发送完 ACK 报文后立即释放连接，那么就无法收到服务器重传的 FIN+ACK 报文段，因而也不会再发送一次确认报文。这样，服务器就无法按照正常步骤进入 CLOSED 状态。 2、防止已失效的连接请求报文段出现在本连接中。客户端在发送完最后一个 ACK 确认报文段后，再经过时间 2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。  13. 说说 HTTP 和 HTTPS 区别？ HTTP 协议传输的数据都是未加密的，也就是明文的，因此使用 HTTP 协议传输隐私信息非常不安全，为了保证这些隐私数据能加密传输，于是网景公司设计了 SSL（Secure Sockets Layer）协议用于对 HTTP 协议传输的数据进行加密，从而就诞生了 HTTPS。简单来说，HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、身份认证的网络协议，要比 http 协议安全。 HTTPS 和 HTTP 的区别主要如下： 1、https 协议需要到 ca 申请证书，一般免费证书较少，因而需要一定费用。 2、http 是超文本传输协议，信息是明文传输，https 则是具有安全性的 ssl 加密传输协议。 3、http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。 4、http 的连接很简单，是无状态的；HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、 身份认证的网络协议，比 http 协议安全。  14. 谈一下 HTTP 协议以及协议头部中表示数据类型的字段？ HTTP 协议是 Hyper Text Transfer Protocol（超文本传输协议）的缩写，是用于从万维网WWW:World Wide Web）服务器传输超文本到本地浏览器的传送协议。 HTTP 是一个基于 TCP/IP 通信协议来传递数据（HTML 文件， 图片文件， 查询结果等）。 HTTP 是一个属于应用层的面向对象的协议，由于其简捷、快速的方式，适用于分布式超媒体信息系统。它于 1990 年提出，经过几年的使用与发展，得到不断地完善和扩展。目前在 WWW 中使用的是 HTTP/0 的第六版，HTTP/1 的规范化工作正在进行之中，而且 HTTP-NG(NextGeneration of HTTP) 的建议已经提出。 HTTP 协议工作于客户端 - 服务端架构为上。浏览器作为 HTTP 客户端通过 URL 向 HTTP 服务端即 WEB 服务器发送所有请求。Web 服务器根据接收到的请求后，向客户端发送响应信息。 表示数据类型字段： Content-Type  15. HTTP 请求方法都有什么？ 根据 HTTP 标准，HTTP 请求可以使用多种请求方法。 HTTP0 定义了三种请求方法： GET， POST 和 HEAD 方法。 HTTP1 新增了五种请求方法：OPTIONS， PUT， DELETE， TRACE 和 CONNECT 方法。 1、 GET 请求指定的页面信息，并返回实体主体。 2、HEAD 类似于 get 请求，只不过返回的响应中没有具体的内容，用于获取报头 3、POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和 / 或已有资源的修改。 4、PUT 从客户端向服务器传送的数据取代指定的文档的内容。 5、DELETE 请求服务器删除指定的页面。 6、CONNECT HTTP/1 协议中预留给能够将连接改为管道方式的代理服务器。 7、OPTIONS 允许客户端查看服务器的性能。 8、TRACE 回显服务器收到的请求，主要用于测试或诊断。  16. 使用 Socket 套接字需要传入哪些参数 ？ Address Family 和 Type，分别表示套接字应用场景和类型。 family 的值可以是 AF_UNIX(Unix 域，用于同一台机器上的进程间通讯)，也可以是 AF_INET（对于 IPV4 协议的 TCP 和 UDP），至于 type 参数，SOCK_STREAM（流套接字）或者SOCK_DGRAM（数据报文套接字）,SOCK_RAW（raw 套接字）。  17. HTTP 常见请求头？ Host(主机和端口号) Connection(链接类型) Upgrade-Insecure-Requests(升级为 HTTPS 请求) User-Agent(浏览器名称) Accept(传输文件类型) Referer(页面跳转处) Accept-Encoding（文件编解码格式） Cookie （Cookie） x-requested-with :XMLHttpRequest (是 Ajax 异步请求)  18. 七层模型？ IP ，TCP/UDP ，HTTP ，RTSP ，FTP 分别在哪层？ IP： 网络层 TCP/UDP： 传输层 HTTP、RTSP、FTP： 应用层协议  19. url 的形式？ 形式： `scheme://host [:port#]/path/…/[?query-string][#anchor]` scheme：协议(例如：http， https， ftp) host：服务器的 IP 地址或者域名 port：服务器的端口（如果是走协议默认端口，80 or 443） path：访问资源的路径 query-string：参数，发送给 http 服务器的数据 anchor：锚（跳转到网页的指定锚点位置） http://localhost:8000/file/part01/html  20. 幂等 Idempotence HTTP 方法的幂等性是指一次和多次请求某一个资源应该具有同样的副作用。(注意是副作用) GET http://www.bank.com/account/123456，不会改变资源的状态，不论调用一次还是 N 次都没有副作用。请注意，这里强调的是一次和 N 次具有相同的副作用，而不是每次 GET 的结果相同。GET http://www.news.com/latest-news 这个 HTTP 请求可能会每次得到不同的结果，但它本身并没有产生任何副作用，因而是满足幂等性的。 DELETE 方法用于删除资源，有副作用，但它应该满足幂等性。比如：DELETE http://www.forum.com/article/4231，调用一次和 N 次对系统产生的副作用是相同的，即删掉 id 为 4231 的帖子；因此，调用者可以多次调用或刷新页面而不必担心引起错误。 POST 所对应的 URI 并非创建的资源本身，而是资源的接收者。比如：POST http://www.forum.com/articles 的语义是在 http://www.forum.com/articles 下创建一篇帖子，HTTP 响应中应包含帖子的创建状态以及帖子的 URI。两次相同的 POST 请求会在服务器端创建两份资源，它们具有不同的 URI；所以，POST 方法不具备幂等性。 PUT 所对应的 URI 是要创建或更新的资源本身。比如：PUT http://www.forum/articles/4231 的语义是创建或更新 ID 为 4231 的帖子。对同一 URI 进行多次 PUT 的副作用和一次 PUT 是相同的；因此，PUT 方法具有幂等性。  21. SOAP SOAP（原为 Simple Object Access Protocol 的首字母缩写，即简单对象访问协议）是交换数据的一种协议规范，使用在计算机网络 Web 服务（web service）中，交换带结构信息。SOAP 为了简化网页服务器（Web Server）从 XML 数据库中提取数据时，节省去格式化页面时间，以及不同应用程序之间按照 HTTP 通信协议，遵从 XML 格式执行资料互换，使其抽象于语言实现、平台和硬件。\n22. RPC RPC（Remote Procedure Call Protocol）—— 远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC 协议假定某些传输协议的存在，如 TCP 或 UDP，为通信程序之间携带信息数据。在 OSI 网络通信模型中，RPC 跨越了传输层和应用层。RPC 使得开发包括网络分布式多程序在内的应用程序更加容易。\n总结：服务提供的两大流派。传统意义以方法调用为导向通称 RPC。为了企业 SOA, 若干厂商联合推出 webservice, 制定了 wsdl 接口定义，传输 soap. 当互联网时代，臃肿 SOA 被简化为 http+xml/json. 但是简化出现各种混乱。以资源为导向，任何操作无非是对资源的增删改查，于是统一的 REST 出现了.\n进化的顺序: RPC -\u0026gt;SOAP -\u0026gt;RESTful\n","date":"2023-01-09","permalink":"/post/python_/","tags":["InterviewQuestions","Python"],"title":"python_"},{"content":"一、基础语法 1. 输入与输出 1.1 代码中要修改不可变数据会出现什么问题？抛出什么异常？ 代码不会正常运行，抛出 TypeError 异常。\n1.2 a=1,b=2, 不用中间变量交换 a 和 b 的值？ 方法一：\na = a + b b = a - b a = a - b  方法二：\na = a ^ b b = b ^ a a = a ^ b  方法三： a,b = b,a\n可参考: https://www.python87.com/pythonbasic/chapter01/1_04.html\n1.3 print 调用 Python 中底层的什么方法？ print 方法默认调用 sys.stdout.write 方法，即往控制台打印字符串。\n1.4 下面这段代码的输出结果将是什么？请解释？ class Parent(object): x = 1 class Child1(Parent): pass class Child2(Parent): pass print(Parent.x, Child1.x, Child2.x) Child1.x = 2 print(Parent.x, Child1.x, Child2.x) Parent.x = 3 print(Parent.x, Child1.x, Child2.x)  结果为：\n1 1 1 # 继承自父类的类属性 x，所以都一样，指向同一块内存地址。 1 2 1 # 更改 Child1，Child1 的 x 指向了新的内存地址。 3 2 3 # 更改 Parent，Parent 的 x 指向了新的内存地址。  1.5 简述你对input()函数的理解？  在 Python3 中，input() 获取用户输入，不论用户输入的是什么，获取到的都是字符串类型的。 在 Python2 中有 raw_input() 和 input(), raw_input() 和 Python3 中的 input() 作用是一样的，input() 输入的是什么数据类型的，获取到的就是什么数据类型的。\n 2. 条件与循环 2.1 阅读下面的代码，写出 A0，A1 至 An 的最终值。 A0 = dict(zip(('a', 'b', 'c', 'd', 'e'),(1, 2, 3, 4, 5))) A1 = range(0, 10) A2 = [i for i in A1 if i in A0] A3 = [A0[s] for s in A0] A4 = [i for i in A1 if i in A3] A5 = {i: i*i for i in A1} A6 = [[i, i*i] for i in A1]  答：\n{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5} range(0, 10) [] [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81} [[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49], [8, 64], [9, 81]]  2.2range和xrange的区别？ xrange 在 python3 中已经没有了\n两者用法相同，不同的是 range 返回的结果是一个列表，而 xrange 的结果是一个生成器，前者是直接开辟一块内存空间来保存列表，后者是边循环边使用，只有使用时才会开辟内存空间，所以当列表很长时，使用 xrange 性能要比 range 好。\n总结，python3 弃用了 xrange，但 python3 的 range 功能和 python2 的 xrange 一样，都是一个生成器。\n参考文章：https://www.python87.com/p/946.html\n2.3 考虑以下 Python 代码，如果运行结束，命令行中的运行结果是什么？ l = [] for i in range(10): l.append({'num':i}) print(l)  在考虑以下代码，运行结束后的结果是什么？\nl = [] a = {'num':0} for i in range(10): a['num'] = i l.append(a) print(l) print(a)  以上两段代码的运行结果是否相同，如果不相同，原因是什么？\n 上方代码的结果：\n [{'num':0}，{'num':1}，{'num':2}，{'num':3}，{'num':4}，{'num':5}，{'num':6}，{'num':7}，{'num':8}， {'num':9}]   下方代码结果：\n [{'num':9}，{'num':9}，{'num':9}，{'num':9}，{'num':9}，{'num':9}，{'num':9}，{'num':9}，{'num':9}， {'num':9}]   原因是 ： 字典是可变对象，在下方的l.append(a)的操作中是把字典a的引用传到列表l中，当后续操作修改a[num']的值的时候，l中的值也会跟着改变，相当于浅拷贝。\n 2.4 以下 Python 程序的输出？ for i in range(5, 0, -1): print(i)   答：5 4 3 2 1\n 3. 文件操作 3.1 4G 内存怎么读取一个 5G 的数据？  方法一： 可以通过生成器，分多次读取，每次读取数量相对少的数据（比如 500MB）进行处理，处理结束后在读取后面的 500MB 的数据。\n  方法二： 可以通过 linux 命令 split 切割成小文件，然后再对数据进行处理，此方法效率比较高。可以按照行数切割，可以按照文件大小切割。\n 参考文章：https://www.python87.com/p/947.html\n3.2 现在考虑有一个 jsonline 格式的文件 file.txt 大小约为 10K，之前处理文件的 代码如下所示：\nfrom multiprocessing import Process def get_lines(): l = [] with open('file.txt', 'rb') as f: for eachline in f: l.append(eachline) return l if __name__ == '__main__': for e in get_lines(): Process(e) # 处理每一行数据   现在要处理一个大小为 10G 的文件，但是内存只有 4G，如果在只修改 get_lines 函数而其他代码保持不变的情况下，应该如何实现？需要考虑的问题都有哪些？\n def get_lines(): l = [] with open('file.txt', 'rb') as f: data = f.readlines(60000) l.append(data) yield l    要考虑到的问题有： 内存只有 4G 无法一次性读入 10G 的文件，需要分批读入。分批读入数据要记录每次读入数据的位置。分批每次读入数据的大小，太小就会在读取操作上花费过多时间。\n 3.3 read、readline 和 readlines 的区别？  read: 读取整个文件。 readline： 读取下一行， 使用生成器方法。 readlines：读取整个文件到一个迭代器以供我们遍历。\n 3.4. 补充缺失的代码？ def print_directory_contents(sPath): \u0026quot;\u0026quot;\u0026quot; 这个函数接收文件夹的名称作为输入参数 返回该文件夹中文件的路径 以及其包含文件夹中文件的路径 \u0026quot;\u0026quot;\u0026quot; # 补充代码 # ------------ 代码如下 -------------------- import os for sChild in os.listdir(sPath): sChildPath = os.path.join(sPath, sChild) if os.path.isdir(sChildPath): print_directory_contents(sChildPath) else: print(sChildPath)  4. 异常 4.1 在 except 中 return 后还会不会执行 finally 中的代码？怎么抛出自定义异常？  会继续处理 finally 中的代码；用 raise 方法可以抛出自定义异常。\n 4.2 介绍一下 except 的作用和用法？ except: #捕获所有异常 except: \u0026lt;异常名\u0026gt;: #捕获指定异常 except: \u0026lt;异常名 1, 异常名 2\u0026gt; : #捕获异常 1 或者异常 2 except: \u0026lt;异常名\u0026gt;,\u0026lt;数据\u0026gt;: #捕获指定异常及其附加的数据 except: \u0026lt;异常名 1, 异常名 2\u0026gt;:\u0026lt;数据\u0026gt;: #捕获异常名 1 或者异常名 2, 及附加的数据  5. 模块与包 5.1 常用的 Python 标准库都有哪些？  os 操作系统，time 时间，random 随机，pymysql 连接数据库，threading 线程，multiprocessing 进程，queue 队列。 第三方库： django 和 flask 也是第三方库，requests，virtualenv，selenium，scrapy，xadmin，celery，re，hashlib，md5。 常用的科学计算库（如 Numpy，Scipy，Pandas)。\n 5.2 赋值、浅拷贝和深拷贝的区别？ 一、赋值  在 Python 中，对象的赋值就是简单的对象引用，这点和 C++ 不同，如下所示：\n a = [1,2,\u0026quot;hello\u0026quot;,['python', 'C++']] b = a   在上述情况下，a 和 b 是一样的，他们指向同一片内存，b 不过是 a 的别名，是引用。 我们可以使用 b is a 去判断，返回 True，表明他们地址相同，内容相同，也可以使用 id() 函数来查看两个列表的地址是否相同。 赋值操作(包括对象作为参数、返回值) 不会开辟新的内存空间，它只是复制了对象的引用。也就是说除了 b 这个名字之外，没有其他的内存开销。修改了 a，也就影响了 b，同理，修改了 b，也就影响了 a。\n 二、浅拷贝(shallow copy)  浅拷贝会创建新对象，其内容非原对象本身的引用，而是原对象内第一层对象的引用。 浅拷贝有三种形式：切片操作、工厂函数、copy 模块中的 copy 函数。\n  比如上述的列表 a； 切片操作：b = a [:] 或者 b = [x for x in a]； 工厂函数：b = list(a)； copy 函数：b = copy.copy(a)；\n  浅拷贝产生的列表 b 不再是列表 a 了，使用 is 判断可以发现他们不是同一个对象，使用 id 查看，他们也不指向同一片内存空间。但是当我们使用 id(x) for x in a 和 id(x) for x in b 来查看 a 和 b 中元 素的地址时，可以看到二者包含的元素的地址是相同的。 在这种情况下，列表 a 和 b 是不同的对象，修改列表 b 理论上不会影响到列表 a。 但是要注意的是，浅拷贝之所以称之为浅拷贝，是它仅仅只拷贝了一层，在列表 a 中有一个嵌套的 list，如果我们修改了它，情况就不一样了。 比如：a[3].append(java)。查看列表 b，会发现列表 b 也发生了变化，这是因为，我们修改了嵌套的 list，修改外层元素，会修改它的引用，让它们指向别的位置，修改嵌套列表中的元素，列表的地址并未发生变化，指向的都是用一个位置。\n 三、深拷贝(deep copy)  深拷贝只有一种形式，copy 模块中的 deepcopy() 函数。 深拷贝和浅拷贝对应，深拷贝拷贝了对象的所有元素，包括多层嵌套的元素。因此，它的时间和空间开销要高。\n  同样的对列表 a，如果使用 b = copy.deepcopy(a)，再修改列表 b 将不会影响到列表 a，即使嵌套的列表具有更深的层次，也不会产生任何影响，因为深拷贝拷贝出来的对象根本就是一个全新的对象， 不再与原来的对象有任何的关联。\n 四、拷贝的注意点？  对于非容器类型，如数字、字符，以及其他的原子类型，没有拷贝一说，产生的都是原对象的引用。 如果元组变量值包含原子类型对象，即使采用了深拷贝，也只能得到浅拷贝。\n 5.3 __init__ 和 __new__ 的区别？  init 在对象创建后，对对象进行初始化。 new 是在对象创建之前创建一个对象，并将该对象返回给 init。\n 5.4 Python 里面如何生成随机数？ 在 Python 中用于生成随机数的模块是 random，在使用前需要 import. 如下例子可以酌情列举： random.random()：生成一个 0-1 之间的随机浮点数； random.uniform(a, b)：生成 [a,b] 之间的浮点数； random.randint(a, b)：生成 [a,b] 之间的整数； random.randrange(a, b, step)：在指定的集合 [a,b) 中，以 step 为基数随机取一个数； random.choice(sequence)：从特定序列中随机取一个元素，这里的序列可以是字符串，列表，元组等。  5.5 输入某年某月某日，判断这一天是这一年的第几天？(可以用 Python 标准库) import datetime def dayofyear(): year = input(\u0026quot;请输入年份：\u0026quot;) month = input(\u0026quot;请输入月份：\u0026quot;) day = input(\u0026quot;请输入天：\u0026quot;) date1 = datetime.date(year=int(year), month=int(month), day=int(day)) date2 = datetime.date(year=int(year), month=1, day=1) return(date1 - date2 + 1).days  5.6 打乱一个排好序的 list 对象 alist？ import random random.shuffle(alist)  5.7 说明一下 os.path 和 sys.path 分别代表什么？  os.path 主要是用于对系统路径文件的操作。 sys.path 主要是对 Python 解释器的系统环境参数的操作（动态的改变 Python 解释器搜索路径）。\n 5.8 Python 中的 os 模块常见方法？ os.remove() 删除文件 os.rename() 重命名文件 os.walk() 生成目录树下的所有文件名 os.chdir() 改变目录 os.mkdir/makedirs 创建目录 / 多层目录 os.rmdir/removedirs 删除目录 / 多层目录 os.listdir() 列出指定目录的文件 os.getcwd() 取得当前工作目录 os.chmod() 改变目录权限 os.path.basename() 去掉目录路径，返回文件名 os.path.dirname() 去掉文件名，返回目录路径 os.path.join() 将分离的各部分组合成一个路径名 os.path.split() 返回（dirname(),basename()) 元组 os.path.splitext()(返回 filename,extension) 元组 os.path.getatime\\\\ctime\\\\mtime 分别返回最近访问、创建、修改时间 os.path.getsize() 返回文件大小 .path.exists() 是否存在 os.path.isabs() 是否为绝对路径 os.path.isdir() 是否为目录 os.path.isfile() 是否为文件  5.9 Python 的 sys 模块常用方法？ sys.argv 命令行参数 List，第一个元素是程序本身路径 sys.modules.keys() 返回所有已经导入的模块列表 sys.exc_info() 获取当前正在处理的异常类，exc_type、exc_value、exc_traceback 当前处理的异常详细信息\u0026gt; sys.exit(n) 退出程序，正常退出时 exit(0) sys.hexversion 获取 Python 解释程序的版本值，16 进制格式如：0x020403F0 sys.version 获取 Python 解释程序的版本信息 sys.maxint 最大的 Int 值 sys.maxunicode 最大的 Unicode 值 sys.modules 返回系统导入的模块字段，key 是模块名，value 是模块 sys.path 返回模块的搜索路径，初始化时使用 PYTHONPATH 环境变量的值 sys.platform 返回操作系统平台名称 sys.stdout 标准输出 sys.stdin 标准输入 sys.stderr 错误输出 sys.exc_clear() 用来清除当前线程所出现的当前的或最近的错误信息 sys.exec_prefix 返回平台独立的 python 文件安装的位置 sys.byteorder 本地字节规则的指示器，big-endian 平台的值是'big',little-endian 平台的值是 'little' sys.copyright 记录 python 版权相关的东西 sys.api_version 解释器的 C 的 API 版本 sys.version_info 元组则提供一个更简单的方法来使你的程序具备 Python 版本要求功能  5.10 unittest 是什么？  在 Python 中，unittest 是 Python 中的单元测试框架。它拥有支持共享搭建、自动测试、在测试 中暂停代码、将不同测试迭代成一组，等的功能。\n 5.11 模块和包是什么？  在 Python 中，模块是搭建程序的一种方式。每一个 Python 代码文件都是一个模块，并可以引用 其他的模块，比如对象和属性。 一个包含许多 Python 代码的文件夹是一个包。一个包可以包含模块和子文件夹。\n 6. Python 特性 6.1 Python 是强语言类型还是弱语言类型？  Python 是强类型的动态脚本语言。 强类型：不允许不同类型相加。 动态：不使用显示数据类型声明，且确定一个变量的类型是在第一次给它赋值的时候。 脚本语言：一般也是解释型语言，运行代码只需要一个解释器，不需要编译。\n 6.2 谈一下什么是解释性语言，什么是编译性语言？  计算机不能直接理解高级语言，只能直接理解机器语言，所以必须要把高级语言翻译成机器语言， 计算机才能执行高级语言编写的程序。 解释性语言在运行程序的时候才会进行翻译。 编译型语言写的程序在执行之前，需要一个专门的编译过程，把程序编译成机器语言（可执行文件）。\n 6.3 Python 中有日志吗？怎么使用？  有日志。 Python 自带 logging 模块，调用 logging.basicConfig() 方法，配置需要的日志等级和相应的参数， Python 解释器会按照配置的参数生成相应的日志。\n 6.4 Python 是如何进行类型转换的？  内建函数封装了各种转换函数，可以使用目标类型关键字强制类型转换，进制之间的转换可以用 int(str\u0026rsquo;，base=\u0026lsquo;n\u0026rsquo;) 将特定进制的字符串转换为十进制，再用相应的进制转换函数将十进制转换为目标进制。\n  可以使用内置函数直接转换的有：\n list----\u0026gt;tuple tuple(list) tuple----\u0026gt;list list(tuple)  6.5 Python2 与 Python3 的区别？ 1) 核心类差异 1.Python3 对 Unicode 字符的原生支持。\n Python2 中使用 ASCII 码作为默认编码方式\n 支持 unicode 的 string。Python2 和 Python3 字节和字符对应关系为：\n   Python2 Python3 表现 转换 作用     str bytes 字节 encode 存储   unicode str 字节 encode 显示    2.Python3 采用的是绝对路径的方式进行 import。\n1. 字典 dict: 字典，字典是一组键(key) 和值(value) 的组合，通过键(key) 进行查找，没有顺序， 使用大括号”{}”;\n应用场景：dict，使用键和值进行关联的数据；\n1.1 现有字典 d={\u0026lsquo;a\u0026rsquo;:24，\u0026lsquo;g\u0026rsquo;:52，\u0026lsquo;i\u0026rsquo;:12，\u0026lsquo;k\u0026rsquo;:33} 请按字典中的 value 值进行排序？ sorted(d.items()，key = lambda x:x[1]) 。  1.2 说一下字典和 json 的区别？  字典是一种数据结构，json 是一种数据的表现形式，字典的 key 值只要是能 hash 的就行，json 的必须是字符串。\n 1.3 什么是可变、不可变类型？  可变不可变指的是内存中的值是否可以被改变 不可变类型指的是对象所在内存块里面的值不可以改变，有数值、字符串、元组； 可变类型则是可以改变，主要有列表、字典。\n 1.4 存入字典里的数据有没有先后排序？  存入的数据不会自动排序，可以使用 sort 函数 对字典进行排序。\n 1.5 字典推导式？ # 变量 = {key,value for key,value in 可迭代对象 if 条件} # 变量 = {key:value for key,value in 可迭代对象 if 条件} d = {key: value for(key, value) in iterable}  1.6 现有字典 d={\u0026lsquo;a\u0026rsquo;:24，\u0026lsquo;g\u0026rsquo;:52，\u0026rsquo;l\u0026rsquo;:12，\u0026lsquo;k\u0026rsquo;:33} 请按字典中的 value 值进行排序？ sorted(d.items(), key=lambda x: x [1])  2、字符串 str: 字符串是 Python 中最常用的数据类型。我们可以使用引号(\u0026rsquo; 或\u0026quot;) 来创建字符串。\n2.1 如何理解 Python 中字符串中的 \\ 字符？  有三种不同的含义： 1、转义字符 2、路径名中用来连接路径名 3、编写太长代码手动软换行。\n 2.2 请反转字符串 “aStr”? print('aStr'[::-1])\n2.3 将字符串”k:1|k1:2|k2:3|k3:4”，处理成 Python 字典：{k:1， k1:2， …} # 字典里的 K 作为字符串处理 str1 =\u0026quot;k:1|k1:2|k2:3|k3:4\u0026quot; def str2dict(str1): dict1 = {} for iterms in strsplit('|'): key, value = iterms.split(':') dict1[key] = value return dict1  2.4 请按 alist 中元素的 age 由大到小排序 alist = [{'name': 'a', 'age': 20}, {'name': 'b', 'age': 30}, {'name': 'c', 'age': 25}] def sort_by_age(list1): return sorted(alist, key=lambda x: x['age'], reverse=True) if __name__ == '__main__': print sort_by_age(alist)) # [{'name': 'b', 'age': 30}, {'name': 'c', 'age': 25}, {'name': 'a', 'age': 20}]  3、列表 list: 是 Python 中使用 最频繁的数据类型，在其他语言中通常叫做 数组，通过索引进行查找，使用方括号 [],\u0026gt; 列表是有序的集合。 应用场景：定义列表使用 [] 定义，数据之间使用 ， 分割。\u0026gt; \u0026gt; 列表的索引从 0 开始：索引就是数据在列表中的位置编号，索引又可以被称为下标。 【注意】: 从列表中取值时，如果超出索引范围，程序会产生异常。IndexError: list index out of range\n3.2 写一个列表生成式，产生一个公差为 11 的等差数列 print([x*11 for x in range(10)]) # [0, 11, 22, 33, 44, 55, 66, 77, 88, 99]  3.3 给定两个列表，怎么找出他们相同的元素和不同的元素？ list1 = [1, 2, 3] list2 = [3, 4, 5] set1 = set(list1) set2 = set(list2) print(set1 \u0026amp; set2) # {3} print(set1 ^ set2) # {1, 2, 4, 5}  3.4 请写出一段 Python 代码实现删除一个 list 里面的重复元素？ 使用用内置的 set：\nl1 = ['b', 'c', 'd', 'b', 'c', 'a', 'a'] l2 = list(set(l1)) print(l2) # ['b', 'd', 'c', 'a']  如果想要保持他们原来的排序 ：用 list 类的 sort 方法：\nl1 = ['b','c','d','b','c','a','a'] # print(list(set(l1))) l2 = list(set(l1)) l2.sort(key=l1.index) # 这里的l1.index 刚开始有点迷惑，以为是写错了，去用lambda 自己写了下才反映过来是差不多的 print(l2) # l1 = ['b', 'c', 'd', 'b', 'c', 'a', 'a'] # l2 = list(set(l1)) # l2.sort(key=lambda x:l1.index(x)) # print(l2) # ['b', 'c', 'd', 'a']  也可以这样写：\nl1 = ['b','c','d','b','c','a','a'] l2 = sorted(set(l1),key=l1.index) print(l2) # l1 = ['b', 'c', 'd', 'b', 'c', 'a', 'a'] # l2 = sorted(set(l1), key=lambda x:l1.index(x)) # print(l2) # ['b', 'c', 'd', 'a']  也可以用遍历：\nl1 = ['b', 'c', 'd', 'b', 'c', 'a', 'a'] l2 = [] for i in l1: if not i in l2: lappend(i) print(l2) # ['b', 'c', 'd', 'a']  3.5 给定两个 list A ,B，请用找出 A ,B 中相同的元素，A ,B 中不同的元素 A、B 中相同元素：print(set(A)\u0026amp;set(B)) A、B 中不同元素：print(set(A)^set(B))  3.6 有如下数组 list = range(10) 我想取以下几个数组，应该如何切片？ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [0, 1, 2, 3, 4, 5, 6] [3, 4, 5, 6] [9] [1, 3, 5, 7, 5, 9]  答：\n[1:] [1:7] [3:7] [-1] [1::2]  3.7 下面这段代码的输出结果是什么？请解释？ def extendlist(val, list=[]): list.append(val) return list list1 = extendlist(10) list2 = extendlist(123, []) list3 = extendlist('a') print(\u0026quot;list1 = % s\u0026quot; % list1) print(\u0026quot;list2 = % s\u0026quot; % list2) print(\u0026quot;list3 = % s\u0026quot; % list3)  输出结果：\nlist1 = [10, 'a'] list2 = [123] list3 = [10, 'a']  新的默认列表只在函数被定义的那一刻创建一次。当 extendList 被没有指定特定参数 list 调用时，这组 list 的值随后将被使用。这是因为带有默认参数的表达式在函数被定义的时候被计算，不是在调用的时候被计算。\n3.8. 将以下 3 个函数按照执行效率高低排序 def f1(lIn): l1 = sorted(lIn) l2 = [i for i in l1 if i\u0026lt;0.5] return [i*i for i in l2] def f2(lIn): l1 = [i for i in l1 if i\u0026lt;0.5] l2 = sorted(l1) return [i*i for i in l2] def f3(lIn): l1 = [i*i for i in lIn] l2 = sorted(l1) return [i for i in l1 if i\u0026lt;(0.5*0.5)]  按执行效率从高到低排列：f2、f1 和 f3。要证明这个答案是正确的，你应该知道如何分析自己代码的性能。Python 中有一个很好的程序分析包，可以满足这个需求。\nimport random import cProfile lIn = [random.random() for i in range(100000)] cProfile.run('f1(lIn)') cProfile.run('f2(lIn)') cProfile.run('f3(lIn)')  3.9 获取 1~100 被 6 整除的偶数？ def A(): L=[] for i in range(1,100): if i%6==0: L.append(i) return L print(A())  4、元祖 tuple: 元组，元组将多样的对象集合到一起，不能修改，通过索引进行查找，使用括号 ();\n应用场景：把一些数据当做一个整体去使用，不能修改；\n5、集合 set:集合，在 Python 中的书写方式的 {}，集合与之前列表、元组类似，可以存储多个数据，但是这些数据是不重复的。\n集合对象还支持 union(联合), intersection(交), difference(差) 和 sysmmetric_difference(对称差集) 等数学运算.\n快速去除列表中的重复元素 a = [11, 22, 33, 33, 44, 22, 55] print(set(a)) # {33, 11, 44, 22, 55}  交集：共有的部分 a = {11, 22, 33, 44, 55} b = {22, 44, 55, 66, 77} print(a \u0026amp; b) # {44, 22, 55}  并集：总共的部分 a = {11, 22, 33, 44, 55} b = {22, 44, 55, 66, 77} print(a | b) # {33, 66, 11, 44, 77, 22, 55}  差集：另一个集合中没有的部分 a = {11, 22, 33, 44, 55} b = {22, 44, 55, 66, 77} print(b - a) # {66, 77}  对称差集(在 a 或 b 中，但不会同时出现在二者中) a = {11, 22, 33, 44, 55} b = {22, 44, 55, 66, 77} print(a ^ b) # {33, 66, 11, 77}  ","date":"2023-01-09","permalink":"/post/python_base/","tags":["InterviewQuestions","Python"],"title":"python_base"},{"content":"面试前的准备 首先我们要穿的得体，因为第一印象对一个面试官来说真的很重要，如果我们面试的时候都不能以一种非常认真的态度去对待，那么可想而知其实我们离面试成功的路渐行渐远，当然这只是说第一印象，并不能代表你面试是否成功的全部，如果你是技术大牛，我想只要你有敏捷的思维，精湛的技术，良好的团队合作精神，那么面试也许对你来说并不是什么问题，我更多的是想对那些小白，或者是培训机构出来的it屌丝们说的！另外面试前一定要规划好面试地点的路线，最好提前半个小时出发，以免人在囧途，面试时迟到，这样给人的第一印象就不好，面试官可能会认为你上班估计也是这个样子吧！所以要提前准备好该准备的，这样我们就可以把更多的时间投入到与面试官斗志斗勇的当中去！\n面试中注意的细节 面试中一般情况，面试官一般都会让你先进行自我介绍，这样他有足够的时间去阅读你的简历，所以自我介绍是一个很好的把自己推销出去的方式，让面试官对你有一个全面的了解，一般分为基本信息的介绍比如叫什么、来自哪、性格呀什么的因人而异，要想让面试官记住你就要表现出你的与众不同，一定要将自己的闪光点亮出来，闪瞎面试官的24k钛合金眼，比如大学期间获得过国家奖学金、参加过什么什么牛逼的比赛获得过什么奖，以及在大学有什么丰富的人生经历，让面试官觉得嗯这小子果然有两把刷子，这样他可能更像往下继续与你聊下去，对于面试it工作经历自然也少不了要介绍一下，我们一定要介绍清楚自己在项目中担任的角色，主要做哪些事情，自己在这个项目中的突出贡献是什么，这样才能体现你在这个团队中的价值，因为有些面试官喜欢问项目，有些面试官喜欢问技术点，所以这一块也要做好充分的准备，以及应对面试官后面的狂轰乱战，所以你说出来的项目技术点自己一定要清楚来龙去脉，如果你不清楚，或者只是知道这个概念，那么面试官对你的印象会瞬间从你刚才的丰富的人生阅历跌到低谷，怀疑你工作年限、甚至会怀疑你的工作经历造假，只是拿别人的项目放到你的简历上而已，这样就会造成你的面试成功的概率急剧下降，所以在介绍你的项目是千万别两句话就讲完了，让人觉得你好像并没有什么人生阅历一样。。。。在面试过程中我们要掌握主动权，这个真的很重要，你要尽量将面试官朝着你知道的地方引，让面试官从你引出的话题中继续提问，这样因为你提前做好了准备所以完全不用害怕，但是如果你等着面试官去问你，那么你可能要准备的更多去应对他可能提出的未知问题，这样当他问到你清楚的技术点的时候你可能就会不知所措，但是也不要慌！因为面试中不会其实也很正常的，但是不是说你什么都不会，那么你就直接被pass掉了，即使问道你不会的知识点也要表现的足够自信，你要告诉他遇到这种问题你的解决思路，工作中也会遇到新的技术我们不会不是说这个事情就不能做了，比如你可以说你是在遇到问题之后如何将问题解决掉的，这才是一个公司看重的，对于it人才快速的学习能力是很重要的，所以千万不要畏惧，表现的很不自信！\n面试官的反问 你为什么要离开上一家公司 像这种问题，我们在面试前就应该想到并做好充分的准备，正所谓不打无准备之仗，兵来将挡水来土掩，我们只有事先做好充分的准备才能在真正荷枪实弹的场景下表现的从容淡定，千万不要因为自己本能应该获得工作的机会，因为自己的疏忽而离我们渐行渐远，在回答问题上也要注意，并不是说这个问题回答问了就算完了，一定要回答的要让面试官满意，比如你说你和上一家公司同事关系处的不好、上一家薪水给的太少、公司经常加班、公司压力太大、项目组人太少，项目根本不能在项目周期内完成等等这些回答看似很真实，但是并不是很好的答案，因为面试官觉得你这个人即使我们要了你，但是你在我们公司可能也待不长，或者工作中承受不住压力，不能很好的融入团队等，所以回答的时候一定不要说是公司的原因，人际关系处的不好，或者自身的某些缺点的原因上去回答，尽量从一些客观上的原因进行回答，这样更好一点，比如上一家公司的地方距离自己家乡太远了，自己爸妈希望自己在这边发展，或者自己女朋友也在这边上班所以想来这边发展，等等吧。。。\n技术相关的问题 其实这方面，对于学的并不是很好的同仁或者工作经历欠缺的同仁，我想说的是在找工作的时候也千万不要害怕，很多人之所以面试失败还是在面试的过程中，觉得自己这也不会那也不会，都不是很精通，表现的很不自信，最后面试就以失败告终！还是那句话你要让他知道即使不会，你也可以在工作中，通过自己的学习，看一些技术博客，将问题很好的解决掉，让他能够看到你在工作中能够独立完成任务，能够很好的解决问题，能够独挡一面，这就好像你觉得一件事情，在自己看来根本不可能完成，那么你一定不可能在这件事中获得很好的结果，因为你在做之前就已经将自己否定了，即使你做了更多的还是抱有侥幸心理，所以我们凡是不要觉得不可能，我不行，我没有别人优秀等借口！来阻止自己继续向前，你一定要相信你自己，你只有把自己说服了，才有可能在面试中将面试说服，否则你有什么理由让面试官将这份offer给你呢，有拿什么相信你可以胜任这份工作呢？所以找工作更多的是靠能力和实力不是让你去说你经历过什么挫折，有多可怜，有种跪舔面试官的感觉，像卖不掉的秫秸，甚至像廉价推出去，如果你是这么想的，恭喜你廉价也不会有公司要你！因为公司也要生存！不会因为可怜就会将工作交给你，天下这么多可怜的人，那谁又可怜公司呢，毕竟公司也是以盈利为目的的！\n你还有什么想问我的吗？ 一般这个问题就代表面试即将结束，千万不要在最后因为前面的表现都很好，在这里翻了跟头，比如一些不好的问题千万不要问，比如公司加班吗？最晚可以几点到公司？这个职位的薪资待遇是多少？等等，这些都不是很好的回答，你可以从自己的职业规划来回答，比如贵公司会有相关的业务培训吗？假如我被贵公司录用了，你希望我在短期内解决哪些问题？你在贵公司工作多久了？您觉得让你在贵公司长期做下去的动力是什么？公司相关的技术团队有多少人?目前使用的什么框架？这个岗位的晋升路线是什么样子的？主要从哪些方面进行考核？等等。。。这些问题一方面可以让你了解这个公司是不是你想去的公司，另一方面也能让面试官觉得你是一个很爱学习的“三好学生”，有自己理想和自己职业规划的人，这样如果一轮面试下来没什么问题，面试官觉得你各方面表现的都还不错，那么恭喜你面试官可能就会和你谈offer的问题了，这样也就能水到渠成的拿到你要的offer!\n一：请你自我介绍一下你自己？ 回答提示：一般人回答这个问题过于平常，只说姓名、年龄、爱好、工作经验，这些在简历上都有。其实，企业最希望知道的是求职者能否胜任工作，包括：最强的技能、最深入研究的知识领域、个性中最积极的部分、做过的最成功的事，主要的成就等，这些都可以和学习无关，也可以和学习有关，但要突出积极的个性和做事的能力，说得合情合理企业才会相信。企业很重视一个人的礼貌，求职者要尊重考官，在回答每个问题之后都说一句“谢谢”，企业喜欢有礼貌的求职者。\n二：你最大的优点和缺点是什么？ 回答提示：这个问题外企问的概率很大，通常不希望听到直接回答的缺点是什么等，如果求职者说自己小心眼、爱忌妒人、非常懒、脾气大、工作效率低，外企肯定不会录用你。外企喜欢求职者从自己的优点说起，中间加一些小缺点，最后再把问题转回到优点上，突出优点的部分。外企喜欢聪明的求职者。\n三：谈谈你对公司加班的看法？ 回答提示：你可以这样说“如果是工作需要我会义不容辞加班，我现在单身，没有任何家庭负担，可以全身心的投入工作。但同时，我也会提高工作效率，减少不必要的加班；或者说如果是工作需要我会义不容辞加班，我现在已经成家，生活上已经稳定，这样更能全身心的投入工作。但同时，我也会提高工作效率，减少不必要的加班。\u0026ldquo;这么回答之后，这道题你就顺利的过关了！\n四：你对薪资的要求？ 回答提示：说实话，大家找工作，都希望找个高薪的，那我们如何和公司去谈薪酬呢？如果你对薪酬的要求太低，那显然贬低自己的能力；如果你对薪酬的要求太高，那又会显得你分量过重，公司受用不起。一些公司通常都事先对求聘的职位定下开支预算，因而他们第一次提出的价钱往往是他们所能给予的最高价钱，他们问你只不过想证实一下这笔钱是否足以引起你对该工作的兴趣。如果你自己必须说出具体数目，请不要说一个宽泛的范围，那样你将只能得到最低限度的数字。最好给出一个具体的数字，这样表明你已经对当今的人才市场作了调查，知道像自己这样学历的雇员有什么样的价值。只要你之前技术测试都很顺利，那么谈薪酬的时候就会更有底气，我们可以这么回答：我对工资没有硬性要求，我相信贵公司在处理我的问题上会友善合理。我注重的是找对工作机会，所以只要条件公平，我则不会计较太多或者说：我受过系统的软件编程的训练，不需要进行大量的培训，而且我本人也对编程特别感兴趣。因此，我希望公司能根据我的情况和市场标准的水平，给我合理的薪水。 在此，希望所有的面试者都能高薪就业！\n五：在未来的五年的时间内，你的职业规划是什么？ 回答提示：这是每一个应聘者都不希望被问到的问题，但是几乎每个人都会被问到，比较多的答案是“管理者”。但是近几年来，许多公司都已经建立了专门的技术途径。这些工作地位往往被称作“顾问”、“参议技师”或“高级软件工程师”等等。当然，说出其他一些你感兴趣的职位也是可以的，比如产品销售部经理，生产部经理等一些与你的专业有相关背景的工作。要知道，考官总是喜欢有进取心的应聘者，此时如果说“不知道”，所以你在回答这道题的时候，一定要符合自己实际情况，千万不要大而空，这样面试官会觉得你是一个好高骛远之人，就会使你丧失一个好机会。最普通的回答应该是“我准备在技术领域有所作为，比如**技术上有所突破”或“我希望能按照公司的管理思路发展”。\n六：谈谈你朋友对你的评价？ 回答提示： 想从侧面了解一下你的性格及与人相处的问题。\n　回答样本一：我的朋友都说我是一个可以信赖的人。因为，我一旦答应别人的事情，就一定会做到。如果我做不到，我就不会轻易许诺。\n　回答样本二：我觉的我是一个比较随和的人，与不同的人都可以友好相处。在我与人相处时，我总是能站在别人的角度考虑问题\n七：你还有什么问题要问吗？ 回答提示：企业面试官问的这个问题看上去可有可无，其实很关键，企业不喜欢说“没问题”的人，因为其很注重员工的个性和创新能力。企业不喜欢求职者问个人福利之类的问题，如果有人这样问：贵公司对新入公司的员工有没有什么培训项目，我可以参加吗？或者说贵公司的晋升机制是什么样的？企业将很欢迎，因为体现出你对学习的热情和对公司的忠诚度以及你的上进心。其实面试很多时候都是打心理战，看你如何去理解，如何去应对了！见招拆招，灵活应用！千万不要死在冲锋的路上，技术面试没问题，但最后挂在人事身上，那岂不是比窦娥还要冤！\n八：如果我们公司录用了你，但你却在工作后发现根本不适合这个职位，你怎么办？ 回答提示：在公司工作一段时间之后发现工作不适合自己，一般会有两种情况：1.如果你确实热爱这个职业，那你就要不断学习，虚心向领导和同事学习业务知识和处事经验，了解这个职业的精神内涵和职业要求，力争减少差距；2.如果你自己觉得这个职业对你来说可有可无，那还是趁早换个职业，去发现适合你的，你热爱的职业，那样你的发展前途也会大点，对公司和个人都有好处。对于这类问题的回答，无论你的实际情况如何，你尽量按照第一种情况去说，而且一定要表明在面试之前，对该份工作做了深入的了解。 俗语有云：管中窥豹可见一斑！面试官都是久经职场，千万不要忽略面试中的任何一个问题哦！\n九：在完成某项工作时，你认为自己的方式比领导要求的更好，你应该怎么做？ 回答提示：1.原则上我会尊重和服从领导的工作安排，同时私底下找机会以请教的口吻，婉转地表达自己的想法，看看领导是否能改变想法。2.如果领导没有采纳我的建议，我也同样会按领导的要求认真地去完成这项工作，并私下找领导分析下我的想法有哪些考虑是不周到的，这样也可以去提高自己。3.还有一种情况，假如领导要求的方式违背原则，我会坚决提出反对意见，如领导仍固执己见，我会毫不犹豫地再向上级领导反映。（注：这个你可以在面试的时候这么说，但一定要明确是面试官不是你将来的直接主管，而且实际中你也不要这么干！）\n十：如果你出现工作失误，给公司造成经济损失，你认为该怎么办？ 回答提示：这个问题主要考查了面试者在出现意外时的处理能力，是能够勇敢地解决问题，还是不知所措地选择逃避。思路：分析失误原因，承担应该承担的部分，总结经验教训。参考样板：1.我本意是为公司努力工作，如果造成经济损失，我认为首要的问题是想方设法去弥补或挽回经济损失。如果我无能力负责，希望单位帮助解决。2.责任问题：分清责任，各负其责，如果是我的责任，我甘愿受罚；如果是一个我负责的团队中别人的失误，也不能幸灾乐祸，作为一个团队，需要互相帮助共同完成工作，安慰同事并且帮助同事查找原因并总结经验。3.总结经验教训：一个人的一生不可能不犯错误，重要的是能从自己或者是别人的错误中吸取经验教训，并在今后的工作中避免发生同类的错误。检讨自己的工作方法、分析问题的深度和力度是否不够，以致出现了本可以避免的错误。\n十一：如果你做的一项工作受到上级表扬，主管却说是他做的，你该怎样？ 回答提示：开头：每一件工作的完成都离不开同事们的互相合作，更离不开领导的指路带队。如果在我的工作中，发生材料中的情况，我会从以下几个方面着手，妥善处理问题：主体：第一，理解主管领导的做法。主管领导主动向上级领导表态他在这项工作中的作用，不仅体现了他对这项工作的重视，也体现出这项工作的成果能对主管领导有促进作用。第二，肯定主管领导的付出。军有头，将有主。在每一项工作中，如果没有了带头的人，就难于团结力量，发挥最大的团队效力。如果这项工作没有主管领导劳心劳力的指导，就不可能如此顺利、完好的完成，也不可能获得这次表扬。这就好像三只千里马驮货物，如果没有主人去引导前进的方向，就会朝着不同的方向发力奔跑，最终只能空耗马力，而无法到达货物需要运送的地方。第三，更谦虚的向领导学习。“活到老，学到老\u0026rdquo;,每个人都不可能不学习就能完成好工作，在今后的工作中，要进一步向主管领导学习，多汇报，多总结。结尾：良好的人际关系有利于促进工作环境的融洽，能促进同事间的团结协作。在今后的工作中，我会尊重领导，和同事和谐相处，让自己尽快尽好的融入团队，以务实的态度做好每一项工作，不断提高自己的综合能力。\n十二：谈谈你对跳槽的看法？\n 回答提示：说实话，这个问题很尖锐！但是更现实，这是很多人不的不遇见的重大问题之一！也是公司考虑员工稳定性的条件之一! 良禽择木而已，虽然很简单的一个道理！但是在众多中小型企业的人事或者技术面前考虑的事情就比较多了！在求职者方面往往考虑的是发展和待遇！而在公司方面考虑的确实你的稳定性和未来给公司带来哪些东西？反过来想，你如果在一年之内跳槽两次，说实话，这就是大问题！最起码我这样认为，刨去使用期，你也仅仅是带了三个月多点而已，三个月说实话，可能你连企业的背景和文化都没了解清楚，甚至于企业的业务设计都还不知道!一个这样的员工在半年之后拍拍屁股走人，这样企业损失大了！但是请记住，损失的不是人才，是公司的各自资源！6个月等于养了一个废物，虽然话很难听，但是事实如此！ 所以这个问题重中之重！小心谨慎哦!下面的一些思考留给在职或者求职的兄弟姐妹们：1、确认你现在是否应该跳槽，是否有能力跳槽若是想跳槽了，不妨在纸上列出两个表，问问自己：第一，跳槽的理由是什么，给自己带来的好处是什么；第二，如果留下，又能够给自己带来什么样的收益。如果你的跳槽理由无非一个月拿更多的钱，那么，你可能需要明确的是，这个阶段对于你而言最重要的是什么？，如果只是盲目地追逐收入的高低的话，很有可能你最后会发现什么也没有得到。2、尽可能多地了解你未来公司的实际情况经常有跳槽者觉得“刚出虎穴，又入狼窝”，为此后悔跳槽者也不在少数。所谓的薪水和职位只是一个表象，你必须要弄清楚，未来的公司的发展计划、晋升机制是否有利于你的发展，而其中最最重要的是，这个公司的企业文化是否适合你的风格。  十三：谈谈工作中你难以和同事、上级相处，你该怎么办？ 回答提示：1.我会服从领导的指挥，配合同事的工作。2.我会从自身找原因，仔细分析是不是自己工作做得不好让领导不满意，同事看不惯。还要看看是不是为人处世方面做得不好，如果是这样的话我会努力改正。3.如果我找不到原因，我会找机会跟他们沟通，请他们指出我的不足，有问题就及时改正。4.作为优秀的员工，应该时刻以大局为重，即使在一段时间内，领导和同事对我不理解，我也会做好本职工作，虚心向他们学习，我相信，他们会看见我在努力，总有一天会对我微笑的。\n十四：你因工作比较突出，虽说得到领导肯定，但同事越来越孤立你，你该怎么办？ 回答提示：我给大家总结了一下可以从四点去说: 1.成绩比较突出得到领导的肯定是件好事情,以后更加努力。 2.检讨一下自己是不是对工作的热心度超过同事间交往的热心了，加强同事间的交往及共同的兴趣爱好。 3.工作中切勿伤害别人的自尊心. 4.不再领导前拨弄是非。\n十五：说说你选择这份工作的动机？ 回答提示：其实这道面试题是面试官想知道面试者对这份工作的热忱及理解 ，并筛选因一时兴起而来应试的人，如果你是在这个领域是无经验者，也可以强调“就算职种不同，也希望有机会发挥之前的经验”,所以你在面试时候一定要对该公司做好备课往往公司不希望自己的公司成为别人的职业跳板。\n十六：你能为我们公司带来什么呢？ 回答提示： 企业很想知道未来的员工能为企业做什么，求职者应再次重复自己的优势，然后说：“就我的能力，我可以做一个优秀的员工在组织中发挥能力，给组织带来高效率和更多的收益”。企业喜欢求职者就申请的职位表明自己的能力。比如申请营销之类的职位，可以说：“我可以开发大量的新客户。同时，对老客户做更全面周到的服务，开发老客户的新需求和消费。”\n十七：说说喜欢这份工作的哪一点？ 回答提示：每个人的价值观不同自然评断的标准也会不同。但是，在回答面试官这个问题时可不能太直接就把自己心理的话说出来，尤其是薪资方面的问题，不过一些无伤大雅的回答是不错的考虑，如交通方便，工作性质及内容颇能符合自己的兴趣等等都是不错的答案，不过如果这时自己能仔细思考出这份工作的与众不同之处，相信在面试上会大大加分。\n十八：你是怎样看待学历和能力的？ 回答提示：学历我想只要是大学专科的学历，就表明觉得我具备了根本的学习能力。剩下的,你是学士也好还是博士也好。对于这一点的讨论，不是看你学了多少知识，而是看你在这个领域上发挥了什么。也就是所说的能力问题。一个人工作能力的高低直接决定其职场命运，而学历的高低只是进入一个企业的敲门砖。如果贵公司把学历卡在博士上，我就无法进入贵公司，当然这不一定只是我个人的损失。如果一个专科生都能完成的工作，您又何必非要招聘一位博士生呢\n","date":"2023-01-09","permalink":"/post/%E9%9D%9E%E6%8A%80%E6%9C%AF/","tags":["InterviewQuestions","Other"],"title":"非技术"},{"content":"第六章 shell 与自动化运维 使用变量的两种方法？各自适合的场景？区别？ 单双引号的区别？ $* 和 $@ 区别？ 字符串的只读、删除、长度输出 shell中命令替换的两种方式，以及有什么区别 变量的间接引用 文件按行处理的多种方式 过去数组单个元素、所有元素 declare 创建关联数组的使用 for 循环的两种方式 select in 的使用 ","date":"2023-01-09","permalink":"/post/shell/","tags":["InterviewQuestions","Linux"],"title":"Shell"},{"content":"Mysql 基本语句 1、from 子句组装来自不同数据源的数据； 2、where 子句基于指定的条件对记录行进行筛选； 3、group by 子句将数据划分为多个分组； 4、使用聚集函数进行计算； 5、使用 having 子句筛选分组； 6、计算所有的表达式； 7、select 的字段； 8、使用 order by 对结果集进行排序。  SQL 语言不同于其他编程语言的最明显特征是处理代码的顺序。在大多数据库语言中，代码按编码顺序被处理。但在 SQL 语句中，第一个被处理的子句式 FROM，而不是第一出现的 SELECT。\nSQL 查询处理的步骤序号：\n(1) FROM \u0026lt;left_table\u0026gt; (2) \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; (3) ON \u0026lt;join_condition\u0026gt; (4) WHERE \u0026lt;where_condition\u0026gt; (5) GROUP BY \u0026lt;group_by_list\u0026gt; (6) WITH {CUBE | ROLLUP} (7) HAVING \u0026lt;having_condition\u0026gt; (8) SELECT (9) DISTINCT (9) ORDER BY \u0026lt;order_by_list\u0026gt; (10) \u0026lt;TOP_specification\u0026gt; \u0026lt;select_list\u0026gt;  以上每个步骤都会产生一个虚拟表，该虚拟表被用作下一个步骤的输入。这些虚拟表对调用者 (客户端应用程序或者外部查询) 不可用。只有最后一步生成的表才会会给调用者。如果没有在查询中指定某一个子句，将跳过相应的步骤。\n逻辑查询处理阶段简介：\n1、 FROM：对 FROM 子句中的前两个表执行笛卡尔积 (交叉联接)，生成虚拟表 VT1。 2、 ON：对 VT1 应用 ON 筛选器，只有那些使为真才被插入到 TV2。 3、 OUTER (JOIN): 如果指定了 OUTER JOIN (相对于 CROSS JOIN 或 INNER JOIN)，保留表中未找到匹配的行将作为外部行添加到 VT2，生成 TV3。如果 FROM 子句包含两个以上的表，则对上一个联接生成的结果表和下一个表重复执行步骤 1 到步骤 3，直到处理完所有的表位置。 4、 WHERE：对 TV3 应用 WHERE 筛选器，只有使为 true 的行才插入 TV4。 5、 GROUP BY：按 GROUP BY 子句中的列列表对 TV4 中的行进行分组，生成 TV5。 6、 CUTE|ROLLUP：把超组插入 VT5，生成 VT6。 7、 HAVING：对 VT6 应用 HAVING 筛选器，只有使为 true 的组插入到 VT7。 8、 SELECT：处理 SELECT 列表，产生 VT8。 9、 DISTINCT：将重复的行从 VT8 中删除，产品 VT9。 10、 ORDER BY：将 VT9 中的行按 ORDER BY 子句中的列列表顺序，生成一个游标 (VC10)。 11、 TOP：从 VC10 的开始处选择指定数量或比例的行，生成表 TV11，并返回给调用者。 where 子句中的条件书写顺序  MySQL 的常见命令如下： create database name; # 创建数据库 use databasename; # 选择数据库 drop database name # 直接删除数据库，不提醒 show tables; # 显示表 describe tablename; # 表的详细描述 select 中加上 distinct ## 去除重复字段 mysqladmin drop databasename # 删除数据库前，有提示。 select version (),current_date; # 显示当前 mysql 版本和当前日期  什么是事务，MySQL 是如何支持事务的？ 事务就是一段 sql 语句的批处理，但是这个批处理是一个原子 ，不可分割，要么都执行，要么回滚（rollback）都不执行。\n事务具体四大特性，也就是经常说的 ACID ：\n1. 原子性（所有操作要么全部成功，要么全部失败回滚） 2. 一致性（事务执行之前和执行之后都必须处于一致性状态。） 3. 隔离性（数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离） 4. 持久性（一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即使遭遇故障依然能够通过日志恢复最后一次更新在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务  MYSQL 事务处理主要有两种方法：\n1、用 BEGIN, ROLLBACK, COMMIT 来实现 BEGIN 开始一个事务 ROLLBACK 事务回滚 COMMIT 事务确认 2、直接用 SET 来改变 MySQL 的自动提交模式: SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交  说一下 Mysql 数据库存储的原理？ 储存过程是一个可编程的函数，它在数据库中创建并保存。它可以有 SQL 语句和一些特殊的控制结构组成。当希望在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有 用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟。它允许控制数据的访问方式。\n存储过程通常有以下优点：\n1、存储过程能实现较快的执行速度 2、存储过程允许标准组件是编程。 3、存储过程可以用流程控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算。 4、存储过程可被作为一种安全机制来充分利用。 5、存储过程能够减少网络流量  数据库索引种类？ 索引是一种特殊的文件 (InnoDB 数据表上的索引是表空间的一个组成部分)，更通俗的说，数据库索引好比是一本书前面的目录，能加快数据库的查询速度\nMySQL 索引的类型：\n　1. 普通索引：这是最基本的索引，它没有任何限制 2. 唯一索引：索引列的值必须唯一，但允许有空值，如果是组合索引，则列值的组合必须唯一 3. 全文索引：全文索引仅可用于 MyISAM 表，可以从 CHAR、VARCHAR 或 TEXT 列中作为 CREATE TABLE 语句的一部分被创建，或是随后使用 ALTER TABLE 或 CREATE INDEX 被添加（切记对于大容量的数据表，生成全文索引是一个非常消耗时间非常消耗硬盘空间的做法） 4. 单列索引、多列索引：多个单列索引与单个多列索引的查询效果不同，因为执行查询时，MySQL 只能使用一个索引，会从多个索引中选择一个限制最为严格的索引。 5. 组合索引（最左前缀）：简单的理解就是只从最左面的开始组合（实在单列索引的基础上进一步压榨索引效率的一种方式）\n索引在什么情况下遵循最左前缀的规则？ mysql 在使用组合索引查询的时候需要遵循最左前缀规则\nMySQL 常见的函数？ 聚合函数：\n　AVG (col) 返回指定列的平均值 COUNT (col) 返回指定列中非 NULL 值的个数 MIN (col) 返回指定列的最小值 MAX (col) 返回指定列的最大值 SUM (col) 返回指定列的所有值之和 GROUP_CONCAT (col) 返回由属于一组的列值连接组合而成的结果\n数学函数：\n　ABS (x) 返回 x 的绝对值 BIN (x) 返回 x 的二进制（OCT 返回八进制，HEX 返回十六进制）\n如何开启慢日志查询？ 1 执行 SHOW VARIABLES LIKE % slow%，获知 mysql 是否开启慢查询 slow_query_log 慢查询开启状态 OFF 未开启 ON 为开启 slow_query_log_file 慢查询日志存放的位置（这个目录需要 MySQL 的运行帐号的可写权限，一般设置为 MySQL 的数据存放目录）\n2 修改配置文件（ 放在 mysqld 下），重启 long_query_time 查询超过多少秒才记录\n3 测试是否成功\n4 慢查询日志文件的信息格式\n数据库怎么优化查询效率？ 1、储存引擎选择：如果数据表需要事务处理，应该考虑使用 InnoDB，因为它完全符合 ACID 特性。如果不需要事务处理，使用默认存储引擎 MyISAM 是比较明智的 2、分表分库，主从。 3、对查询进行优化，要尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引 4、应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描 5、应尽量避免在 where 子句中使用！= 或 \u0026lt;\u0026gt; 操作符，否则将引擎放弃使用索引而进行全表扫描 6、应尽量避免在 where 子句中使用 or 来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描 7、Update 语句，如果只更改 1、2 个字段，不要 Update 全部字段，否则频繁调用会引起明显的性能消耗，同时带来大量日志 8、对于多张大数据量（这里几百条就算大了）的表 JOIN，要先分页再 JOIN，否则逻辑读会很高，性能很差。  Mysql 集群的优缺点？ 优点：\n99.999% 的高可用性 快速的自动失效切换 灵活的分布式体系结构，没有单点故障 高吞吐量和低延迟 可扩展性强，支持在线扩容  缺点：\n存在很多限制，比如：不支持外键 部署、管理、配置很复杂 占用磁盘空间大、内存大 备份和恢复不方便 重启的时候，数据节点将数据 load 到内存需要很长的时间  你用的 Mysql 是哪个引擎，各引擎之间有什么区别？ 主要 MyISAM 与 InnoDB 两个引擎，其主要区别如下：\nInnoDB 支持事务，MyISAM 不支持，这一点是非常之重要。事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而 MyISAM 就不可以了； MyISAM 适合查询以及插入为主的应用，InnoDB 适合频繁修改以及涉及到安全性较高的应用； InnoDB 支持外键，MyISAM 不支持； MyISAM 是默认引擎，InnoDB 需要指定； InnoDB 不支持 FULLTEXT 类型的索引； InnoDB 中不保存表的行数，如 select count () from table 时，InnoDB；需要扫描一遍整个表来计算有多少行，但是 MyISAM 只要简单的读出保存好的行数即可。注意的是，当 count () 语句包含 where 条件时 MyISAM 也需要扫描整个表； 对于自增长的字段，InnoDB 中必须包含只有该字段的索引，但是在 MyISAM 表中可以和其他字段一起建立联合索引；清空整个表时，InnoDB 是一行一行的删除，效率非常慢。MyISAM 则会重建表； InnoDB 支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like % lee%’  数据库的优化？ 1. 优化索引、SQL 语句、分析慢查询； 2. 设计表的时候严格根据数据库的设计范式来设计数据库； 3. 使用缓存，把经常访问到的数据而且不需要经常变化的数据放在缓存中，能节约磁盘 IO 4. 优化硬件；采用 SSD，使用磁盘队列技术 (RAID0,RAID1,RDID5) 等 5. 采用 MySQL 内部自带的表分区技术，把数据分层不同的文件，能够提高磁盘的读取效率； 6. 垂直分表；把一些不经常读的数据放在一张表里，节约磁盘 I/O； 7. 主从分离读写；采用主从复制把数据库的读操作和写入操作分离开来； 8. 分库分表分机器（数据量特别大），主要的的原理就是数据路由； 9. 选择合适的表引擎，参数上的优化 10. 进行架构级别的缓存，静态化和分布式； 11. 不采用全文索引； 12. 采用更快的存储方式，例如 NoSQL 存储经常访问的数据。  Mysql 数据库如何分区、分表？ 分表可以通过三种方式：Mysql 集群、自定义规则和 merge 存储引擎。\n分区有四类：\nRANGE 分区：基于属于一个给定连续区间的列值，把多行分配给分区。 LIST 分区：类似于按 RANGE 分区，区别在于 LIST 分区是基于列值匹配一个离散值集合中的某个值来进行选择。 HASH 分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含 MySQL 中有效的、产生非负整数值的任何表达式。 KEY 分区：类似于按 HASH 分区，区别在于 KEY 分区只支持计算一列或多列，且 MySQL 服务器提供其自身的哈希函数。必须有一列或多列包含整数值。  Sql 注入是如何产生的，如何防止？ 程序开发过程中不注意规范书写 sql 语句和对特殊字符进行过滤，导致客户端可以通过全局变量 POST 和 GET 提交一些 sql 语句正常执行。产生 Sql 注入。下面是防止办法：\na. 过滤掉一些常见的数据库操作关键字，或者通过系统函数来进行过滤。 b. 在 PHP 配置文件中将 Register\\_globals=off; 设置为关闭状态 c. SQL 语句书写的时候尽量不要省略小引号 (tab 键上面那个) 和单引号 d. 提高数据库命名技巧，对于一些重要的字段根据程序的特点命名，取不易被猜到的 e. 对于常用的方法加以封装，避免直接暴漏 SQL 语句 f. 开启 PHP 安全模式：Safe\\_mode=on; g. 打开 magic\\_quotes\\_gpc 来防止 SQL 注入 h. 控制错误信息：关闭错误提示信息，将错误信息写到系统日志。 i. 使用 mysqli 或 pdo 预处理。  NoSQL 和关系数据库的区别？  a. SQL 数据存在特定结构的表中；而 NoSQL 则更加灵活和可扩展，存储方式可以省是 JSON 文档、哈希表或者其他方式。 b. 在 SQL 中，必须定义好表和字段结构后才能添加数据，例如定义表的主键 (primary key)，索引 (index), 触发器 (trigger), 存储过程 (stored procedure) 等。表结构可以在被定义之后更新，但是如果有比较大的结构变更的话就会变得比较复杂。在 NoSQL 中，数据可以在任何时候任何地方添加，不需要先定义表。 c. SQL 中如果需要增加外部关联数据的话，规范化做法是在原表中增加一个外键，关联外部数据表。而在 NoSQL 中除了这种规范化的外部数据表做法以外，我们还能用如下的非规范化方式把外部数据直接放到原数据集中，以提高查询效率。缺点也比较明显，更新审核人数据的时候将会比较麻烦。 d. SQL 中可以使用 JOIN 表链接方式将多个关系数据表中的数据用一条简单的查询语句查询出来。NoSQL 暂未提供类似 JOIN 的查询方式对多个数据集中的数据做查询。所以大部分 NoSQL 使用非规范化的数据存储方式存储数据。 e. SQL 中不允许删除已经被使用的外部数据，而 NoSQL 中则没有这种强耦合的概念，可以随时删除任何数据。 f. SQL 中如果多张表数据需要同批次被更新，即如果其中一张表更新失败的话其他表也不能更新成功。这种场景可以通过事务来控制，可以在所有命令完成后再统一提交事务。而 NoSQL 中没有事务这个概念，每一个数据集的操作都是原子级的。 g. 在相同水平的系统设计的前提下，因为 NoSQL 中省略了 JOIN 查询的消耗，故理论上性能上是优于 SQL 的。   简述触发器、函数、视图、存储过程？ 触发器：触发器是一个特殊的存储过程，它是 MySQL 在 insert、update、delete 的时候自动执行的代码块。\ncreate trigger trigger_name after/before insert /update/delete on 表名 for each row begin sql 语句：（触发的语句一句或多句） end  函数：MySQL 中提供了许多内置函数，还可以自定义函数（实现程序员需要 sql 逻辑处理）\n自定义函数创建语法： 创建：CREATE FUNCTION 函数名称 (参数列表) RETURNS 返回值类型 函数体 修改：ALTER FUNCTION 函数名称 [characteristic ...] 删除：DROP FUNCTION [IF EXISTS] 函数名称 调用：SELECT 函数名称 (参数列表)  视图：视图是由查询结果形成的一张虚拟表，是表通过某种运算得到的一个投影\ncreate view view_name as select 语句  存储过程：把一段代码封装起来，当要执行这一段代码的时候，可以通过调用该存储过程来实现（经过第一次编译后再次调用不需要再次编译，比一个个执行 sql 语句效率高）\ncreate procedure 存储过程名 (参数，参数，…) begin // 代码 end  列举 创建索引但是无法命中索引的 8 种情况。 1、如果条件中有 or，即使其中有条件带索引也不会使用 (这也是为什么尽量少用 or 的原因） 2、对于多列索引，不是使用的第一部分 (第一个)，则不会使用索引 3、like 查询是以 % 开头 4、如果列类型是字符串，那一定要在条件中将数据使用引号引用起来，否则不使用索引 5、如果 mysql 估计使用全表扫描要比使用索引快，则不使用索引 6、对小表查询 7、提示不使用索引 8、统计数据不真实 9、单独引用复合索引里非第一位置的索引列.  优化数据库？提高数据库的性能 1. 对语句的优化 ①用程序中，保证在实现功能的基础上，尽量减少对数据库的访问次数；通过搜索参数，尽量减少对表的访问行数，最小化结果集，从而减轻网络负担； ②能够分开的操作尽量分开处理，提高每次的响应速度；在数据窗口使用 SQL 时，尽量把使用的索引放在选择的首列；算法的结构尽量简单； ③在查询时，不要过多地使用通配符如 SELECT \\* FROM T1 语句，要用到几列就选择几列如：SELECT COL1,COL2 FROM T1； ④在可能的情况下尽量限制尽量结果集行数如：SELECT TOP 300 COL1,COL2,COL3 FROM T1, 因为某些情况下用户是不需要那么多的数据的。 ⑤不要在应用中使用数据库游标，游标是非常有用的工具，但比使用常规的、面向集的 SQL 语句需要更大的开销；按照特定顺序提取数据的查找。 2. 避免使用不兼容的数据类型 例如 float 和 int、char 和 varchar、binary 和 varbinary 是不兼容的。 数据类型的不兼容可能使优化器无法执行一些本来可以进行的优化操作。 例如: SELECT name FROM employee WHERE salary ＞ 60000 在这条语句中，如 salary 字段是 money 型的，则优化器很难对其进行优化，因为 60000 是个整型数。我们应当在编程时将整型转化成为钱币型，而不要等到运行时转化。 若在查询时强制转换，查询速度会明显减慢。 3. 避免在 WHERE 子句中对字段进行函数或表达式操作。若进行函数或表达式操作，将导致引擎放弃使用索引而进行全表扫描。 4. 避免使用！= 或＜＞、IS NULL 或 IS NOT NULL、IN ，NOT IN 等这样的操作符 5. 尽量使用数字型字段 6. 合理使用 EXISTS,NOT EXISTS 子句。 7. 尽量避免在索引过的字符数据中，使用非打头字母搜索。 8. 分利用连接条件 9. 消除对大型表行数据的顺序存取 10. 避免困难的正规表达式 11. 使用视图加速查询 12. 能够用 BETWEEN 的就不要用 IN 13. DISTINCT 的就不用 GROUP BY 14. 部分利用索引 15. 能用 UNION ALL 就不要用 UNION 16. 不要写一些不做任何事的查询 17. 尽量不要用 SELECT INTO 语句 18. 必要时强制查询优化器使用某个索引 19. 虽然 UPDATE、DELETE 语句的写法基本固定，但是还是对 UPDATE 语句给点建议： a) 尽量不要修改主键字段。 b) 当修改 VARCHAR 型字段时，尽量使用相同长度内容的值代替。 c) 尽量最小化对于含有 UPDATE 触发器的表的 UPDATE 操作。 d) 避免 UPDATE 将要复制到其他数据库的列。 e) 避免 UPDATE 建有很多索引的列。 f) 避免 UPDATE 在 WHERE 子句条件中的列。  数据库负载均衡 负载均衡集群是由一组相互独立的计算机系统构成，通过常规网络或专用网络进行连接，由路由器衔接在一起，各节点相互协作、共同负载、均衡压力，对客户端来说，整个群集可以视为一台具有超高性能的独立服务器。\n1、 实现原理：\n实现数据库的负载均衡技术，首先要有一个可以控制连接数据库的控制端。在这里，它截断了数据库和程序的直接连接，由所有的程序来访问这个中间层，然后再由中间层来访问数据库。这样，我们就可以具体控制访问某个数据库了，然后还可以根据数据库的当前负载采取有效的均衡策略，来调整每次连接到哪个数据库。\n2、 实现多据库数据同步：\n对于负载均衡，最重要的就是所有服务器的数据都是实时同步的。这是一个集群所必需的，因为，如果数不据实时、不同步，那么用户从一台服务器读出的数据，就有别于从另一台服务器读出的数据，这是不能允许的。所以必须实现数据库的数据同步。这样，在查询的时候就可以有多个资源，实现均衡。比较常用的方法是 Moebius for SQL Server 集群，Moebius for SQL Server 集群\n采用将核心程序驻留在每个机器的数据库中的办法，这个核心程序称为 Moebius for SQL Server 中间件，主要作用是监测数据库内数据的变化并将变化的数据同步到其他数据库中。数据同步完成后客户端才会得到响应，同步过程是并发完成的，所以同步到多个数据库和同步到一个数据库的时间基本相等；另外同步的过程是在事务的环境下完成的，保证了多份数据在任何时刻数据的一致性。正因为 Moebius 中间件宿主在数据库中的创新，让中间件不但能知道数据的变化，而且知道引起数据变化的 SQL 语句，根据 SQL 语句的类型智能的采取不同的数据同步的策略以保证数据同步成本的最小化。\n数据条数很少，数据内容也不大，则直接同步数据。数据条数很少，但是里面包含大数据类型，比如文本，二进制数据等，则先对数据进行压缩然后再同步，从而减少网络带宽的占用和传输所用的时间。 数据条数很多，此时中间件会拿到造成数据变化的 SQL 语句， 然后对 SQL 语句进行解析，分析其执行计划和执行成本，并选择是同步数据还是同步 SQL 语句到其他的数据库中。此种情况应用在对表结构进行调整或者批量更改数据的时候非常有用。\n3、 优缺点\n优点： 1. 扩展性强：当系统要更高数据库处理速度时，只要简单地增加数据库服务器就 可以得到扩展。 2. 可维护性：当某节点发生故障时，系统会自动检测故障并转移故障节点的应用，保证数据库的持续工作。 3. 安全性：因为数据会同步的多台服务器上，可以实现数据集的冗余，通过多份数据来保证安全性。另外它成功地将数据库放到了内网之中，更好地保护了数据库的安全性。 4. 易用性：对应用来说完全透明，集群暴露出来的就是一个 IP 缺点： a) 不能够按照 Web 服务器的处理能力分配负载。 b) 负载均衡器 (控制端) 故障，会导致整个数据库系统瘫痪。  数据库三大范式？ 什么是范式：简言之就是，数据库设计对数据的存储性能，还有开发人员对数据的操作都有莫大的关系。所以建立科学的，规范的的数据库是需要满足一些 规范的来优化数据数据存储方式。在关系型数据库中这些规范就可以称为范式。\n什么是三大范式：\n第一范式：当关系模式 R 的所有属性都不能在分解为更基本的数据单位时，称 R 是满足第一范式的，简记为 1NF。满足第一范式是关系模式规范化的最低要求，否则，将有很多基本操作在这样的关系模式中实现不了。 第二范式：如果关系模式 R 满足第一范式，并且 R 得所有非主属性都完全依赖于 R 的每一个候选关键属性，称 R 满足第二范式，简记为 2NF。 第三范式：设 R 是一个满足第一范式条件的关系模式，X 是 R 的任意属性集，如果 X 非传递依赖于 R 的任意一个候选关键字，称 R 满足第三范式，简记为 3NF.  注：关系实质上是一张二维表，其中每一行是一个元组，每一列是一个属性\n简述数据库设计中一对多和多对多的应用场景？ 一对多：学生与班级 — 一个学生只能属于一个班级，一个班级可以有多个学生 多对多：学生与课程 — 一个学生可以选择多个课程，一个课程也可以被多个学生选择  如何基于数据库实现商城商品计数器？ 创建一个商城表 — 包含（id，商品名，每一个商品对应数量）\ncreate table product (id primary key auto_increment, pname varchar (64), pcount int);  char 和 varchar 的区别？ char： 定长，char 的存取数度相对快 varchar： 不定长，存取速度相对慢\n到底如何取舍可以根据一下几个方面考虑：\n 1、对于 MyISAM 表，尽量使用 Char，对于那些经常需要修改而容易形成碎片的 myisam 和 isam 数据表就更是如此，它的缺点就是占用磁盘空间； 2、对于 InnoDB 表，因为它的数据行内部存储格式对固定长度的数据行和可变长度的数据行不加区分（所有数据行共用一个表头部分，这个标头部分存放着指向各有关数据列的指针），所以使用 char 类型不见得会比使用 varchar 类型好。事实上，因为 char 类型通常要比 varchar 类型占用更多的空间，所以从减少空间占用量和减少磁盘 i/o 的角度，使用 varchar 类型反而更有利； 3、存储很短的信息，比如门牌号码 101，201…… 这样很短的信息应该用 char，因为 varchar 还要占个 byte 用于存储信息长度，本来打算节约存储的现在得不偿失。 4、固定长度的。比如使用 uuid 作为主键，那用 char 应该更合适。因为他固定长度，varchar 动态根据长度的特性就消失了，而且还要占个长度信息。 5、十分频繁改变的 column。因为 varchar 每次存储都要有额外的计算，得到长度等工作，如果一个非常频繁改变的，那就要有很多的精力用于计算，而这些对于 char 来说是不需要的。  在对 name 做了唯一索引前提下，简述以下区别： select * from tb where name = ‘Oldboy’ ------------- 查找到 tb 表中所有 name = ‘Oldboy’的数据 select * from tb where name = ‘Oldboy’ limit 1------ 查找到 tb 表中所有 name = ‘Oldboy’的数据只取其中的第一条  ","date":"2023-01-09","permalink":"/post/mysql/","tags":["InterviewQuestions","MySQL"],"title":"mysql"},{"content":"数据库的一些基本操作命令（列举一些常用命令即可）？ MongoDB 的常见命令如下：\ndb.help (); Help 查看命令提示 use yourDB; 切换 / 创建数据库 show dbs; 查询所有数据库 db.dropDatabase (); 删除当前使用数据库 db.getName (); 查看当前使用的数据库 db.version (); 当前 db 版本 db.addUser (\u0026quot;name\u0026quot;); 添加用户 db.addUser (\u0026quot;userName\u0026quot;, \u0026quot;pwd123\u0026quot;, true); show users; 显示当前所有用户 db.removeUser (\u0026quot;userName\u0026quot;); 删除用户 db.yourColl.count (); 查询当前集合的数据条数  MongoDB 是什么？ 应用场景及优缺点？ MongoDB 是一个面向文档的数据库系统。使用 C++ 编写，不支持 SQL，但有自己功能强大的查询语法。\nMongoDB 使用 BSON 作为数据存储和传输的格式。BSON 是一种类似 JSON 的二进制序列化文档，支持嵌套对象和数组。\nMongoDB 很像 MySQL，document 对应 MySQL 的 row，collection 对应 MySQL 的 table\n应用场景：\na) 网站数据：mongo 非常适合实时的插入，更新与查询，并具备网站实时数据存储所需的复制及高度伸缩性。 b) 缓存：由于性能很高，mongo 也适合作为信息基础设施的缓存层。在系统重启之后，由 mongo 搭建的持久化缓存可以避免下层的数据源过载。 c) 大尺寸、低价值的数据：使用传统的关系数据库存储一些数据时可能会比较贵，在此之前，很多程序员往往会选择传统的文件进行存储。 d) 高伸缩性的场景：mongo 非常适合由数十或者数百台服务器组成的数据库。 e) 用于对象及 JSON 数据的存储：mongo 的 BSON 数据格式非常适合文档格式化的存储及查询。 f) 重要数据：mysql，一般数据：mongodb，临时数据：memcache g) 对于关系数据表而言，mongodb 是提供了一个更快速的视图 view；而对于 PHP 程序而言，mongodb 可以作为一个持久化的数组来使用，并且这个持久化的数组还可以支持排序、条件、限制等功能。 h) 将 mongodb 代替 mysql 的部分功能，主要一个思考点就是：把 mongodb 当作 mysql的一个 view（视图），view 是将表数据整合成业务数据的关键。比如说对原始数据进行报表，那么就要先把原始数据统计后生成 view，在对 view 进行查询和报表。  不适合的场景：\na) 高度事物性的系统：例如银行或会计系统。传统的关系型数据库目前还是更适用于需要大量原子性复杂事务的应用程序。 b) 传统的商业智能应用：针对特定问题的 BI 数据库会对产生高度优化的查询方式。对于此类应用，数据仓库可能是更合适的选择。 c) 需要 SQL 的问题 d) 重要数据，关系数据  优点: 1）弱一致性（最终一致），更能保证用户的访问速度 2）文档结构的存储方式，能够更便捷的获取数 3） 内置 GridFS，高效存储二进制大对象 (比如照片和视频) 4） 支持复制集、主备、互为主备、自动分片等特性 5） 动态查询 6） 全索引支持，扩展到内部对象和内嵌数组\n缺点: 1）不支持事务 2）MongoDB 占用空间过大，维护工具不够成熟\nMySQL 与 MongoDB 本质之间最基本的差别是什么 差别在多方面，例如：数据的表示、查询、关系、事务、模式的设计和定义、速度和性能。\nMongoDB 是由 C++ 语言编写的，是一个基于分布式文件存储的开源数据库系统。在高负载的情况下，添加更多的节点，可以保证服务器性能。\nMongoDB 旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。\nMongoDB 将数据存储为一个文档，数据结构由键值 (key=\u0026gt;value) 对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。\nMongoDB 是一个面向文档的数据库，目前由 10gen 开发并维护，它的功能丰富齐全，所以完全可以替代 MySQL。\n与 MySQL 等关系型数据库相比，MongoDB 的优点如下：\n①弱一致性，更能保证用户的访问速度。 ②文档结构的存储方式，能够更便捷的获取数据。 ③内置 GridFS，支持大容量的存储。 ④内置 Sharding。 ⑤第三方支持丰富。(这是与其他的 NoSQL 相比，MongoDB 也具有的优势) ⑥性能优越：MongoDB 本身它还算比较年轻的一个产品，所以它的问题，就是成熟度肯定没有传统 MySQL那么成熟稳定。所以在使用的时候，第一，尽量使用稳定版，不要在线上使用开发版，这是一个大原则；另外一点，备份很重要，MongoDB 如果出现一些异常情况，备份一定是要能跟上。除了通过传统的复制的方式来做备份，离线备份也还是要有，不管你是用什么方式，都要有一个完整的离线备份。往往最后出现了特殊情况，它能帮助到你；另外，MongoDB 性能的一个关键点就是索引，索引是不是能有比较好的使用效率，索引是不是能够放在内存中，这样能够提升随机读写的性能。如果你的索引不能完全放在内存中，一旦出现随机读写比较高的时候，它就会频繁地进行磁盘交换，这个时候，MongoDB 的性能就会急剧下降，会出现波动。另外，MongoDB 还有一个最大的缺点，就是它占用的空间很大，因为它属于典型空间换时间原则的类型。那么它的磁盘空间比普通数据库会浪费一些，而且到目前为止它还没有实现在线压缩功能， 在MongoDB 中频繁的进行数据增删改时，如果记录变了，例如数据大小发生了变化，这时候容易产生一些数据碎片，出现碎片引发的结果，一个是索引会出现性能问题,另外一个就是在一定的时间后，所占空间会莫明其妙地增大，所以要定期把数据库做修复，定期重新做索引，这样会提升 MongoDB 的稳定性和效率。在最新的版本里，它已经在实现在线压缩，估计应该在 2.0 版左右，应该能够实现在线压缩，可以在后台执行现在 repair DataBase 的一些操作。如果那样，就解决了目前困扰我们的大问题。  使用 MongoDB 的优点 面向文件 高性能 高可用 易扩展 可分片 对数据存储友好  MongoDB 成为优秀的 NoSQL 数据库的原因是什么？ 1） 面向文件的 2） 高性能 3） 高可用性 4） 易扩展性 5） 丰富的查询语言  分析器在 MongoDB 中的作用是什么？ MongoDB 中包括了一个可以显示数据库中每个操作性能特点的数据库分析器。通过这个分析器你可以找到比预期慢的查询 (或写操作); 利用这一信息，比如，可以确定是否需要添加索引。\n怎么查看 MongoDB 正在使用的链接？ db._adminCommand(connPoolStats);\n","date":"2023-01-09","permalink":"/post/mongo/","tags":["InterviewQuestions","MongoDB"],"title":"mongo"},{"content":"redis 是什么？ redis 是 nosql(也是个巨大的 map) 单线程，但是可处理 1 秒 10w 的并发（数据都在内存中）\nRedis 的全称是：Remote Dictionary.Server，本质上是一个 Key-Value 类型的内存数据库，很像memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据 flush 到硬盘上进行保存。\n因为是纯内存操作，Redis 的性能非常出色，每秒可以处理超过 10 万次读写操作，是已知性能最快的 Key-Value DB。\nRedis 的出色之处不仅仅是性能，Redis 最大的魅力是支持保存多种数据结构，此外单个 value 的最大限制是 1GB，不像 memcached 只能保存 1MB 的数据，因此 Redis 可以用来实现很多有用的功能。\n比方说用他的 List 来做 FIFO 双向链表，实现一个轻量级的高性 能消息队列服务，用他的 Set 可以做高性能的 tag 系统等等。\n另外 Redis 也可以对存入的 Key-Value 设置 expire 时间，因此也可以被当作一 个功能加强版的 memcached 来用。 Redis 的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此 Redis 适合的场景主要局限在较小数据量的高性能操作和运算上。\nRedis 主要消耗什么物理资源？ 内存。\nRedis 有多少个库？ Redis 一个实例下有 16 个。\nRedis 默认端口，默认过期时间，Value 最多可以容纳的数据长度？ 1. 默认端口：`6379` 2. 默认过期时间：可以说永不过期，一般情况下，当配置中开启了超出最大内存限制就写磁盘的话， 那么没有设置过期时间的 key 可能会被写到磁盘上。假如没设置，那么 REDIS 将使用 LRU 机制，将内存中的老数据删除，并写入新数据。 3.Value 最多可以容纳的数据长度是：512M。  Redis 中 list 底层实现有哪几种？有什么区别？ 列表对象的编码可以是 `ziplist` 或者 `linkedlist` ziplist 是一种 `压缩链表`，它的好处是更能 `节省内存空间`，因为它所存储的内容都是在连续的内存区域当中的。   当列表对象元素不大，每个元素也不大的时候，就采用 ziplist 存储。但当数据量过大时就 ziplist 就不是那么好用了。因为为了保证他存储内容在内存中的连续性，插入的复杂度是 O (N)，即每次插入都会重新进行 realloc。\n redis 数据类型及对应使用场所。  String  String 是 Redis 最为常用的一种数据类型，String 的数据结构为 key/value 类型，可以用来做微博涨粉，点赞关注数变化。 存储 json 类型对象,2 计数器,3 优酷视频点赞等 常用命令: set,get,decr,incr,mget 等。   Hash  Hash 类型可以看成是一个 key/value 都是 String 的 Map 容器。 通常用来存储对象数据类型 常用命令：hget,hset,hgetall 等。   List  List 用于存储一个有序的字符串列表，常用的操作是向队列两 端添加元素或者获得列表的某一片段。可用来做微信朋友圈按时间顺序加载 可以使用 redis 的 list 模拟队列,堆,栈 常用命令：lpush,rpush,lpop,rpop,lrange 等   Set  Set 可以理解为一组无序的字符集合，Set 中相同的元素是不会重复出现的，相同的元素只保留一个。可用来做共同好友，共同关注等 常用命令：sadd,spop,smembers,sunion 等。   Sorted Set（有序集合）  有序集合是在集合的基础上为每一个元素关联一个分数，Redis 通过分数为集合中的成员进行排序。可用来做各类排行榜应用 常用命令：zadd,zrange,zrem,zcard 等。    应用场景: 1. 取最新 N 个数据的操作 2. 排行榜应用，取 TOP N 操作 3. 需要精准设定过期时间的应用 4. 计数器应用 5. uniq 操作，获取某段时间所有数据排重值 6. Pub/Sub 构建实时消息系统 7. 构建队列系统 8. 缓存  Redis 有哪些适合的场景？ （1）会话缓存（Session Cache） 最常用的一种使用 Redis 的情景是会话缓存（sessioncache），用 Redis 缓存会话比其他存储（如 Memcached）的优势在于：Redis 提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用 Redis 来缓存会话的文档。甚至广为人知的商业平台 Magento 也提供 Redis 的插件。 （2）全页缓存（FPC） 除基本的会话 token 之外，Redis 还提供很简便的 FPC 平台。回到一致性问题，即使重启了 Redis 实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似 PHP 本地 FPC。 再次以 Magento 为例，Magento 提供一个插件来使用 Redis 作为全页缓存后端。此外，对 WordPress 的用户来说，Pantheon 有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 （3）队列 Reids 在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得 Redis 能作为一个很好的消息队列平台来使用。Redis 作为队列使用的操作，就类似于本地程序语言（如 Python）对 list 的 push/pop 操作。 如果你快速的在 Google 中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用 Redis 创建非常好的后端工具，以满足各种队列需求。例如，Celery 有一个后台就是使用 Redis 作为 broker，你可以从这里去查看。 （4）排行榜/计数器 Redis 在内存中对数字进行递增或递减的操作实现的非常好。 集合（Set）和有序集合（SortedSet）也使得我们在执行这些操作的时候变的非常简单，Redis 只是正好提供了这两种数据结构。 所以，我们要从排序集合中获取到排名最靠前的 10 个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可： 当然，这是假定你是根据你用户的分数做递增的排序。 如果你想返回用户及用户的分数，你需要这样执行：`ZRANGE user_scores 0 10 WITHSCORES` Agora Games 就是一个很好的例子，用 Ruby 实现的，它的排行榜就是使用 Redis 来存储数据的，你可以在这里看到。 （5）发布/订阅 最后（但肯定不是最不重要的）是 Redis 的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用 Redis 的发布/订阅功能来建立聊天系统！  Redis 有哪几种数据淘汰策略？ Redis 的内存淘汰策略是指在 Redis 的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。\nnoeviction： 当内存不足以容纳新写入数据时，新写入操作会报错。(禁止驱逐数据) allkeys-lru： 当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key。 allkeys-random： 当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 volatile-lru： 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl： 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除()优先回收存活时间（TTL）较短的键)。  说说 Redis 哈希槽的概念？ Redis 集群没有使用一致性 hash,而是引入了哈希槽的概念，Redis 集群有 16384 个哈希槽，每个 key 通过 CRC16 校验后对 16384 取模来决定放置哪个槽，集群的每个节点负责一部分 hash 槽。\n怎么测试 Redis 的连通性？ ping\n查看 Redis 使用情况及状态信息用什么命令？ info\nRedis key 的过期时间和永久有效分别怎么设置？ EXPIRE 和 PERSIST 命令\nRedis 如何做大量数据插入？ Redis2.6 开始 Redis-cli 支持一种新的被称之为管道(pipe mode)的新模式用于执行大量数据插入工作。\nRedis 中的管道有什么用？ 一次请求/响应服务器能实现处理新的请求即使旧的请求还未被响应，这样就可以将多个命令发送到服务器，而不用等待回复，最后在一个步骤中读取该答复。\n这就是管道（pipelining），是一种几十年来广泛使用的技术。例如许多 POP3 协议已经实现支持这个功能，大大加快了从服务器下载新邮件的过程。\nRedis 与其他 key-value 存储有什么不同？ Redis 有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis 的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。\nRedis 运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，应为数据量不能大于硬件内存。在内存数据库方面的另一个优点是，相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样 Redis 可以做很多内部复杂性很强的事情。 同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。\n4.为什么 redis 是单线程的都那么快？ 1.数据存于内存 2.用了多路复用 I/O 3.单线程  分布式 Redis 是前期做还是后期规模上来了再做好？为什么？ 既然 Redis 是如此的轻量（单实例只使用 1M 内存）,为防止以后的扩容，最好的办法就是一开始就启动较多实例。即便你只有一台服务器，你也可以一开始就让Redis 以分布式的方式运行，使用分区，在同一台服务器上启动多个实例。\n一开始就多设置几个 Redis 实例，例如 32 或者 64 个实例，对大多数用户来说这操作起来可能比较麻烦，但是从长久来看做这点牺牲是值得的。\n这样的话，当你的数据不断增长，需要更多的 Redis 服务器时，你需要做的就是仅仅将Redis实例从一台服务迁移到另外一台服务器而已（而不用考虑重新分区的问题）。一旦你添加了另一台服务器，你需要将你一半的 Redis 实例从第一台机器迁移到第二台机器。\nTwemproxy 是什么？ Twemproxy 是 Twitter 维护的（缓存）代理系统，代理 Memcached 的ASCII 协议和 Redis 协议。它是单线程程序，使用 c 语言编写，运行起来非常快。它是采用 Apache 2.0 license 的开源软件。\nTwemproxy 支持自动分区，如果其代理的其中一个 Redis 节点不可用时，会自动将该节点排除（这将改变原来的 keys-instances 的映射关系，所以你应该仅在把 Redis 当缓存时使用 Twemproxy)。\nTwemproxy 本身不存在单点问题，因为你可以启动多个 Twemproxy 实例，然后让你的客户端去连接任意一个 Twemproxy 实例。\nTwemproxy 是 Redis 客户端和服务器端的一个中间层，由它来处理分区功能应该不算复杂，并且应该算比较可靠的。\n支持一致性哈希的客户端有哪些？ Redis-rb、PRedis 等。\nRedis 的内存占用情况怎么样？ 给你举个例子： 100 万个键值对（键是 0 到 999999 值是字符串“hello world”）在我的32 位的 Mac 笔记本上 用了 100MB。同样的数据放到一个 key 里只需要16MB，这是因为键值有一个很大的开销。 在 Memcached 上执行也是类似的结果，但是相对Redis的开销要小一点点，因为 Redis 会记录类型信息引用计数等等。\n当然，大键值对时两者的比例要好很多。\n64 位的系统比 32 位的需要更多的内存开销，尤其是键值对都较小时，这是因为64 位的系统里指针占用了 8 个字节。 但是，当然，64 位系统支持更大的内存，所以为了运行大型的 Redis 服务器或多或少的需要使用 64 位的系统。\nRedis 的内存用完了会发生什么？ 如果达到设置的上限，Redis 的写命令会返回错误信息（但是读命令还可以正常返回）\n或者你可以将 Redis 当缓存来使用配置淘汰机制，当 Redis 达到内存上限时会冲刷掉旧的内容。\nRedis 是单线程的，如何提高多核CPU的利用率？ 可以在同一个服务器部署多个 Redis 的实例，并把他们当作不同的服务器来使用，在某些时候，无论如何一个服务器是不够的，所以，如果你想使用多个 CPU，你可以考虑一下分片（shard）。\n一个 Redis 实例最多能存放多少的keys？List、Set、Sorted Set 他们最多能存放多少元素？ 理论上 Redis 可以处理多达 232 的 keys，并且在实际中进行了测试，每个实例至少存放了2亿 5 千万的 keys。我们正在测试一些较大的值。\n任何 list、set、和 sorted set 都可以放 232 个元素。\n换句话说，Redis 的存储极限是系统中的可用内存值。\nRedis 官方为什么不提供 Windows 版本？ 因为目前 Linux 版本已经相当稳定，而且用户量很大，无需开发 windows 版本，反而会带来兼容性等问题。\n一个字符串类型的值能存储最大容量是多少？ 512M\n为什么 Redis 需要把所有数据放到内存中？ Redis 为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。\n所以 redis 具有快速和数据持久化的特征，如果不将数据放在内存中，磁盘 I/O 速度为严重影响 redis的性能。\n在内存越来越便宜的今天，redis 将会越来越受欢迎， 如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。\nRedis 如何实现延时队列 使用 sortedset，使用时间戳做 score, 消息内容作为 key,调用 zadd 来生产消息，消费者使用 zrangbyscore 获取 n 秒之前的数据做轮询处理。\n使用 Redis 有哪些好处？ (1) 速度快，因为数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是 O(1) (2) 支持丰富数据类型，支持 string，list，set，sorted set，hash (3) 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 (4) 丰富的特性：可用于缓存，消息，按 key 设置过期时间，过期后将会自动删除  Redis 当中有哪些数据结构 字符串 String、 字典 Hash、 列表 List、 集合 Set、 有序集合 SortedSet。  如果是高级用户，那么还会有，如果你是 Redis 中高级用户，还需要加上下面几种数据结构\nHyperLogLog、 Geo、 Pub/Sub。  假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以某个固定的已知的前缀开头的，如果将它们全部找出来？ 使用 keys 指令可以扫出指定模式的 key 列表。\n分区 为什么要做 Redis 分区？ 分区可以让 Redis 管理更大的内存，Redis 将可以使用所有机器的内存。如果没有分区，你最多只能使用一台机器的内存。分区使 Redis 的计算能力通过简单地增加计算机得到成倍提升,Redis 的网络带宽也会随着计算机和网卡的增加而成倍增长。\n你知道有哪些 Redis 分区实现方案？ 客户端分区: 就是在客户端就已经决定数据会被存储到哪个 Redis 节点或者从哪个Redis节点读取。大多数客户端已经实现了客户端分区。\n代理分区: 意味着客户端将请求发送给代理，然后代理决定去哪个节点写数据或者读数据。代理根据分区规则决定请求哪些 Redis 实例，然后根据 Redis 的响应结果返回给客户端。Redis 和 memcached 的一种代理实现就是 Twemproxy\n查询路由(Query routing) : 意思是客户端随机地请求任意一个 Redis 实例，然后由 Redis 将请求转发给正确的 Redis 节点。Redis Cluster 实现了一种混合形式的查询路由，但并不是直接将请求从一个 Redis 节点转发到另一个 Redis 节点，而是在客户端的帮助下直接 redirected 到正确的 Redis 节点。\nRedis 分区有什么缺点？ 涉及多个 key 的操作通常不会被支持。例如你不能对两个集合求交集，因为他们可能被存储到不同的 Redis 实例（实际上这种情况也有办法，但是不能直接使用交集指令）。\n同时操作多个 key,则不能使用 Redis 事务.\n分区使用的粒度是key，不能使用一个非常长的排序key存储一个数据集（The partitioninggranularity is the key, so it is not possible to shard a dataset with a single hugekey like a very big sorted set）.\n当使用分区的时候，数据处理会非常复杂，例如为了备份你必须从不同的Redis 实例和主机同时收集 RDB / AOF 文件。\n分区时动态扩容或缩容可能非常复杂。Redis 集群在运行时增加或者删除Redis 节点，能做到最大程度对用户透明地数据再平衡，但其他一些客户端分区或者代理分区方法则不支持这种特性。然而，有一种预分片的技术也可以较好的解决这个问题。\n如果 Redis 被当做一个持久化存储使用，必须使用固定的 keys-to-nodes 映射关系，节点的数量一旦确定不能变化。否则的话(即 Redis 节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有 Redis 集群可以做到这样。\n同步 Redis 的同步机制了解么？ 从从同步。第一次同步时，主节点做一次 bgsave，并同时将后续修改操作记录到内存 buffer，待完成后将 rdb 文件全量同步到复制节点，复制节点接受完成后将 rdb 镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。\n缓存与数据库不一致怎么办 假设采用的主存分离，读写分离的数据库，如果一个线程 A 先删除缓存数据，然后将数据写入到主库当中，这个时候，主库和从库同步没有完成，线程 B 从缓存当中读取数据失败，从从库当中读取到旧数据，然后更新至缓存，这个时候，缓存当中的就是旧的数据。\n发生上述不一致的原因在于，主从库数据不一致问题，加入了缓存之后，主从不一致的时间被拉长了\n处理思路： 在从库有数据更新之后，将缓存当中的数据也同时进行更新，即当从库发生了数据更新之后，向缓存发出删除，淘汰这段时间写入的旧数据。\n主从数据库不一致如何解决 场景描述，对于主从库，读写分离，如果主从库更新同步有时差，就会导致主从库数据的不一致\n1、忽略这个数据不一致，在数据一致性要求不高的业务下，未必需要时时一致性 2、强制读主库，使用一个高可用的主库，数据库读写都在主库，添加一个缓存，提升数据读取的性能。 3、选择性读主库，添加一个缓存，用来记录必须读主库的数据，将哪个库，哪个表，哪个主键，作为缓存的 key,设置缓存失效的时间为主从库同步的时间，如果缓存当中有这个数据，直接读取主库，如果缓存当中没有这个主键，就到对应的从库中读取。  对方接着追问：如果这个 redis 正在给线上的业务提供服务，那使用 keys 指令会有什么问题？\n这个时候你要回答 redis 关键的一个特性：\nredis 的单线程的。keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用 scan 指令，scan 指令可以无阻塞的提取出指定模式的 key 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用 keys 指令长。  使用 Redis 做过异步队列吗，是如何实现的使用 list 类型保存数据信息 rpush 生产消息，lpop 消费消息，当 lpop 没有消息时，可以 sleep 一段时间，然后再检查有没有信息，如果不想 sleep 的话，可以使用 blpop, 在没有信息的时候，会一直阻塞，直到信息的到来。\nredis 可以通过 pub/sub 主题订阅模式实现一个生产者，多个消费者，当然也存在一定的缺点，当消费者下线时，生产的消息会丢失。\nredis 主从复制如何实现的？redis 的集群模式如何实现？redis 的 key 是如何寻址的？ 主从复制实现： 主节点将自己内存中的数据做一份快照，将快照发给从节点，从节点将数据恢复到内存中。之后再每次增加新数据的时候，主节点以类似于 mysql 的二进制日志方式将语句发送给从节点，从节点拿到主节点发送过来的语句进行重放。\n分片方式：\n--客户端分片 --基于代理的分片 ● Twemproxy ● codis --路由查询分片 ● Redis-cluster（本身提供了自动将数据分散到 Redis Cluster 不同节点的能力，整个数据集合的某个数据子集存储在哪个节点对于用户来说是透明的）  redis-cluster 分片原理：Cluster 中有一个 16384 长度的槽(虚拟槽)，编号分别为 0-16383。\n每个 Master 节点都会负责一部分的槽，当有某个 key 被映射到某个 Master 负责的槽，那么这个 Master 负责为这个 key 提供服务，至于哪个 Master 节点负责哪个槽，可以由用户指定，也可以在初始化的时候自动生成，只有 Master 才拥有槽的所有权。Master 节点维护着一个 16384/8 字节的位序列，Master 节点用 bit 来标识对于某个槽自己是否拥有。比如对于编号为 1 的槽，Master 只要判断序列的第二位（索引从 0 开始）是不是为 1 即可。\n这种结构很容易添加或者删除节点。比如如果我想新添加个节点 D, 我需要从节点 A、B、C 中得部分槽到 D 上。\nredis 过期策略都有哪些？LRU 算法知道吗？写一下 java 代码实现？ 过期策略:\n定时过期(一 key 一定时器)， 惰性过期：只有使用 key 时才判断 key 是否已过期，过期则清除。 定期过期：前两者折中。  LRU 算法实现：\n1.通过双向链表来实现，新数据插入到链表头部； 2.每当缓存命中（即缓存数据被访问），则将数据移到链表头部； 3.当链表满的时候，将链表尾部的数据丢弃。  LinkedHashMap：HashMap 和双向链表合二为一即是 LinkedHashMap。HashMap 是无序的，LinkedHashMap 通过维护一个额外的双向链表保证了迭代顺序。该迭代顺序可以是插入顺序（默认），也可以是访问顺序。\n缓存更新策略（即如何让缓存和 mysql 保持一致性）？ key 过期清除（超时剔除）策略 惰性过期（类比懒加载，这是懒过期）：只有当访问一个 key 时，才会判断该 key是否已过期，过期则清除。该策略可以最大化地节省 CPU 资源，却对内存非常不友好。极端情况可能出现大量的过期 key 没有再次被访问，从而不会被清除，占用大量内存。 定期过期：每隔一定的时间，会扫描一定数量的数据库的 expires 字典中一定数量的 key，并清除其中已过期的 key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得 CPU 和内存资源达到最优的平衡效果。(expires 字典会保存所有设置了过期时间的 key 的过期时间数据，其中，key 是指向键空间中的某个键的指针，value 是该键的毫秒精度的 UNIX 时间戳表示的过期时间。键空间是指该 Redis 集群中保存的所有键。)  问：比如这么个场景，我设计了很多 key，过期时间是 5 分钟，当前内存占用率是 50%。但是 5 分钟到了，内存占用率还是很高，请问为什么？\nRedis 中同时使用了惰性过期和定期过期两种过期策略，即使过期时间到了，但是有部分并没有真正删除，等待惰性删除。  为什么有定期还要有惰性呢？ 其实很简单，比如 10 万个 key 就要过期了，Redis默认是 100ms 检查一波。如果他检查出 10 万个即将要清除，那他接下来的时间基本都是在干这些清空内存的事了，那肯定影响性能，所以他只会部分删除，剩下的等惰性。\n集群 是否使用过 Redis 集群，集群的原理是什么？ Redis Sentinel 着眼于高可用，在 master 宕机时会自动将 slave 提升为 master，继续提供服务。\nRedis Cluster 着眼于扩展性，在单个 redis 内存不足时，使用 Cluster 进行分片存储。\nredis 集群如何保证一致性？ Redis 集群的主从复制模型是怎样的？ 为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型,每个节点都会有 N-1 个复制品.\nRedis 集群会有写操作丢失吗？为什么？ Redis 并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。\nRedis 集群之间是如何复制的？ 异步复制\nRedis 集群最大节点个数是多少？ 16384 个\nRedis 集群如何选择数据库？ Redis 集群目前无法做数据库选择，默认在 0 数据库。\nRedis 集群实现？ 需要结合真实案例去分析，这里给大家推荐一个不错的博客。 https://blog.csdn.net/yfkiss/article/details/38944179\nredis 集群（采用虚拟槽方式，高可用）原理（和哨兵模式原理类似，3.0 版本或以上才有）？ Redis 集群内节点通过 ping/pong 消息实现节点通信，消息不但可以传播节点槽信息，还可以传播其他状态如：主从状态、节点故障等。\n因此故障发现也是通过消息传播机制实现的，主要环节包括：主观下线（pfail）和客观下线（fail）\n主客观下线:\n1 主观下线：集群中每个节点都会定期向其他节点发送 ping 消息，接收节点回复 pong 消息作为响应。如果通信一直失败，则发送节点会把接收节点标记为主观下线（pfail）状态。 2 客观下线：超过半数，对该主节点做客观下线 3 主节点选举出某一主节点作为领导者，来进行故障转移。 4 故障转移（选举从节点作为新主节点）  Redis 集群方案应该怎么做？都有哪些方案？ 1.codis\n2.目前用的最多的集群方案，基本和 twemproxy 一致的效果，但它支持在节点数量改变情况下，旧节点数据可恢复到新 hash 节点。redis cluster3.0 自带的集群，特点在于他的分布式算法不是一致性 hash，而是 hash 槽的概念，以及自身支持节点设置从节点。具体看官方文档介绍。\n3.在业务代码层实现，起几个毫无关联的 redis 实例，在代码层，对 key 进行 hash 计算，然后去对应的 redis 实例操作数据。这种方式对 hash 层代码要求比较高，考虑部分包括，节点失效后的替代算法方案，数据震荡后的自动脚本恢复，实例的监控，等等。\nRedis 集群方案什么情况下会导致整个集群不可用？ 有 A，B，C 三个节点的集群,在没有复制模型的情况下,如果节点 B 失败了，那么整个集群就会以为缺少 5501-11000 这个范围的槽而不可用。\n事务 Redis 的事务？ 一、Redis 事务允许一组命令在单一步骤中执行。事务有两个属性，说明如下：\na) 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 b) Redis 事务是原子的。原子意味着要么所有的命令都执行，要么都不执行；  二、一个事务从开始到执行会经历以下三个阶段：\na) 开始事务 b) 命令入队 c) 执行事务  怎么理解 Redis 事务？ 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行，事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。\n事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。\nRedis 事务相关的命令有哪几个？ MULTI、EXEC、DISCARD、WATCH\n哨兵 redis 中的 sentinel (哨兵模式) 的作用？ 在哨兵模式下，如果主机宕机了，会在从机里面投票选出一个从机当主机，之后如果原来的主机又回来了，在较短的时间内还没有被哨兵模式监控到的时候，回来的主机就是自己一个人单独一套体系自己是光杆司令，但是一会儿功夫，哨兵模式监控到了这个重启的主机后，哨兵模式会告诉这个新来的主机，已经换老大了，你需要跟着新老大混，这个时候新来的就会自动变为从机依附于前面投票选出来的主机\n哨兵模式实现原理？（2.8 版本或更高才有） 1.三个定时监控任务：\n1.1 每隔 10s，每个 S 节点（哨兵节点）会向主节点和从节点发送 info 命令获取最新的拓扑结构 1.2 每隔 2s，每个 S 节点会向某频道上发送该 S 节点对于主节点的判断以及当前 Sl 节点的信息，同时每个 Sentinel 节点也会订阅该频道，来了解其他 S 节点以及它们对主节点的判断（做客观下线依据） 1.3 每隔 1s，每个 S 节点会向主节点、从节点、其余 S 节点发送一条 ping 命令做一次心跳检测(心跳检测机制)，来确认这些节点当前是否可达  2.主客观下线：\n2.1 主观下线：根据第三个定时任务对没有有效回复的节点做主观下线处理 2.2 客观下线：若主观下线的是主节点，会咨询其他 S 节点对该主节点的判断，超过半数，对该主节点做客观下线  3.选举出某一哨兵节点作为领导者，来进行故障转移。 选举方式：raft算法。\n每个 S 节点有一票同意权，哪个 S 节点做出主观下线的时候，就会询问其他 S 节点是否同意其为领导者。获得半数选票的则成为领导者。基本谁先做出客观下线，谁成为领导者。\n4.故障转移（选举新主节点流程）：\n主从复制模式下，主挂了怎么办？ redis 提供了哨兵模式（高可用）\n何谓哨兵模式？就是通过哨兵节点进行自主监控主从节点以及其他哨兵节点，发现主节点故障时自主进行故障转移。\n订阅 redis 也可以进行发布订阅消息吗？ 可以，（然后可以引出哨兵模式（后面会讲）怎么互相监督的，就是因为每隔 2秒哨兵节点会发布对某节点的判断和自身的信息到某频道，每个哨兵订阅该频道获取其他哨兵节点和主从节点的信息，以达到哨兵间互相监控和对主从节点的监控）和很多专业的消息队列系统（例如 Kafka、RocketMQ）相比，Redis 的发布订阅略显粗糙，例如无法实现消息堆积和回溯。但胜在足够简单。\n持久化 redis 能否将数据持久化，如何实现？ 能，将内存中的数据异步写入硬盘中，两种方式：RDB（默认）和 AOF\nRDB 持久化原理：\n通过 bgsave 命令触发，然后父进程执行 fork 操作创建子进程，子进程创建 RDB 文件，根据父进程内存生成临时快照文件，完成后对原有文件进行原子替换（定时一次性将所有数据进行快照生成一份副本存储在硬盘中） 优点：是一个紧凑压缩的二进制文件，Redis 加载 RDB 恢复数据远远快于 AOF的方式。 缺点：由于每次生成 RDB 开销较大，非实时持久化，  AOF 持久化原理：\n开启后，Redis 每执行一个修改数据的命令，都会把这个命令添加到 AOF 文件中。 优点：实时持久化。 缺点：所以 AOF 文件体积逐渐变大，需要定期执行重写操作来降低文件体积，加载慢  Redis 提供了哪几种持久化方式？ RDB 持久化方式能够在指定的时间间隔能对你的数据进行快照存储.\nAOF 持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF 命令以 Redis 协议追加保存每次写的操作到文件末尾.\nRedis 还能对AOF 文件进行后台重写,使得 AOF 文件的体积不至于过大. 如果你只希望你的数据在服务器运行的时候存在,你也可以不使用任何持久化方式.\n你也可以同时开启两种持久化方式, 在这种情况下, 当 Redis 重启的时候会优先载入AOF文件来恢复原始的数据,因为在通常情况下 AOF 文件保存的数据集要比RDB 文件保存的数据集要完整. 最重要的事情是了解 RDB 和 AOF 持久化方式的不同,让我们以 RDB 持久化方式开始。\n如何选择合适的持久化方式？ 一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失，那么你可以只使用 RDB 持久化。\n有很多用户都只使用 AOF 持久化，但并不推荐这种方式：\n因为定时生成RDB快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比AOF恢复的速度要快，除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的bug。\n修改配置不重启 Redis 会实时生效吗？ 针对运行实例，有许多配置选项可以通过 CONFIG SET 命令进行修改，而无需执行任何形式的重启。\n从 Redis 2.2 开始，可以从 AOF 切换到 RDB 的快照持久性或其他方式而不需要重启 Redis。检索 \u0026ldquo;CONFIG GET *\u0026rdquo; 命令获取更多信息。但偶尔重新启动是必须的，如为升级 Redis 程序到新的版本，或者当你需要修改某些目前CONFIG 命令还不支持的配置参数的时候。\nRedis 持久化数据和缓存怎么做扩容？ 如果 Redis 被当做缓存使用，使用一致性哈希实现动态扩容缩容。\n知道 redis 的持久化吗？底层如何实现的？有什么优点缺点？ RDB(Redis DataBase:在不同的时间点将 redis 的数据生成的快照同步到磁盘等介质上):内存到硬盘的快照，定期更新。缺点：耗时，耗性能(fork+io 操作)，易丢失数据。\nAOF(Append Only File：将 redis 所执行过的所有指令都记录下来，在下次 redis 重启时，只需要执行指令就可以了):写日志。缺点：体积大，恢复速度慢。\nbgsave 做镜像全量持久化，aof 做增量持久化。因为 bgsave 会消耗比较长的时间，不够实时，在停机的时候会导致大量的数据丢失，需要 aof 来配合，在 redis 实例重启时，优先使用 aof 来恢复内存的状态，如果没有 aof 日志，就会使用 rdb 文件来恢复。Redis 会定期做\naof 重写，压缩 aof 文件日志大小。Redis4.0 之后有了混合持久化的功能，将 bgsave 的全量和 aof 的增量做了融合处理，这样既保证了恢复的效率又兼顾了数据的安全性。bgsave 的原理，fork 和 cow, fork 是指 redis 通过创建子进程来进行 bgsave 操作，cow 指的是 copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来。\n缓存 什么是缓存穿透？如何避免？ 缓存穿透\n一般的缓存系统，都是按照 key 去缓存查询，如果不存在对应的 value，就应该去后端系统查找（比如DB）。一些恶意的请求会故意查询不存在的 key,请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。 （缓存穿透指的是查询一个根本不存在的数据，缓存层不命中，又去查存储层，又不命中。但如果有大量这种查询不存在的数据的请求过来，会对存储层有较大压力，若是恶意攻击，后果就很严重）  如何避免？\n1：缓存空值存在的问题：对查询结果为空的情况也进行缓存，缓存时间设置短一点，或者该 key 对应的数据 insert 了之后清理缓存。 2：布隆过滤器存在的问题：对一定不存在的 key 进行过滤。可以把所有的可能存在的 key 放到一个大的 Bitmap 中，查询时通过该 bitmap 过滤。  什么是缓存雪崩？何如避免？ 当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，会给后端系统带来很大压力。导致系统崩溃。  如何避免？\n1：在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个 key 只允许一个线程查询数据和写缓存，其他线程等待。 2：做二级缓存，A1 为原始缓存，A2 为拷贝缓存，A1 失效时，可以访问 A2，A1 缓存失效时间设置为短期，A2 设置为长期 3：不同的 key，设置不同的过期时间，让缓存失效的时间点尽量均匀  缓存穿透、缓存击穿、缓存雪崩解决方案？ 缓存穿透：指查询一个一定不存在的数据，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到 DB 去查询，可能导致 DB 挂掉。\n解决方案：\n1.查询返回的数据为空，仍把这个空结果进行缓存，但过期时间会比较短； 2.布隆过滤器：将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对 DB 的查询。  缓存击穿：对于设置了过期时间的 key，缓存在某个时间点过期的时候，恰好这时间点对这个 Key 有大量的并发请求过来，这些请求发现缓存过期一般都会从后端 DB 加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把 DB 压垮。\n解决方案：\n1.使用互斥锁：当缓存失效时，不立即去 load db，先使用如 Redis 的 setnx 去设置一个互斥锁，当操作成功返回时再进行 load db 的操作并回设缓存，否则重试 get 缓存的方法。 2.永远不过期：物理不过期，但逻辑过期（后台异步线程去刷新）。  缓存雪崩：设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到 DB，DB 瞬时压力过重雪崩。与缓存击穿的区别：雪崩是很多 key，击穿是某一个key 缓存。\n解决方案：\n将缓存失效时间分散开，比如可以在原有的失效时间基础上增加一个随机值，比如 1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。  缓存粒度控制？ 锁 redis分布式锁实现原理学习|PDF 使用过 Redis 分布式锁么，它是怎么实现的？ 先拿 setnx 来争抢锁，抢到之后，再用 expire 给锁加一个过期时间防止锁忘记了释放。\n如果在 setnx 之后执行 expire 之前进程意外 crash 或者要重启维护了，那会怎么样？ set 指令有非常复杂的参数，这个应该是可以同时把 setnx 和 expire 合成一条指令来用的！\n使用过 Redis 做异步队列么，你是怎么用的？有什么缺点？ 一般使用 list 结构作为队列，rpush 生产消息，lpop 消费消息。当 lpop 没有消息的时候，要适当 sleep 一会再重试。\n缺点：\n在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如 rabbitmq 等。  能不能生产一次消费多次呢？ 使用 pub/sub 主题订阅者模式，可以实现 1:N 的消息队列。\n使用 redis 如何设计分布式锁？说一下实现思路？使用 zk 可以吗？如何实现？这两种有什么区别？ redis:\n1.线程 A setnx(上锁的对象,超时时的时间戳 t1)，如果返回 true，获得锁。 2.线程 B 用 get 获取 t1,与当前时间戳比较,判断是是否超时,没超时 false,若超时执行第 3 步; 3.计算新的超时时间 t2,使用 getset 命令返回 t3(该值可能其他线程已经修改过),如果t1==t3，获得锁，如果 t1!=t3 说明锁被其他线程获取了。 4.获取锁后，处理完业务逻辑，再去判断锁是否超时，如果没超时删除锁，如果已超时，  不用处理（防止删除其他线程的锁）。\nzk:\n1.客户端对某个方法加锁时，在 zk 上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点 node1; 2.客户端获取该路径下所有已经创建的子节点，如果发现自己创建的 node1 的序号是最小的，就认为这个客户端获得了锁。 3.如果发现 node1 不是最小的，则监听比自己创建节点序号小的最大的节点，进入等待。 4.获取锁后，处理完逻辑，删除自己创建的 node1 即可。  区别:zk 性能差一些，开销大，实现简单。\n优化 无底洞优化？ 造成原因：redis 分布式越来越多，导致性能反而下降，因为键值分布到更多的节点上，所以无论是 Memcache 还是 Redis 的分布式，批量操作通常需要从不同节点上获取，相比于单机批量操作只涉及一次网络操作，分布式批量操作 会涉及多次网络时间。 即分布式过犹不及。\n雪崩优化 如果缓存层由于某些原因不能提供服务，于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会级联宕机的情况。\n热点 key 优化 当前 key 是一个热点 key（例如一个热门的娱乐新闻），并发量非常大。\nRedis 如何做内存优化？ 尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。\n比如你的 web 系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的 key,而是应该把这个用户的所有信息存储到一张散列表里面。\n都有哪些办法可以降低 Redis 的内存使用情况呢？ 如果你使用的是 32 位的 Redis 实例，可以好好利用 Hash,list,sorted set,set 等集合类型数据，因为通常情况下很多小的 Key-Value 可以用更紧凑的方式存放到一起。\nRedis 常见性能问题和解决方案？ (1) Master 最好不要做任何持久化工作，如 RDB 内存快照和 AOF 日志文件 (2) 如果数据比较重要，某个 Slave 开启 AOF 备份数据，策略设置为每秒同步一次 (3) 为了主从复制的速度和连接的稳定性，Master 和 Slave 最好在同一个局域网内 (4) 尽量避免在压力很大的主库上增加从库 (5) 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master \u0026lt;- Slave1 \u0026lt;- Slave2\u0026lt;- Slave3... 这样的结构方便解决单点故障问题，实现 Slave 对 Master 的替换。如果Master 挂了，可以立刻启用 Slave1 做 Master，其他不变。  Redis 常见的性能问题都有哪些？如何解决？ 1. Master 写内存快照，save 命令调度 rdbSave 函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以 Master最好不要写内存快照。 2. Master AOF 持久化，如果不重写 AOF 文件，这个持久化方式对性能的影响是最小的，但是 AOF 文件会不断增大，AOF 文件过大会影响 Master重启的恢复速度。Master 最好不要做任何持久化工作，包括内存快照和AOF 日志文件，特别是不要启用内存快照做持久化,如果数据比较关键，某个 Slave 开启 AOF 备份数据，策略为每秒同步一次。 3. Master 调用 BGREWRITEAOF 重写 AOF 文件，AOF 在重写的时候会占大量的 CPU 和内存资源，导致服务 load 过高，出现短暂服务暂停现象。 4. Redis 主从复制的性能问题，为了主从复制的速度和连接的稳定性，Slave和 Master 最好在同一个局域网内  怎样解决数据库高并发的问题？ 解决数据库高并发的常见方案：\n1） 缓存式的 Web 应用程序架构： 在 Web 层和 DB (数据库) 层之间加一层 cache 层，主要目的：减少数据库读取负担，提高数据读取速度。cache 存取的媒介是内存，可以考虑采用分布式的 cache 层，这样更容易破除内存容量的限制，同时增加了灵活性。 2） 增加 Redis 缓存数据库 3） 增加数据库索引 4） 页面静态化： 效率最高、消耗最小的就是纯静态化的 html 页面，所以我们尽可能使我们的网站上的页面采用静态页面来实现，这个最简单的方法其实也是最有效的方法。 用户可以直接获取页面，不用像 MVC 结构走那么多流程，比较适用于页面信息大量被前台程序调用，但是更新频率很小的情况。 5） 使用存储过程： 处理一次请求需要多次访问数据库的操作，可以把操作整合到储存过程，这样只要一次数据库访问就可以了。 6） MySQL 主从读写分离： 当数据库的写压力增加，cache 层（如 Memcached）只能缓解数据库的读取压力。读写集中在一个数据库上让数据库不堪重负。使用主从复制技术（master-slave 模式）来达到读写分离，以提高读写性能和读库的可扩展性。读写分离就是只在主服务器上写，只在从服务器上读，基本原理是让主数据库处理事务性查询，而从数据库处理 select 查询，数据库复制被用于把事务性查询（增删改） 导致的改变更新同步到集群中的从数据库。 MySQL 读写分离提升系统性能： 1、主从只负责各自的读和写，极大程度缓解 X 锁和 S 锁争用。 2、slave 可以配置 MyISAM 引擎，提升查询性能以及节约系统开销。 3、master 直接写是并发的，slave 通过主库发送来的 binlog 恢复数据是异步的。 4、slave 可以单独设置一些参数来提升其读的性能。 5、增加冗余，提高可用性。 实现主从分离可以使用 MySQL 中间件如：Atlas 7） 分表分库： 在 cache 层的高速缓存，MySQL 的主从复制，读写分离的基础上，这时 MySQL 主库的写压力开始出现瓶颈，而数据量的持续猛增，由于 MyISAM 使用表锁，在高并发下会出现严重的锁问题，大量的高并发 MySQL 应用开始使用 InnoDB 引擎代替 MyISAM。 采用 Master-Slave 复制模式的 MySQL 架构，只能对数据库的读进行扩展，而对数据的写操作还是集中在 Master 上。这时需要对数据库的吞吐能力进一步地扩展，以满足高并发访问与海量数据存储的需求。 对于访问极为频繁且数据量巨大的单表来说，首先要做的是减少单表的记录条数，以便减少数据查询所需的时间，提高数据库的吞吐，这就是所谓的分表【水平拆分】。 在分表之前，首先需要选择适当的分表策略（尽量避免分出来的多表关联查询），使得数据能够较为均衡地分布到多张表中，并且不影响正常的查询。 分表能够解决单表数据量过大带来的查询效率下降的问题，但是却无法给数据库的并发处理能力带来质的提升。 面对高并发的读写访问，当数据库 master 服务器无法承载写操作压力时，不管如何扩展 Slave 服务器都是没有意义的，对数据库进行拆分，从而提高数据库写入能力，即分库【垂直拆分】。 8） 负载均衡集群： 将大量的并发请求分担到多个处理节点。 由于单个处理节点的故障不影响整个服务，负载均衡集群同时也实现了高可用性。 负载均衡将是大型网站解决高负荷访问和大量并发请求采用的终极解决办法。  Redis 的并发竞争问题怎么解决？  方案一：可以使用独占锁的方式，类似操作系统的 mutex 机制，不过实现相对复杂，成本较高。\n  方案二：使用乐观锁的方式进行解决（成本较低，非阻塞，性能较高）\n 如何用乐观锁方式进行解决？ 本质上是假设不会进行冲突，使用 redis 的命令 watch 进行构造条件\n回收 Redis 回收进程如何工作的？ 一个客户端运行了新的命令，添加了新的数据。\nRedi 检查内存使用情况，如果大于 maxmemory 的限制, 则根据设定好的策略进行回收。\n一个新的命令被执行，等等。\n所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下。\n如果一个命令的结果导致大量内存被使用（例如很大的集合的交集保存到一个新的键），不用多久内存限 制就会被这个内存使用量超越。\nRedis 回收使用的是什么算法？ LRU 算法\nmysql 对比 redis 相比 memcached 有哪些优势？ (1) memcached 所有的值均是简单的字符串，redis 作为其替代者，支持更为丰富的数据类型 (2) redis 的速度比 memcached 快很多 (3) redis 可以持久化其数据  redis 和 memcached 什么区别？ 1.mc 可缓存图片和视频。rd 支持除 k/v 更多的数据结构; 2.rd 可以使用虚拟内存，rd 可持久化和 aof 灾难恢复，rd 通过主从支持数据备份; 3.rd 可以做消息队列。  为什么高并发下有时单线程的 redis 比多线程的memcached 效率要高？ 原因：mc 多线程模型引入了缓存一致性和锁，加锁带来了性能损耗。\nMemcache 与 Redis 的区别都有哪些？ 1)、存储方式 Memecache 把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。Redis 有部份存在硬盘上，这样能保证数据的持久性。 2)、数据支持类型 Memcache 对数据类型支持相对简单。Redis 有复杂的数据类型。 3)、使用底层模型不同 它们之间底层实现方式 以及与客户端之间通信的应用协议不一样。Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 4），value 大小 redis 最大可以达到 1GB，而 memcache 只有 1MB  Redis 与 memcached 相比有哪些优势？ 1.memcached 所有的值均是简单的字符串，redis 作为其替代者，支持更为丰富的数据类型 2.redis 的速度比 memcached 快很多 redis 的速度比 memcached 快很多 3.redis 可以持久化其数据 redis 可以持久化其数据  MySQL 和 Redis 高可用性体现在哪些方面？ \u0026gt; a) MySQL Replication 是 MySQL 官方提供的主从同步方案，用于将一个 MySQL 实例的数据，同步到另一个实例中。Replication 为保证数据安全做了重要的保证，也是现在运用最广的 MySQL 容灾方案。Replication 用两个或以上的实例搭建了 MySQL 主从复制集群， 提供单点写入，多点读取的服务，实现了读的 scale out。 \u0026gt; b) Sentinel 是 Redis 官方为集群提供的高可用解决方案。在实际项目中可以使用 sentinel 去做 Redis 自动故障转移，减少人工介入的工作量。另外 sentinel 也给客户端提供了监控消息的通知，这样客户端就可根据消息类型去判断服务器的状态，去做对应的适配操作。 \u0026gt; c) 下面是 Sentinel 主要功能列表： # Monitoring Sentinel 持续检查集群中的 master、slave 状态，判断是否存活。 # Notification 在发现某个 Redis 实例死的情况下，Sentinel 能通过 API 通知系统管理员或其他程序脚本。 # Automatic failover 如果一个 master 挂掉后，sentinel 立马启动故障转移，把某个 slave 提升为 master。其他的 slave 重新配置指向新 master。 # Configuration provider 对于客户端来说 sentinel 通知是有效可信赖的。客户端会连接 sentinel 去请求当前 master 的地址，一旦发生故障 sentinel 会提供新地址给客户端。  MySQL 里有 2000w 数据，redis 中只存 20w 的数据，如何保证 redis 中的数据都是热点数据 相关知识：redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。\nredis 提供 6 种数据淘汰策略：\nvoltile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据  在选择缓存时，什么时候选择 redis，什么时候选择 memcached 选择 redis 的情况：\n1、复杂数据结构，value 的数据是哈希，列表，集合，有序集合等这种情况下，会选择redis, 因为 memcache 无法满足这些数据结构，最典型的的使用场景是，用户订单列表，用户消息，帖子评论等。 2、需要进行数据的持久化功能，但是注意，不要把 redis 当成数据库使用，如果 redis挂了，内存能够快速恢复热数据，不会将压力瞬间压在数据库上，没有 cache 预热的过程。对于只读和数据一致性要求不高的场景可以采用持久化存储 3、高可用，redis 支持集群，可以实现主动复制，读写分离，而对于 memcache 如果想要实现高可用，需要进行二次开发。 4、存储的内容比较大，memcache 存储的 value 最大为 1M。  选择 memcache 的场景：\n1、纯 KV,数据量非常大的业务，使用 memcache 更合适，原因是， a)memcache 的内存分配采用的是预分配内存池的管理方式，能够省去内存分配的时间，redis 是临时申请空间，可能导致碎片化。 b)虚拟内存使用，memcache 将所有的数据存储在物理内存里，redis 有自己的 vm 机制，理论上能够存储比物理内存更多的数据，当数据超量时，引发 swap,把冷数据刷新到磁盘上，从这点上，数据量大时，memcache 更快 c)网络模型，memcache 使用非阻塞的 IO 复用模型，redis 也是使用非阻塞的 IO 复用模型，但是 redis 还提供了一些非 KV 存储之外的排序，聚合功能，复杂的 CPU 计算，会阻塞整个 IO 调度，从这点上由于 redis 提供的功能较多，memcache 更快些 d) 线程模型，memcache 使用多线程，主线程监听，worker 子线程接受请求，执行读写，这个过程可能存在锁冲突。redis 使用的单线程，虽然无锁冲突，但是难以利用多核的特性提升吞吐量。  Mongo 对比 Redis 和 MongoDB 的优缺点 MongoDB 和 Redis 都是 NoSQL，采用结构型数据存储。\n二者在使用场景中，存在一定的区别，这也主要由于二者在内存映射的处理过程，持久化的处理方法不同。 MongoDB 建议集群部署，更多的考虑到集群方案，Redis 更偏重于进程顺序写入，虽然支持集群，也仅限于主 - 从模式。\nRedis 优点：\n1) 读写性能优异 2) 支持数据持久化，支持 AOF 和 RDB 两种持久化方式 3) 支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。 4) 数据结构丰富：数据结构丰富：支持 string、hash、set、sortedset、list 等数据结构。  缺点：\n1) Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的 IP 才能恢复。 2) 主机宕机，宕机前有部分数据未能及时同步到从机，切换 IP 后还会引入数据不一致的问题，降低了系统的可用性。 3) Redis 的主从复制采用全量复制，复制过程中主机会 fork 出一个子进程对内存做一份快照，并将子进程的内存快照保存为文件发送给从机，这一过程需要确保主机有足够多的空余内存。若快照文件较大，对集群的服务能力会产生较大的影响，而且复制过程是在从机新加入集群或者从机和主机网络断开重连时都会进行，也就是网络波动都会造成主机和从机间的一次全量的数据复制，这对实际的系统运营造成了不小的麻烦。 4) Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题， 运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。  MongoDB 优点:\n1）弱一致性（最终一致），更能保证用户的访问速度 2）文档结构的存储方式，能够更便捷的获取数 3）内置 GridFS，高效存储二进制大对象 (比如照片和视频) 4）支持复制集、主备、互为主备、自动分片等特性 5） 动态查询 6）全索引支持，扩展到内部对象和内嵌数组  缺点：\n1）不支持事务 2）MongoDB 占用空间过大 3）维护工具不够成熟  ","date":"2023-01-09","permalink":"/post/redis/","tags":["InterviewQuestions","Redis"],"title":"redis"},{"content":"proxy protocol  Shadowsocks（SS） ShadowsocksR（SSR） V2Ray VrayX Trojan Trojan-Go  详细的就不累赘了，我了解的也不是很完全，墙内目前也有些介绍，但是只要能翻出去，资料就很多了。这里给出几篇文章，里面有详细介绍，防止失效，自己留了PDF文件。\n科学上网的主流协议大对比！这里面有你在使用的吗？ | PDF\n一次搞懂Xray/V2ray/Trojan/Trojan-go/SSR/SS的区别，不再选择困难症  | PDF\nSS、SSR、V2ray、Trojan、Xray 这五种翻墙协议与 VPN 对比有何不同？ | PDF\n软件  Qv2ray 跨平台 V2Ray 客户端，支持 Linux、Windows、macOS，可通过插件系统支持 SSR / Trojan / Trojan-Go / NaiveProxy 等协议 SagerNet SagerNet 是一个基于 V2Ray 的 Android 通用代理应用。 V2rayN V2RayN 是一个基于 V2Ray 内核的 Windows 客户端。 v2rayA 基于 web GUI 的跨平台 V2Ray 客户端，在 Linux 上支持全局透明代理，其他平台上支持系统代理。  命令行代理 浏览器代理 Proxy SwitchyOmega 插件商店可搜索下载，目前感觉是使用最舒适的代理切换工具。\n情景模式： 可以使用 HTTP/Socks 代理访问网站；可以根据多种条件和规则自动切换；可以根据在线或本地的 PAC 脚本规则使用代理。\n规则： 可以在线导入 AutoProxy 和 Switchy 格式的规则，也可以自己添加域名通配符、网址通配符和网址正则等切换规则。可以搭配GFWList使用PAC，有奇效，能够自动区分那些需要代理，不用麻烦的切换代理，自己添加规则也很麻烦，网页太多。\n快速切换： 可以在浏览器菜单中对情景模式进行快速切换，可以快速对当前网址添加过滤规则。\n官网\n谷歌插件商店\ngithub 项目地址\n自用软件代理 路由全局代理 这个比较麻烦，可以买别人刷好系统的路由器，也可以自己刷。还分软路由、硬路由等。\n软件有代理接口 有的软件有接口，在设置里面设置一下就可以使用。\n系统有全局代理设置 Windows 好像默认全局代理。 Linux 的 KDE、Gnome 有全局代理设置（不绝对）。\n手机分享实现全局代理 可以手机使用代理软件，然后通过像 EveryProxy 这类转发软件转发。不过这种方法好像不能全局代理。流量或者wifi（有的设备类似于双网卡，即可以使用wifi也可以热点给别人），数据的流动层不一样，所以不能。不过转发可以实现局域网代理，局域网内的设备通过局域网 IP 端口可共享代理。这样在没有代理配置的设备上也可以使用代理。\n其他分享 https://zgq-inc.github.io/overthefirewall/\nhttps://github.com/Loyalsoldier/v2ray-rules-dat\nhttps://github.com/gfwlist/gfwlist\n","date":"2023-01-08","permalink":"/post/proxy/","tags":["Tools","Proxy"],"title":"Proxy"},{"content":"一些键盘知识及可选方案 PCB 板： 旧键盘的 PCB 板 网上的自打板 网上开源方案自己找人打印 键盘配列： 键位数量：104、84、78、61 等等。 键位占比：100%、75%、60℀ 。 键帽： 材质：PBT、ABS、POM 等。 配色：单一颜色、混搭主题配色。 高度标准： 原厂键帽、OEM、SA、DSA、XDA、DCS 等 不太常见，但已经成为标准的还有 MIX、G20 等。 印刷方式： 正刻：字符刻到键帽正面朝上的方式，最常见的类型 同刻，是阴刻 or 蚀刻，字符会凹陷，一般来说字符和键帽同色 侧刻，把字符刻到键帽侧面、面朝人的方向，字在正对你身体的那一面 无刻，就是键帽上完全没有字符 印刷方式： 1、镭雕：激光蚀刻技术， 使用激光刻字技术在键帽上进行激光烧灼，烧出黑色的凹槽。因刻写的笔画有点状也有线条状，仔细看印刷的笔画就会很容易发觉凹陷下去的并有烧焦痕迹的笔画。激光蚀刻出来的字迹颜色交淡，摸起来能够感觉到文字部分的刻痕。寿命长、成本低，最为常见 2、丝印：丝网印刷技术，将一个特制的丝网覆盖在空白的键盘上，其中有字迹的地方被镂空，然后将油墨从上面刮过，这样在镂空的部分就会印上字迹 3、热升华：通过高温将固态油墨印进键帽的内部。固体树脂类油墨（颜料）会在高温之下升华然后渗透进入键帽材质中，在光泽上以及牢固性上更出色，字符很不容易磨损 4、镂空字体：最大的优点就是其本身自带的透光效果。常见于背光键盘，键帽表面易磨损 5、二色成型： 利用模具将两种不同颜色的材质混合在一起，利用两种塑料颜色来显示字符，字符颜色十分的鲜艳，耐用能很好的保证键帽手感，字符不易磨损，成本较高 6、UV覆膜：原理和丝印差不多，多了一道UV的工艺，字符上有一层薄薄的塑料膜，相较于丝印来说，字符的寿命要稍微长一些 7、类肤涂层喷墨：将半透明的键帽素材喷黑，然后将需要显示的字体留白。类肤涂层键帽的字体大多比较好看，类肤涂层的手感也较为优异，问题是容易脏 8、双色注塑：原理方面就是采用二色的模具，通过注塑机一次成型，相对来说工艺比较简单，成本也比较低，字符大多为开口 字符类型： 传统英文、日文、韩文、精灵文 轴体 ： 黑轴、红轴、茶轴、青轴。 国产轴、品牌轴、个人作坊定制轴。 三脚轴、五脚轴。 轴安装方式： 热插拔: 套筒 热插拔底座 焊接轴。 灯： 安装方式： 热插拔 焊接 灯位： 上、下灯位。 灯光： 有灯：单色灯、RGB 灯、可编程。 无灯。 灯体： 贴片灯。 灯珠。 连接方式： 有线、键线分离。 无线： 2.4G: 品牌厂固件 罗技键盘拆卸固件 私人固件。 蓝牙。 多模。 线： 接口： Mini-USB、Micro-USB、Type-C、PS/2。 形状： 直线 螺旋线 外壳： 老键盘外壳 根据品牌键盘自己用其他材料（亚克力板、金属、木头、其他材料）复刻的、 完全自定义的外壳（这就需要搭配想对应的 PCB 板了） 定位板： PC定位板： 搭配4f线性轴手感较为舒适，材质较软，回弹很舒服，搭配强段落轴会稍微吸收缓解触底段落感，要注意一点就是安装需要稍微精心一点，不然很容易划伤表面，个人最喜欢的搭配。 PC半钢定位板： 字母区由于无钢，心理上有些感觉不稳，但实际上没有什么影响，相比PC手感更软一些，安装上比起PC要稍麻烦一些，个人不太喜欢。 碳纤维定位板： 整体手感依然偏软，但比PC手感要硬一些，回弹有力但不震手，定制价格较便宜，个人第二选择。 FR4定位板： 手感比PC和碳纤维要硬，主观感觉回弹上力度要大一些，尤其在墨玉黑大键上，声音也偏亮，个人不是很喜欢，但很多大佬喜欢，而且定制价格不贵，个人建议可以入手尝试。 铝合金定位板： 手感较为均衡，和碳纤维感觉很像，但不知为何感觉比碳纤维回弹要软，声音也偏闷一些，个人推荐第三选择。 黄铜定位板： 手感比较硬，感觉上就是一个字，震，由于gasket结构的特殊加上黄铜定位板的重量，导致这种震手感觉被放大，个人不是很推荐，适合习武之人搭配类HP等段落轴使用。 卡纸定位板： 卡纸定位板是突发奇想试试新材料X宝随便找的商家切的，实际上十分悲剧，由于卡纸边缘过于柔软，导致轴体不容易卡紧，且拔出轴体后很容易损伤定位板，整体手感偏软，弹性一般，也有可能是我安装不到位导致的，价格十分低廉，个人不推荐。 榉木层板定位板： 与卡纸定位板一样都是突发奇想的搞活定位板，整体回弹较强，声音也相对较为清脆，实际使用比卡纸要强，但是味道实在太大，接受不了，价格十分低廉，个人也不推荐。 外观： 喷漆 贴纸 原皮 电池： 传统电池 锂电池 钮扣电池 其他配件： 消音棉 键盘托 防尘盖 喷漆（这个种类也多） 贴纸 数据线 接口小板  个人经验 自己客制化并不会省钱，无论是买现成套件还是自己买基础配件。这些还都是在不考虑组装工具的情况下。\n电烙铁买一定要买好一点的，那些便宜的小瓦数的，表面很容易氧化，然后就失去作用了，小瓦数的有的用的锡料熔点高，根本就融化不了。\n空心针也是，买盒装的比如4种标准的一盒，实际能用且会使用到的只有一种，质量不好的不小心就折了，也废了。\n吸锡器，便宜的那种铝加塑料的，密封性不好，简单来说就是没用。\n剪线键盘，大多数的质量都很差，无论是什么轴，起码都要换掉一半以上。电路板一般都还好或者能抢救。期待使用原装键帽不现实。\n拆解非热插拔的轴很麻烦，两三个小时是常事，带灯的化更甚，甚至感觉单是拔键帽都很麻烦。\n焊接并不容易，实际焊接的点很小，不熟练完全操作不来，焊点的美观就别想了。如果是接口小板这类小零件要多备几个，不小心烫坏不是不可能。\n自己尝试了三四把键盘，状况百出，最后只有一把情况好点的，换了几个轴，改了 Type-C 键线分离。其他的喷漆瑕疵，漆不匀，起泡，拆灯、轴弄断引脚，工具报废，实在弄不下去最后连同工具、配件打包便宜卖了。\nkeyboard layout 留做备用，换洗键后安装的时候有个对照。 一个可以设置键盘布局的网站 : keyboard-layout-editor\n自己有 json 文件可自己导入（比如78keys），页面不好调整，换成 json 模式调整会方便些。\n机械键盘键帽知识普及 OEM和原厂键帽有什么区别\n【科普】配列、键帽高度以及常见的键帽倍数识别\n有哪些跟机械键盘相关的「黑话」和术语？\n客制化键盘各部件介绍（客制化入门必看）\n","date":"2023-01-08","permalink":"/post/keyboard/","tags":["Tools","Input_Method"],"title":"keyboard"},{"content":"1. tar    参数 说明     -c 建立压缩档案   -x 解压   -t 查看内容   -r 向压缩归档文件末尾追加文件   -u 更新原压缩包中的文件    这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。\n   参数 说明     -z 有gzip属性的   -j 有bz2属性的   -Z 有compress属性的   -v 显示所有过程   -O 将文件解开到标准输出    参数-f是必须的-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。\n1.1. tar 严格来说，tar文件只是归档但并未压缩\ntar -cvf test.tar test1.log test2.log #归档多个文件 tar -cvf test.tar test/* \u0026amp;emsp;[[归档test]]目录下所有文件 tar -cvf test.tar *.log \u0026amp;emsp;#归档所有以.log结尾的文件   tar -xvf file.tar //解压 tar包   1.2. tar.gz tar -zxvf file.tar.gz //解压tar.gz   tar -zcvf test.tar.gz file1 file2 [[打包，并以gzip]]压缩  1.3. tar.bz2 tar -jxvf test.tar.bz2 file1 file2 [[打包，并以bzip2]]压缩   tar -jcvf test.tar.bz2 file1 file2 [[打包，并以bzip2]]压缩  1.4. tar.Z tar -xZvf file.tar.Z //解压tar.Z   tar -cZvf file.tar.Z //压缩tar.Z  1.5. tar.xz 压缩tar.xz包 先创建xxx.tar文件 tar -cvf xxx.tar xxx 再创建xxx.tar.xz文件 xz -z xxx.tar 如果要保留被压缩的文件，需要加上参数-k   解压tar.xz包 这是两层压缩，外面是xz压缩，里层是tar压缩，所以分两步实现解压。 xz -d filename.tar.xz tar -xvf filename.tar.xz 也可以直接解压 tar -xvJf filename.tar.xz  2. zip/unzip zip和unzip命令主要用于处理zip包。记得linux默认也是没有的，需要额外安装。\nzip 功能说明：压缩文件。\n语　法：zip [-AcdDfFghjJKlLmoqrSTuvVwXyz$][-b \u0026lt;工 作目录\u0026gt;][-ll][-n \u0026lt;字 尾字符串\u0026gt;][-t \u0026lt;日 期时间\u0026gt;][-\u0026lt;压 缩效率\u0026gt;][压 缩文件][文件...][-i \u0026lt;范本样式\u0026gt;][-x \u0026lt;范本样式\u0026gt;]\n补充说明：zip是个使用广泛的压缩程序，文件经它压缩后会另外产生具 有\u0026quot;.zip\u0026quot;扩展名 的压缩文件。\n   参数 说明     -A 调整可执行的自动解压缩文件。   -b\u0026lt;工作目录\u0026gt; 指定暂时存放文件的目录。   -c 替每个被压缩的文件加上注释。   -d 从压缩文件内删除指定的文件。   -D 压缩文件内不建立目录名称。   -f 此参数的效果和指定\u0026quot;-u\u0026quot;参 数类似，但不仅更新既有文件，如果某些文件原本不存在于压缩文件内，使用本参数会一并将其加入压缩文件中。   -F 尝试修复已损坏的压缩文件。   -g 将文件压缩后附加在既有的压缩文件之后，而非另行建立新的压缩文件。   -h 在线帮助。   -i\u0026lt;范本样式\u0026gt; 只压缩符合条件的文件。   -j 只保存文件名称及其内容，而不存放任何目录名称。   -J 删除压缩文件前面不必要的数据。   -k 使用MS-DOS兼容格 式的文件名称。   -l 压缩文件时，把LF字符 置换成LF+CR字 符。   -ll 压缩文件时，把LF+CR字 符置换成LF字符。   -L 显示版权信息。   -m 将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中。   -n\u0026lt;字尾字符串\u0026gt; 不压缩具有特定字尾字符串的文件。   -o 以压缩文件内拥有最新更改时间的文件为准，将压缩文件的更改时间设成和该文件相同。   -q 不显示指令执行过程。   -r 递归处理，将指定目录下的所有文件和子目录一并处理。   -S 包含系统和隐藏文件。   -t\u0026lt;日期时间\u0026gt; 把压缩文件的日期设成指定的日期。   -T 检查备份文件内的每个文件是否正确无误。   -u 更换较新的文件到压缩文件内。   -v 显示指令执行过程或显示版本信息。   -V 保存VMS操作系统的文 件属性。   -w 在文件名称里假如版本编号，本参数仅在VMS操 作系统下有效。   -x\u0026lt;范本样式\u0026gt; 压缩时排除符合条件的文件。   -X 不保存额外的文件属性。   -y 直接保存符号连接，而非该连接所指向的文件，本参数仅在UNIX之 类的系统下有效。   -z 替压缩文件加上注释。   -$ 保存第一个被压缩文件所在磁盘的卷册名称。   -\u0026lt;压缩效率\u0026gt; 压缩效率是一个介于1-9的 数值。    unzip 功 能说明：解压缩zip文 件\n语　法：unzip [-cflptuvz][-agCjLMnoqsVX][-P \u0026lt;密 码\u0026gt;][.zip文 件][文件][-d \u0026lt;目录\u0026gt;][-x \u0026lt;文件\u0026gt;] 或 unzip [-Z]\n补充说明：unzip为.zip压缩文件的解压缩程序。\n   参数 说明     -c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。   -f 更新现有的文件。   -l 显示压缩文件内所包含的文件。   -p 与 -c 参数类似，会将解压缩的结果显示到屏幕上，但不会执行任 何的转换。   -t 检查压缩文件是否正确。但不解压。   -u 与 -f 参数类似，但是除了更新现有的文件外，也会将压缩文件中 的其他文件解压缩到目录中。   -v 执行是时显示详细的信息。或查看压缩文件目录，但不解压。   -z 仅显示压缩文件的备注文字。   -a 对文本文件进行必要的字符转换。   -b 不要对文本文件进行字符转换。   -C 压缩文件中的文件名称区分大小写。   -j 不处理压缩文件中原有的目录路径。   -L 将压缩文件中的全部文件名改为小写。   -M 将输出结果送到more程 序处理。   -n 解压缩时不要覆盖原有的文件。   -o 不必先询问用户，unzip执行后覆盖原有文件。   -P\u0026lt;密码\u0026gt; 使用zip的密码选项。   -q 执行时不显示任何信息。   -s 将文件名中的空白字符转换为底线字符。   -V 保留VMS的文件版本信 息。   -X 解压缩时同时回存文件原来的UID/GID。   [.zip文件] 指定.zip压缩文件。   [文件] 指定要处理.zip压缩文 件中的哪些文件。   -d\u0026lt;目录\u0026gt; 指定文件解压缩后所要存储的目录。   -x\u0026lt;文件\u0026gt; 指定不要处理.zip压 缩文件中的哪些文件。   -Z unzip -Z 等于执行zipinfo指 令。    2.1. 压缩 zip test.zip filename # 压缩文件 zip -r test.zip test/ # 打包test目录下的文件 zip -rj test.zip test/ # 打包test目录下文件，且压缩包不带test目录  压缩时如果需要对压缩包进行加密，可使用-P参数：\nzip -r test.zip test1 test -P 66666 # 使用密码66666加密  2.2. 解压 unzip test.zips # 解压文件 unzip -o test.zip -d dir # 将test.zip解压到dir目录  2.3. jar包 jar包是java归档包，但同样可用unzip解压查看里面的文件：\nunzip -o java.jar -d dir  3. gzip    参数 说明     -k 保留源文件   -d 解开压缩文件   -r 递归处理，将指定目录下的所有文件及子目录一并处理   -v 显示指令执行过程    tar命令带有-z参数，并且打包成tar.gz文件时，便调用gzip进行了压缩。gzip对文本的压缩率约有60%~70%，压缩包文件常以gz为后缀。使用-k参数保留源文件：\ngzip -k ./* [[当前目录下所有文件进行压缩，每个文件一个gz]]包 gzip -rkv ./* 递归压缩  解压也很简单：\ngzip -dv test.gz  4. bzip2 tar命令使用-j参数将文件打包为tar.bz2时，便调用了bzip2进行压缩。bzip2压缩或解压后，会将源文件删除。如果需要保留源文件，可使用-k参数:\nbzip2 -zk test [[压缩test]]文件 bzip2 -dk test.bz2 #解压  5. rar/unrar rar和unrar命令并非linux发行版自带命令，需要另外安装。常见用法如下：\nrar a test.tar test [[将test文件压缩为test]].tar unrar e test.rar [[解压test]].tar  6. war    参数 说明     -c 创建war包   -v 显示过程信息   -f 指定归档文件名   -M 不创建条目的清单文件   -0 这个是阿拉伯数字，只打包不压缩的意思    jar -xvf game.war # 解压war包并存储在当前目录下 jar -cvf filename.war filename 压缩 jar -cvfM0 game.war ./ # 把当前目录下的所有文件打包成game.war  7. 压缩率比较  压缩率一般来说： tar.bz2 \u0026gt; tar.gz \u0026gt; zip \u0026gt; tar 压缩率越高，压缩以及解压的时间也就越长。 总结 对文件进行压缩能够节省磁盘空间，进行网络传输时，也能节省带宽，但是需要注意的是，空间和时间是需要根据实际应用进行权衡的。\n 参考： 看完这篇Linux下的解压缩你还不会吗？ Linux：linux下解压*压缩tar.xz、等文件方法\n","date":"2023-01-08","permalink":"/post/compress/","tags":["Tools"],"title":"compress"},{"content":"陆陆续续使用了不少 linux 发行版的不同桌面环境，对几个常见的桌面环境也算是有了些使用心得，简单记录一下。\n KDE  基于 Qt，桌面环境提供了丰富的可选项，可直接通过图形化的方式配置、调整自己的桌面样式、配色。DIY 自己心仪的独一无二的桌面。\nGnome  Gnome 的相对 KDE 来说，少了很多基本选项，不过可以通过拓展来增添功能，但也很有限，这次使用的 Gnome 版本为 42.6 ，在添加拓展的时候发现很多拓展已经不被支持了。\n安装的方式也比较奇怪，通过浏览器安装插件后，再通过插件安装，以前可以下载相应文件到相应目录后编译后安装，现在通过这种方式有的拓展无法生效。\n拓展的功能也不是如想象中那样好，说一个自己比较需要的功能，顶部状态栏的隐藏，不使用拓展不能隐藏，对于小屏幕感觉空间挤压的感觉很强烈，而且在图形化编辑的软件中，又增加了一栏编辑菜单栏，就更明显了。使用拓展后，拓展的触发不是很灵敏，正常鼠标焦点移动到顶部会被呼出，实际反应很慢，甚至无反应。系统的很多东西是跟桌面环境有很强的耦合的，在不清楚的情况下，不要轻易卸载任何自带的软件，自己后期安装的软件，卸载的时候也要注意，不要触碰到原有的依赖。\n下面是一些基于 Gnome 的旧版本继续开发的，有的是 gnome2，有的是 gnome3，就不一一做介绍了。\n2.1.Unity | Ububntu 早期使用的就是 Unity，明显差异是在视觉上有些不同，使用拓展的方式，整体的性能都差不多。 2.2.MATE 2.3.Cinnamon  Xfxe  可拓展性不及 Gnome，但是自身的基本配置比较齐全，有的 Gnome 需要拓展实现的功能，比如之前提到的顶栏自动隐藏，以及剪切板（Mnajaro-Xfce 的剪切板使用体验极好），Xfce 自带了而且感觉都很好用，桌面比 Gnome 要轻量很多。但有的功能没有，比如 KDE，Gnome 都有的全局代理设置功能。\nLXDE  比 Xfce 更轻量的桌面环境，不过有点太轻量了，桌面的整体视觉氛围很简洁，甚至设置都是一些零散的工具。\nLXQt  介绍说是 LXDE 的下一代产品，但可能是新开发的，目前的观感有些不及 LXDE，不过大多数内容是基本相同的。\n不过并不能以桌面环境作为系统初始占用空间的依据，不同发行版的实际情况不太一样，内置的应用也不一样。\n以 Fedora 37 新发布的镜像包大小了解个大概。这里没有选用 Manjaro，Manjaro 的最新发行版镜像大小都在 3.0G 以上了，早期的版本也在 1.9G。\n   桌面发行版 镜像大小     Fedora KDE Live x86_64 37 2.2GB   Fedora Workstation Live x86_64 37 (Gnome) 1.9GB   Fedora Cinnamon Live x86_64 37 2.2GB   Fedora MATE_Compiz Live x86_64 37 2.1GB   Fedora Xfce Live x86_64 37 1.6GB   Fedora LXDE Live x86_64 37 1.4GB   Fedora LXQt Live x86_64 37 1.4GB    ","date":"2023-01-05","permalink":"/post/desktopenvironment/","tags":["Linux"],"title":"DesktopEnvironment"},{"content":"OS : Fedora Linux 36 (Workstation Edition) cat /etc/os-release\nNAME=\u0026quot;Fedora Linux\u0026quot; VERSION=\u0026quot;36 (Workstation Edition)\u0026quot; ID=fedora VERSION_ID=36 VERSION_CODENAME=\u0026quot;\u0026quot; PLATFORM_ID=\u0026quot;platform:f36\u0026quot; PRETTY_NAME=\u0026quot;Fedora Linux 36 (Workstation Edition)\u0026quot; ANSI_COLOR=\u0026quot;0;38;2;60;110;180\u0026quot; LOGO=fedora-logo-icon CPE_NAME=\u0026quot;cpe:/o:fedoraproject:fedora:36\u0026quot; HOME_URL=\u0026quot;https://fedoraproject.org/\u0026quot; DOCUMENTATION_URL=\u0026quot;https://docs.fedoraproject.org/en-US/fedora/f36/system-administrators-guide/\u0026quot; SUPPORT_URL=\u0026quot;https://ask.fedoraproject.org/\u0026quot; BUG_REPORT_URL=\u0026quot;https://bugzilla.redhat.com/\u0026quot; REDHAT_BUGZILLA_PRODUCT=\u0026quot;Fedora\u0026quot; REDHAT_BUGZILLA_PRODUCT_VERSION=36 REDHAT_SUPPORT_PRODUCT=\u0026quot;Fedora\u0026quot; REDHAT_SUPPORT_PRODUCT_VERSION=36 PRIVACY_POLICY_URL=\u0026quot;https://fedoraproject.org/wiki/Legal:PrivacyPolicy\u0026quot; SUPPORT_END=2023-05-16 VARIANT=\u0026quot;Workstation Edition\u0026quot; VARIANT_ID=workstation  输入法 默认的是ibus框架，直接进入输入法配置添加了Intelligent Pinyin ，每次配置 fcitx 挺麻烦，这次使用发现挺好的。\n虽然麻烦但是后面还是换成了 fcitx5，但是不要卸载 ibus，ibus 是 gnome 内置的，可能会导致桌面系统崩溃。\n换的原因是有时候会卡键，按一下后面连续输出该键的字母，敲击其他的键才会停下。\n看图软件 geeqie 还可以，但是系统的暗色主题不生效\ngwenview 还行，系统暗色主题下菜单栏会模糊化看不清字。\ngThumb 毕竟是 gnome 自己出的，主题适配很好，使用起来感觉也很棒。\ngnome-extensions Clipboard Clipboard Indicator 设置更为丰富，但是感觉不是很需要这些设置 亮点是有个星标功能，可能对着重的进行标记，会将优先级提到最前 对于常用的或者可以作为临时备忘录之类的\nClipman 可以快速将复制的内容转为二维码。\nGnome Clipboard 问题：系统 suspend 后内容会消失\nProxy Switcher 在状态栏快速设置系统代理的切换，不必再一步步进入设置去开启关闭\n问题 安装问题 安装时选择了自主分区，但是一直提示一个 “biosboot error”，就是要创建一个新的/boot/efi 分区，无法像其他系统一样通过挂载现有的分区就行。千万不要创建类型biosboot的分区，那样重启时会什么启动项都读取不到，而直接进入bios或者从u盘启动。\ngrub2 问题 manjaro的是grub管理，至少之前安装的是，安装的fedora是grub2管理，在前期是明确有两个系统的启动项的，但是后面因为grub2的分辨率问题更新了grub2,回头发现grub文件夹没了，自然也就无法启动了，资料也都备份了，新系统使用了一天也没有发现什么不合适的地方，就没有作引导的修复。也不太确定问题是不是两个grub的冲突导致的，但是如果下次遇到两个不同版本grub的系统，还是最好资料备份及引导修复重建合并的准备吧。\nupdate-grub 命令 在 grub2 中不存在 update-grub 命令了，而 update-grub 实际也是二次包装的，grub2-mkconfig -o /boot/grub2/grub.cfg可实现相同的效果,更改 grub2 配置的文件还是/etc/default/grub\n分辨率问题 启动页面选择引导的分辨率模糊，被放大了，字体模糊，文字行超出显示屏，大概确定是分辨的问题，但是后期因为被查找到的资料的误导及自己经验不足失败了很多次。 过程 1、调小分辨率，既然有这种情况，那估计是默认的分辨率大于自己的显示器分辨率或者判断错了自己的分辨率导致分辨率不对，尝试指定GRUB_GFXMODE为自己的1366x768，无效 2、自己的分辨率不被支持，某个 grub 资料说是一些分辨率不被支持，通过命令vbeinfo or videoinfo可以查看适配的分辨率。无赖进入 grub 命令行查看（非系统命令行），grub2 不存在这些命令，查文档也没有看到说有替换的。那尝试改为普遍支持的 640x480等等。无效。 3、在grub.cfg的构建文件中做修改，即/etc/grub.d/00_header，也无效。 4、尝试对grub.cfg 的分辨率的更改的所有操作都无效。一直没有去查看 grub.cfg 文件中的分辨率最初为什么，更改后为什么，或者有没有被修改，而是通过不断重启看有没有改变。想想简直哭死。/etc/grub.d/00_header文件中的构建过程是建立在一个判断结构下的。对比查看其与/etc/default/grub，又往这方面查。  /etc/default/grub中GRUB_TERMINAL_OUTPUT=console是被注释着的，也就是模式是console，但是 gfx 相关的操作生效的前提是应该为GRUB_TERMINAL_OUTPUT=\u0026quot;gfxterm\u0026quot;。\nNautilus 文件管理排序，文件、文件夹变混合排序 个性化设置可以更改，包括要显示的内容，用户、创建时间，类型等等等等。\nsublime 3 主题异常 Gnome version : GNOME Shell 42.0\n在换了系统后继续在就像使用原来的 sublime 3 时,在对 Fedora 系统设置了全局暗色主题的前提下，原本的暗色主题设置在标题栏无效。侧边栏和编辑区正常。尝试切换其他主题也是同样的结果。sublime 4 使用正常，尝试将 sublime 4 的默认主题包替换过去也没效果。查了一下可能是一些 gnome 的设置项参数在新版本调整导致的。\n近期在vscode相同异常上看到vscode对标题栏有特有的设置，查看了；sublime 3、4 的配置，sublime 4 新增加了\u0026quot;themed_title_bar\u0026quot;: true,,这个会对其有影响，而sublime 3没有类似的，可能也就无法通过简单设置去解决这个问题了。\n状态栏不显示有后台运行的应用图标 Fedora 相对于 Ubuntu 桌面更简介，一些配置是没有的，可以通过添加 gnome-extensions 来扩充、补全自己的需求。\n添加AppIndicator and KStatusNotifierItem Support插件可解决问题。\nvscode 标题栏白色（标题栏颜色不随主题更换变化） 打开设置，Title Bar Style,native更改为custom\n软件崩溃 在使用软件时界面常常崩溃，然后弹出提示页面无响应，有时候弹窗自动消失恢复，有时候整个软件崩溃，更有甚整个桌面系统崩溃，回到登陆界面。主要是 Chrome 浏览器引起的次数居多，可能不是 chrome,查的时候看到较早的版本有人是使用 firefox 经常有类似的问题，可能是内存资源的占用、gpu 驱动方面的问题吧，暂时没解决。\n后面进行了一次整体升级，这个问题就没出现了。\n窗口切换问题 使用默认的快捷键切换窗口的使用，同个应用开启的多个窗口会被认为是一个，切换到的是最新使用的。这并不是快捷键的问题，而是整个系统的设计。\n","date":"2023-01-05","permalink":"/post/fedora/","tags":["Linux"],"title":"Fedora"},{"content":"OS : Fedora Linux 36 (Workstation Edition) cat /etc/os-release\nNAME=\u0026quot;Fedora Linux\u0026quot; VERSION=\u0026quot;36 (Workstation Edition)\u0026quot; ID=fedora VERSION_ID=36 VERSION_CODENAME=\u0026quot;\u0026quot; PLATFORM_ID=\u0026quot;platform:f36\u0026quot; PRETTY_NAME=\u0026quot;Fedora Linux 36 (Workstation Edition)\u0026quot; ANSI_COLOR=\u0026quot;0;38;2;60;110;180\u0026quot; LOGO=fedora-logo-icon CPE_NAME=\u0026quot;cpe:/o:fedoraproject:fedora:36\u0026quot; HOME_URL=\u0026quot;https://fedoraproject.org/\u0026quot; DOCUMENTATION_URL=\u0026quot;https://docs.fedoraproject.org/en-US/fedora/f36/system-administrators-guide/\u0026quot; SUPPORT_URL=\u0026quot;https://ask.fedoraproject.org/\u0026quot; BUG_REPORT_URL=\u0026quot;https://bugzilla.redhat.com/\u0026quot; REDHAT_BUGZILLA_PRODUCT=\u0026quot;Fedora\u0026quot; REDHAT_BUGZILLA_PRODUCT_VERSION=36 REDHAT_SUPPORT_PRODUCT=\u0026quot;Fedora\u0026quot; REDHAT_SUPPORT_PRODUCT_VERSION=36 PRIVACY_POLICY_URL=\u0026quot;https://fedoraproject.org/wiki/Legal:PrivacyPolicy\u0026quot; SUPPORT_END=2023-05-16 VARIANT=\u0026quot;Workstation Edition\u0026quot; VARIANT_ID=workstation  官方的包管理器并没有 fcitx 相关的包，能不能自己编译安装没有测试。由于该版本的桌面环境是 gnome，内置了 ibus 框架，在安装使用较其他的要复杂一些。\n安裝 sudo dnf install fcitx5 fcitx5-rime im-chooser  Fedora 沒有內置的工具im-config，但是可以安装使用类似的工具im-chooser。\nim-chooser 安装后使用前有些步骤是必须的，不然会一直有异常，提示不支持当前桌面，开始时看的文章没有介绍，我还以为时工具太久没更新，跟不上gnome的升级造成的，后来去看了文档，做好准备工作后排除了异常。\n\n修改默认为繁体的问题 查看默认的配置文件cat ~/.local/share/fcitx5/rime/build/default.yaml,\n这里只选取了重要部分\nschema_list: - schema: luna_pinyin - schema: luna_pinyin_simp - schema: luna_pinyin_fluency - schema: bopomofo - schema: bopomofo_tw - schema: cangjie5 - schema: stroke - schema: terra_pinyin  第一个便是默认的，第二个就是需要的简体，修改顺序就可以了，我对于其他的不是很需要，干脆把其他的直接给注释了。修改保存后重新加载配置文件就行了。\nfcitx5 wiki，里面有很详细的安装，配置介绍，以及在一些软件上的使用问题解决方案。\nfcitx5 对 vim 支持的插件fcitx.vim\n","date":"2023-01-04","permalink":"/post/fedorainputmethod/","tags":["Tools","Input_Method"],"title":"FedoraInputMethod"},{"content":"局域网传输 Feem: 局域网文本传送 消息互传 (端对端传输，非开源产品，免费用户有广告，切到后台传输会被中断，但是有断点续传，优点感觉是可以整体传输文件夹吧)\nLANDrop: 端对端传输，对平台适配。不能消息通信。 开源软件。\nAirdroid: 剪切板同步，android10后不支持 电话、信息监听（起始感觉有些多余了） 手机相册、音乐、视频、文件查看（一些文件管理器开启wifi分享功能也可以实现） 可以登录帐号，是否可以跨局域网实现传输就不清楚了。可以的话也尽量不要使用吧，隐私很难保证。 （优点：有一个URL接口可以实现从浏览器发送链接在手机端利用默认浏览器打开，免去了二次复制粘贴的操作）\n局域网快传： 差不多的手机通过网页传输的应用，但功能很有限。不推荐。\nSweech: 打开应用启动服务后可以在其他的网页端读取、下载各文件内容及app。 （现在来增加使用感受一时想不起了，估计应该是当时UI设计比较对自己的喜好吧）\nAirMore: 支持所有 Airdroid 支持的局域网功能。 不过视频需要下载，无法在线浏览。 (剪切板不是最理想的情况但也不错了，通过在网页发送消息的方法，手机端会将消息自动复制到剪切板，但是手机到浏览器不会自动读取，是单纯发送消息的方式，某种程度上来说，内嵌了消息交互功能) 不确定是否是软件的问题，一加氧OS刷氢OS后不少软件后台待机都出现问题，待机长时间后传输的任务会中断，浏览器端刷新会相当于重新请求，但是前一次请求不会被中断，所以一直不能刷新网页，也就不能继续其他文件的操作，有时候手机一直保持前台待机，传输也会暂停。\nx-plore 不得不说功能是十分强大的，软件小巧但是各种功能一应俱全。不过传输效率相对低了一些。\nkde connect 之前使用过kdeconnect，那时没有这些需求，也就感觉一一般般，由于是端对端，连接可以保持稳定，不用考虑页面的问题，传输效率也快了。可以一次选中多个文件，但是传输是单进程的。不能保留文件夹的结构。\n播放器 MX 播放器 视频播放器\nPoweramp 音乐播放器\nToDesk 手机/电脑 远程连接电脑利器，无需帐号，稳定，多平台兼容。 现在使用RustDesk代替了。\nApk Analyzer 查看设备上的安装包的 ‘应用名’， 在一些需要查看的时候不至于那么麻烦\n简洁日历 如同名字一般干净简洁\n几何天气 一款轻便的天气应用\nES文件浏览器 功能很多，但我几乎是用来做安装包提取备份的。\nXAPKInstaller 协助安装 *.xapk 格式的安装包\n浏览器 神奇浏览器被起诉停止广告拦截功能后一直在寻找替换的，但是其他浏览器的广告拦截都不如意，使用体验也不是很好，目前仍然使用该浏览器。2022:3:26提示老版本已经完全停止使用了，必须强制更新到新版本，新版本去掉了很多功能（资源嗅探等），弃。\nvia 浏览器小巧功能也丰富，但是在一加6上使用有bug，有时缓存的页面在会被全部清理掉，遇到好几次，上次没看完的内容都找不回来了，放弃了。\nx 浏览器，ui不是很喜欢，里面的内容过于冗杂，也放弃了。\niceraven 浏览器，开源浏览器，可安装插件及油猴脚本，目前使用还不错\nrains 浏览器、嗅觉浏览器，目前在用的两个。\nchrome ，不得不说很垃圾，唯一可取的是能同步，其他功能一般般，而且之前好好的分组功能，更新给取消了，然后又加又砍，无语了都。\nSoul ,自带的阅读模式很舒服，屏蔽广告，专注于文本，还可以保存内容为文件。\nFirefox，搭配屏蔽广告插件，常用的。\nLemur Browser，Firefox 能够添加的插件有限，这个能够像pc端那样直接添加插件，虽然不知道是不是全部兼容，但是目前使用的几个还可以。而且能够快速复制url，这个功能很方便。\n其他 绿色守护 一加6刷氧OS后前台显示应用停止实际仍然在后台，该应用可查看后台运行的，进行强制停止。\nRAR 解压工具\n文档处理 电子书阅读 eboox 不显示章节信息\nReadEra 纯电子书阅读器，可以打开pdf或者其他格式的电子书。但软件的字体不完全适配中文，只有pro版本提供了自选字体的选项。\neReader Prestigio 没有前面的问题而且有我一直期待的按照文件夹分类,且能更具文件夹文件的变更。eboox有类似书架功能，但是当文件被删除时需要手动从书架上移除。\noffice 使用过一段时间的 wps，使用感受还是不错的，即使的付费版本也不贵，但是，由于后面的对用户隐私的侵犯的暴露，虽然说是在意料之中，还是有被恶心到，只能说给过机会了吧，国产软件还是谨慎使用吧。\noffice（office的手机版本，轻便简洁好用），OfficeSuit使用也还可以，但是对于文档的自动格式调整差一些，在一加6上的阅读感受不如office\n输入法 搜狗、百度、QQ 等尽可能的不要使用,如果实在要用，尽可能不要登录帐号及让程序联网。 讯飞、手心输入法（之前使用的，但是后面也爆出问题），也建议断网使用。 必应输入法的按钮大小适合，样式也可以，但是功能较少，缺少主要的如剪贴板等。\nGbord 由于是google的，需要下载语言包的时候需要使用代理，一开始切换过来不太习惯，熟练后还是很不错的,而且剪切板的记录数量太少。\ntrime，开源浏览器rime的手机版本，需要一定的配置，但目前一加6上使用体验不是很好，有明显卡顿，有时输入法长时间无法呼出，配置输入法的时候应用会卡死。\nSwiftKey 是微软开发的，使用不需要登录，但是主题需要登录后才能下载，剪切板还不错，但有个致命的问题，会在栏中一直显示推荐字词，而且自己打过的词语优先推荐，于隐私不太好。\nfcitx5 也推出了手机端的输入法软件，尝试了一下，目前还没有九宫格的版本，使用时也有一些小问题，但未来可期。\n","date":"2023-01-03","permalink":"/post/android_soft/","tags":["Phone"],"title":"Android_soft"},{"content":"在 Manjaro-KDE 和 Manjaro-XFCE 正常配置。\nfcitx 安装 sudo pacman -S --noconfirm fcitx-im kcm-fcitx fcitx-configtool  配置 fcitx\nsudo echo -e \u0026quot;export GTK_IM_MODULE=fcitx\\nexport QT_IM_MODULE=fcitx\\nexport XMODIFIERS=@im=fcitx\u0026quot;\u0026gt;\u0026gt;~/.xprofile  googlepinyin sudo pacman -S fcitx-googlepinyin\n重启后在状态栏找到键盘右键 -- config -- input method -- 添加后确认应用生效，默认的切换输入法快捷键为ctrl + 空格\n可自行安装搜狗输入法，manjaro 软件管理软件中也可以直接安装谷歌输入法。（目前用 googlepinyin，之前用搜狗有乱码，不知道为什么）\nrime rime 在 ibus 中有无法正常将待选字由纵向选择改为横向，改底层配置解决后重新部署会恢复原来的\nyay -S fictx-rime 安装即可，在 fcitx 中选择配置即可 默认的为繁体字，需要简体的F4后选择合适的即可优先简体\n默认情况下，Rime 只会列出 5 个候选项，可以通过修改 \u0026ldquo;menu/page_size\u0026rdquo; 的值来手动更改列出候选项的个数\ncd ~/.config/fcitx/rime \u0026amp;\u0026amp; rm default.yaml \u0026amp;\u0026amp; touch default.custom.yaml \u0026amp;\u0026amp; vim default.custom.yaml  后写入\npatch: \u0026quot;menu/page_size\u0026quot;: 9  默认情况下，如果输入\\，默认选择三个选项，不利于效率,希望的时输入\\，可以直接上屏中文的顿号、。\ncd ~/.config/fcitx/rime \u0026amp;\u0026amp; touch luna_pinyin.custom.yaml \u0026amp;\u0026amp; vim luna_pinyin.custom.yaml  后写入\npatch: punctuator: full_shape: # ' ' : { commit: '　' } # ',' : { commit: ， } # '.' : { commit: 。 } # '\u0026lt;' : [ '《', '〈', '«', '‹' ] # '\u0026gt;' : [ '》', '〉', '»', '›' ] # '/' : [ '／', '÷' ] # '?' : { commit: ？ } # ';' : { commit: ； } # ':' : { commit: ： } # '''' : { pair: [ '‘', '’' ] } # '\u0026quot;' : { pair: [ '“', '”' ] } # '\\' : [ '、', '＼' ] # '|' : [ '·', '｜', '§', '¦' ] # '`' : { commit: ｀ } # '~' : '～' # '!' : { commit: ！ } # '@' : { commit: ＠ } # '#' : { commit: ＃ } # '%' : { commit: ％ } # '$' : [ ￥, '$', '€', '£', '¥', '¢', '¤', ₩ ] # '^' : { commit: …… } # '\u0026amp;' : '＆' # '*' : [ ＊, ·, ・, ×, ※, ❂ ] # '(' : '（' # ')' : '）' # '-' : '－' # '_' : '——' # '+' : '＋' # '=' : [ '＝'] # '[' : [ '「', '【', '〔', '［' ] # ']' : [ '」', '】', '〕', '］' ] # '{' : [ '『', '〖', '｛' ] # '}' : [ '』', '〗', '｝' ] half_shape: ',' : { commit: '，' } '.' : { commit: '。' } '\u0026lt;' : [ 《, 〈, «, ‹, ˂, ˱ ] '\u0026gt;' : [ 》, 〉, », ›, ˃, ˲ ] '/' : [ '//', '/**/', '/', ÷, ／, '\\', ＼ ] '?' : { commit: ？ } ';' : { commit: ； } ':' : { commit: ： } '''' : { pair: [ '‘', '’' ] } '\u0026quot;' : { pair: [ '“', '”' ] } '\\' : { commit: '、' } '|' : [ '|', '｜', '§', '¦', '‖' ] '`' : { commit: '``' } # [ '``', '`', ·, ・, ‵, ‶, ‷, ′, ″, ‴, ⁗ ] '~' : [ '~', ·, ～, ˜, ˷, ⸯ, ≈, ≋, ≃, ≅, ≇, ∽, ⋍, ≌, ﹏, ﹋, ﹌ ] '!' : { commit: ！ } '@' : [ '@', ©, ®, ℗ ] # \u0026quot;@\u0026quot; '#' : '#' # [ '#', № ] '%' : [ '%', ％, '°', '℃', ‰, ‱, ℉, '℅', '℆', '℀', '℁', '⅍' ] '$' : [ ￥, '$', '€', '£', '¥', '¢', '¤', ₩ ] '^' : { commit: …… } '\u0026amp;' : '\u0026amp;' '*' : '*' # [ '*', ＊, ·, ・, ×, ※, ❂, ⁂, ☮, ☯, ☣ ] '(' : '（' # \u0026quot;（\u0026quot; ')' : '）' # \u0026quot;）\u0026quot; '-' : '-' # \u0026quot;－\u0026quot; '_' : [ '——', '_' ] '+' : '+' # \u0026quot;＋\u0026quot; '=' : '=' '[' : '[' ']' : ']' '{' : '{' # \u0026quot;｛\u0026quot; '}' : '}' # \u0026quot;｝\u0026quot; symbols: #符號、電腦 '/fh': [ ©, ®, ℗, ℠, ™, ℡, ℻, ☇, ☈, ☉, ☊, ☋, ☌, ☍, ☎, ☏, ☐, ☑, ☒, ☓, ☕, ☖, ☗, ⛉, ⛊, ☘, ☙, ☚, ☛, ☜, ☝, ☞, ☟, ☠, ☡, ☢, ☣, ☤, ☥, ☦, ☧, ☨, ☩, ☪, ☫, ☬, ☭, ☮, ☯, ☸, ♨, ♰, ♱, ♲, ♳, ♴, ♵, ♶, ♷, ♸, ♹, ♺, ♻, ♼, ♽, ♾, ♿, ⚆, ⚇, ⚈, ⚉, ⚐, ⚑, ⚒, ⚓, ⚔, ⚕, ⚖, ⚗, ⚘, ⚙, ⚚, ⚛, ⚜, ⚝, ⚞, ⚟, ⚠, ⚡, ⚰, ⚱, ⚲, ⚳, ⚴, ⚵, ⚶, ⚷, ⚸, ⚹, ⚺, ⚻, ⚼, ⚽, ⚾, ⚿, ⛀, ⛁, ⛂, ⛃, ⛋, ⛌, ⛍, ⛎, ⛏, ⛐, ⛑, ⛒, ⛓, ⛔, ⛕, ⛖, ⛗, ⛘, ⛙, ⛚, ⛛, ⛜, ⛝, ⛞, ⛟, ⛠, ⛡, ⛢, ⛣, ⛨, ⛩, ⛪, ⛫, ⛬, ⛭, ⛮, ⛯, ⛰, ⛱, ⛲, ⛳, ⛴, ⛵, ⛶, ⛷, ⛸, ⛹, ⛺, ⛻, ⛼, ⛽, ⛾, ⛿ ] '/dn': [ , ❖, ◁, ⌘, ⌥, ⎇, ⇧, ⇪, ↩, ⌅, ⌤, ⌫, ⌦, ⌧, ⌨, ⌀, ⌖, ⌗, ⏏, ↖, ↘, ⇞, ⇟, ⌚, ⏰, ⏱, ⏲, ⏳, ⌛, ⌜, ⌝⌞⌟, ⍑, ⏩, ⏪, ⏫, ⏬, ⏭, ⏮, ⏯ ] #象棋、麻將、色子、撲克 '/xq': [ ♔, ♕, ♖, ♗, ♘, ♙, ♚, ♛, ♜, ♝, ♞, ♟ ] '/mj': [ 🀀, 🀁, 🀂, 🀃, 🀄, 🀅, 🀆, 🀇, 🀈, 🀉, 🀊, 🀋, 🀌, 🀍, 🀎, 🀏, 🀐, 🀑, 🀒, 🀓, 🀔, 🀕, 🀖, 🀗, 🀘, 🀙, 🀚, 🀛, 🀜, 🀝, 🀞, 🀟, 🀠, 🀡, 🀢, 🀣, 🀤, 🀥, 🀦, 🀧, 🀨, 🀩, 🀪, 🀫 ] '/sz': [ ⚀, ⚁, ⚂, ⚃, ⚄, ⚅ ] '/pk': [ ♠, ♡, ♢, ♣, ♤, ♥, ♦, ♧ ] #表情 '/bq': [ ☻, ☺, ☹ ] #天氣 '/tq': [ ☀, ☁, ⛅, ⛈, ⛆, ☂, ☔, ☃, ⛄, ⛇ ] #音樂 '/yy': [ 𝄞, ♩, ♪, ♫, ♬, ♭, ♮, ♯ ] #兩性 '/lx': [ ♂, ♀, ⚢, ⚣, ⚤, ⚥, ⚦, ⚧, ⚨, ⚩, ⚪, ⚫, ⚬, ⚭, ⚮, ⚯ ] #八卦、八卦名、六十四卦、六十四卦名、太玄經 '/bg': [ ☰, ☱, ☲, ☳, ☴, ☵, ☶, ☷ ] '/bgm': [ 乾, 兌, 離, 震, 巽, 坎, 艮, 坤 ] '/lssg': [ ䷀, ䷁, ䷂, ䷃, ䷄, ䷅, ䷆, ䷇, ䷈, ䷉, ䷊, ䷋, ䷌, ䷍, ䷎, ䷏, ䷐, ䷑, ䷒, ䷓, ䷔, ䷕, ䷖, ䷗, ䷘, ䷙, ䷚, ䷛, ䷜, ䷝, ䷞, ䷟, ䷠, ䷡, ䷢, ䷣, ䷤, ䷥, ䷦, ䷧, ䷨, ䷩, ䷪, ䷫, ䷬, ䷭, ䷮, ䷯, ䷰, ䷱, ䷲, ䷳, ䷴, ䷵, ䷶, ䷷, ䷸, ䷹, ䷺, ䷻, ䷼, ䷽, ䷾, ䷿ ] '/lssgm': [ 乾, 坤, 屯, 蒙, 需, 訟, 師, 比, 小畜, 履, 泰, 否, 同人, 大有, 謙, 豫, 隨, 蠱, 臨, 觀, 噬嗑, 賁, 剝, 復, 无妄, 大畜, 頤, 大過, 坎, 離, 咸, 恆, 遯, 大壯, 晉, 明夷, 家人, 睽, 蹇, 解, 損, 益, 夬, 姤, 萃, 升, 困, 井, 革, 鼎, 震, 艮, 漸, 歸妹, 豐, 旅, 巽, 兌, 渙, 節, 中孚, 小過, 既濟, 未濟 ] '/txj': [ ⚊, ⚋, ⚌, ⚍, ⚎, ⚏, 𝌀, 𝌁, 𝌂, 𝌃, 𝌄, 𝌅, 𝌆, 𝌇, 𝌈, 𝌉, 𝌊, 𝌋, 𝌌, 𝌍, 𝌎, 𝌏, 𝌐, 𝌑, 𝌒, 𝌓, 𝌔, 𝌕, 𝌖, 𝌗, 𝌘, 𝌙, 𝌚, 𝌛, 𝌜, 𝌝, 𝌞, 𝌟, 𝌠, 𝌡, 𝌢, 𝌣, 𝌤, 𝌥, 𝌦, 𝌧, 𝌨, 𝌩, 𝌪, 𝌫, 𝌬, 𝌭, 𝌮, 𝌯, 𝌰, 𝌱, 𝌲, 𝌳, 𝌴, 𝌵, 𝌶, 𝌷, 𝌸, 𝌹, 𝌺, 𝌻, 𝌼, 𝌽, 𝌾, 𝌿, 𝍀, 𝍁, 𝍂, 𝍃, 𝍄, 𝍅, 𝍆, 𝍇, 𝍈, 𝍉, 𝍊, 𝍋, 𝍌, 𝍍, 𝍎, 𝍏, 𝍐, 𝍑, 𝍒, 𝍓, 𝍔, 𝍕, 𝍖 ] #天體、星座、星座名、十二宮 '/tt': [ ☄, ☼, ☽, ☾, ☿, ♀, ♁, ♂, ♃, ♄, ♅, ♆, ♇ ] '/xz': [ ♈, ♉, ♊, ♋, ♌, ♍, ♎, ♏, ♐, ♑, ♒, ♓ ] '/xzm': [ 白羊座, 金牛座, 雙子座, 巨蟹座, 獅子座, 室女座, 天秤座, 天蠍座, 人馬座, 摩羯座, 寶瓶座, 雙魚座 ] '/seg': [ 白羊宮, 金牛宮, 雙子宮, 巨蟹宮, 獅子宮, 室女宮, 天秤宮, 天蠍宮, 人馬宮, 摩羯宮, 寶瓶宮, 雙魚宮 ] #星號 '/xh': [ ★, ☆, ⛤, ⛥, ⛦, ⛧, ✡, ❋, ❊, ❉, ❈, ❇, ❆, ❅, ❄, ❃, ❂, ❁, ❀, ✿, ✾, ✽, ✼, ✻, ✺, ✹, ✸, ✷, ✶, ✵, ✴, ✳, ✲, ✱, ✰, ✯, ✮, ✭, ✬, ✫, ✪, ✩, ✧, ✦, ✥, ✤, ✣, ✢ ] #方塊 '/fk': [ ▀, ▁, ▂, ▃, ▄, ▅, ▆, ▇, █, ▉, ▊, ▋, ▌, ▍, ▎, ▏, ▐, ░, ▒, ▓, ▔, ▕, ▖, ▗, ▘, ▙, ▚, ▛, ▜, ▝, ▞, ▟ ] #幾何 '/jh': [ ■, □, ▢, ▣, ▤, ▥, ▦, ▧, ▨, ▩, ▪, ▫, ▬, ▭, ▮, ▯, ▰, ▱, ▲, △, ▴, ▵, ▶, ▷, ▸, ▹, ►, ▻, ▼, ▽, ▾, ▿, ◀, ◁, ◂, ◃, ◄, ◅, ◆, ◇, ◈, ◉, ◊, ○, ◌, ◍, ◎, ●, ◐, ◑, ◒, ◓, ◔, ◕, ◖, ◗, ◘, ◙, ◚, ◛, ◜, ◝, ◞, ◟, ◠, ◡, ◢, ◣, ◤, ◥, ◦, ◧, ◨, ◩, ◪, ◫, ◬, ◭, ◮, ◯, ◰, ◱, ◲, ◳, ◴, ◵, ◶, ◷, ◸, ◹, ◺, ◻, ◼, ◽, ◾, ◿ ] #箭頭 '/jt': [ →, ←, ↑, ↓, ↚, ↛, ↔, ↮, ↕, ↖, ↗, ↘, ↙, ↜, ↝, ↞, ↟, ↠, ↡, ↢, ↣, ↤, ↥, ↦, ↧, ↨, ↩, ↪, ↫, ↬, ↭, ↯, ↰, ↱, ↲, ↳, ↴, ↵, ↶, ↷, ↸, ↹, ↺, ↻, ↼, ↽, ↾, ↿, ⇀, ⇁, ⇂, ⇃, ⇄, ⇅, ⇆, ⇇, ⇈, ⇉, ⇊, ⇋, ⇌, ⇐, ⇍, ⇑, ⇒, ⇏, ⇓, ⇔, ⇎, ⇕, ⇖, ⇗, ⇘, ⇙, ⇚, ⇛, ⇜, ⇝, ⇞, ⇟, ⇠, ⇡, ⇢, ⇣, ⇤, ⇥, ⇦, ⇧, ⇨, ⇩, ⇪, ⇫, ⇬, ⇭, ⇮, ⇯, ⇰, ⇱, ⇲, ⇳, ⇴, ⇵, ⇶, ⇷, ⇸, ⇹, ⇺, ⇻, ⇼, ⇽, ➔, ➘, ➙, ➚, ➛, ➜, ➝, ➞, ➟, ➠, ➡, ➢, ➣, ➤, ➥, ➦, ➧, ➨, ➩, ➪, ➫, ➬, ➭, ➮, ➱, ➲, ➳, ➴, ➵, ➶, ➷, ➸, ➹, ➺, ➻, ➼, ➽, ➾ ] #數學 '/sx': [ ︴, －, ∈, ∏, ∑, ＋, ±, ÷, ×, ＜, ≮, ＝, ≠, ＞, ≯, ∕, √, ∝, ∞, ∟, ∠, ∥, ∧, ∨, ∩, ∪, ∫, ∮, ∴, ∵, ∷, ∽, ≈, ≌, ≒, ≡, ≤, ≥, ≦, ≧, ⊕, ⊙, ⊥, ⊿, ㏑, ㏒ ] #數字+圈/弧/點 '/szq': [ ⓪, ①, ②, ③, ④, ⑤, ⑥, ⑦, ⑧, ⑨, ⑩, ⑪, ⑫, ⑬, ⑭, ⑮, ⑯, ⑰, ⑱, ⑲, ⑳, ㉑, ㉒, ㉓, ㉔, ㉕, ㉖, ㉗, ㉘, ㉙, ㉚, ㉛, ㉜, ㉝, ㉞, ㉟, ㊱, ㊲, ㊳, ㊴, ㊵, ㊶, ㊷, ㊸, ㊹, ㊺, ㊻, ㊼, ㊽, ㊾, ㊿, ⓿, ❶, ❷, ❸, ❹, ❺, ❻, ❼, ❽, ❾, ❿, ⓫, ⓬, ⓭, ⓮, ⓯, ⓰, ⓱, ⓲, ⓳, ⓴ ] '/szh': [ ⑴, ⑵, ⑶, ⑷, ⑸, ⑹, ⑺, ⑻, ⑼, ⑽, ⑾, ⑿, ⒀, ⒁, ⒂, ⒃, ⒄, ⒅, ⒆, ⒇ ] '/szd': [ ⒈, ⒉, ⒊, ⒋, ⒌, ⒍, ⒎, ⒏, ⒐, ⒑, ⒒, ⒓, ⒔, ⒕, ⒖, ⒗, ⒘, ⒙, ⒚, ⒛ ] #字母+圈/弧 '/zmq': [ ⓐ, Ⓐ, ⓑ, Ⓑ, ⓒ, Ⓒ, ⓓ, Ⓓ, ⓔ, Ⓔ, ⓕ, Ⓕ, ⓖ, Ⓖ, ⓗ, Ⓗ, ⓘ, Ⓘ, ⓙ, Ⓙ, ⓚ, Ⓚ, ⓛ, Ⓛ, ⓜ, Ⓜ, ⓝ, Ⓝ, ⓞ, Ⓞ, ⓟ, Ⓟ, ⓠ, Ⓠ, ⓡ, Ⓡ, ⓢ, Ⓢ, ⓣ, Ⓣ, ⓤ, Ⓤ, ⓥ, Ⓥ, ⓦ, Ⓦ, ⓧ, Ⓧ, ⓨ, Ⓨ, ⓩ, Ⓩ ] '/zmh': [ ⒜, ⒝, ⒞, ⒟, ⒠, ⒡, ⒢, ⒣, ⒤, ⒥, ⒦, ⒧, ⒨, ⒩, ⒪, ⒫, ⒬, ⒭, ⒮, ⒯, ⒰, ⒱, ⒲, ⒳, ⒴, ⒵ ] #數字、分數 '/0': [ 〇, 零, ₀, ⁰, ⓪, ⓿ , ０] '/1': [ 一, 壹, ₁, ¹, Ⅰ, ⅰ, ①, ➀, ❶, ➊, ⓵, ⑴, ⒈, １, ㊀, ㈠, 弌, 壱, 幺, ㆒ ] '/2': [ 二, 貳, ₂, ², Ⅱ, ⅱ, ②, ➁, ❷, ➋, ⓶, ⑵, ⒉, ２, ㊁, ㈡, 弍, 弐, 貮, 㒃, 㒳, 兩, 倆, ㆓] '/3': [ 三, 叄, ₃, ³, Ⅲ, ⅲ, ③, ➂, ❸, ➌, ⓷, ⑶, ⒊, ３, ㊂, ㈢, 參, 参, 叁, 弎, 仨, ㆔] '/4': [ 四, 肆, ₄, ⁴, Ⅳ, ⅳ, ④, ➃, ❹, ➍, ⓸, ⑷, ⒋, ４, ㊃, ㈣, 亖] '/5': [ 五, 伍, ₅, ⁵, Ⅴ, ⅴ, ⑤, ➄, ❺, ➎, ⓹, ⑸, ⒌, ５, ㊄, ㈤, 㐅, 㠪, 𠄡 ] '/6': [ 六, 陸, ₆, ⁶, Ⅵ, ⅵ, ⑥, ➅, ❻, ➏, ⓺, ⑹, ⒍, ６, ㊅, ㈥, ↅ] '/7': [ 七, 柒, ₇, ⁷, Ⅶ, ⅶ, ⑦, ➆, ❼, ➐, ⓻, ⑺, ⒎, ７, ㊆, ㈦, 漆] '/8': [ 八, 捌, ₈, ⁸, Ⅷ, ⅷ, ⑧, ➇, ❽, ➑, ⓼, ⑻, ⒏, ８, ㊇, ㈧ ] '/9': [ 九, 玖, ₉, ⁹, Ⅸ, ⅸ, ⑨, ➈, ❾, ➒, ⓽, ⑼, ⒐, ９, ㊈, ㈨ ] '/10': [ 十, 拾, ₁₀, ¹⁰, Ⅹ, ⅹ, ⑩, ➉, ❿, ➓, ⓾, ⑽, ⒑, １０, ㊉, ㈩, 什 ] '/fs': [ ⅟, ½, ↉, ⅓, ⅔, ¼, ⅕, ⅖, ⅗, ⅘, ⅙, ⅚, ⅐, ⅛, ⅜, ⅝, ⅞, ⅑, ⅒ ] #蘇州碼 '/szm': [ 〡, 〢, 〣, 〤, 〥, 〦, 〧, 〨, 〩, 〸, 〹, 〺 ] #羅馬數字 '/lm': [ ⅰ, ⅱ, ⅲ, ⅳ, ⅴ, ⅵ, ⅶ, ⅷ, ⅸ, ⅹ, ⅺ, ⅻ, ⅼ, ⅽ, ⅾ, ⅿ ] '/lmd': [ Ⅰ, Ⅱ, Ⅲ, Ⅳ, Ⅴ, Ⅵ, Ⅶ, Ⅷ, Ⅸ, Ⅹ, Ⅺ, Ⅻ, Ⅼ, Ⅽ, Ⅾ, Ⅿ ] #拉丁 '/a': [ ₐ, ᵃ, ª, ᵄ, á, à, ȧ, â, ä, ǎ, ă, ā, ã, å, ą, ⱥ, ấ, ầ, ắ, ằ, ǡ, ǻ, ǟ, ẫ, ẵ, ả, ȁ, ȃ, ẩ, ẳ, ᶏ, ạ, ḁ, ậ, ẚ, ặ, ɐ, ɑ, ɒ, ᶛ, ᵅ ] '/b': [ ᵇ, ḃ, ᵬ, ƀ, ɓ, ᶀ, ḅ, ḇ, ƃ, ᵦ, ᵝ, β ] '/c': [ ᶜ, ć, ċ, ĉ, č, ç, ȼ, ḉ, ƈ, ᵓ, ɔ, ᶗ, ɕ, ᶝ ] '/d': [ ᵈ, ḋ, ď, ď, ᵭ, ḑ, đ, ƌ, ɗ, ᶁ, ḍ, ᶑ, ḓ, ḏ, ð, ʤ, ƍ, ᶞ, ǳ, ǆ, ɖ, ʣ, ʥ, ȡ, ẟ ] '/e': [ ₑ, ᵉ, é, è, ė, ê, ë, ě, ĕ, ē, ẽ, ę, ȩ, ɇ, ế, ề, ḗ, ḕ, ễ, ḝ, ẻ, ȅ, ȇ, ể, ẹ, ᶒ, ḙ, ḛ, ᶟ, ệ, ɛ, ǝ, ə, ₔ, ᵊ, ɚ, ɘ, ɜ, ɝ, ɞ, ʚ, ȝ, ᶾ, ᶕ, ᶚ, ᴈ, ᶓ, ᶔ, ᵋ, ᵌ, ⱸ ] '/f': [ ᶠ, ḟ, ᵮ, ƒ, ᶂ, ﬀ, ﬃ, ﬄ, ﬁ, fʲ, ﬂ, ʩ, ɟ, ɸ, ᶲ, ᵩ, ᵠ ] '/g': [ ᵍ, ᵷ, ǵ, ġ, ĝ, ǧ, ğ, ḡ, ģ, ǥ, ɠ, ᶃ, ɣ, ᶢ, ɡ, ˠ, ᵧ, ᵞ ] '/h': [ ͪ, ḣ, ĥ, ḧ, ȟ, ḩ, ħ, ɦ, ḥ, ḫ, ẖ, ⱨ, ɥ, ᶣ, ʱ, ƕ, ʮ, ʯ, ꜧ, ɧ ] '/i': [ ᵢ, ı, ᴉ, í, ì, î, ï, ǐ, ĭ, ī, ĩ, į, ɨ, ḯ, ᶤ, ỉ, ȉ, ȋ, ị, ᶖ, ḭ, ᵎ, ɩ, ᶥ, ᵼ, ĳ ] '/j': [ ⱼ, ʲ, ȷ, ĵ, ǰ, ɉ, ɟ, ᶡ, ʄ, ᶨ, ʝ ] '/k': [ ᵏ, ḱ, ǩ, ķ, ƙ, ᶄ, ḳ, ḵ, ⱪ, ʞ ] '/l': [ ˡ, ĺ, ŀ, ľ, ɫ, ⱡ, ļ, ƚ, ł, ƛ, ᶅ, ᶪ, ᶩ, ḷ, ɭ, ḽ, ḻ, ḹ, ɬ, ɮ, ǉ, ỻ, ʪ, ʫ, ȴ ] '/m': [ ᵐ, ḿ, ṁ, ᵯ, ᶬ, ɱ, ᶆ, ṃ, ɯ, ᵚ, ɰ, ᶭ, ᴟ ] '/n': [ ⁿ, ń, ǹ, ṅ, ň, ñ, ᵰ, ņ, ᶮ, ɲ, ŉ, ƞ, ᶇ, ṇ, ɳ, ᶯ, ṋ, ṉ, ȵ ] '/o': [ ₒ, ᵒ, º, ó, ò, ȯ, ô, ö, ǒ, ŏ, ō, õ, ǫ, ő, ố, ồ, ɵ, ø, ṓ, ṑ, ȱ, ṍ, ȫ, ổ, ọ, ớ, ờ, ỡ, ộ, ɷ, ở, ợ, ᵔ, ᵕ, œ, ȣ, ᴔ, ⱺ ] '/p': [ ᵖ, ṕ, ṗ, ᵱ, ᵽ, ƥ, ᶈ ] '/q': [ ʠ, ɋ, ȹ ] '/r': [ ᵣ, ŕ, ṙ, ř, ᵲ, ŗ, ɍ, ᵳ, ɽ, ȑ, ȓ, ᶉ, ṛ, ṟ, ṝ, ɹ, ɺ, ɻ, ɼ, ɾ, ɿ, ʳ, ʴ, ʵ, ᵨ ] '/s': [ ˢ, ś, ṡ, ŝ, š, ᵴ, ş, ṥ, ṧ, ᶳ, ʂ, ᶊ, ṣ, ș, ȿ, ṩ, ʃ, ᶴ, ƨ, ʆ, ʅ, ƪ, ß, ſ, ẛ ] '/t': [ ᵗ, ṫ, ť, ᵵ, ţ, ƭ, ᶵ, ƫ, ṭ, ʈ, ț, ṱ, ṯ, ⱦ, ʇ, ʧ, ʨ, ᶿ, ȶ, ŧ ] '/u': [ ᵤ, ᵘ, ú, ù, û, ü, ǔ, ŭ, ū, ũ, ů, ų, ű, ᶶ, ʉ, ǘ, ǜ, ǚ, ṹ, ǖ, ṻ, ủ, ȕ, ȗ, ư, ᶙ, ụ, ṳ, ứ, ừ, ṷ, ṵ, ữ, ʉ̞, ʊ, ᶷ, ᵙ, ử, ᵿ, ự, ᴝ, ᴞ, ᵫ ] '/v': [ ᵥ, ᵛ, ṽ, ᶹ, ᶌ, ṿ, ⱴ, ʋ, ᶺ, ʌ ] '/w': [ ʷ, ẃ, ẁ, ẇ, ŵ, ẅ, ẘ, ẉ, ƿ, ʍ, ⱳ ] '/x': [ ₓ, ᶍ, ˣ, χ, ᵪ, ᵡ ] '/y': [ ʸ, ý, ỳ, ẏ, ŷ, ÿ, ȳ, ỹ, ẙ, ɏ, ỷ, ƴ, ỵ, ʎ, ỿ ] '/z': [ ᶻ, ź, ż, ẑ, ž, ᵶ, ƶ, ȥ, ᶎ, ᶼ, ẓ, ʐ, ɀ, ẕ, ⱬ, ʑ, ᶽ, ʒ ] #上標、下標 '/sb': [ ⁰, ¹, ², ³, ⁴, ⁵, ⁶, ⁷, ⁸, ⁹, ˜, ⁺, ⁻, ⁼, ⁽, ⁾, ᴬ, ᵃ, ᵄ, ᵅ, ᶛ, ᴭ, ᵆ, ᴮ, ᴯ, ᵇ, ᵝ, ᶜ, ᵓ, ᶝ, ᴰ, ᵈ, ᶞ, ᵟ, ᴱ, ᵉ, ᴲ, ᵊ, ᵋ, ᶟ, ᵌ, ᶠ, ᶡ, ᶲ, ᵠ, ᴳ, ᵍ, ᶢ, ˠ, ᵞ, ᴴ, ʰ, ᶣ, ʱ, ᴵ, ⁱ, ᶤ, ᵎ, ᶥ, ᴶ, ʲ, ᶨ, ᴷ, ᵏ, ᴸ, ᶫ, ˡ, ᶩ, ᶪ, ᴹ, ᵐ, ᶬ, ᵚ, ᶭ, ᴺ, ᴻ, ⁿ, ᵑ, ᶮ, ᶯ, ᴼ, ᵒ, ᶱ, ᴽ, ᴾ, ᵖ, ᴿ, ʳ, ʶ, ʴ, ʵ, ˢ, ᶴ, ᶳ, ᵀ, ᵗ, ᶵ, ᶿ, ᵁ, ᵘ, ᶶ, ᶷ, ᵙ, ⱽ, ᵛ, ᶺ, ᶹ, ᵂ, ʷ, ˣ, ᵡ, ʸ, ᶻ, ᶾ, ᶽ, ᶼ ] '/xb': [ ₀, ₁, ₂, ₃, ₄, ₅, ₆, ₇, ₈, ₉, ₊, ₋, ₌, ₍, ₎, ‸, ᴀ, ₐ, ᴁ, ʙ, ᴃ, ᵦ, ᴄ, ᴐ, ᴒ, ᴅ, ᴆ, ᴇ, ₑ, ₔ, ᵩ, ɢ, ʛ, ᴦ, ᵧ, ʜ, ₕ, ɪ, ᵻ, ᵢ, ᴊ, ⱼ, ᴋ, ₖ, ʟ, ₗ, ᴌ, ᴧ, ᴍ, ₘ, ꟺ, ɴ, ᴎ, ₙ, ᴏ, ₒ, ɶ, ʘ, ᴓ, ᴑ, ᴘ, ₚ, ᴨ, ᴪ, ʀ, ᵣ, ᴙ, ʁ, ᴚ, ᵨ, ₛ, ᴛ, ₜ, ᴜ, ᵤ, ᵾ, ᴠ, ᵥ, ᴡ, ₓ, ᵪ, ʏ, ᴢ, ᴣ ] #希臘 '/xl': [ α, β, γ, δ, ε, ζ, η, θ, ι, κ, λ, μ, ν, ξ, ο, π, ρ, σ, ς, τ, υ, φ, χ, ψ, ω ] '/xld': [ Α, Β, Γ, Δ, Ε, Ζ, Η, Θ, Ι, Κ, Λ, Μ, Ν, Ξ, Ο, Π, Ρ, Σ, Τ, Υ, Φ, Χ, Ψ, Ω ] #俄語 '/ey': [ а, б, в, г, д, е, ё, ж, з, и, й, к, л, м, н, о, п, р, с, т, у, ф, х, ц, ч, ш, щ, ъ, ы, ь, э, ю, я ] '/eyd': [ А, Б, В, Г, Д, Е, Ё, Ж, З, И, Й, К, Л, М, Н, О, П, Р, С, Т, У, Ф, Х, Ц, Ч, Ш, Щ, Ъ, Ы, Ь, Э, Ю, Я ] #月份、日期、曜日等 '/yf': [ ㋀, ㋁, ㋂, ㋃, ㋄, ㋅, ㋆, ㋇, ㋈, ㋉, ㋊, ㋋ ] '/rq': [ ㏠, ㏡, ㏢, ㏣, ㏤, ㏥, ㏦, ㏧, ㏨, ㏩, ㏪, ㏫, ㏬, ㏭, ㏮, ㏯, ㏰, ㏱, ㏲, ㏳, ㏴, ㏵, ㏶, ㏷, ㏸, ㏹, ㏺, ㏻, ㏼, ㏽, ㏾ ] '/yr': [ 月, 火, 水, 木, 金, 土, 日, ㊊, ㊋, ㊌, ㊍, ㊎, ㊏, ㊐, ㊗, ㊡, ㈪, ㈫, ㈬, ㈭, ㈮, ㈯, ㈰, ㈷, ㉁, ㉀ ] #時間 '/sj': [ ㍘, ㍙, ㍚, ㍛, ㍜, ㍝, ㍞, ㍟, ㍠, ㍡, ㍢, ㍣, ㍤, ㍥, ㍦, ㍧, ㍨, ㍩, ㍪, ㍫, ㍬, ㍭, ㍮, ㍯, ㍰ ] #天干、地支、干支 '/tg': [ 甲, 乙, 丙, 丁, 戊, 己, 庚, 辛, 壬, 癸 ] '/dz': [ 子, 丑, 寅, 卯, 辰, 巳, 午, 未, 申, 酉, 戌, 亥 ] '/gz': [ 甲子, 乙丑, 丙寅, 丁卯, 戊辰, 己巳, 庚午, 辛未, 壬申, 癸酉, 甲戌, 乙亥, 丙子, 丁丑, 戊寅, 己卯, 庚辰, 辛巳, 壬午, 癸未, 甲申, 乙酉, 丙戌, 丁亥, 戊子, 己丑, 庚寅, 辛卯, 壬辰, 癸巳, 甲午, 乙未, 丙申, 丁酉, 戊戌, 己亥, 庚子, 辛丑, 壬寅, 癸卯, 甲辰, 乙巳, 丙午, 丁未, 戊申, 己酉, 庚戌, 辛亥, 壬子, 癸丑, 甲寅, 乙卯, 丙辰, 丁巳, 戊午, 己未, 庚申, 辛酉, 壬戌, 癸亥 ] #節氣 '/jq': [ 立春, 雨水, 驚蟄, 春分, 清明, 穀雨, 立夏, 小滿, 芒種, 夏至, 小暑, 大暑, 立秋, 處暑, 白露, 秋分, 寒露, 霜降, 立冬, 小雪, 大雪, 冬至, 小寒, 大寒 ] #單位 '/dw': [ Å, ℃, ％, ‰, ‱, °, ℉, ㏃, ㏆, ㎈, ㏄, ㏅, ㎝, ㎠, ㎤, ㏈, ㎗, ㎙, ㎓, ㎬, ㏉, ㏊, ㏋, ㎐, ㏌, ㎄, ㎅, ㎉, ㎏, ㎑, ㏍, ㎘, ㎞, ㏎, ㎢, ㎦, ㎪, ㏏, ㎸, ㎾, ㏀, ㏐, ㏓, ㎧, ㎨, ㎡, ㎥, ㎃, ㏔, ㎆, ㎎, ㎒, ㏕, ㎖, ㎜, ㎟, ㎣, ㏖, ㎫, ㎳, ㎷, ㎹, ㎽, ㎿, ㏁, ㎁, ㎋, ㎚, ㎱, ㎵, ㎻, ㏘, ㎩, ㎀, ㎊, ㏗, ㏙, ㏚, ㎰, ㎴, ㎺, ㎭, ㎮, ㎯, ㏛, ㏜, ㎔, ㏝, ㎂, ㎌, ㎍, ㎕, ㎛, ㎲, ㎶, ㎼ ] #貨幣 '/hb': [ ￥, ¥, ¤, ￠, ＄, $, ￡, £, ৳, ฿, ₠, ₡, ₢, ₣, ₤, ₥, ₦, ₧, ₩, ₪, ₫, €, ₭, ₮, ₯, ₰, ₱, ₲, ₳, ₴, ₵, ₶, ₷, ₸, ₹, ₺, ₨, ﷼ ] #結構、偏旁、康熙（部首）、筆畫、標點 '/jg': [ ⿰, ⿱, ⿲, ⿳, ⿴, ⿵, ⿶, ⿷, ⿸, ⿹, ⿺, ⿻, 〾 ] '/pp': [ 乛, 冫, 丷, 龹, ⺌, 龸, 亻, 亼, 亽, 仒, 冖, 冂, 冃, 冄, 宀, 罒, 㓁, 罓, 冈, 凵, 厶, 刂, 勹, 匚, 匸, 卩, 阝, 厂, 丆, 广, 壬, 訁, 讠, 釒, 钅, 飠, 饣, 龺, 攵, 夂, 夊, 尢, 尣, 兂, 旡, 巜, 巛, 彐, 彑, 彡, 彳, 龰, 辶, 廴, 㞢, 忄, 㣺, 扌, 爫, 龵, 廾, 歺, 癶, 氵, 氺, 火, 灬, 爿, 丬, 疒, 牜, ⺶, 犭, 豕, 豸, 虍, 艹, 卝, 龷, 丗, 龶, 芈, 丵, 菐, 黹, 礻, 衤, 糸, 糹, 纟, 龻, 镸, 髟, 襾, 覀, 吅, 㗊, 㠭, 㸚, 叕] '/kx': [ 一, 丨, 丶, 丿, 乙, 亅, 二, 亠, 人, 儿, 入, 八, 冂, 冖, 冫, 几, 凵, 刀, 力, 勹, 匕, 匚, 匸, 十, 卜, 卩, 厂, 厶, 又, 口, 囗, 土, 士, 夂, 夊, 夕, 大, 女, 子, 宀, 寸, 小, 尢, 尸, 屮, 山, 巛, 工, 己, 巾, 干, 幺, 广, 廴, 廾, 弋, 弓, 彐, 彡, 彳, 心, 戈, 戶, 手, 支, 攴, 文, 斗, 斤, 方, 无, 日, 曰, 月, 木, 欠, 止, 歹, 殳, 毋, 比, 毛, 氏, 气, 水, 火, 爪, 父, 爻, 爿, 片, 牙, 牛, 犬, 玄, 玉, 瓜, 瓦, 甘, 生, 用, 田, 疋, 疒, 癶, 白, 皮, 皿, 目, 矛, 矢, 石, 示, 禸, 禾, 穴, 立, 竹, 米, 糸, 缶, 网, 羊, 羽, 老, 而, 耒, 耳, 聿, 肉, 臣, 自, 至, 臼, 舌, 舛, 舟, 艮, 色, 艸, 虍, 虫, 血, 行, 衣, 襾, 見, 角, 言, 谷, 豆, 豕, 豸, 貝, 赤, 走, 足, 身, 車, 辛, 辰, 辵, 邑, 酉, 釆, 里, 金, 長, 門, 阜, 隶, 隹, 雨, 靑, 非, 面, 革, 韋, 韭, 音, 頁, 風, 飛, 食, 首, 香, 馬, 骨, 高, 髟, 鬥, 鬯, 鬲, 鬼, 魚, 鳥, 鹵, 鹿, 麥, 麻, 黃, 黍, 黑, 黹, 黽, 鼎, 鼓, 鼠, 鼻, 齊, 齒, 龍, 龜, 龠 ] '/bh': [ ㇀, ㇁, ㇂, ㇃, ㇄, ㇅, ㇆, ㇇, ㇈, ㇉, ㇊, ㇋, ㇌, ㇍, ㇎, ㇏, ㇐, ㇑, ㇒, ㇓, ㇔, ㇕, ㇖, ㇗, ㇘, ㇙, ㇚, ㇛, ㇜, ㇝, ㇞, ㇟, ㇠, ㇡, ㇢, ㇣ ] '/bd': [ ₋, ⁻, ―, ˗, ˉ, ＿, ﹍, ﹎, ．, ¡, ‼, ⁉, ¿, ؟, ⁈, ⁇, ､, ｡, 、, 。, 〃, 〄, 々, 〆, 〇, 〈, 〉, 《, 》, 「, 」, 『, 』, 【, 】, 〒, 〓, 〔, 〕, 〖, 〗, 〘, 〙, 〚, 〛, 〜, 〝, 〞, 〟, 〠, 〰, 〱, 〲, 〳, 〴, 〵, 〶, 〷, 〻, 〼, 〽 ] '/bdz': [ ﹅, ﹆, ﹁, ﹂, ﹃, ﹄, ︙, ︱, ︻, ︼, ︗, ︘, ︵, ︶, ︷, ︸, ︹, ︺, ︿, ﹀, ︽, ︾, ︰, ︲, ︳, ︴, ﹉, ﹊, ﹋, ﹌, ﹍, ﹎, ﹏, ﹇, ﹈, ︐, ︑, ︒, ︔, ︕, ︖ ] #拼音、註音、聲調 '/py': [ ā, á, ǎ, à, ō, ó, ǒ, ò, ê, ē, é, ě, è, ī, í, ǐ, ì, ū, ú, ǔ, ù, ü, ǖ, ǘ, ǚ, ǜ, , ń, ň,  ] '/zy': [ ㄅ, ㄆ, ㄇ, ㄈ, ㄉ, ㄊ, ㄋ, ㄌ, ㄍ, ㄎ, ㄏ, ㄐ, ㄑ, ㄒ, ㄓ, ㄔ, ㄕ, ㄖ, ㄗ, ㄘ, ㄙ, ㄧ, ㄨ, ㄩ, ㄚ, ㄛ, ㄜ, ㄝ, ㄞ, ㄟ, ㄠ, ㄡ, ㄢ, ㄣ, ㄤ, ㄥ, ㄦ, ㄪ, ㄫ, ㄬ, ㄭ, ㆠ, ㆡ, ㆢ, ㆣ, ㆤ, ㆥ, ㆦ, ㆧ, ㆨ, ㆩ, ㆪ, ㆫ, ㆬ, ㆭ, ㆮ, ㆯ, ㆰ, ㆱ, ㆲ, ㆳ, ㆴ, ㆵ, ㆶ, ㆷ ] '/sd': [ ˉ, ˊ, ˇ, ˋ, ˆ, ˙, ˜, ˥, ˦, ˧, ˨, ˩, ꜀, ꜁, ꜂, ꜃, ꜄, ꜅, ꜆, ꜇ ] #漢字+圈/弧 '/hzq': [ ㊀, ㊁, ㊂, ㊃, ㊄, ㊅, ㊆, ㊇, ㊈, ㊉, ㊊, ㊋, ㊌, ㊍, ㊎, ㊏, ㊐, ㊑, ㊒, ㊓, ㊔, ㊕, ㊖, ㊗, ㊘, ㊙, ㊚, ㊛, ㊜, ㊝, ㊞, ㊟, ㊠, ㊡, ㊢, ㊣, ㊤, ㊥, ㊦, ㊧, ㊨, ㊩, ㊪, ㊫, ㊬, ㊭, ㊮, ㊯, ㊰, ㉄, ㉅, ㉆, ㉇ ] '/hzh': [ ㈠, ㈡, ㈢, ㈣, ㈤, ㈥, ㈦, ㈧, ㈨, ㈩, ㈪, ㈫, ㈬, ㈭, ㈮, ㈯, ㈰, ㈱, ㈲, ㈳, ㈴, ㈵, ㈶, ㈷, ㈸, ㈹, ㈺, ㈻, ㈼, ㈽, ㈾, ㈿, ㉀, ㉁, ㉂, ㉃ ] #いろは順 '/iro': [ い, ろ, は, に, ほ, へ, と, ち, り, ぬ, る, を, わ, か, よ, た, れ, そ, つ, ね, な, ら, む, う, ゐ, の, お, く, や, ま, け, ふ, こ, え, て, あ, さ, き, ゆ, め, み, し, ゑ, ひ, も, せ, す ] #假名 '/jm': [ あ, ぁ, い, ぃ, う, ぅ, え, ぇ, お, ぉ, か, ゕ, が, き, ぎ, く, ぐ, け, ゖ, げ, こ, ご, さ, ざ, し, じ, す, ず, せ, ぜ, そ, ぞ, た, だ, ち, ぢ, つ, っ, づ, て, で, と, ど, な, に, ぬ, ね, の, は, ば, ぱ, ひ, び, ぴ, ふ, ぶ, ぷ, へ, べ, ぺ, ほ, ぼ, ぽ, ま, み, む, め, も, や, ゃ, ゆ, ゅ, よ, ょ, ら, り, る, れ, ろ, わ, ゎ, ゐ, ゔ, ゑ, を, ん, ・, ー, ゝ, ゞ, ゟ ] '/pjm': [ ア, ァ, イ, ィ, ウ, ゥ, エ, ェ, オ, ォ, カ, ヵ, ガ, キ, ギ, ク, グ, ケ, ヶ, ゲ, コ, ゴ, サ, ザ, シ, ジ, ス, ズ, セ, ゼ, ソ, ゾ, タ, ダ, チ, ヂ, ツ, ッ, ヅ, テ, デ, ト, ド, ナ, ニ, ヌ, ネ, ノ, ハ, バ, パ, ヒ, ビ, ピ, フ, ブ, プ, ヘ, ベ, ペ, ホ, ボ, ポ, マ, ミ, ム, メ, モ, ヤ, ャ, ユ, ュ, ヨ, ョ, ラ, リ, ル, レ, ロ, ワ, ヮ, ヰ, ヸ, ヴ, ヱ, ヹ, ヲ, ヺ, ン, ・, ー, ヽ, ヾ, ヿ, ㇰ, ㇱ, ㇲ, ㇳ, ㇴ, ㇵ, ㇶ, ㇷ, ㇸ, ㇹ, ㇺ, ㇻ, ㇼ, ㇽ, ㇾ, ㇿ ] '/jmk': [ か, ゕ, き, く, け, ゖ, こ, カ, ヵ, キ, ク, ケ, ヶ, コ ] '/jmg': [ が, ぎ, ぐ, げ, ご, ガ, ギ, グ, ゲ, ゴ ] '/jms': [ さ, し, す, せ, そ, サ, シ, ス, セ, ソ ] '/jmz': [ ざ, じ, ず, ぜ, ぞ, ザ, ジ, ズ, ゼ, ゾ ] '/jmt': [ た, ち, つ, っ, て, と, タ, チ, ツ, ッ, テ, ト ] '/jmd': [ だ, ぢ, づ, で, ど, ダ, ヂ, ヅ, デ, ド ] '/jmn': [ な, に, ぬ, ね, の, ん, ナ, ニ, ヌ, ネ, ノ, ン ] '/jmh': [ は, ひ, ふ, へ, ほ, ハ, ヒ, フ, ヘ, ホ ] '/jmb': [ ば, び, ぶ, べ, ぼ, バ, ビ, ブ, ベ, ボ ] '/jmp': [ ぱ, ぴ, ぷ, ぺ, ぽ, パ, ピ, プ, ペ, ポ ] '/jmm': [ ま, み, む, め, も, マ, ミ, ム, メ, モ ] '/jmy': [ や, ゃ, ゆ, ゅ, よ, ょ, ヤ, ャ, ユ, ュ, ヨ, ョ ] '/jmr': [ ら, り, る, れ, ろ, ラ, リ, ル, レ, ロ ] '/jmw': [ わ, ゐ, ゑ, を, ワ, ヰ, ヱ, ヲ ] '/jma': [ あ, か, が, さ, ざ, た, だ, な, は, ば, ぱ, ま, や, ら, わ, ア, カ, ガ, サ, ザ, タ, ダ, ナ, ハ, バ, パ, マ, ヤ, ラ, ワ ] '/jmi': [ い, き, ぎ, し, じ, ち, ぢ, に, ひ, び, ぴ, み, り, ゐ, イ, キ, ギ, シ, ジ, チ, ヂ, ニ, ヒ, ビ, ピ, ミ, リ, ヰ ] '/jmu': [ う, く, ぐ, す, ず, つ, づ, ぬ, ふ, ぶ, ぷ, む, る, ウ, ク, グ, ス, ズ, ツ, ヅ, ヌ, フ, ブ, プ, ム, ル ] '/jme': [ え, け, げ, せ, ぜ, て, で, ね, へ, べ, ぺ, め, れ, ゑ, エ, ケ, ゲ, セ, ゼ, テ, デ, ネ, ヘ, ベ, ペ, メ, レ, ヱ ] '/jmo': [ お, こ, ご, そ, ぞ, と, ど, の, ほ, ぼ, ぽ, も, ろ, を, オ, コ, ゴ, ソ, ゾ, ト, ド, ノ, ホ, ボ, ポ, モ, ロ, ヲ ] #假名+圈 '/jmq': [ ㋐, ㋑, ㋒, ㋓, ㋔, ㋕, ㋖, ㋗, ㋘, ㋙, ㋚, ㋛, ㋜, ㋝, ㋞, ㋟, ㋠, ㋡, ㋢, ㋣, ㋤, ㋥, ㋦, ㋧, ㋨, ㋩, ㋪, ㋫, ㋬, ㋭, ㋮, ㋯, ㋰, ㋱, ㋲, ㋳, ㋴, ㋵, ㋶, ㋷, ㋸, ㋹, ㋺, ㋻, ㋼, ㋽, ㋾ ] #假名+半角 '/jmbj': [ ｱ, ｧ, ｲ, ｨ, ｳ, ｩ, ｴ, ｪ, ｵ, ｫ, ｶ, ｷ, ｸ, ｹ, ｺ, ｻ, ｼ, ｽ, ｾ, ｿ, ﾀ, ﾁ, ﾂ, ｯ, ﾃ, ﾄ, ﾅ, ﾆ, ﾇ, ﾈ, ﾉ, ﾊ, ﾋ, ﾌ, ﾍ, ﾎ, ﾏ, ﾐ, ﾑ, ﾒ, ﾓ, ﾔ, ｬ, ﾕ, ｭ, ﾖ, ｮ, ﾗ, ﾘ, ﾙ, ﾚ, ﾛ, ﾜ, ｦ, ﾝ, ･, ｰ, ﾞ, ﾟ ] #韓文 '/hw': [ ㄱ, ㄴ, ㄷ, ㄹ, ㅁ, ㅂ, ㅅ, ㅇ, ㅈ, ㅊ, ㅋ, ㅌ, ㅍ, ㅎ ] #韓文+圈/弧 '/hwq': [ ㉠, ㉡, ㉢, ㉣, ㉤, ㉥, ㉦, ㉧, ㉨, ㉩, ㉪, ㉫, ㉬, ㉭, ㉮, ㉯, ㉰, ㉱, ㉲, ㉳, ㉴, ㉵, ㉶, ㉷, ㉸, ㉹, ㉺, ㉻, ㉼, ㉽, ㉾, ㉿ ] '/hwh': [ ㈀, ㈁, ㈂, ㈃, ㈄, ㈅, ㈆, ㈇, ㈈, ㈉, ㈊, ㈋, ㈌, ㈍, ㈎, ㈏, ㈐, ㈑, ㈒, ㈓, ㈔, ㈕, ㈖, ㈗, ㈘, ㈙, ㈚, ㈛, ㈜, ㈝, ㈞ ] '/zz': [\u0026quot;→_→\u0026quot;, \u0026quot;←_←\u0026quot;, \u0026quot;O(∩_∩)O\u0026quot;, \u0026quot;╮(╯▽╰)╭\u0026quot;, \u0026quot;┑(￣▽ ￣)┍ \u0026quot;, \u0026quot;(≧﹏ ≦)\u0026quot;, \u0026quot;(￣ε ￣) \u0026quot;, \u0026quot;(*▔＾▔*)\u0026quot;, \u0026quot;O__O\u0026quot;, \u0026quot;(^_^)\u0026quot;, \u0026quot;(╰_╯)\u0026quot;, \u0026quot;ψ(￣︶￣)ψ\u0026quot;, \u0026quot;-_-||\u0026quot;, \u0026quot;(╯#-_-)╯╧═╧\u0026quot;, \u0026quot;◔‸◔\u0026quot;, \u0026quot;= =囧~~\u0026quot;, \u0026quot;•﹏•\u0026quot;] '/wz': [\u0026quot;zhihu.com\u0026quot;, \u0026quot;google.com\u0026quot;, \u0026quot;gmail.com\u0026quot;, \u0026quot;mail.qq.com\u0026quot;, \u0026quot;zNote.html\u0026quot; ]  fcitx5 需要卸載之前的fcitxsudo pacman -Rsc fcitx\n安裝fcitx5-im,包括了fcitx5、fcitx5-configtool、fcitx5-gtk、fcitx5-qt\nsudo pacman -S fcitx5-im # fcitx5-im 包括了 fcitx5、fcitx5-configtool、fcitx5-gtk、fcitx5-qt yay -S fcitx5-rime  配置和fcitx的差不多，位置有所不同，在~/.local/share/fcitx5/rime/ 主题：\ncd /home/fiki/.local/share/fcitx5/themes \u0026amp;\u0026amp; mkdir Material-Color \u0026amp;\u0026amp; cd Material-Color git clone https://github.com/hosxy/Fcitx5-Material-Color.git ~/.local/share/fcitx5/themes/Material-Color ln -sf theme-blue.conf theme.conf cd ~/.config/fcitx5/conf \u0026amp;\u0026amp; vim classicui.conf # 将最后的 theme 更改 Material-Color 即可  候选的字体太小，在~/.config/fcitx5/conf/classicui.conf修改font、menufont的字号即可，建议‘PerScreenDPI=False‘\n# Vertical Candidate List Vertical Candidate List=False # Use Per Screen DPI PerScreenDPI=True # Use mouse wheel to go to prev or next page WheelForPaging=True # Font Font=\u0026quot;Sans 12\u0026quot; # Menu Font MenuFont=\u0026quot;Sans 12\u0026quot; # Tray Font TrayFont=\u0026quot;Sans Bold 10\u0026quot; # Tray Label Outline Color TrayOutlineColor=#000000 # Tray Label Text Color TrayTextColor=#ffffff # Prefer Text Icon PreferTextIcon=False # Show Layout Name In Icon ShowLayoutNameInIcon=True # Use input method language to display text UseInputMethodLangaugeToDisplayText=True # Theme Theme=Material-Color  其他 fcitx 修改 luna_pinyin.custom.yaml 可以对全半角符号进行修改且有效，但是在换到 fcitx5 后就无效了，建议先去看 user.yaml 文件的内容 比如内容为:\nvar: last_build_time: 1655690864 previously_selected_schema: luna_pinyin_simp schema_access_time: luna_pinyin_simp: 1647270274  则文件名为 luna_pinyin_simp.custom.yaml\n","date":"2023-01-02","permalink":"/post/manjaroinputmethod/","tags":["Tools","Input_Method"],"title":"ManjaroInputMethod"},{"content":"[toc]\nWin10 win10配置 一、系统安装 WinPE安装比较直观、简单的，过程先放放吧，忘记了 激活：\n二、软件 看图 Honeyview(蜂蜜浏览器) 搭配bandizip看图简直不要太爽，唯一不足的是只能作为一款纯看图软件，裁剪什么的很鸡肋(版本5.31) FastStone Capture Viewer 预加载，内置文件管理器功能，丰富的编辑功能，安装后占用空间下只有19M 解压 bandizip 很干净的解压软件，效率什么的也有保障，但是有些功能个人觉得有些别扭，选中多个文件打开会一个文件一个窗口，如果都需要密码，不能一次解决，配合winrar绿色版刚刚好\n两款搭配着使用最好\n播放器 potpaly 强大的看视频软件，会配置的看国外流的很不错\nmindmanager： 思维导图工具 不知道官方版本的能不能用激活码激活，下载的第三方的版本，能够激活破解获取所有功能，文件(MindManager2020.emsi) 破解： 安装后输入激活码：（随意一个）\nMP20-888-MP11-AAA5-BBBB MP20-345-DP56-7778-919A  打开host屏蔽；路径（C:\\Windows\\System32\\drivers\\etc\\hosts）\n0.0.0.0 http://www.mindjet.com 0.0.0.0 http://www.mindjet.com/ 0.0.0.0 mindjet.com 0.0.0.0 ipm.corel.com   Notepad++ # 一款免费、开源的、十分小巧、功能强大、好用的文字编辑工具 官网：https://notepad-plus-plus.org/ # 其目前的最新版本为：Notepad++ 7.8.9: Stand with Hong Kong # 爱了爱了  vnote markdown写作软件\n office全家桶 网上破解的很多，各个版本的都有\nAdobe全家桶 首选嬴政天下系列\njdk https://download.java.net/下载自己想要的包解压到合适的位置 配置环境变量 新建JAVA_HOME 点击系统变量新建：\n”变量名“： JAVA_HOME ”变量值“为JDK安装路径，笔者的路径： D:\\jdk1.8.0_202  设置Path 在”系统变量“中找到”Path“点击“编辑”选项，点击新建添加一个：\n%JAVA_HOME%\\bin  测试安装 调出cmd命令行输出：\njava  有非提示命令不存在的输出就是可以了\n其他小工具 FastStone Capture 截图、取色、录屏、屏幕标尺等等功能集合一体的神器\n轻音少女桌宠 展示视频,没有调整好，没录下声音，声音也挺可爱搞怪的😂\n鲁大师绿色版 有时候需要看些配置啥的，安装官网的只偶尔使用一两次，卸载一般还卸载不干净，很多功能也用不上\nbootice 自己在装双、多系统的时候设置启动项很好的工具\nDiskGenius 磁盘分区、管理利器\n火柴(原火萤酱) 最初是叫火萤酱的，现在好像分为两边了，火萤酱做动态桌面壁纸的一个社区及控制软件了，而原来火萤酱的工作由现在的火柴来做\n功能简单介绍 1、快速启动，可以添加第三方的软件，无须像win的开始桌面一样，只能添加写入注册表的软件，要添加第三方的话需要新建快捷键再复制到能够读取的位置 2、快速搜索 能够快速的检索磁盘里面的文件并显示出来，而且能够记忆使用过的，下次使用搜索时，按使用频率排序 3、一些其他的小功能，譬如ip查询、进制转换、读度盘解析等等\nUtool 和火柴有很多相似的地方，也略有差异，最明显的特征是utool的很多功能是通过类似于第三方插件的形式来的，要就安装，不要就删除\n火绒(安全、杀毒) 受够了360、腾讯管家 遇到危险文件，什么消息就没有直接删除，找回费老大劲，刚找回又给删了 安装的时候真得孙悟空附体，火眼金睛大显神通，不然疏忽漏掉一个半个选项，这次新装得系统算是废了，一大堆得垃圾软件自动安装，停都没法停 使用过程中有时候没有开几个软件，就提示可加速让电脑运行更快，加速来加速去还是那个样，并且最后往往是它自己占用的资源最严重 卸载的时候各种软磨硬泡，当你以为卸载了的时候再回头一看，还在那里躺得好好的\n火绒除了刚安装的时候需要稍微调试一下(不调也可以)，其他时候就安静的在状态栏里，不占用资源也不会弹窗打扰你，在网络的使用，文件的报毒都会有提示让其自行选择。\n","date":"2022-12-26","permalink":"/post/windows/","tags":["Windows"],"title":"Windows"},{"content":"官网\n插件： 插件的管理工具在 sublime 3 中的安装方式如下，sublime 4 可以通过 Ctrl+Shift+P 查找安装。\n按 ctrl + \\ 或者View \u0026gt; Show Console，打开 Console 一次性输入如下代码，回车：\nimport urllib.request,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)  为 Package Control 设置代理（不设置也可以正常使用，但是在安装插件及更新插件的时候可能会比较慢）\nPrefrences --\u0026gt; Packages Settings --\u0026gt; Package Control --\u0026gt; Settngs 增添下面的代码\n{ // proxy setting \u0026quot;http_proxy\u0026quot;:\u0026quot;http://127.0.0.1:12333\u0026quot;, \u0026quot;https_proxy\u0026quot;:\u0026quot;http://127.0.0.1:12333\u0026quot;, }  安装方法： 1、自动安装： Ctrl+Shift+P 打开命令行模式，输入 Package Control: Install Package中任意几个字母，会自动模糊匹配出来），然后回车选择：Package Control: Install Package 。搜索插件名称即可自动安装，注意留意底部的安装进程，如果安装失败的话可以选择手动下载安装。\n这种方法只能一次安装一个插件，Package Control: Advanced Install Package 可以一起性安装多个插件，每个插件使用,进行分隔，插件名不需要使用'、\u0026quot;包裹，名字由多个单词如HTML-CSS-JS Prettify中间有空格也不需要。\n2、手动安装 去 github 上或者你能找到的渠道下载相应的包。 如果插件是打包好的话复制到 Installed Packages下，是文件目录的话复制到 Packages 文件夹下。\nAdvancedNewFile AdvancedNewFile 用 Sublime text 新建文件的快捷键是，ctrl+ N会新建一个名为untitled的文件，CTRL+S 保存，然后在弹出的 Finder 中填写真正需要的文件名，然后点击 Save 。可谓很是麻烦，而且如果需要新建多层次的文件夹，则需要转换到 Finder 进入到相应的工程目录下，一层一层新建，操作步骤十分繁琐而且费事。\n然而，如果用 AdvancedNewFile，一切将会变得十分简单。\nctrl+ art + N 即新建，然后 sublime text 底部会弹出一个文本输入框，只要在里面输入文件名，或多层次的路径，然后回车即可。 相应文件或多层次的路径就会立刻在工程目录下新建完成。 另外再输入的时候也支持tab补全操作。\nAll Autocomplete AllAutocomplete Sublime Text 默认的 Autocomplete 功能只考虑当前的文件，而 AllAutocomplete 插件会搜索所有打开的文件来寻找匹配的提示词。\n用此插件替换自带的Autocomplete在使用jedi进行语法补全的时候会有很大的延迟，而且全局搜索反而有时候引用到包外的变量，暂时不用了。\nBracketHighlighter 括号以及标签层级显示，不用担心选中的代码属于哪个代码块，一目了然。 查看介绍\nKeymaps Keymaps\n Find a keymap for\u0026hellip; and show all enabled keymaps in a Cheat Sheet.\n 很有帮助，之前安装的插件只知道一些快捷键，不能完全发挥插件的作用，而且插件增多后加上自己的快捷键设置，有时候快捷键冲突而不能使用却一直找不出原因。\nsublime4 安装遇到的问题：\n但是能生效而且结果准确\n是使用sublime的插件管理器安装的，安装到了目录~/.config/sublime-text/Installed\\ Packages下，而sublime对这个包的默认地址是~/.config/sublime-text/Packages下，但是如果在一个目录中找不到就会去另一个文件夹中寻找，所以出现了报错但仍然正常使用的情况，觉得别扭的话将其~/.config/sublime-text/Installed\\ Packages下的keymaps解压移动到~/.config/sublime-text/Packages并删除~/.config/sublime-text/Installed\\ Packages下的keymaps包即可\nAutoFileName 文件名自动补全\nIndexError处理，将autofilename.py中的代码做个简单修改即可，其实就算报错也能正常使用，但是看着挺别扭，对这个异常的意见挺多，自己又懒得去细究先这样用着吧。\nOpen autofilename.py, using PackageResourceView. Go to line 162 sel = view.sel()[0].a txt = view.substr(sublime.Region(sel-4,sel-3)) if (self.showing_win_drives and txt == FileNameComplete.sep): self.showing_win_drives = False view.run_command('afn_delete_prefixed_slash') 替换为 try: sel = view.sel()[0].a txt = view.substr(sublime.Region(sel-4,sel-3)) if (self.showing_win_drives and txt == FileNameComplete.sep): self.showing_win_drives = False view.run_command('afn_delete_prefixed_slash') except IndexError: pass  PackageResourceViewer 对安装的插件进行预览、编辑，压缩包形式的也可以，无需自己解压修改后再压缩。\nFindKeyConflicts 找出所有插件有冲突的快捷键\n一些配置 { // 主题、配色相关 \u0026quot;theme\u0026quot;: \u0026quot;Material-Theme.sublime-theme\u0026quot;, // 整体界面 \u0026quot;color_scheme\u0026quot;: \u0026quot;Packages/Material Theme/schemes/Material-Theme.tmTheme\u0026quot;, // 配色方案 // \u0026quot;sidebar_no_icon\u0026quot;: true, \u0026quot;sidebar_size_13\u0026quot;: true, \u0026quot;sidebar_row_padding_medium\u0026quot;: true, \u0026quot;folder_no_icon\u0026quot;: true, \u0026quot;tabs_small\u0026quot;: true, \u0026quot;tabs_padding_small\u0026quot;: true, \u0026quot;tabs_padding_medium\u0026quot;: true, // \u0026quot;tabs_label_not_italic\u0026quot;: true, \u0026quot;status_bar_brighter\u0026quot;: true, \u0026quot;color_inactive_tabs\u0026quot;: true, // \u0026quot;gutter\u0026quot;: false, // 显示行号边栏 // \u0026quot;margin\u0026quot;: 0, // 行号边栏和文字的间距 \u0026quot;line_padding_top\u0026quot;: 2, // 行的上间距 \u0026quot;line_padding_bottom\u0026quot;: 2, // 行的下间距 // \u0026quot;draw_white_space\u0026quot;: \u0026quot;all\u0026quot;, // 显示空白符 \u0026quot;show_encoding\u0026quot;: true, // 状态栏显示当前文件编码 \u0026quot;always_show_minimap_viewport\u0026quot;: true, // 右侧总是显示代码地图可视区域 // \u0026quot;draw_minimap_border\u0026quot;: true, // 显示可视区域部分的边框 // \u0026quot;bold_folder_labels\u0026quot;: true, // 左侧边栏文字加粗 \u0026quot;indent_guide_options\u0026quot;: [ \u0026quot;draw_normal\u0026quot;, \u0026quot;draw_active\u0026quot; ], // 制表位的对齐线 \u0026quot;remember_open_files\u0026quot;: true, // 记忆之前打开的文件 // \u0026quot;overlay_scroll_bars\u0026quot;: \u0026quot;system\u0026quot;, \u0026quot;dpi_scale\u0026quot;: 1.0, // 高分屏必须调整此设置 \u0026quot;show_full_path\u0026quot;: true, // 标题栏显示打开文件的完整路径 // 编辑行为 \u0026quot;default_encoding\u0026quot;: \u0026quot;UTF-8\u0026quot;, // 默认编码格式 \u0026quot;tab_size\u0026quot;: 4, // Tab键制表符宽度 \u0026quot;translate_tabs_to_spaces\u0026quot;: true, // 设为true时，缩进和遇到Tab键时使用空格替代 \u0026quot;scroll_past_end\u0026quot;: false, // 设置为false时，滚动到文本的最下方时，没有缓冲区 // \u0026quot;highlight_modified_tabs\u0026quot;: true, // 高亮内容有修改的标签 // \u0026quot;find_selected_text\u0026quot;: true, // 匹配选中的文本 \u0026quot;trim_trailing_white_space_on_save\u0026quot;: true, // 保存文件时是否删除每行结束后多余的空格 // \u0026quot;ensure_newline_at_eof_on_save\u0026quot;: false, // 保存文件时光标是否在文件的最后向下换一行 // \u0026quot;save_on_focus_lost\u0026quot;: false, // 切换到其它文件标签或点击其它非本软件区域，文件是否自动保存 // \u0026quot;auto_close_tags\u0026quot;: true, // 自动闭合标签 // \u0026quot;tab_completion\u0026quot;: true, // \u0026quot;auto_complete\u0026quot;: true, // 代码提示 // \u0026quot;auto_complete_delay\u0026quot;: 50, // 代码提示延迟显示 \u0026quot;auto_complete_triggers\u0026quot;: [ // 设置触发代码提醒的关键字 { \u0026quot;selector\u0026quot;: \u0026quot;text.html\u0026quot;, \u0026quot;characters\u0026quot;: \u0026quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.\u0026lt;\u0026quot; }, { \u0026quot;selector\u0026quot;: \u0026quot;text.xml\u0026quot;, \u0026quot;characters\u0026quot;: \u0026quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.\u0026lt;\u0026quot; }, { \u0026quot;selector\u0026quot;: \u0026quot;text.php\u0026quot;, \u0026quot;characters\u0026quot;: \u0026quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.\u0026lt;\u0026quot; }, { \u0026quot;selector\u0026quot;: \u0026quot;text.css\u0026quot;, \u0026quot;characters\u0026quot;: \u0026quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.\u0026lt;\u0026quot; }, { \u0026quot;selector\u0026quot;: \u0026quot;text.js\u0026quot;, \u0026quot;characters\u0026quot;: \u0026quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.\u0026lt;\u0026quot; } ], // \u0026quot;auto_match_enabled\u0026quot;: true, // 自动匹配引号，括号等 // 光标样式 \u0026quot;caret_style\u0026quot;: \u0026quot;smooth\u0026quot;, // 光标闪动方式 \u0026quot;smooth\u0026quot;, \u0026quot;phase\u0026quot;, \u0026quot;blink\u0026quot;, \u0026quot;wide\u0026quot; and \u0026quot;solid\u0026quot; \u0026quot;caret_extra_bottom\u0026quot;: 1, \u0026quot;caret_extra_top\u0026quot;: 1, \u0026quot;caret_extra_width\u0026quot;: 1, // Word wrapping - follow PEP 8 recommendations // \u0026quot;rulers\u0026quot;: [ 82, 92 ], // \u0026quot;wrap_width\u0026quot;: 80, // 设置窗口内文字区域的宽度 \u0026quot;word_wrap\u0026quot;: false, // true | false | auto // 禁止自动更新 \u0026quot;update_check\u0026quot;: false }  破解 1、 2020-6-6 日测试可用。\nsudo vim /etc/hosts #添加如下内容到文件里面 127.0..0.1 www.sublimetext.com 127.0.0.1 license.sublimehq.com 127.0.0.1 45.55.255.55 127.0.0.1 45.55.41.223  注册码：\n ZYNGA INC. 50 User License EA7E-811825 927BA117 84C9300F 4A0CCBC4 34A56B44 985E4562 59F2B63B CCCFF92F 0E646B83 0FD6487D 1507AE29 9CC4F9F5 0A6F32E3 0343D868 C18E2CD5 27641A71 25475648 309705B3 E468DDC4 1B766A18 7952D28C E627DDBA 960A2153 69A2D98A C87C0607 45DC6049 8C04EC29 D18DFA40 442C680B 1342224D 44D90641 33A3B9F2 46AADB8F  由于这个是一个Sublime Text 2的验证码，注册成功后会弹出提示说这个注册码是Sublime Text 2的，是否要升级之类的，选择取消就好了，接下来就可以正常使用了。\n条件允许的情况下还是建议购买官方注册码。\n2、 上面的方法在linux的时候可用，但是在 win 的时候不成功，找了另一个方法\n改hosts： 加上下面这段代码：\n127.0.0.1 www.sublimetext.com 127.0.0.1 sublimetext.com 127.0.0.1 sublimehq.com 127.0.0.1 license.sublimehq.com 127.0.0.1 45.55.255.55 127.0.0.1 45.55.41.223 0.0.0.0 license.sublimehq.com  输入注册码： 一定要改一下hosts在输入注册码，否则可能失败。Help-\u0026gt;enter license,复制粘贴下列全部粘贴至打开的对话框。\n----- BEGIN LICENSE ----- Member J2TeaM Single User License EA7E-1011316 D7DA350E 1B8B0760 972F8B60 F3E64036 B9B4E234 F356F38F 0AD1E3B7 0E9C5FAD FA0A2ABE 25F65BD8 D51458E5 3923CE80 87428428 79079A01 AA69F319 A1AF29A4 A684C2DC 0B1583D4 19CBD290 217618CD 5653E0A0 BACE3948 BB2EE45E 422D2C87 DD9AF44B 99C49590 D2DBDEE1 75860FD2 8C8BB2AD B2ECE5A4 EFC08AF2 25A9B864 ------ END LICENSE ------  接下来应该是显示激活成功，激活成功后可将之前hosts文件添加内容删掉\n快捷键： 默认快捷键 Ctrl+Shift+P：打开命令面板 Ctrl+P：搜索项目中的文件 Ctrl+G：跳转到第几行 Ctrl+W：关闭当前打开文件 Ctrl+Shift+W：关闭所有打开文件 Ctrl+Shift+V：粘贴并格式化 Ctrl+D：选择单词，重复可增加选择下一个相同的单词 Ctrl+L：选择行，重复可依次增加选择下一行 Ctrl+Shift+L：选择多行 Ctrl+Shift+Enter：在当前行前插入新行 Ctrl+X：删除当前行 Ctrl+M：跳转到对应括号 Ctrl+U：软撤销，撤销光标位置 Ctrl+J：选择标签内容 Ctrl+F：查找内容 Ctrl+Shift+F：查找并替换 Ctrl+H：替换 Ctrl+R：前往 method Ctrl+N：新建窗口 Ctrl+K+B：开关侧栏 Ctrl+Shift+M：选中当前括号内容，重复可选着括号本身 Ctrl+F2：设置/删除标记 Ctrl+/：注释当前行 Ctrl+Shift+/：当前位置插入注释 Ctrl+Alt+/：块注释，并Focus到首行，写注释说明用的 Ctrl+Shift+A：选择当前标签前后，修改标签用的 F11：全屏 Shift+F11：全屏免打扰模式，只编辑当前文件 Alt+F3：选择所有相同的词 Alt+.：闭合标签 Alt+Shift+数字：分屏显示 Alt+数字：切换打开第N个文件 Shift+右键拖动：光标多不，用来更改或插入列内容 鼠标的前进后退键可切换Tab文件 按Ctrl，依次点击或选取，可需要编辑的多个位置 按Ctrl+Shift+上下键，可替换行  便携版本 安装的时候选择一个比较容易找到的位置，默认的也可以，不过得去找到安装的位置，安装好之后先不要打开，在 Sublime Text 3 文件夹下新建一个 Data (注意大小写) 文件夹并删除原来的的 Sublime Text 3 配置文件夹，这时候启动安装插件就会在Data目录中了，也可以直接复制自己的配置文件到Data目录中，这个文件夹就可以复制进 u 盘随身带着了，但是依赖的 python、node、go 等需要在新设备上进行安装并修改相应 sublime 配置\n问题 连续的输入gh或者gd会快速显示h、d后消失。 原因是godef定义的快捷键冲突了，更改快捷键或者慢速输入即可。\npath 问题 sublime 默认使用的是bash, 默认 path 路径为\n/usr/local/bin /usr/local/sbin /usr/bin /usr/sbin  自己安装的一些软件不在默认 path 中，\n这时要么在.bash_profile中添加环境变量，但是设置后要重启系统才生效。\n要么创建其软链接在/usr/local/bin中，创建软链接的位置非强制但建议，这样在删除软件时更方便也不会和其他的系统软件混淆。\n","date":"2022-12-26","permalink":"/post/sublime_base/","tags":["Tools","Sublime"],"title":"sublime_base"},{"content":"安装及使用： 项目地址https://github.com/ytdl-org/youtube-dl 具体的安装方式去看挑选一个适合且可用的即可，我在这使用的是 pip 安装\npip3 install youtube-dl  最简单的使用 youtube-dl url即可下载，但是默认下载的是最高画质，这时候可以使用 -F 参数查看可供下载的格式，\n","date":"2022-12-26","permalink":"/post/youtube-dl/","tags":["Tools"],"title":"Youtube-dl"},{"content":"emmet： 前端神器。一个可以极大提高 web 开发者 HTML 和 CSS 工作效率的工具箱组件。 查看介绍\n问题： 如下报错： 因为下载下来的 PyV8 的包是一个文件夹，按照正常情况是放在 packages 下的，但是安装之后一直无法使用，以为是快捷键冲突，又是改快捷键、又是删除其余的包、又是重装、又是更换不同版本的 PyV8 的包的都没有解决，然后想着是不是包管理器给安装错误了，调出命令面板想重装一下，然后留意到加载emmet包然后加载 PyV8 插件的时候是报错，也就是包未加载成功。 最后终于找到解决方法：PyV8 包的位置应该是在与 emmet 包同级目录下。也就是 .config/sublime-text-3/Installed Packages/下，而不是Reference－－Browser Packages指定的.config/sublime-text-3/Packages/下。\n解决方法： 去提示的网址：https://github.com/emmetio/pyv8-binaries 下载相应的包，解压到里面的文件 /home/fish/.config/sublime-text-3/Packages/PyV8下，没有PyV8文件夹就创建一个（比如我的 pyv8-linux64-p3，或者我在测试的时候发现把里面的文件复制到 PyV8 里面也是可以的，但直接复制比较方便快捷，也易于下次区分是什么系统的文件），然后重启解决。\n做完这些后不会弹窗报错了，但是启动的时候地下的任务栏都会提示加载 PyV8,调出命令板也是每次都会有提示，但那不是错误，而是其默认会在启动时检查并更新 PyV8，阻止PyV8 的更新设置方法是：Preferences-\u0026gt;Packages Settings--\u0026gt;Emmet--\u0026gt;Settings User，修改 Emmet 的配置文件即可，内容是：{\u0026quot;disable_pyv8_update\u0026quot;:true}\n使用方法： https://docs.emmet.io/cheat-sheet/ https://code.z01.com/Emmet/\nColorHighLight ColorHighLight 可以展示你所选择的颜色代码（ 像 (\u0026lsquo;FFFFFF\u0026rsquo;, \u0026lsquo;rgb(255,255,255)\u0026rsquo;, \u0026lsquo;white\u0026rsquo;) 的真正颜色。同时它还包含一个颜色选择器让你可以方便地更改颜色。\nHTML-CSS-JS Prettify 报错:安装后需要配合nodejs使用所以需要安装nodejs并设置node的位置 改快捷键：\n{ \u0026quot;keys\u0026quot;: [\u0026quot;ctrl+shift+o\u0026quot;], \u0026quot;command\u0026quot;: \u0026quot;htmlprettify\u0026quot; }  为了正常使用建议更改后测试一下是否有效，防止与其他的互相冲突\nCSS3： CSS3语法高亮、CSS语法提示，美中不足的是缺少游览器私有属性高亮。 查看介绍\nCSS Extended Completions： 关联CSS文件，智能提示css文件中的类名，非常好用。 [查看介绍](https://packagecontrol.io/packages/CSS Extended Completions)\nJavaScript Completions 支持javascript原生语法提示，妈妈再也不用担心我输入document.getElementById(id)。 [查看介绍](https://packagecontrol.io/packages/JavaScript Completions)\njQuery 为jQuery的大部分方法提供了示例代码段，让jQuery的API更加容易使用。 查看介绍\n","date":"2022-12-26","permalink":"/post/sublime_frontend/","tags":["Tools","Sublime"],"title":"sublime_frontEnd"},{"content":"Golang Build The official Sublime Text package for Go build system integration.\n安装后需要配置后使用，比如\n{ \u0026quot;PATH\u0026quot;: \u0026quot;/opt/go/bin\u0026quot;, \u0026quot;GOPATH\u0026quot;: \u0026quot;/home/aiyoyo/go\u0026quot; }  GoSublime 如果没法通过包管理器安装，可以选择下载安装 cd /~.config/sublime-text/Packages \u0026amp;\u0026amp; git clone https://margo.sh/GoSublime\n依赖：go get golang.org/x/tools/cmd/goimports\n { \u0026quot;env\u0026quot;: { \u0026quot;GOPATH\u0026quot;: \u0026quot;/home/aiyoyo/go\u0026quot;, \u0026quot;GOROOT\u0026quot;: \u0026quot;/opt/go\u0026quot; } , \u0026quot;gscomplete_enabled\u0026quot;: true, \u0026quot;fmt_enabled\u0026quot;: true, \u0026quot;fmt_tab_indent\u0026quot;: false, \u0026quot;fmt_tab_width\u0026quot;: 4, \u0026quot;autocomplete_snippets\u0026quot;: true, \u0026quot;autocomplete_tests\u0026quot;: true, \u0026quot;autocomplete_builtins\u0026quot;: true, \u0026quot;autocomplete_closures\u0026quot;: true, \u0026quot;autocomplete_suggest_imports\u0026quot;: true, \u0026quot;calltips\u0026quot;: true, \u0026quot;use_named_imports\u0026quot;: true, \u0026quot;autoinst\u0026quot;: true, \u0026quot;ipc_timeout\u0026quot;: 1, \u0026quot;fmt_cmd\u0026quot;: [\u0026quot;goimports\u0026quot;], \u0026quot;on_save\u0026quot;: [ {\u0026quot;cmd\u0026quot;: \u0026quot;gs_comp_lint\u0026quot;}, {\u0026quot;cmd\u0026quot;: \u0026quot;goimports\u0026quot;} ], \u0026quot;lint_enabled\u0026quot;: true, \u0026quot;linters\u0026quot;: [ {\u0026quot;cmd\u0026quot;: [\u0026quot;go\u0026quot;, \u0026quot;run\u0026quot;]} ], \u0026quot;comp_lint_enabled\u0026quot;: true, \u0026quot;comp_lint_commands\u0026quot;: [ {\u0026quot;cmd\u0026quot;: [\u0026quot;go\u0026quot;, \u0026quot;install\u0026quot;]} ], }  ","date":"2022-12-26","permalink":"/post/sublime_go/","tags":["Tools","Sublime"],"title":"sublime_go"},{"content":"设备信息 Linux VERSIONS ： Fedora release 36 (Thirty Six) Sublime VERSIONS： Sublime Text 4143\n简介 1、各个插件的依赖，比如LSP-pylsp依赖python-lsp-server包，并不需要自己进行安装配置，只需要有基本的环境，比如go python rust node npm等，自己安装的无论设置不设置环境变量都不会被应用，依赖是在~/.cache/sublime-text/Package Storage相应的目录中。\n2、LSP 配置中的各个插件的 \u0026ldquo;clients\u0026rdquo; 不再需要自己写，也不再需要自己在配置中开启。\n3、安装后会自动安装依赖，打开输出台留意安装进程，有报错进行调整重启会重新尝试安装，直至安装成功，在各格式文件中自动开启适合的插件服务。\n4、安装后可以对各自的配置进行调整。\n基本环境 Python 3.10.7\nnode v16.18.0\nnpm 8.19.2 换源，命令行npm config set registry https://registry.npmjs.org/设置后，查看源被改变，但是安装的时候并没有用，npm install -g vscode-html-languageserver-bin --registry=https://registry.npm.taobao.org 却可以，修改~/.npmrc文件中registry=https://registry.npm.taobao.org/可以不使用代理成功安装。\ngo version go1.18 linux/amd64 在设置了环境变量外部测试能找到命令，但是 sublime 中安装LSP-gopls一直报错go binary could not be found in $PATH,创建软连接在/bin或/usr/bin中即可。\n插件 LSP VERSIONS： 1.20.0\nLSP-gopls VERSION\t1.2.3\n安装报错： 看了源码，安装命令没有问题，但是会指定一些参数，为了安装在~/.cache/sublime-text/Package Storage/LSP-gopls/目录下，会指定虚拟环境，因该是该虚拟环境与GOPATH冲突导致一直未安装成功。自己安装gopls后将相应的文件复制过去再添加一个VERSION文件写入版本号,例如0.10.1,重启即可。\nLSP-json VERSION\t1.9.0\nLSP-bash VERSION\t1.1.5\n提示change \u0026quot;SHELLCHECK_PATH\u0026quot; to \u0026quot;shellchcek\u0026quot;，文件未正确添加shabang头导致.\nLSP-pylsp VERSION\t2.6.0\n下面的配置根据自己的喜好可进行调整 LSP-pylsp.sublime-settings:\n{ // \u0026quot;command\u0026quot;: [\u0026quot;$server_path\u0026quot;], // \u0026quot;env\u0026quot;: { // \u0026quot;PYTHONPATH\u0026quot;: \u0026quot;${sublime_py_files_dir}${pathsep}${packages}\u0026quot;, // \u0026quot;MYPYPATH\u0026quot;: \u0026quot;${sublime_py_files_dir}${pathsep}${packages}\u0026quot; // }, \u0026quot;settings\u0026quot;: { // --- JEDI configuration --------------------------------------------- \u0026quot;pylsp.plugins.jedi.extra_paths\u0026quot;: [\u0026quot;$sublime_py_files_dir\u0026quot;, \u0026quot;$packages\u0026quot;], \u0026quot;pylsp.plugins.jedi.environment\u0026quot;: null, \u0026quot;pylsp.plugins.jedi.env_vars\u0026quot;: null, \u0026quot;pylsp.plugins.jedi.auto_import_modules\u0026quot;: [\u0026quot;numpy\u0026quot;], \u0026quot;pylsp.plugins.jedi_completion.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_completion.cache_for\u0026quot;: [\u0026quot;pandas\u0026quot;, \u0026quot;numpy\u0026quot;, \u0026quot;tensorflow\u0026quot;, \u0026quot;matplotlib\u0026quot;], \u0026quot;pylsp.plugins.jedi_completion.eager\u0026quot;: false, \u0026quot;pylsp.plugins.jedi_completion.fuzzy\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_completion.include_class_objects\u0026quot;: false, \u0026quot;pylsp.plugins.jedi_completion.include_function_objects\u0026quot;: false, \u0026quot;pylsp.plugins.jedi_completion.include_params\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_completion.resolve_at_most\u0026quot;: 25, \u0026quot;pylsp.plugins.jedi_definition.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_definition.follow_builtin_imports\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_definition.follow_imports\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_hover.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_references.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_signature_help.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_symbols.all_scopes\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_symbols.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_symbols.include_import_symbols\u0026quot;: true, \u0026quot;pylsp.rope.extensionModules\u0026quot;: null, \u0026quot;pylsp.rope.ropeFolder\u0026quot;: null, // --- Linters -------------------------------------------------------- \u0026quot;pylsp.configurationSources\u0026quot;: [\u0026quot;pycodestyle\u0026quot;], \u0026quot;pylsp.plugins.flake8.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pycodestyle.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.pydocstyle.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pyflakes.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pylint.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pylsp_mypy.enabled\u0026quot;: false, // --- flake8 settings --- \u0026quot;pylsp.plugins.flake8.config\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.exclude\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.executable\u0026quot;: \u0026quot;flake8\u0026quot;, \u0026quot;pylsp.plugins.flake8.filename\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.hangClosing\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.ignore\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.maxLineLength\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.perFileIgnores\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.select\u0026quot;: null, // --- mccabe settings --- \u0026quot;pylsp.plugins.mccabe.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.mccabe.threshold\u0026quot;: 15, // --- preload settings --- \u0026quot;pylsp.plugins.preload.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.preload.modules\u0026quot;: null, // --- pycodestyle settings --- \u0026quot;pylsp.plugins.pycodestyle.aggressive\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.exclude\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.filename\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.hangClosing\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.ignore\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.maxLineLength\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.select\u0026quot;: null, // --- pydocstyle settings --- \u0026quot;pylsp.plugins.pydocstyle.addIgnore\u0026quot;: null, \u0026quot;pylsp.plugins.pydocstyle.addSelect\u0026quot;: null, \u0026quot;pylsp.plugins.pydocstyle.convention\u0026quot;: null, \u0026quot;pylsp.plugins.pydocstyle.ignore\u0026quot;: null, \u0026quot;pylsp.plugins.pydocstyle.match\u0026quot;: \u0026quot;(?!test_).*\\\\.py\u0026quot;, \u0026quot;pylsp.plugins.pydocstyle.matchDir\u0026quot;: \u0026quot;[^\\\\.].*\u0026quot;, \u0026quot;pylsp.plugins.pydocstyle.select\u0026quot;: null, // --- pylint settings --- \u0026quot;pylsp.plugins.pylint.args\u0026quot;: null, \u0026quot;pylsp.plugins.pylint.executable\u0026quot;: \u0026quot;\u0026quot;, // --- pylsp_mypy settings --- \u0026quot;pylsp.plugins.pylsp_mypy.dmypy\u0026quot;: false, \u0026quot;pylsp.plugins.pylsp_mypy.live_mode\u0026quot;: true, \u0026quot;pylsp.plugins.pylsp_mypy.strict\u0026quot;: false, // --- rope_completion settings --- \u0026quot;pylsp.plugins.rope_completion.eager\u0026quot;: false, \u0026quot;pylsp.plugins.rope_completion.enabled\u0026quot;: false, // --- Formatters ----------------------------------------------------- // By default, autopep8 is enabled \u0026quot;pylsp.plugins.autopep8.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.yapf.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pyls_isort.enabled\u0026quot;: false, // Enabling black disables the autopep8 and yapf plugins. \u0026quot;pylsp.plugins.pylsp_black.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pylsp_black.cache_config\u0026quot;: false, \u0026quot;pylsp.plugins.pylsp_black.line_length\u0026quot;: 88, \u0026quot;pylsp.plugins.pylsp_black.preview\u0026quot;: false, }, \u0026quot;selector\u0026quot;: \u0026quot;source.python\u0026quot;, }  LSP-pyright VERSION\t1.1.199\n不使用虚拟环境\n设置python.pythonPath可以使用虚拟环境中的 python，也就能实现对虚拟环境中的包引用和自动补全了。\nLSP-pyright.sublime-settings\n{ \u0026quot;settings\u0026quot;: { // Path to Python. Leave empty to attempt automatic resolution. \u0026quot;python.pythonPath\u0026quot;: \u0026quot;/home/aiyoyo/Documents/vir/flask_b/bin/python\u0026quot;, // Path to folder with a list of Virtual Environments. // \u0026quot;python.venvPath\u0026quot;: \u0026quot;/home/aiyoyo/Documents/vir\u0026quot;, } }  使用虚拟环境 虚拟环境配置：name.sublime-project\n{ \u0026quot;folders\u0026quot;: [ { // \u0026quot;folder_exclude_patterns\u0026quot;: [\u0026quot;Backup/\u0026quot;], \u0026quot;path\u0026quot;: \u0026quot;.\u0026quot;, } ], \u0026quot;settings\u0026quot;: { \u0026quot;LSP\u0026quot;: { \u0026quot;LSP-pyright\u0026quot;: { \u0026quot;enabled\u0026quot;: true, \u0026quot;settings\u0026quot;: { \u0026quot;pyright.dev_environment\u0026quot;: \u0026quot;sublime_text_38\u0026quot;, // python 包的查找路径 \u0026quot;python.analysis.extraPaths\u0026quot;: [ \u0026quot;/home/aiyoyo/Documents/vir/flask_b/lib/python3.8/site-packages\u0026quot;, ], \u0026quot;python.analysis.logLevel\u0026quot;: \u0026quot;Information\u0026quot;, \u0026quot;python.venvPath\u0026quot;: \u0026quot;\u0026quot;, }, }, }, }, // 虚拟环境路径，最初可以不用指定,使用 `virtualenv`插件进行环境切换时会自动创建、切换，注释 \u0026quot;virtualenv\u0026quot;: \u0026quot;/home/aiyoyo/Documents/vir/flask_b\u0026quot;, }  Project \u0026gt; Edit Project会打开相应的项目配置文件编辑即可。为灰色则是没有项目配置文件，Project \u0026gt; Save Project As会创建相应的文件，再去编辑即可。\n","date":"2022-12-26","permalink":"/post/sublime_lsp/","tags":["Tools","Sublime"],"title":"sublime_LSP"},{"content":"MarkdownPreview Markdown转HTML，提供在浏览器中的预览功能 LiveReload\nLiveReload 一个提供md/html等文档的实时刷新预览的的插件 ,插件安装后需要手动开启相应的功能并且需要在浏览器上安装相应插件。不过浏览器的预览不会随着 sublime 上文件焦点的移动而相应的移动。只是会在保存后刷新修改的部分。\nLSP-marksman ","date":"2022-12-26","permalink":"/post/sublime_markdown/","tags":["Tools","Sublime"],"title":"sublime_markdown"},{"content":"方案一 Virtualenv # 虚拟环境切换,需要 `virtualenv` 包支持，还可通过设置中的 `\u0026quot;virtualenv_directories\u0026quot;=[] // virtualenvwrapper $WORKON_HOME` 配置自己的虚拟环境目录，这样可调用在外设置的虚拟环境 Jedi - Python autocompletion # 语法补全，而且与 Virtualenv 无冲突，也可以手动配置虚拟环境，但麻烦些 AutoPEP8 # 快捷方式快速对文件格式进行调整 SublimeLinter # 语法检查框架，不能单独使用 SublimeLinter-flake8 # 需要安装 `flake8` ，配合 SublimeLinter 实现对语法的检查  SublimeLinter + SublimeLinter-flake8 语法检查工具，支持多个检查工具，可自己配置，SublimeLinter-flake8 需要安装相应的 python 包。\n如果使用 Anaconda 插件需要关闭其自带的语法检查，否则会有冲突\nJedi - Python autocompletion 通过修改 Jedi 配置中的\u0026quot;python_virtualenv\u0026quot;， \u0026ldquo;python_interpreter\u0026rdquo; 也可以实现对 python 版本及外部创建的虚拟环境的使用\nAnaconda 有虚拟环境管理，但是以前用 Anaconda 作为包管理器创建的环境不是很纯净，这个就默认没有去尝试了。 SublimeCodeIntel 初初使用了一下，可能是没有配置好，使用时会提供很多的其他的不需要的可选项。\nAutoPEP8：  Automatically formats Python code to conform to the PEP 8 style guide using autopep8 and pep8 modules\n 快速完成对代码的格式化，让代码更优雅。\nVirtualenv 在 sublime 4 中安装后无法正常使用，查看加载有如下报错：\nreloading python 3.3 plugin Virtualenv.commands Traceback (most recent call last): File \u0026quot;/opt/sublime_text/Lib/python33/sublime_plugin.py\u0026quot;, line 308, in reload_plugin m = importlib.import_module(modulename) File \u0026quot;./python3.3/importlib/__init__.py\u0026quot;, line 90, in import_module File \u0026quot;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026quot;, line 1584, in _gcd_import File \u0026quot;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026quot;, line 1565, in _find_and_load File \u0026quot;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026quot;, line 1532, in _find_and_load_unlocked File \u0026quot;/opt/sublime_text/Lib/python33/sublime_plugin.py\u0026quot;, line 1692, in load_module exec(compile(source, source_path, 'exec'), mod.__dict__) File \u0026quot;/home/aiyoyo/.config/sublime-text/Installed Packages/Virtualenv.sublime-package/commands.py\u0026quot;, line 126, in \u0026lt;module\u0026gt; AttributeError: 'module' object has no attribute 'exec' reloading python 3.3 plugin Virtualenv.integrations Traceback (most recent call last): File \u0026quot;/opt/sublime_text/Lib/python33/sublime_plugin.py\u0026quot;, line 308, in reload_plugin m = importlib.import_module(modulename) File \u0026quot;./python3.3/importlib/__init__.py\u0026quot;, line 90, in import_module File \u0026quot;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026quot;, line 1584, in _gcd_import File \u0026quot;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026quot;, line 1565, in _find_and_load File \u0026quot;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026quot;, line 1532, in _find_and_load_unlocked File \u0026quot;/opt/sublime_text/Lib/python33/sublime_plugin.py\u0026quot;, line 1692, in load_module exec(compile(source, source_path, 'exec'), mod.__dict__) File \u0026quot;/home/aiyoyo/.config/sublime-text/Installed Packages/Virtualenv.sublime-package/integrations.py\u0026quot;, line 5, in \u0026lt;module\u0026gt; File \u0026quot;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026quot;, line 1565, in _find_and_load File \u0026quot;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026quot;, line 1532, in _find_and_load_unlocked File \u0026quot;/opt/sublime_text/Lib/python33/sublime_plugin.py\u0026quot;, line 1692, in load_module exec(compile(source, source_path, 'exec'), mod.__dict__) File \u0026quot;/home/aiyoyo/.config/sublime-text/Installed Packages/Virtualenv.sublime-package/commands.py\u0026quot;, line 126, in \u0026lt;module\u0026gt; AttributeError: 'module' object has no attribute 'exec'  解决方法是在包中添加一个文件.python-version 并在其中指明其他版本的 python 即可。\ncd ~ echo \u0026quot;3.8\u0026quot; \u0026gt; .python-version # 如果是包管理器安装的如下将文件合进去即可 zip -u ~/.config/sublime-text/Installed\\ Packages/Virtualenv.sublime-package .python-version # 下载手动安装的话 cp .python-version ~/.config/sublime-text/Packages/Virtualenv/ rm .python-version  这样加载 Virtualenv 时会使用python3.8而不是默认的python3.3就能正常使用了。\n因该是sublime 4考虑了兼容新增的python3.8。其他插件如果有类似的异常也可以试试这种方法。不过不建议替换之前版本的python内核，可能与sublime程序本身不兼容。\n方案二 Virtualenv LSP # Client implementation of the Language Server Protocol for Sublime Text LSP-pylsp # 搭配 LSP 使用，可以实现代码补全，检查等功能，需要`python-lsp-server`包  不支持虚拟环境，Virtualenv插件是用来切换运行时的环境。\n对于其他环境的包应用补全可以通过设置\u0026quot;pylsp.plugins.jedi.environment\u0026quot;:来实现，但是代码检查时还是不能识别其他环境的包。\n下面的配置根据自己的喜好可进行调整 LSP-pylsp.sublime-settings:\n{ // \u0026quot;command\u0026quot;: [\u0026quot;$server_path\u0026quot;], // \u0026quot;env\u0026quot;: { // \u0026quot;PYTHONPATH\u0026quot;: \u0026quot;${sublime_py_files_dir}${pathsep}${packages}\u0026quot;, // \u0026quot;MYPYPATH\u0026quot;: \u0026quot;${sublime_py_files_dir}${pathsep}${packages}\u0026quot; // }, \u0026quot;settings\u0026quot;: { // --- JEDI configuration --------------------------------------------- \u0026quot;pylsp.plugins.jedi.extra_paths\u0026quot;: [\u0026quot;$sublime_py_files_dir\u0026quot;, \u0026quot;$packages\u0026quot;], \u0026quot;pylsp.plugins.jedi.environment\u0026quot;: null, \u0026quot;pylsp.plugins.jedi.env_vars\u0026quot;: null, \u0026quot;pylsp.plugins.jedi.auto_import_modules\u0026quot;: [\u0026quot;numpy\u0026quot;], \u0026quot;pylsp.plugins.jedi_completion.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_completion.cache_for\u0026quot;: [\u0026quot;pandas\u0026quot;, \u0026quot;numpy\u0026quot;, \u0026quot;tensorflow\u0026quot;, \u0026quot;matplotlib\u0026quot;], \u0026quot;pylsp.plugins.jedi_completion.eager\u0026quot;: false, \u0026quot;pylsp.plugins.jedi_completion.fuzzy\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_completion.include_class_objects\u0026quot;: false, \u0026quot;pylsp.plugins.jedi_completion.include_function_objects\u0026quot;: false, \u0026quot;pylsp.plugins.jedi_completion.include_params\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_completion.resolve_at_most\u0026quot;: 25, \u0026quot;pylsp.plugins.jedi_definition.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_definition.follow_builtin_imports\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_definition.follow_imports\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_hover.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_references.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_signature_help.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_symbols.all_scopes\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_symbols.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.jedi_symbols.include_import_symbols\u0026quot;: true, \u0026quot;pylsp.rope.extensionModules\u0026quot;: null, \u0026quot;pylsp.rope.ropeFolder\u0026quot;: null, // --- Linters -------------------------------------------------------- \u0026quot;pylsp.configurationSources\u0026quot;: [\u0026quot;pycodestyle\u0026quot;], \u0026quot;pylsp.plugins.flake8.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pycodestyle.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.pydocstyle.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pyflakes.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pylint.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pylsp_mypy.enabled\u0026quot;: false, // --- flake8 settings --- \u0026quot;pylsp.plugins.flake8.config\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.exclude\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.executable\u0026quot;: \u0026quot;flake8\u0026quot;, \u0026quot;pylsp.plugins.flake8.filename\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.hangClosing\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.ignore\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.maxLineLength\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.perFileIgnores\u0026quot;: null, \u0026quot;pylsp.plugins.flake8.select\u0026quot;: null, // --- mccabe settings --- \u0026quot;pylsp.plugins.mccabe.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.mccabe.threshold\u0026quot;: 15, // --- preload settings --- \u0026quot;pylsp.plugins.preload.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.preload.modules\u0026quot;: null, // --- pycodestyle settings --- \u0026quot;pylsp.plugins.pycodestyle.aggressive\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.exclude\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.filename\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.hangClosing\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.ignore\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.maxLineLength\u0026quot;: null, \u0026quot;pylsp.plugins.pycodestyle.select\u0026quot;: null, // --- pydocstyle settings --- \u0026quot;pylsp.plugins.pydocstyle.addIgnore\u0026quot;: null, \u0026quot;pylsp.plugins.pydocstyle.addSelect\u0026quot;: null, \u0026quot;pylsp.plugins.pydocstyle.convention\u0026quot;: null, \u0026quot;pylsp.plugins.pydocstyle.ignore\u0026quot;: null, \u0026quot;pylsp.plugins.pydocstyle.match\u0026quot;: \u0026quot;(?!test_).*\\\\.py\u0026quot;, \u0026quot;pylsp.plugins.pydocstyle.matchDir\u0026quot;: \u0026quot;[^\\\\.].*\u0026quot;, \u0026quot;pylsp.plugins.pydocstyle.select\u0026quot;: null, // --- pylint settings --- \u0026quot;pylsp.plugins.pylint.args\u0026quot;: null, \u0026quot;pylsp.plugins.pylint.executable\u0026quot;: \u0026quot;\u0026quot;, // --- pylsp_mypy settings --- \u0026quot;pylsp.plugins.pylsp_mypy.dmypy\u0026quot;: false, \u0026quot;pylsp.plugins.pylsp_mypy.live_mode\u0026quot;: true, \u0026quot;pylsp.plugins.pylsp_mypy.strict\u0026quot;: false, // --- rope_completion settings --- \u0026quot;pylsp.plugins.rope_completion.eager\u0026quot;: false, \u0026quot;pylsp.plugins.rope_completion.enabled\u0026quot;: false, // --- Formatters ----------------------------------------------------- // By default, autopep8 is enabled \u0026quot;pylsp.plugins.autopep8.enabled\u0026quot;: true, \u0026quot;pylsp.plugins.yapf.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pyls_isort.enabled\u0026quot;: false, // Enabling black disables the autopep8 and yapf plugins. \u0026quot;pylsp.plugins.pylsp_black.enabled\u0026quot;: false, \u0026quot;pylsp.plugins.pylsp_black.cache_config\u0026quot;: false, \u0026quot;pylsp.plugins.pylsp_black.line_length\u0026quot;: 88, \u0026quot;pylsp.plugins.pylsp_black.preview\u0026quot;: false, }, \u0026quot;selector\u0026quot;: \u0026quot;source.python\u0026quot;, }  方案三 Virtualenv LSP # Client implementation of the Language Server Protocol for Sublime Text LSP-pyright # LSP-pylsp 类似，但更快，且补全检查不依赖 python ，支持虚拟环境  虚拟环境配置：name.sublime-project\n{ \u0026quot;folders\u0026quot;: [ { // \u0026quot;folder_exclude_patterns\u0026quot;: [\u0026quot;Backup/\u0026quot;], \u0026quot;path\u0026quot;: \u0026quot;.\u0026quot;, } ], \u0026quot;settings\u0026quot;: { \u0026quot;LSP\u0026quot;: { \u0026quot;LSP-pyright\u0026quot;: { \u0026quot;enabled\u0026quot;: true, \u0026quot;settings\u0026quot;: { \u0026quot;pyright.dev_environment\u0026quot;: \u0026quot;sublime_text_38\u0026quot;, // python 包的查找路径 \u0026quot;python.analysis.extraPaths\u0026quot;: [ \u0026quot;/home/aiyoyo/Documents/vir/flask_b/lib/python3.8/site-packages\u0026quot;, ], \u0026quot;python.analysis.logLevel\u0026quot;: \u0026quot;Information\u0026quot;, \u0026quot;python.venvPath\u0026quot;: \u0026quot;\u0026quot;, }, }, }, }, // 虚拟环境路径，最初可以不用指定,使用 `virtualenv`插件进行环境切换时会自动创建、切换，注释 \u0026quot;virtualenv\u0026quot;: \u0026quot;/home/aiyoyo/Documents/vir/flask_b\u0026quot;, }  Project \u0026gt; Edit Project会打开相应的项目配置文件编辑即可。为灰色则是没有项目配置文件，Project \u0026gt; Save Project As会创建相应的文件，再去编辑即可。\n使用了一段时间还是换回lsp-pylsp了，补全的响应速度快些但是并不是刚需，对于代码检查的不准确，英文的逗号,写成中文的，提示为括号未正确关闭，而且代码检查的工具不能切换或增加，对于包的补全，在针对protobuf项目时虽然都不能正常补全，但是pyright在引用后因为不是基于python的仍然会提示异常实际却是可以正常使用，自然也就不能跳转到相应的定义处了。\n","date":"2022-12-26","permalink":"/post/sublime_python/","tags":["Tools","Sublime"],"title":"sublime_python"},{"content":"脚本的使用是在安装了 Tampermonkey 的基础上\nall-search 全搜 一个搜索引擎快捷跳转菜单, 支持图形界面自定义 除了支持谷歌、百度、duck搜索等搜索引擎外，还支持图片、翻译、开发工具（github）等的快速跳转\nBaiduMonkeyW 改善百度的搜索结果界面，提高阅读效率\ncsdn 代码块自由复制 (() =\u0026gt; document.querySelectorAll(\u0026quot;#content_views pre code\u0026quot;).forEach(v =\u0026gt; v.style.userSelect = 'text'))()  Search By Image 通过ctrl开启热键后在想要搜索的图片右键即可跳出搜索图片的选择，多种搜索可选\n去百度搜索置顶推广 百度去广告，blur(毛玻璃)搜索框，美化 在百度搜索的时候，为整个画面提供美化\n百度热点搜索屏蔽 屏蔽点搜索后右侧的百度热点新闻，之前使用的，后来使用 Adblock 手动选择屏蔽就不用了\n隐藏知乎登录框 ","date":"2022-12-26","permalink":"/post/tampermonkey_script/","tags":["Tools","Chrome"],"title":"Tampermonkey_script"},{"content":"Adblock AdGuard AdBlocker 屏蔽广告\nBookmark Sidebar 收藏夹侧边栏显示，且打开可设置为在新标签打开 原来的收藏夹在上面会占用一定的空间，且如果一直显示有时候在给别人展示的时候，会比较尴尬，快捷键去打开也比较麻烦\nClear history, Cache \u0026amp; Cookies for Chrome 每次进入 Setting 去清除缓存以及一些垃圾的时候总是很繁琐，这个插件可以简化这些操作，一键清理\nEdge Translate 划词翻译\nExplain and Send Screenshots 页面截图工具，可截取整个网页或者部分（如视频的暂停）\nfatkun图片批量下载 很强大的图片下载器，一张张的图片下载很麻烦，这个可以快速选择下载图片并且保存位置可修改，选择时可以按自己设定的大小选择，快速去除不符合要求的图片\nfehelpr(前端助手) 里面有很多小工具很实用\nFireShot 一键滚动截屏整个网页。\nonetab 标签打开太多，占用大量运行空间，但是暂时用不到，或者看不完，又不必完全保存为收藏标签，这个插件可以很好的保存标签页，且可导出导入，很方便\nPrint Friendly \u0026amp; PDF 自带的print输出pdf时在遇到大网页时加载预览时常卡住，且可定制的内容太少，该插件提供更好的pdf输出\nProxy SwitchyOmega 很好用的代理切换插件\nQuick QR 可以不借助任何通讯软件，通过手机扫码，获取PC浏览器上任意一段文字信息。\nReader Mode 开启阅读模式，能去掉广告而且地址栏不会改变，附加的保存等功能也很好\nTampermonkey 脚本收集 不解释，提供了很多其他丰富的脚本，不用每一种功能去装一个插件了\nTwitter Media Downloader User-Agent Switcher and Manager XPath Helper 京淘助手 Vimium 让操作浏览器像 vim 一样 使用教程：https://xiaoheidiannao.com/articles/Vim-For-Chrome.html#link-operation\nuBlacklist 能够利用正则匹配对谷歌等多种搜索引擎的搜索结果进行过滤，以前没有太在乎这个，有但不多，近期不知道什么原因多了很多，有的搜索结果，20个里面只有4-5个是正常的，其他的虽然域名不一，但是log、域名后的内容都是一样的，比如一些盗版小说群站。而且该插件支持订阅，标题规则过滤，可谓是很强大了。\n订阅源：Google-Chinese-Results-Blocklist，由于是国人收集的，也更有帮助。\n","date":"2022-12-26","permalink":"/post/chrome_extensions/","tags":["Tools","Chrome"],"title":"Chrome_extensions"},{"content":"tools shellcheck bash-language-server 的语法检查工具，这个网站可以在线进行语法检查，而且会给出一些修改建议，有的能运行但是不符合其检查要求的可以根据相应的提示去跳转到相应的介绍，有详细的实例介绍，可以对照着修改，慢慢就能养成好习惯，写出更优雅的 shell 文件。\nvimawesome vim 插件收集分类。需要 vim 插件可以去看看。\nawesome-vim 多人维护的 (Vim plugin shortlist) github 项目,没有前面的全，但应该大多是比较受欢迎的插件。\nawesome-neovim (NeoVim plugin shortlist) github 项目。\nvim-bootstrap 该网站在页面上会给出多个热门的编程语言，选择语言后点击可生成相应的 vim或neovim配置，包括插件、配置、快捷键等，且在文件内有较为详细的规划和介绍。不过目前其对语言的补全、语法检查不是依赖于 LSP。\nvivify A ColorScheme Editor for Vim,可以预览一些默认的主题配置在各种语言下的情况，也可以进行编辑，配置自己喜欢的主题。\n","date":"2022-12-26","permalink":"/post/webresource/","tags":["Others"],"title":"webResource"},{"content":"（1）请按照这样的日期格式（xxxx-xx-xx）每日生成一个文件，例如今天生成的文件为2017-07-05.log， 并且把磁盘的使用情况写到到这个文件中，（不用考虑cron，仅仅写脚本即可）！ #! /bin/bash d=`date +%F` logfile=$d.log df -h \u0026gt; $logfile  (2)需求：\u0026ndash;统计日志 有日志1.log，内容如下： 日志片段： 112.111.12.248 - [25/Sep/2013:16:08:31 +0800]formula-x.haotui.com \u0026ldquo;/seccode.php?update=0.5593110133088248\u0026rdquo; 200\u0026quot;http://formula-x.haotui.com/registerbbs.php\u0026quot; \u0026ldquo;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1;)\u0026rdquo; 61.147.76.51 - [25/Sep/2013:16:08:31 +0800]xyzdiy.5d6d.com \u0026ldquo;/attachment.php?aid=4554\u0026amp;k=9ce51e2c376bc861603c7689d97c04a1\u0026amp;t=1334564048\u0026amp;fid=9\u0026amp;sid=zgohwYoLZq2qPW233ZIRsJiUeu22XqE8f49jY9mouRSoE71\u0026rdquo; 301\u0026quot;http://xyzdiy.5d6d.com/thread-1435-1-23.html\u0026quot; \u0026ldquo;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\u0026rdquo; 要求： 统计出每个IP的访问量有多少？ awk '{print $1}' 1.log |sort -n|uniq -c\n(3)需求：\u0026ndash;统计内存使用 写一个脚本计算一下linux系统所有进程占用内存大小的和。（提示，使用ps或者top命令）\n#! /bin/bash sum=0 for mem in `ps aux |awk '{print $6}' |grep -v 'RSS'` do sum=$[$sum+$mem] done echo \u0026quot;The total memory is $sum\u0026quot;\u0026quot;k\u0026quot;  也可以使用awk 一条命令计算： ps aux | grep -v 'RSS TTY' |awk '{(sum=sum+$6)};END{print sum}'\n(4)需求：\u0026ndash;设计监控脚本 监控远程的一台机器(假设ip为123.23.11.21)的存活状态，当发现宕机时发一封邮件给你自己。 提示： 你可以使用ping命令 ping -c10 123.23.11.21 发邮件脚本可以参考 https://coding.net/u/aminglinux/p/aminglinux-book/git/blob/master/D22Z/mail.py 脚本可以搞成死循环，每隔30s检测一次\n#!/bin/bash ip=123.23.11.21 ma=abc@139.com while 1 do ping -c10 $ip \u0026gt;/dev/null 2\u0026gt;/dev/null if [ $? != \u0026quot;0\u0026quot; ] then python /usr/local/sbin/mail.py $ma \u0026quot;$ip down\u0026quot; \u0026quot;$ip is down,plese check.\u0026quot; #假设mail.py已经编写并设置好了 fi sleep 30 done  (5) 需求：\u0026ndash;批量更改文件名 找到/123目录下所有后缀名为.txt的文件 批量修改.txt为.txt.bak 把所有.bak文件打包压缩为123.tar.gz 批量还原文件的名字，即把增加的.bak再删除\n#!/bin/bash # 查找txt文件 find /123 -type f -name \u0026quot;*.txt\u0026quot; \u0026gt; /tmp/txt.list # 批量修改文件名 for f in `cat /tmp/txt.list` do mv $f $f.bak done # 创建一个目录，为了避免目录已经存在，所以要加一个复杂的后缀名 d=`date +%y%m%d%H%M%S` mkdir /tmp/123_$d # 把.bak文件拷贝到/tmp/123_$d for f in `cat /tmp/txt.list` do cp $f.bak /tmp/123_$d done # 打包压缩 cd /tmp/ tar czf 123.tar.gz 123_$d/ # 还原 for f in `cat /tmp/txt.list` do mv $f.bak $f done  (6)需求：\u0026ndash;监控80端口 写一个脚本，判断本机的80端口是否开启着，如果开启着什么都不做，如果发现端口不存在，那么重启一下httpd服务，并发邮件通知你自己。脚本写好后，可以每一分钟执行一次，也可以写一个死循环的脚本，30s检测一次。 #! /bin/bash mail=123@123.com if netstat -lnp |grep ':80' |grep -q 'LISTEN'; then exit else /usr/local/apache2/bin/apachectl restart \u0026gt;/dev/null 2\u0026gt; /dev/null python mail.py $mail \u0026quot;check_80\u0026quot; \u0026quot;The 80 port is down.\u0026quot; n=`ps aux |grep httpd|grep -cv grep` if [ $n -eq 0 ]; then /usr/local/apache2/bin/apachectl start 2\u0026gt;/tmp/apache_start.err fi if [ -s /tmp/apache_start.err ]; then python mail.py $mail 'apache_start_error' `cat /tmp/apache_start.err` fi fi  (7) 需求：\u0026ndash;域名代理 内网有一台机器不能连外网，所以没有办法使用yum，考虑过使用iptables nat 转发上网，但因为一些原因，放弃使用。所以想到nginx代理，原理很简单。 A 不能访问 1网站， B可以访问，A和B可以内网通信，所以可以让B作为A的代理。 并且可以限定访问的来源IP，配置文件如下：\nserver { listen 80; server_name aaa.com bbb.com ccc.com ddd.com eee.com; location / { resolver 119.29.29.29; proxy_pass http://$host; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; allow 192.168.5.0/24; deny all; } }  说明：这里的119.29.29.29 为一个DNS的ip，用resolver来指定。 假如B机器内网ip为 192.168.5.11，只需要在A上加一条hosts 192.168.5.11 aaa.com bbb.com ccc.com ddd.com eee.com\n(8)需求：\u0026ndash;备份数据库 设计一个shell脚本来备份数据库，首先在本地服务器上保存一份数据，然后再远程拷贝一份，本地保存一周的数据，远程保存一个月。 假定，我们知道mysql root账号的密码，要备份的库为discuz，本地备份目录为/bak/mysql, 远程服务器ip为192.168.123.30，远程提供了一个rsync服务，备份的地址是 192.168.123.30::backup . 写完脚本后，需要加入到cron中，每天凌晨3点执行。\n#! /bin/bash ### backup mysql data ### Writen by Aming. PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/mysql/bin d1=`date +%w` d2=`date +%d` pass=\u0026quot;your_mysql_password\u0026quot; bakdir=/bak/mysql r_bakdir=192.168.123.30::backup exec 1\u0026gt;/var/log/mysqlbak.log 2\u0026gt;/var/log/mysqlbak.log echo \u0026quot;mysql backup begin at `date +\u0026quot;%F %T\u0026quot;`.\u0026quot; mysqldump -uroot -p$pass --default-character-set=gbk discuz \u0026gt;$bakdir/$d1.sql rsync -az $bakdir/$d1.sql $r_bakdir/$d2.sql echo \u0026quot;mysql backup end at `date +\u0026quot;%F %T\u0026quot;`.\u0026quot;  然后加入cron 0 3 * * * /bin/bash /usr/local/sbin/mysqlbak.sh\n（9）需求：\u0026ndash;自动重启nginx服务 服务器上跑的是LNMP环境，近期总是有502现象。502为网站访问的状态码，200正常，502错误是nginx最为普通的错误状态码。由于502只是暂时的，并且只要一重启php-fpm服务则502消失，但不重启的话，则会一直持续很长时间。所以有必要写一个监控脚本，监控访问日志的状态码，一旦发生502，则自动重启一下php-fpm。\n我们设定： access_log /data/log/access.log 脚本死循环，每10s检测一次（假设每10s钟的日志条数为300左右） 重启php-fpm的方法是 /etc/init.d/php-fpm restart\n#! /bin/bash log=/data/log/access.log N=10 while :; do ##因为10秒钟大概产生300条日志 tail -n 300 $log \u0026amp;gt; /tmp/log n_502=`grep -c ' 502\u0026quot;' /tmp/log` if [ $n_502 -ge $N ]; then ##记录系统的状态 top -bn1 \u0026gt;/tmp/`date +%H%M%S`-top.log vmstat 1 5 \u0026gt;/tmp/`date +%H%M%S`-vm.log /etc/init.d/php-fpm restart 2\u0026gt;/dev/null ##重启php-fpm服务后，应先暂缓1分钟，而后继续每隔10s检测一次 sleep 60 fi sleep 10 done  （10）需求：\u0026ndash;删除文本中的字母 要求： 把一个文本文档的前5行中包含字母的行删除掉，同时把6到10行中的全部字母删除掉。 假设文本名字叫做1.txt，并且文本行数大于10，脚本如下\n#!/bin/bash # 先获取该文本的行数 nu=`wc -l 1.txt |awk '{print $1}'` # 对前5行进程处理 for i in `seq 1 5` do # 使用sed把每一行的内容赋值给变量 l=`sed -n \u0026quot;$i\u0026quot;p 1.txt` # 用grep 判定是否匹配字母,-v取反，-q不输出内容 if echo $l |grep -vq '[a-zA-Z]' then echo $l fi done # 对6-10行做删除字母处理 for i in `seq 6 10` do l=`sed -n \u0026quot;$i\u0026quot;p 1.txt` echo $l|sed 's/[a-zA-Z]//g' done # 剩余的直接输出 for i in `seq 11 $nu` do sed -n \u0026quot;$i\u0026quot;p 1.txt done  若想把更改内容写入到1.txt，还需要把以上内容重定向到一个文本中，然后删除1.txt，再把刚刚重定向的文件更名为1.txt\n（11）需求：\u0026ndash;查找字母数小于6的单词 用shell打印下面这句话中字母数小于6的单词。 Bash also interprets a number of multi-character options.\n#!/bin/bash for s in Bash also interprets a number of multi-character options do n=`echo $s|wc -c` if [ $n -lt 6 ] then echo $s fi done  （12）需求：\u0026ndash;输入数字执行对应命令 写一个脚本实现如下功能： 输入一个数字，然后运行对应的一个命令。显示命令如下： cmd meau* 1\u0026mdash;date 2\u0026ndash;ls 3\u0026ndash;who 4\u0026ndash;pwd 当输入1时，会运行date, 输入2时运行ls, 依此类推。\necho \u0026quot;*cmd meau** 1---date 2--ls 3--who 4--pwd\u0026quot; read -p \u0026quot;please input a number 1-4: \u0026quot; n case $n in 1) date ;; 2) ls ;; 3) who ;; 4) pwd ;; *) echo \u0026quot;Please input a number: 1-4\u0026quot; ;; esac  （13）需求：\u0026ndash;监控httpd进程 在服务器上，写一个监控脚本。 每隔10s去检测一次服务器上的httpd进程数，如果大于等于500的时候，就需要自动重启一下apache服务，并检测启动是否成功？ 若没有正常启动还需再一次启动，最大不成功数超过5次则需要理解发邮件通知管理员，并且以后不需要再检测！ 如果启动成功后，1分钟后再次检测httpd进程数，若正常则重复之前操作（每隔10s检测一次），若还是大于等于500，那放弃重启并需要发邮件给管理员，然后自动退出该脚本。假设其中发邮件脚本为之前咱们使用的 mail.py\n#!/bin/bash check_service() { n=0 for i in `seq 1 5` do /usr/local/apache2/bin/apachectl restart 2\u0026gt;/tmp/apache.err if [ $? -ne 0 ] then n=$[$n+1] else break fi done if [ $n -eq 5 ] then ##下面的mail.py参考https://coding.net/u/aminglinux/p/aminglinux-book/git/blob/master/D22Z/mail.py python mai.py \u0026quot;123@qq.com\u0026quot; \u0026quot;httpd service down\u0026quot; `cat /tmp/apache.err` exit fi } while : do t_n=`ps -C httpd --no-heading |wc -l` if [ $t_n -ge 500 ] then /usr/local/apache2/bin/apachectl restart if [ $? -ne 0 ] then check_service fi sleep 60 fi sleep 10 done  ##（14）需求：\u0026ndash;封ip 需求： 根据web服务器上的访问日志，把一些请求量非常高的ip给拒绝掉！ 分析： 我们要做的，不仅是要找到哪些ip请求量不合法，并且还要每隔一段时间把之前封掉的ip（若不再继续请求了）给解封。 所以该脚本的关键点在于定一个合适的时间段和阈值。\n比如， 我们可以每一分钟去查看一下日志，把上一分钟的日志给过滤出来分析，并且只要请求的ip数量超过100次那么就直接封掉。 而解封的时间又规定为每半小时分析一次，把几乎没有请求量的ip给解封！\n参考日志文件片段：\n157.55.39.107 [20/Mar/2015:00:01:24 +0800] www.aminglinux.com \u0026ldquo;/bbs/thread-5622-3-1.html\u0026rdquo; 200 \u0026ldquo;-\u0026rdquo; \u0026ldquo;Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\u0026rdquo; 61.240.150.37 [20/Mar/2015:00:01:34 +0800] www.aminglinux.com \u0026ldquo;/bbs/search.php?mod=forum\u0026amp;srchtxt=LNMP\u0026amp;formhash=8f0c7da9\u0026amp;searchsubmit=true\u0026amp;source=hotsearch\u0026rdquo; 200 \u0026ldquo;-\u0026rdquo; \u0026ldquo;Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\u0026rdquo;\n#! /bin/bash logfile=/home/logs/access.log d1=`date -d \u0026quot;-1 minute\u0026quot; +%H:%M` d2=`date +%M` ipt=/sbin/iptables ips=/tmp/ips.txt block(){ grep \u0026quot;$d1:\u0026quot; $logfile|awk '{print $1}' |sort -n |uniq -c |sort -n \u0026gt;$ips for ip in `awk '$1\u0026gt;100 {print $2}' $ips`; do $ipt -I INPUT -p tcp --dport 80 -s $ip -j REJECT echo \u0026quot;`date +%F-%T` $ip\u0026quot; \u0026gt;\u0026gt; /tmp/badip.txt done } unblock(){ for i in `$ipt -nvL --line-numbers |grep '0.0.0.0/0'|awk '$2\u0026lt;15 {print $1}'|sort -nr`; do $ipt -D INPUT $i done $ipt -Z } if [ $d2 == \u0026quot;00\u0026quot; ] || [ $d2 == \u0026quot;30\u0026quot; ]; then unblock block else block fi  （15）需求：\u0026ndash;找规律打印数字 请详细查看如下几个数字的规律，并使用shell脚本输出后面的十个数字。 10 31 53 77 105 141 \u0026hellip;\u0026hellip;.\n试题解析： 我想大多数人都会去比较这些数字的差值： 10 31 53 77 105 141 21 22 24 28 36 但是这个差值看，并没有什么规律，而我们再仔细看的时候，发现这个差值的差值是有规律的： 10 31 53 77 105 141 21 22 24 28 36 1 2 4 8\n#! /bin/bash x=21 m=10 echo $m for i in `seq 0 14`; do j=$[2**$i] m=$[$m+$x] echo $m x=$[$x+$j] done  （16）需求：\u0026ndash;统计普通用户 写个shell，看看你的Linux系统中是否有自定义用户（普通用户），若是有，一共有几个？并输出姓名！ 假设所有普通用户都是uid大于1000的\n#!/bin/bash n=`awk -F ':' '$3\u0026gt;=1000' /etc/passwd|wc -l` if [ $n -gt 0 ] then echo \u0026quot;There are $n common users.\u0026quot; uname=`awk -F ':' '$3\u0026gt;=1000' /etc/passwd | awk -F ':' '{print $1}'` echo \u0026quot;the user were $uname .\u0026quot; else echo \u0026quot;No common users.\u0026quot; fi  （17）需求：\u0026ndash;监控磁盘使用率 写一个shell脚本，检测所有磁盘分区使用率和inode使用率并记录到以当天日期为命名的日志文件里，当发现某个分区容量或者inode使用量大于85%时，发邮件通知你自己。 思路：就是先df -h 然后过滤出已使用的那一列，然后再想办法过滤出百分比的整数部分，然后和85去比较，同理，inode也是一样的思路。\n#!/bin/bash # This script is for record Filesystem Use%,IUse% everyday and send alert mail when % is more than 85%. log=/var/log/disk/`date +%F`.log date +'%F %T' \u0026gt; $log df -h \u0026gt;\u0026gt; $log echo \u0026gt;\u0026gt; $log df -i \u0026gt;\u0026gt; $log for i in `df -h|grep -v 'Use%'|sed 's/%//'|awk '{print $5}'`; do if [ $i -gt 85 ]; then use=`df -h|grep -v 'Use%'|sed 's/%//'|awk '$5=='$i' {print $1,$5}'` echo \u0026quot;$use\u0026quot; \u0026gt;\u0026gt; use fi done if [ -e use ]; then # 这里可以使用咱们之前介绍的 mail.py 发邮件 mail -s \u0026quot;Filesystem Use% check\u0026quot; root@localhost \u0026lt; use rm -rf use fi for j in `df -i|grep -v 'IUse%'|sed 's/%//'|awk '{print $5}'`; do if [ $j -gt 85 ]; then iuse=`df -i|grep -v 'IUse%'|sed 's/%//'|awk '$5=='$j' {print $1,$5}'` echo \u0026quot;$iuse\u0026quot; \u0026gt;\u0026gt; iuse fi done if [ -e iuse ]; then mail -s \u0026quot;Filesystem IUse% check\u0026quot; root@localhost \u0026lt; iuse rm -rf iuse fi  思路： 1、df -h、df -i 记录磁盘分区使用率和inode使用率，date +%F 日志名格式 2、取出使用率(第5列)百分比序列，for循环逐一与85比较，大于85则记录到新文件里，当for循环结束后，汇总超过85的一并发送邮件(邮箱服务因未搭建，发送本地root账户)。\n此脚本正确运行前提： 该系统没有逻辑卷的情况下使用，因为逻辑卷df -h、df -i 时，使用率百分比是在第4列，而不是第5列。如有逻辑卷，则会漏统计逻辑卷使用情况。\n（18）需求：\u0026ndash;获取文件列表 有一台服务器作为web应用，有一个目录（/data/web/attachment）不定时地会被用户上传新的文件，但是不知道什么时候会上传。所以，需要我们每5分钟做一次检测是否有新文件生成。 请写一个shell脚本去完成检测。检测完成后若是有新文件，还需要将新文件的列表输出到一个按年、月、日、时、分为名字的日志里。请不要想的太复杂，核心命令只有一个 find /data/web/attachment -mmin -5\n思路： 每5分钟检测一次，那肯定需要有一个计划任务，每5分钟去执行一次。脚本检测的时候，就是使用find命令查找5分钟内有过更新的文件，若是有更新，那这个命令会输出东西，否则是没有输出的。固，我们可以把输出结果的行数作为比较对象，看看它是否大于0。\n#!/bin/bash d=`date -d \u0026quot;-5 min\u0026quot; +%Y%m%d%H%M` basedir=/data/web/attachment find $basedir/ -type f -mmin -5 \u0026gt; /tmp/newf.txt n=`wc -l /tmp/newf.txt` if [ $n -gt 0 ]; then /bin/mv /tmp/newf.txt /tmp/$d fi  （19）需求：\u0026ndash;统计最常用命令 写一个shell脚本来看看你使用最多的命令是哪些，列出你最常用的命令top10。 思路：我们要用到一个文件就是.bash_history，然后再去sort、uniq，剩下的就不用我多说了吧。很简单一个shell。\nsort /root/.bash_history |uniq -c |sort -nr |head\n（20）需求：\u0026ndash;统计日志大小 假如我们需要每小时都去执行你写的脚本。在脚本中实现这样的功能，当时间是0点和12点时，需要将目录/data/log/下的文件全部清空，注意只能清空文件内容而不能删除文件。而其他时间只需要统计一下每个文件的大小，一个文件一行，输出到一个按日期和时间为名字的日志里。 需要考虑/data/log/目录下的二级、三级、\u0026hellip; 等子目录里面的文件。\n#!/bin/bash logdir=\u0026quot;/data/log\u0026quot; t=`date +%H` d=`date +%F-%H` [ -d /tmp/log_size ] || mkdir /tmp/log_size for log in `find $logdir -type f` do if [ $t == \u0026quot;0\u0026quot; ] || [ $t == \u0026quot;12\u0026quot; ] then true \u0026gt; $log else du -sh $log \u0026gt;\u0026gt;/tmp/log_size/$d fi done  ##（21）需求：\u0026ndash;统计数字并求和 计算文档a.txt中每一行中出现的数字个数并且要计算一下整个文档中一共出现了几个数字。例如a.txt内容如下： 12aa*lkjskdj alskdflkskdjflkjj 我们脚本名字为 ncount.sh, 运行它时： bash ncount.sh a.txt 输入结果应该为： 2 0 sum:2\n#!/bin/bash n=`wc -l a.txt|awk '{print $1}'` sum=0 for i in `seq 1 $n` do line=`sed -n \u0026quot;$i\u0026quot;p a.txt` n_n=`echo -n $line|sed 's/[^0-9]//g'|wc -c` echo line $i number: $n_n sum=$[$sum+$n_n] done echo sum is $sum  ##（22）需求：\u0026ndash;检测文件改动\n有两台Linux服务器A和B，假如A可以直接ssh到B，不用输入密码。A和B都有一个目录叫做/data/web/ 这下面有很多文件，当然我们不知道具体有几层子目录，假若之前A和B上该目录下的文件都是一模一样的。但现在不确定是否一致了。固需要我们写一个脚本实现这样的功能，检测A机器和B机器/data/web/目录下文件的异同，我们以A机器上的文件作为标准。比如，假若B机器少了一个a.txt文件，那我们应该能够检测出来，或者B机器上的b.txt文件有过改动，我们也应该能够检测出来（B机器上多了文件我们不用考虑）。 提示： 使用核心命令 md5sum a.txt 算出md5值，去和B机器上的比较。\n#!/bin/bash #假设A机器到B机器已经做了无密码登录设置 dir=/data/web ##假设B机器的IP为192.168.0.100 B_ip=192.168.0.100 find $dir -type f |xargs md5sum \u0026gt;/tmp/md5.txt ssh $B_ip \u0026quot;find $dir -type f |xargs md5sum \u0026gt;/tmp/md5_b.txt\u0026quot; scp $B_ip:/tmp/md5_b.txt /tmp for f in `awk '{print $2}' /tmp/md5.txt` do if grep -q \u0026quot;$f\u0026quot; /tmp/md5_b.txt then md5_a=`grep $f /tmp/md5.txt|awk '{print $1}'` md5_b=`grep $f /tmp/md5_b.txt|awk '{print $1}'` if [ $md5_a != $md5_b ] then echo \u0026quot;$f changed.\u0026quot; fi else echo \u0026quot;$f deleted. \u0026quot; fi done  （23）需求：\u0026ndash;统计网卡流量 写一个脚本,检测你的网络流量，并记录到一个日志里。需要按照如下格式，并且一分钟统计一次（只需要统计外网网卡，假设网卡名字为eth0)： 2017-08-04 01:11 eth0 input: 1000bps eth0 output : 200000bps ################ 2017-08-04 01:12 eth0 input: 1000bps eth0 output : 200000bps\n提示：使用sar -n DEV 1 59 这样可以统计一分钟的平均网卡流量，只需要最后面的平均值。另外，注意换算一下，1byt=8bit 查找并删除符合要求的文件 1、find / -name “test*” | xargs rm -rf 2、find / -name “test*” -exec rm -rf {} ; 3、rm -rf $(find / -name “test”)\nLinux截取文件指定行数之间的内容 sed -n ‘开始行数，结束行数p’ 被截取文件 \u0026gt; 另存为文件\n从ps aux的结果中提取pid ps aux | grep process_name | tr -s | cut -d -f 2 首先应该使用tr命令压缩各个列之间的空格，将多个空格压缩为一个，接着使用cut命令根据空格对列进行分割并取出第二个位置的值，也就是PID\n算法题： 一个长字符串，去除重复字符串，且保持字典序最小\n","date":"2022-12-26","permalink":"/post/shell_practive/","tags":["Linux"],"title":"shell_practive"},{"content":"questions Failed to load module \u0026ldquo;xapp-gtk3-module\u0026rdquo; Installing xapp installs the missing modules and removes the warning\nsudo apt install xapp\n某个命令平时能正常使用，加sudo或者进入root用户显示命令未找到 以node，npm为例，自己下载了包解压到/opt并且在当前用户.bashrc文件配置了环境变量，所以能使用，但是在root用户的环境下却是没有的，为其创建软链接（使用绝对路径）到/usr/local/bin,或者/usr/bin即可,建议前者吧，不用了要删除也方便分辨些\nAppImage文件无法正常启动 通过命令行./启动,发现是缺少相应的库，补全即可 sudo apt install libfuse2\nconfigure: error: glib-compile-schemas not found. apt-get install libglib2.0-dev\nconfigure: error: Your intltool is too old. You need intltool 0.40.4 or later. apt install intltool\nubuntu 字体异常 系统使用全英文时，默认字体Noto Sans CJK优先显示日文汉字，打开文件/etc/fonts/conf.avail/64-language-selector-prefer.conf将SC的优先级提到最高后重启电脑即可。\nln 命令错误 Too many levels of symbolic links 原因在于生成软连接是没有写完整的路径，所以只要写绝对路径就行了\n安装 python 后，使用python使用的不是系统的版本 /bin ,/usr/bin,usr/local/bin中的软链接指向的都是系统版本\n因为在.bashrc或.zshrc中设置了环境变量，优先级覆盖了\n崩溃了，早期那会不再使用 ubuntu，就是因为安装软件的依赖无法解决，现在看来还是太菜了，不适合我，新换的系统，一个星期，配置得七七八八了,从 grub 进登录页面也需要花一定时间，可能10秒左右，使用中也有一些卡顿，文件管理器在复制文件的时候会突然崩溃。\n","date":"2022-12-26","permalink":"/post/ubuntu/","tags":["Linux"],"title":"Ubuntu"},{"content":"数组作为函数参数的问题 ` #!/bin/bash\nurlOne=\u0026ldquo;https://raw.githubusercontent.com/ngosang/trackerslist/master/\" arrayOne=(\u0026ldquo;trackers_all.txt\u0026rdquo; \u0026ldquo;trackers_all_http.txt\u0026rdquo; \u0026ldquo;trackers_all_https.txt\u0026rdquo; \u0026ldquo;trackers_all_ip.txt\u0026rdquo; \u0026ldquo;trackers_all_udp.txt\u0026rdquo; \u0026ldquo;trackers_all_ws.txt\u0026rdquo; \u0026ldquo;trackers_best.txt\u0026rdquo; \u0026ldquo;trackers_best_ip.txt\u0026rdquo;)\nfunction getTracker(){ array=$2 echo ${array[]} for i in ${array[]} do url=\u0026quot;${1}${i}\u0026rdquo; echo \u0026ldquo;wget $url\u0026rdquo; done }\ngetTracker ${urlOne} \u0026ldquo;${arrayOne[]}\u0026rdquo; ` 因为不是只传一个数组，要考虑其他参数的情况，而且数组不一定在最前最后，即尽量不在函数内使用\u0026quot;$@\u0026quot;、\u0026quot;@\u0026quot; 接收再切片的方式。\n1、传参\n如果使用@的方式，无论有没有\u0026quot;\u0026ldquo;数组都会被展开，结果就变成了函数接受参数1和数组里面的每个值作为参数，总参数个数就变成了1+数组长度，如果数组有多个值就会影响到脚本的结果了。\n不使用\u0026quot;\u0026ldquo;包裹则不管是@、* 也都会被展开。\n只有使用\u0026quot;\u0026quot; 和 * 传递 在函数内才能使用位置参数去接收。\n2、函数内部 因为语法限制，无法使用\u0026rdquo;${${}}\u0026ldquo;的方式，所以要使用一个变量去接收再使用。\n下面的情况有点迷惑了。\n直接输出\u0026rdquo;$2\u0026quot;得到的是一个整体，这个整体按照shell的理念应该是字符串，无论是直接使用变量名、\u0026quot;${name[*]}\u0026quot;、\u0026quot;${name[@]}\u0026ldquo;都能正确的循环，也就排除了会动态转为数组的可能。\n单写了一个测试的，对于不管是\u0026rdquo;\u0026quot;、\u0026lsquo;\u0026lsquo;包裹的有空格的字符串，for循环都会根据空格去拆分。\n于是猜测是不是会自动识别分隔符，更改了IFS的值，果然遍历就只输出了一个整体，再对字符串中的某些空格改为新的分隔符，的确是根据分隔符来拆分。\n但为了整体的统一，还是在读取循环时也使用\u0026rdquo;${name[*]}\u0026ldquo;的方式吧。\n“sudo echo ＞＞”或类似命令串提示权限不够的解决办法 这是因为重定向符号 “\u0026gt;” 和 “\u0026raquo;” 也是 bash 的命令。sudo 只是让 echo 命令具有了 root 权限，但是没有让 “\u0026gt;” 和 “\u0026raquo;” 命令也具有root 权限，所以 bash 会认为这两个命令都没有写入信息的权限。\n解决方法一： 利用 “sh -c” 命令，它可以让 bash 将一个字串作为完整的命令来执行，这样就可以将 sudo 的影响范围扩展到整条命令。具体用法如下：\nsudo sh -c 'command line' sudo sh -c 'echo \u0026quot;114.250.64.34 translate.googleapis.com\u0026quot; \u0026gt;\u0026gt; /etc/hosts'  解决方法二： 利用管道和 tee 命令，该命令可以从标准输入中读入信息并将其写入标准输出或文件中，具体用法如下：\necho \u0026quot;strings\u0026quot; | sudo tee -a filename echo \u0026quot;114.250.64.34 translate.googleapis.com\u0026quot; | sudo tee -a /etc/hosts  读取文件的几种方法 while read line do echo $line done \u0026lt; filename   cat filename | while read line do echo $line done  IFS 分隔符；cat 逐行读取文件 for line in `cat filename` do echo $line done  使用上面的方法读取文件时，当行内有空白符(空格、tab、换行)时就不会按行输出了。\na b 1 2 3 4  for i in `cat test.txt` do echo $i done   a b 1 2 3 4  除了更换之前的 while 方法外，还可以通过指定分隔符来实现。\nIFS=$'\\n' # 定义分割符 # for i in $(cat file) # better # for i in $(\u0026lt;file) # in bash for i in `cat file` do echo \u0026quot;$i\u0026quot; done   IFS=\u0026quot;\\n\u0026rdquo; # 将字符 n 作为 IFS 的换行符。 IFS=$\u0026quot;\\n\u0026quot; # 这里\\n确实通过$转化为了换行符，但仅当被解释时（或被执行时）才被转化为换行符;第一个和第二个是等价的 IFS=$\u0026rsquo;\\n\u0026rsquo; # 这才是真正的换行符。\n Shell 脚本中有个变量叫 IFS(Internal Field Seprator) ，内部域分隔符。 Shell 的环境变量分为 set, env 两种，其中 set 变量可以通过 export 工具导入到 env 变量中。 其中，set 是显示设置 shell 变量，仅在本 shell 中有效；env 是显示设置用户环境变量 ，仅在当前会话中有效。\nIFS 是一种 set 变量，当 shell 处理\u0026quot;命令替换\u0026quot;和\u0026quot;参数替换\u0026quot;时，shell 根据 IFS 的值，默认是 space, tab, newline 来拆解读入的变量，然后对特殊字符进行处理，最后重新组合赋值给该变量.\n","date":"2022-12-26","permalink":"/post/shell_question/","tags":["Linux"],"title":"shell_question"},{"content":"国外和国内性价比比较高的云服务器对比： 一、国外性价比较高的云服务器的厂商 1、google云 官网：https://cloud.google.com/ 优势：注册即送300美元的免费试用赠金，有效期是1年 创建一个最低配的实例【1 vCPU；0.6GB内存；10GB硬盘】（我注册下来后，从来没用过，每个月大约花费1.14美元）， 使用1年绝对是绰绰有余的。谷歌在这块还是挺良心的。 有两点需要注意一下： 1、注册的时候，需要填写一个美国地址。在这个http://www.shenfendaquan.com/网站可随机生成一个地址 2、需要持有一张双币信用卡去注册，有网友说visa不能注册，但是我的就是招商银行的visa信用卡，也是能正常注册的。\n2、AWS 官网：https://amazonaws-china.com/cn/\n首页上的大Banner就挺吸引人的。 对性能没有什么要求的同学，可以看 申请EC2，12个月内，每个月可以免费使用750个小时（31天），如果不追求服务器的性能，这个性价比还是很高的。\n3、Vultr 官网：https://www.vultr.com/ 使用的Vultr的人还是挺多的，稳定性也还行，关键还可以支持支付宝支付。 以前注册时好像还有优惠码，但是现在好久没有用Vultr了，也不知道还有没有优惠。 4、搬瓦工 官网：https://www.bwh88.net/ 配置和价格大概是这样婶的。 总结：\n如果要买国外的服务器，推荐用谷歌云和AWS，白嫖即艺术。\n二、国内比较好的云服务器的厂商（腾讯云、阿里云） 虽然国外有限时免费的云服务器，但是我还是不推荐买国外的厂商，有点经济实力的，还是用国内的云服务器厂商比较靠谱。\n优势在于： 1、腾讯云 国内的云服务器厂商，稳定性、安全性、工单响应速度、性价比等方面，都还算不错，而腾讯云的性价比又是一大卖点，对于初级玩家来说，算是相当不错的了。\n现在恰逢618，腾讯云这么喜欢搞活动，肯定是少不了促销的啦！\n一年才95元，每天约等于0.26元，对于学生党来说，是相当友好的了。试想一下，去网咖开黑，一个小时好像都得10元了吧，如果拿这些钱省下来买个服务器，岂不美哉。\n2、阿里云 口碑最佳，机器性能也十分不错，好多企业也都在用阿里云，毕竟阿里的技术真的没得黑。\n最令我敬佩的是，之前加了一个阿里云的销售，咨询阿里云服务器的问题，当时因为没有什么活动，就没有买。\n没想到每当阿里云有活动的时候，那个销售总会亲自通知我，云服务器搞优惠，这种敬业精神也是值得我们学习的。\n目前阿里云也是有活动的。\n总结：如果是学习和工作的话，还是建议买国内的服务器，毕竟稳定、好用、售后服务更好。\n 服务器有关知识 不知道大家是不是跟我一样，刚入行时总是搞不清物理服务器（独立服务器）、VPS服务器（虚拟专用）、云服务器、虚拟主机、裸金属服务器这些概念。说实话，我刚开始接触的时候也有点头大。后来随着了解地不断加深，逐渐清楚了这些服务器之间的区别，并且学会了如何挑选服务器。今天，我就将自己掌握的服务器有关知识全部分享给大家！希望对大家以后识别服务器、选购服务器能有帮助。\n既然提到了上面这些概念，那就先从区别这些服务器开始吧！\n物理服务器：又叫独立服务器或者传统服务器。顾名思义，物理服务器就是一台台看得见摸得着，摆在机房的实实在在的服务器。可以理解为是一台具有高计算能力、高性能、高安全性、高稳定性、高可控性，独立又完整的电脑。有硬盘、内存、CPU，可以自行分配实行多种网络功能服务，将各种软硬件资源集于一身。\n应用场景：适合大型网站及应用。\n优劣势：优点在于性能稳定、安全性高、更可控；缺点是需要专业的服务器运维人员管理，价格高，灵活性弱，扩展限制大。\n价格：高。\n VPS服务器：又叫虚拟专用服务器。VPS服务器是用虚拟技术把物理服务器划分成若干个独立空间，每一个独立的空间都是虚拟专用服务器，也就是VPS服务器。由于是从物理服务器上虚拟出来的产品，因此没有物理服务器稳定，运算速度也要慢一些，性能不高，适合对配置要求不高的客户。\n应用场景：适合预算不多的网站和应用。\n优劣势：最主要的优势就是价格了，价格比物理服务器便宜很多，比虚拟主机贵不了多少，管理起来比较方便；缺点是在扩容、存储、稳定性方面都不及云服务器，这也是为什么大多数企业选择云服务器的原因。\n价格：便宜。\n 云服务器：又叫云主机，简称ECS。在一组集群服务器上虚拟出多个类似独立服务器的部分。云服务器不是单个的物理服务器，它们之间通过网络技术连接起来形成一台超级计算机，拥有独立的宽带和IP。云服务器具有安全可靠性高、易部署、扩展性高（即时扩展，按需扩展）、性价比高等特点。因为是多个服务器的集群，云服务器还具有硬件独享、资源独享、风险共享的优势。云服务器还具备容错性，故障恢复快，操作系统和软件环境皆可备份，恢复后无需重新配置软件环境。\n应用场景：适用于中小型网站和应用。\n优劣势：优点相比物理服务器更灵活，弹性伸缩管理，价格可按需实时制定，避免造成网络资源的浪费，降低了运营成本；缺点在安全性能方面，用户缺乏对云服务器的控制，因此出于企业数据安全层面考虑，建议选择大厂商。\n价格：低。\n 虚拟主机：又叫虚拟服务器或共享主机。是一种在单一主机或主机群上，比如物理服务器、VPS服务器或者云服务器上安装例如CPanel、Plesk等面板搭建而成的。虚拟主机市场比较混乱，使用云服务器最好（推荐阿里云），物理服务器也可以，但一般不推荐VPS作为虚拟主机的服务器。如果说用租房来比喻VPS和虚拟主机之间的区别，那VPS相当于整租，虚拟主机就是合租。但整租是毛坯房，合租是精装房。\n应用场景：适用于非Java项目，且流量、内存较小的网站应用。\n优劣势：优点在于价格便宜，和物理服务器相比降低了不少运营成本；缺点是稳定性、安全性等都很弱，网站在应对大流量访问时性能不足，对速度和流量有非常大的限制。\n价格：最便宜。\n 裸金属服务器：类似云上的专属物理服务器，在拥有弹性灵活的基础上，具有高性能的计算能力。计算性能与传统物理机无差别，具有安全物理隔离的特点。相当于传统物理服务器的“变态版”。裸金属服务器是集物理服务器的稳定性能，和云服务器高度弹性的资源优势于一身的超级平台。兼具超高计算性能的同时，满足用户对核心应用场景和服务器稳定性的要求。比如，阿里云弹性裸金属服务器-神龙，就具备这样的特点。\n应用场景：适用于对数据安全、性能配置、安全监管等都要求非常严格的Web网站、中大型企业等重量级数据库应用、游戏和金融等高性能网站和应用。\n优劣势：集合了物理服务器的稳定性和云服务器云上资源高度弹性的优势，性能更高，更灵活；缺点是价格相对较高。\n价格：较高。\n 讲完上面几种服务器的区别，可能有人会说还不知道服务器是什么呢，能不能系统地讲一下服务器到底是个什么东东啊？OK，继续往下看。\n服务器是指能向网络用户提供特定服务的软件和硬件。\n服务器的作用：\n是为网络提供特定的服务，人们通常用服务器所能提供的主要服务来命名服务器，比如提供文件共享服务的服务器称为文件服务器，提供打印队列共享服务的服务器称为打印服务器等。\n服务器工作原理：\n如果把服务器比作人，处理器就是服务器的大脑，各种总线就像是分布于全身肌肉中的神经。芯片组有点像骨架，I/O设备就像是通过神经系统支配的人的手、眼睛、耳朵和嘴。电源系统相当于血液循环系统，将能量输送到身体的各个地方。\n服务器作为软件，有很多形式的服务器：文件服务器、数据库服务器、Web服务器、邮件服务器、网页服务器、FTP服务器、域名服务器、应用程序服务器、代理服务器、游戏服务器等。\n服务器系统的硬件构成包括中央处理器、硬盘、内存、芯片组、I/O总线、I/O设备、电源、机箱和相关软件等，和我们平时所接触的电脑有诸多相似之处。但是由于需要提供高可靠的服务，因此在处理能力、稳定性、可靠性、安全性、可扩展性、可管理性等方面要求较高。\n作为服务器大脑的中央处理器，即服务器CPU，是衡量服务器性能的首要指标。接触过局域网络的朋友一定知道，服务器是网络中的重要设备，承载着成千上万用户的访问。因此对服务器有大数据量的快速吞吐、超强稳定性、长时间运行等严格要求。目前，服务器CPU仍按CPU的指令系统来区分，通常分为CISC型CPU和RISC型CPU。后来又出现了一种64位的VLIM指令系统的CPU。\n服务器常见的外型有四种：塔式服务器（又称台式服务器）、机架服务器（rack）、刀片服务器（blade server）、机柜式服务器。\n根据不同的计算能力，按网络规模划分，服务器又分为：工作组级服务器，部门级服务器和企业级服务器。这三者之间的关系是，由工作组级服务器到部门级服务器，再到企业级服务器，对所要服务的联网计算机的数量、处理速度和数据安全性、硬件配置、系统可靠性等要求依次递增。并且对应服务的是小型网络、中型网络、大型网络。\n按架构划分，服务器可分为：CISC架构服务器和RISC架构服务器。\n按用途划分，服务器又可分为通用型服务器和专用型（又称功能型）服务器。通用型服务器，顾名思义就是可以提供各种服务功能的服务器。当前大多数服务器属于通用型服务器。专用型服务器是为某一种或某几种功能专门设计的服务器，在某些方面与通用型服务器不同，比如光盘镜像服务器是用来存放光盘镜像的，就需要配置大容量、高速的硬盘以及光盘镜像软件。\n科普了这么多，可能大家最关心的还是如何选择适合自己的服务器。一般来说，选用服务器需要从以下几个维度来衡量性能指标：\n1.可用性\n可用性是指在一段时间内服务器可供访问者正常使用的时间的百分比。提高可用性可从两方面着手：减少硬件平均故障时间和利用专用功能机制。专用功能机制可在出现故障时自动执行系统或部件切换机制，避免或减少意外停机。\n2.高性能\n顾名思义，指服务器综合性能指标要高。主要要求在运行速度、磁盘空间、容错能力、扩展能力、稳定性、监测功能及电源等方面具有较高的性能指标。尤其是硬盘和电源的热插拔性能、网卡的自适应能力等性能指标要高。\n3.模块化\n模块化是指电源、网卡、SCSI卡、硬盘等部件为模块化结构，且都具有热插拔功能，可在线维护，从而使系统停机的可能性大大减小。特别是分布式电源技术可使每个重要的部件都有独立的电源。\n4.可扩展性\n为了使服务器随负荷的增加可以平稳升级，并保证服务器工作的安全性与稳定性，必须将服务器的可扩展性能作为一项重要衡量指标。首先，在机架上要有为硬盘和电源的增加而预留充分的空间。其次主机上的插槽不但要种类齐全，而且要有一定的余量。\n5.可管理性\n可管理性是指服务器的管理是否方便、快捷，应用软件是否丰富。在可管理性方面，基于Windows NT／2000平台的个人计算机服务器要优于Unix服务器。\n当然，除了以上这些因素是在选购服务器时需要重点考虑的之外。品牌、价格、售后服务以及厂商实力等因素也是需要考虑在内的。\n讲到这里，相信大家对服务器的种类区别，以及如何选择适合自己的服务器已经心里有数了。但我最近也收到一些用户的反馈，他们对另外一些服务器相关的问题还存在疑惑，希望我能帮忙解答一下。借此机会，我也一并分享下。\n问题一：一个网站需要多少服务器？\n这个问题没法直接给出具体数字，因为影响一个网站所需服务器数量的因素有很多，最简单的比如网站源代码。事实上源代码写得越牛，需要的服务器数量就可以越少。反之，则越多。其次，网站业务量越大，整体架构就会越复杂。服务器数量的评估，需要根据不同业务系统的特点具体分析。\n中小型企业需要多少服务器：\n情况一：企业自研发网站，就可以根据业务规模以及业务系统特点，选用不同配置的云服务器。比如阿里云服务器常用的配置有1核1G、1核2G、2核4G、2核8G、4核8G、4核16G、16核32G等，这些都是中小企业用户购买最多的，可以说是爆款云服务器配置。至于数量，这个需要根据网站的实际情况而定。\n情况二：如果企业是购买的阿里云模板建站、半定制化建站产品，则不需要购买服务器。比如阿里云速成美站和云企业官网，服务器是包含在建站费用里的。拥有云服务器（ECS）、负载匀衡(SLB)、云数据库（RDS）、云存储（OSS）、网络加速（CDN）等云计算资源集群，以SaaS的方式提供给用户，让每一个网页都能秒开，同时确保网站稳定。\n阿里云建站“速美”，高性价比，价格低至500元 ​ aliyun3.wezhan.cn 图标\n大型网站需要多少服务器：\n对于电商网站来说，每天都会有大量用户访问，进行购买操作。所以服务器需要进行大量的数据请求处理，因此用于电商网站的服务器在CPU、内存上的要求会比较高。对于视频类网站，除了用户访问请求和下载数据外，还需要有大硬盘、大带宽的配置，才能保障用户在观看视频时不会出现卡顿。\n对于大型网站来说，无论是图片网站、视频网站、门户网站、企业网站、还是电商网站，在租用服务器时，需要注意基本的配置标准。比如CPU、硬盘、内存、带宽、防御上的配置都是需要我们考虑的。通常情况下，CPU建议最好是选择8核以上的，视频网站的话，内存不要低于16G，硬盘至少要1T，带宽独享100M会更好些。\n问题二：有没有便宜好用的云服务器推荐？\n现在买云服务器，大多数用户优先考虑的都是阿里云服务器。阿里云服务器以稳定、安全、方便以及高性价比等优势，一直以来都深受用户欢迎。阿里云服务器是一种高效，计算能力可弹性伸缩的云计算服务，用户可根据业务需要，随时创建、修改、释放云服务器ECS配置。\n运行于阿里云自研的飞天操作系统，具有计算性能可弹性伸缩，存储空间可扩展，网络配置可自定义的低耦合特性。实例规格、磁盘、网络、操作系统等作为云服务器ECS的组成部分，可以像搭积木一样任意组合卸载，满足用户多样化的需求。\n价格方面，阿里云服务器有各种各样便宜到爆的特惠活动。比如：个人新用户低至1折（限购1-2台）。企业新用户低至2.6折（限购3-5台）。每日10点限量抢爆款产品，先到先得。此外还有新注册用户专享，爆款云服务器免费试用12个月等优惠活动，0成本上云，0元试用18+款产品。了解详情可咨询阿里云官网。\n问题三：阿里云服务器怎么样？腾讯云与阿里云，哪个更好些？\n腾讯云，和青云、景安云、UCloud这几家比。平心而论，腾讯云是有一些优势的。但是和阿里云比，腾讯云目前还处于推广阶段，优惠比较多。但是性能方面，我没用过，所以特意问了一些用过的朋友，据他们反馈不是很好。\n阿里云，作为毫无争议的中国云服务第一品牌。从2009年创立，发展至今已有11个年头。拥有弹性计算10年的深厚技术积淀，技术领先，性能优异，稳如磐石。阿里云目前的服务范围已经覆盖200多个国家和地区，在全球18个地域开放了49个可用区，在全球部署了200多个飞天数据中心。\n阿里云服务全面覆盖IaaS、PaaS、SaaS三大云服务类型，产品涉及云计算基础、安全、大数据、人工智能、企业应用、物联网等众多领域。自主研发的超大规模通用计算操作系统飞天系统，已经全面服务于全球范围内的客户。飞天系统能够将遍布全球的百万级服务器连成一台超级计算机，以在线公共服务的方式提供计算能力。\n云服务器方面，阿里云目前从云服务器ECS已经衍生出了多个云服务器系列，包括适用于初级用户的轻量应用服务器、适合高性能场景的多种云服务器等。为中小型企业提供一站式互联网服务，专注于云计算领域的研究和研发，在全球云计算领域构建起强大的技术及市场优势。多年来一直保持着高增长，增速已经超越亚马逊云计算的业务增速，成为全球增速最快的云计算服务商。\n在云服务器市场，阿里云服务器占据近五成的国内市场份额，国内市场排名第一无可争议。阿里云服务器成为企业在部署各种应用场景的时候考虑云计算服务商的首选。跟其他云服务器厂商比，阿里云服务器有什么优势？\n1.弹性计算服务ECS：支持分钟级别创建1000台实例，多种弹性付费选择更贴合业务现状，同时带来弹性的扩容能力，实例与带宽均可随时升降配，云盘可扩容。帮助用户构建更稳定、更安全的应用。提升运维效率，降低IT整体成本，使用户更专注于核心业务创新。提供GPU和FPGA等异构计算服务器、弹性裸金属服务器以及通用的x86架构服务器。\n2.稳定可靠：同地域多可用区（机房）可为用户提供超高的容灾能力。单实例可用性达99.975%，多可用区多实例可用性达99.995%，云盘可靠性达99.9999999%。可实现自动宕机迁移、快照备份，进一步保障企业服务和数据安全。\n3.高安全性：提供虚拟防火墙、角色权限控制、内网隔离、防病毒攻击及流量监控等多重安全方案。免费提供 DDoS 防护、木马查杀、防暴力破解等服务，通过多方国际安全认证，ECS云盘支持数据加密功能等。\n4.高性能：单实例最高可选88vCPU，内存704GB，单实例性能最高可达到700万PPS网络收发包，35Gbps带宽。提供性能监控框架和主动运维体系。\n5.易部署：分钟级交付，快速部署，缩短应用上线周期。快速接入部署在全球范围内的数据中心和BGP机房。\n6.高性价：成本透明，按需使用，支持根据业务波动随时扩展和释放资源。支持包年月预付费，按需计费，满足不同用户场景需求。无需自建机房，无需采购以及配置硬件设施，无需服务器、网络和硬件等维护，零成本运维。提供10大类、40多个不同版本的免费基础操作系统。\n7.可扩展性：支持通过内网访问其他阿里云服务，形成丰富的行业解决方案，降低公网流量成本。ECS 可与阿里云各种丰富的云产品无缝衔接，可持续为业务发展提供完整的计算、存储、安全等解决方案。\n8.易用性：丰富的操作系统和应用软件，通过镜像可一键简单部署，同一镜像可在多台 ECS 中快速复制环境，轻松扩展。提供行业通用标准API，提高易用性和适用性。\n（配图来源于网络，侵删）\n参考资料：\n1.计算机网络与应用技术\n2.电子商务师、高级电子商务师\n3.大学计算机基础\n","date":"2022-12-26","permalink":"/post/server_selection/","tags":["Linux"],"title":"Server_Selection"},{"content":"wget -m -e robots=off -k -E \u0026quot;https://python3webspider.cuiqingcai.com/\u0026quot;   wget整站抓取、网站抓取功能；下载整个网站；下载网站到本地 wget -r -p -np -k -E http://www.xxx.com 抓取整站 wget -l 1 -p -np -k http://www.xxx.com 抓取第一级 -r 递归抓取 -k 抓取之后修正链接，适合本地浏览 wget -m -e robots=off -k -E \u0026quot;http://www.abc.net/\u0026quot; 可以将全站下载以本地的当前工作目录，生成可访问、完整的镜像。 解释： -m //镜像，就是整站抓取 -e robots=off //忽略robots协议，强制、流氓抓取 -k //将绝对URL链接转换为本地相对URL -E //将所有text/html文档以.html扩展名保存  一、回到上次操作的目录 cd - 进入上次访问目录\n命令跳转 在终端中按捉 [Ctrl] 键的同时 [r] 键，出现提示：(reverse-i-search) 此时你尝试一下输入你以前输入过的命令，当你每输入一个字符的时候，终端都会滚动显示你的历史命令。 当显示到你想找的合适的历史命令的时候，直接 [Enter]，就执行了历史命令。\n另外， [Ctrl + p] 或 [Ctrl + n] 快速向前或向后滚动查找一个历史命令， 对于快速提取刚刚执行过不久的命令很有用。\n三、命令行内快速操作键\n  移动操作快捷键 Ctrl + f\u0026ndash; 向右移动一个字符，当然多数人用→ Ctrl + b\u0026ndash; 向左移动一个字符， 多数人用← ESC + f\u0026ndash; 向右移动一个单词，MAC下建议用ALT + → ESC + b\u0026ndash; 向左移动一个单词，MAC下建议用ALT + ← Ctrl + a\u0026ndash; 跳到行首 Ctrl + e\u0026ndash; 跳到行尾\n  删除操作快捷键 Ctrl + d\u0026ndash; 向右删除一个字符 Ctrl + h\u0026ndash; 向左删除一个字符 Ctrl + u\u0026ndash; 删除当前位置字符至行首（输入密码错误的时候多用下这个） Ctrl + k\u0026ndash; 删除当前位置字符至行尾 Ctrl + w\u0026ndash; 删除从光标到当前单词开头\n  3.其他操作快捷键 Ctrl + y\u0026ndash; 插入最近删除的单词 Ctrl + c\u0026ndash; 终止操作 Ctrl + d\u0026ndash; 当前操作转到后台 Ctrl + l\u0026ndash; 清屏 （有时候为了好看）\n双（多）系统启动项修复 一般推荐的是先安装linux 系统再安装win系统，这样一般不会出现启动项被win强占的后果，因为win的启动项只支持自身的系统，不会检测启动其他启动项，回导致明明安装了多个系统，但是启动总是进入win。\n也有的启动项因为启动项本事设置的问题，硬件的兼容问题，有时也会启动项失效，安装deepin时有这种情况出现过，ubuntu、mnajaro、kali都正常。\n或者电脑维修、清灰等时启动盘未完全归位的开机也会导致启动项重置。并且这种情况是在bios中也找不到其他系统的启动项的。\n在确定磁盘内的各文件都还在的情况下可通过软件DiskGenius的工具-设置UEFI-BIOS启动项，点击添加找到相应目录下的文件即可，然后调整linux的为第一位。如果安装混乱也可以通过这个工具删除多余的启动项。\n","date":"2022-12-26","permalink":"/post/other/","tags":["Linux"],"title":"other"},{"content":"Manjaro_KDE的大多数内容都是可以应用到这上面来的。\n之前的 manajaro-kde 为18版本，当时分区/boot/efi并没有出错，但是在这次的 manjaro-xfce-20.1 如此分区时安装到后期报错，大概看后解释说是因为先安装有 win10,win10已经存在一个/boot/efi分区，因此造成冲突，将win10的分区挂载但是保持不格式化，然后分区/boot就可以了。 美化什么的就不要有太多期望了，可操作性很小，但是其本身虽然是很轻巧的，但是从同图标到整体风格都刚刚好，没有任何粗糙的感觉。安装的东西过程大多相同，但也有些差异。 代理的全局设置一直是一个很头疼的问题，以前在 ubuntu 没有解决，在 manjaro-kde 前期也一直在寻找合适的方法，后面在系统里面有设置全局代理解决了，但这个因为是轻量级所以没有了。\n软件 yay -S geeqie # 开源看图软件 yay -S ark # 解压缩，自带的 engrampa 解压速度快但是解压的类型很少  之前配置过zsh，为了方便留了文件，直接本地安装吧 https://github.com/ohmyzsh/ohmyzsh下载oh-my-zsh:\ncp oh-my-zsh/ ~/.oh--my-zsh cp oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc  插件： autojump: zsh-autosuggestions zsh-syntax-highlighting\nqbittorrent 能添加trackers的磁力下载器\nliferea rss 阅读器，重要的是可以配置代理，这样就可以添加一些墙外的源了。\n问题 1、.Appimages文件无法打开 不知道是不是动了什么东西，导致最开始能打开的 .Appimages 文件无法打开了，报错忘记截图了，大概意思是 linux 的 fuse 模块没有自动加载，\nsudo vim /etc/rc.moudules 添加 modprobe fuse  2、快捷方式 在一开始的时候 win 键盘可以正常使用，但是后来可能是更新了还是什么的，不起作用了，而且之前设置的 xfce4-terminal 下拉终端也 不起作用了，去快捷方式里面找，直接不见之前的一些设置了，而且重置也和最开始配置的那会不一样，最后查询了终于补齐了，但是有新的问题，比如调用文件夹的是WIN + E ，会先打开开始模块再打开文件夹，相当于是个bug吧，只能用右边的 WIN 键，但是不方便，只好改一个键。\n而且有两个设置快捷键的地方，都去改动一下。删除不需要的，以免是全局的影响了其他软件的使用 3、pycharm 中 md 文件预览乱码 \u0026amp; 一些中文字体的不正常(如门，画等) 方法来自http://panqiincs.me/2019/06/05/after-installing-manjaro/，应该大多数linux都可以用吧\nsudo pacman -S ttf-roboto noto-fonts ttf-dejavu # 文泉驿 sudo pacman -S wqy-bitmapfont wqy-microhei wqy-microhei-lite wqy-zenhei # 思源字体 sudo pacman -S noto-fonts-cjk adobe-source-han-sans-cn-fonts adobe-source-han-serif-cn-fonts  创建文件~/.config/fontconfig/fonts.conf，加入下面的配置：\n\u0026lt;?xml \u0026lt;!DOCTYPE fontconfig SYSTEM \u0026quot;fonts.dtd\u0026quot;\u0026gt; \u0026lt;fontconfig\u0026gt; \u0026lt;its:rules xmlns:its=\u0026quot;http://www.w3.org/2005/11/its\u0026quot; version=\u0026quot;1.0\u0026quot;\u0026gt; \u0026lt;its:translateRule translate=\u0026quot;no\u0026quot; selector=\u0026quot;/fontconfig/*[not(self::description)]\u0026quot;/\u0026gt; \u0026lt;/its:rules\u0026gt; \u0026lt;description\u0026gt;Manjaro Font Config\u0026lt;/description\u0026gt; \u0026lt;!-- Font directory list --\u0026gt; \u0026lt;dir\u0026gt;/usr/share/fonts\u0026lt;/dir\u0026gt; \u0026lt;dir\u0026gt;/usr/local/share/fonts\u0026lt;/dir\u0026gt; \u0026lt;dir prefix=\u0026quot;xdg\u0026quot;\u0026gt;fonts\u0026lt;/dir\u0026gt; \u0026lt;dir\u0026gt;~/.fonts\u0026lt;/dir\u0026gt; \u0026lt;!-- this line will be removed in the future --\u0026gt; \u0026lt;!-- 自动微调 微调 抗锯齿 内嵌点阵字体 --\u0026gt; \u0026lt;match target=\u0026quot;font\u0026quot;\u0026gt; \u0026lt;edit name=\u0026quot;autohint\u0026quot;\u0026gt; \u0026lt;bool\u0026gt;false\u0026lt;/bool\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;edit name=\u0026quot;hinting\u0026quot;\u0026gt; \u0026lt;bool\u0026gt;true\u0026lt;/bool\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;edit name=\u0026quot;antialias\u0026quot;\u0026gt; \u0026lt;bool\u0026gt;true\u0026lt;/bool\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;edit name=\u0026quot;embeddedbitmap\u0026quot; mode=\u0026quot;assign\u0026quot;\u0026gt; \u0026lt;bool\u0026gt;false\u0026lt;/bool\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;!-- 英文默认字体使用 Roboto 和 Noto Serif ,终端使用 DejaVu Sans Mono. --\u0026gt; \u0026lt;match\u0026gt; \u0026lt;test qual=\u0026quot;any\u0026quot; name=\u0026quot;family\u0026quot;\u0026gt; \u0026lt;string\u0026gt;serif\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;edit name=\u0026quot;family\u0026quot; mode=\u0026quot;prepend\u0026quot; binding=\u0026quot;strong\u0026quot;\u0026gt; \u0026lt;string\u0026gt;Noto Serif\u0026lt;/string\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;match target=\u0026quot;pattern\u0026quot;\u0026gt; \u0026lt;test qual=\u0026quot;any\u0026quot; name=\u0026quot;family\u0026quot;\u0026gt; \u0026lt;string\u0026gt;sans-serif\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;edit name=\u0026quot;family\u0026quot; mode=\u0026quot;prepend\u0026quot; binding=\u0026quot;strong\u0026quot;\u0026gt; \u0026lt;string\u0026gt;Roboto\u0026lt;/string\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;match target=\u0026quot;pattern\u0026quot;\u0026gt; \u0026lt;test qual=\u0026quot;any\u0026quot; name=\u0026quot;family\u0026quot;\u0026gt; \u0026lt;string\u0026gt;monospace\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;edit name=\u0026quot;family\u0026quot; mode=\u0026quot;prepend\u0026quot; binding=\u0026quot;strong\u0026quot;\u0026gt; \u0026lt;string\u0026gt;DejaVu Sans Mono\u0026lt;/string\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;!-- 中文默认字体使用思源宋体,不使用 Noto Sans CJK SC 是因为这个字体会在特定情况下显示片假字. --\u0026gt; \u0026lt;match\u0026gt; \u0026lt;test name=\u0026quot;lang\u0026quot; compare=\u0026quot;contains\u0026quot;\u0026gt; \u0026lt;string\u0026gt;zh\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;test name=\u0026quot;family\u0026quot;\u0026gt; \u0026lt;string\u0026gt;serif\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;edit name=\u0026quot;family\u0026quot; mode=\u0026quot;prepend\u0026quot;\u0026gt; \u0026lt;string\u0026gt;Source Han Serif CN\u0026lt;/string\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;match\u0026gt; \u0026lt;test name=\u0026quot;lang\u0026quot; compare=\u0026quot;contains\u0026quot;\u0026gt; \u0026lt;string\u0026gt;zh\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;test name=\u0026quot;family\u0026quot;\u0026gt; \u0026lt;string\u0026gt;sans-serif\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;edit name=\u0026quot;family\u0026quot; mode=\u0026quot;prepend\u0026quot;\u0026gt; \u0026lt;string\u0026gt;Source Han Sans CN\u0026lt;/string\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;match\u0026gt; \u0026lt;test name=\u0026quot;lang\u0026quot; compare=\u0026quot;contains\u0026quot;\u0026gt; \u0026lt;string\u0026gt;zh\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;test name=\u0026quot;family\u0026quot;\u0026gt; \u0026lt;string\u0026gt;monospace\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;edit name=\u0026quot;family\u0026quot; mode=\u0026quot;prepend\u0026quot;\u0026gt; \u0026lt;string\u0026gt;Noto Sans Mono CJK SC\u0026lt;/string\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;!-- 把Linux没有的中文字体映射到已有字体，这样当这些字体未安装时会有替代字体 --\u0026gt; \u0026lt;match target=\u0026quot;pattern\u0026quot;\u0026gt; \u0026lt;test qual=\u0026quot;any\u0026quot; name=\u0026quot;family\u0026quot;\u0026gt; \u0026lt;string\u0026gt;SimHei\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;edit name=\u0026quot;family\u0026quot; mode=\u0026quot;assign\u0026quot; binding=\u0026quot;same\u0026quot;\u0026gt; \u0026lt;string\u0026gt;Source Han Sans CN\u0026lt;/string\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;match target=\u0026quot;pattern\u0026quot;\u0026gt; \u0026lt;test qual=\u0026quot;any\u0026quot; name=\u0026quot;family\u0026quot;\u0026gt; \u0026lt;string\u0026gt;SimSun\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;edit name=\u0026quot;family\u0026quot; mode=\u0026quot;assign\u0026quot; binding=\u0026quot;same\u0026quot;\u0026gt; \u0026lt;string\u0026gt;Source Han Serif CN\u0026lt;/string\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;match target=\u0026quot;pattern\u0026quot;\u0026gt; \u0026lt;test qual=\u0026quot;any\u0026quot; name=\u0026quot;family\u0026quot;\u0026gt; \u0026lt;string\u0026gt;SimSun-18030\u0026lt;/string\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;edit name=\u0026quot;family\u0026quot; mode=\u0026quot;assign\u0026quot; binding=\u0026quot;same\u0026quot;\u0026gt; \u0026lt;string\u0026gt;Source Han Serif CN\u0026lt;/string\u0026gt; \u0026lt;/edit\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;!-- Load local system customization file --\u0026gt; \u0026lt;include ignore_missing=\u0026quot;yes\u0026quot;\u0026gt;conf.d\u0026lt;/include\u0026gt; \u0026lt;!-- Font cache directory list --\u0026gt; \u0026lt;cachedir\u0026gt;/var/cache/fontconfig\u0026lt;/cachedir\u0026gt; \u0026lt;cachedir prefix=\u0026quot;xdg\u0026quot;\u0026gt;fontconfig\u0026lt;/cachedir\u0026gt; \u0026lt;!-- will be removed in the future --\u0026gt; \u0026lt;cachedir\u0026gt;~/.fontconfig\u0026lt;/cachedir\u0026gt; \u0026lt;config\u0026gt; \u0026lt;!-- Rescan in every 30s when FcFontSetList is called --\u0026gt; \u0026lt;rescan\u0026gt; \u0026lt;int\u0026gt;30\u0026lt;/int\u0026gt; \u0026lt;/rescan\u0026gt; \u0026lt;/config\u0026gt; \u0026lt;/fontconfig\u0026gt;  4、fcitx-googlepinyin 也就是谷歌输入法打不出顿号 前一次在 pycharm 编辑的时候记得就出现了，但是自己当时只不过是当作是一些小问题，没准重启能够解决，但是今天在编辑markdown文本的时候，迫切的需要，但是已经明确切换到中文输入法状态下，点击回车键上的反斜杠“\\”，出来的还是反斜杆，一直查不出什么原因，无论是重启还是修改一些配置，最终在网上找到了解决方法。\n使用中英文标点切换快捷键（ctrl+.）[ 以前不知道这个快捷键]\n估计是在pycharm使用快捷方式时不小心给触碰到了导致的。\n5、Qv2ray代理突然无法使用 以前在代理失效时会有类似的情况，但是在更换了手机上测试可以使用的代理，确定其端口没有被占用，使用的网络没有问题，其 v2ray 启动成功 等等等等之后，还是无法使用代理。\n最后想到自己在前一晚切换到 Win 环境下，而那时 Win 的时间被自己修改过，回想起自己最初使用 Linux 时候由于没有配置时间导致其使用斗鱼无法使用，于是乎去修改了自己的时间，重启代理，问题就解决了。\n6、vim进入后行不会根据屏幕自动换行，以及左侧目录栏显示异常 关闭重新进入vim即可，估计是插件为完全加载的原因\n7、Failed to register AppImage in AppImageLauncherFS: could not open map file 起因是双击打开了一个appimage文件，在设备了设置了appimagelauncher管理，然后退出后通过命令行启动，可能就无法通过appimagelauncher管理了，但是文件又关联了appimagelauncer，就冲突了，设备重启了下就解决了。\n8、无故卡死 没有解决！！！ 安装了某个软件自启动一段时间后导致卡死？ 系统不断的升级某个驱动、库、依赖不兼容导致卡死？ 硬件老化导致的问题？\n而且有的软件本应在状态栏的显示不显示，实际应用未退出。 基于 arch，一些软件没有兼容到，无法安装，或者是不方便安装。 为了更好的开发，暂时换了系统。\n","date":"2022-12-26","permalink":"/post/manjaro_xfce/","tags":["Linux"],"title":"Manjaro_Xfce"},{"content":"一、（图形化软件）Qv2ray 全平台： linux、win 、mac 都可有可使用的版本 是开源的的第三方图形化界面，可以直接使用v2ray，在而且在添加插件后可使用 ssr、torjan 等其他，现在在使用的就是这个，还可以查看速度以及使用量，但是其默认监听端口不是1080,需要的可以自己改动一下。 项目地址： https://github.com/Qv2ray/Qv2ray/releases \n1、安装： 直接去项目地址下载 AppImage 包， 直接双击打开就能使用，但是会提示缺少核心文件\n去查看核心文件的所在位置，两种解决方法，一是去安装v2ray后更改路径，二是把去下载相应的vcore文件复制移动到其默认路径下\n去 https://github.com/v2ray/v2ray-core/releases/下载v2ray-linux-64.zip文件，解压位置自定，然后改路径\n2、注意： 留意这个设置，该客户端的默认端口不是1080，需要改的自己修改下才能使用\n二、代理的应用： 1、Firefox 填入代理端口：（例如：1080） 然后可以正常爬墙，不过是全局代理\n2、Chrome （插件）（ SwitchOmega ） (1)、把情景模式下的 proxy 和 auto switch 都删除\n(2)、新建情景模式： 1 )名称：Proxy；类型：代理服务器。 2) 名称：Auto；类型：自动切换模式。 3). 设置 Proxy\n代理协议：HTTP；代理服务器：127.0.0.1；代理端口：1080 4). 设置 Auto 个人感觉：实际应用中并不是很好用。所以自己在后期都不设置这项了。就直接连接和使用代理总有一个是能用的。\n在上面规则列表规则的情景模式里选择 proxy 规则列表格式选 AutoProxy 在 添加规则列表 里面输入如下网址\nhttps://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt  并点击下载链接里面的内容到本地\n5). 应用选项，测试 上面有一个细节，在点击下载文件的时候需要事先去SwitchOmega切换到代理，链接是被墙的，没有先开代理的话会一直提示下载失败（无法连接到网络）\n3、Telegram的使用： telegram 也是需要手动去配置代理，上面说的什么方法对它都不管用，使用是必须使用代理的。\n这样就可以正常的使用了。\n4、终端代理 终端需要额外配置： 以上面的 qv2ray 为例，该客户端有两种方法，一个是 socket 通信,一个是 http 通信,两个方式对应的端口不同\n(1)一次性的： 如果有想要 http 通信，http 的代理端口设置 12333，想执行 wget 或者 curl来下载国外的东西，可以使用如下命令：\nexport http_proxy=\u0026quot;http://127.0.0.1:12333\u0026quot; export https_proxy=\u0026quot;http://127.0.0.1:12333\u0026quot; 或者直接： export ALL_PROXY=http://127.0.0.1:12333  或者走 socket5 协议的话，代理端口设置 1080\nexport http_proxy=\u0026quot;socks5://127.0.0.1:1080\u0026quot; export https_proxy=\u0026quot;socks5://127.0.0.1:1080\u0026quot; 或者： export ALL_PROXY=\u0026quot;socks5://127.0.0.1:1080\u0026quot;  (2)永久的： 把代理服务器地址写入shell配置文件.bashrc或者.zshrc 直接在.bashrc或者.zshrc添加下面内容\nexport http_proxy=\u0026quot;http://127.0.0.1:12333\u0026quot; export https_proxy=\u0026quot;http://127.0.0.1:12333\u0026quot;  或者走socket5协议的话，代理端口是1080\nexport http_proxy=\u0026quot;socks5://127.0.0.1:1080\u0026quot; export https_proxy=\u0026quot;socks5://127.0.0.1:1080\u0026quot;  然后刷新设置\nsource ~/.bashrc 或者： source ~/.zshrc  (3)自动开启关闭： 或者通过设置 alias 简写来简化操作，每次要用的时候输入setproxy，不用了就unsetproxy。\nalias setSocks=\u0026quot;export ALL_PROXY=socks5://127.0.0.1:1080\u0026quot; alias setHttp=\u0026quot;export ALL_PROXY=\u0026quot;http://127.0.0.1:12333\u0026quot; alias unsetproxy=\u0026quot;unset ALL_PROXY\u0026quot;  或在~/.zshrc中添加如下脚本后刷新\n[[设置socks5]]代理 function setsocks5() { export ALL_PROXY=\u0026quot;socks5://127.0.0.1:1080\u0026quot; } [[设置http]]代理 function http() { export ALL_PROXY=\u0026quot;http://127.0.0.1:12333\u0026quot; } # 取消终端代理 function unsetProxy() { unset ALL_PROXY } # 测试终端代理 function testProxy() { curl -i 'http://ip.cn' }  #通常端口号是固定的，不过如果有需要随时变换的参考如下脚本\n function setProxy() { export ALL_PROXY=\u0026quot;socks5://127.0.0.1:${1}\u0026quot; } 启动为： setProxy \u0026lt;port\u0026gt; # 启动终端代理连接  可以使用curl -i httpbin.org/get来查看自己的 ip，如果确实发生了改变，那应该就是成功地设置代理了。\n5、git加速 git很明显是被墙的额而且经常使用，直接配置git的命令。使用 ss/ssr ( socks5 类的都可以)来加快 git 的速度 分辨需要设置的代理\n HTTP 形式：  git clone https://github.com/owner/git.git\n  SSH 形式：  git clone git@github.com/owner/git.git\n   一、HTTP 形式 法一： 走 HTTP 代理\ngit config --global http.proxy \u0026quot;http://127.0.0.1:12333\u0026quot; git config --global https.proxy \u0026quot;http://127.0.0.1:12333\u0026quot;  走 socks5 代理（如 Shadowsocks）\ngit config --global http.proxy \u0026quot;socks5://127.0.0.1:1080\u0026quot; git config --global https.proxy \u0026quot;socks5://127.0.0.1:1080\u0026quot;  取消设置\ngit config --global --unset http.proxy git config --global --unset https.proxy  法二： 在 ~/.gitconfig 文件中加入以下配置:\n[http] proxy = socks5://127.0.0.1:1080  注意，上明配置等同于命令 git config --global http.proxy 'socks5://127.0.0.1:1080'\nGit不认https.proxy，设置http.proxy就可以支持 https 了。\n二、SSH 形式 在 ~/.ssh/config 文件中加入以下配置:\nHost github.com HostName github.com User git Port 22 ProxyCommand /usr/bin/ncat --proxy 127.0.0.1:1080 --proxy-type socks5 %h %p  端口被占用解决方法： 有时候会出现弹窗警告1080端口被占用提示： manjaro的端口管理工具：net-tools是\nyay -S net-tools  查看相应的端口号占用的程序：\nnetstat -lnp|grep 1080  关闭应用：\nkill -9 [42058]  不知道这个原因的出现是bug还是什么的吧，切换节点但是端口释放不成功需要像上面那样，有时候退出后也会有端口释放不成功，被长期占用的情况，而且使用会导致电脑卡，在退出的时候尤为明显，有个3、5秒的卡顿。\n二、 脚本安装 1、ssr cd /opt # 下载 curl https://raw.githubusercontent.com/the0demiurge/CharlesScripts/master/charles/bin/ssr -o \u0026quot;ssr\u0026quot; # 或者 wget https://raw.githubusercontent.com/the0demiurge/CharlesScripts/master/charles/bin/ssr -O \u0026quot;ssr\u0026quot; # 添加执行权限 chmod a+x ssr # 为脚本创造软链接 sudo ln -s /home/fish/opt/ssr /usr/bin/ssr # 安装依赖 yaourt -S jq tsocks # 安装ssr客户端 ssr install # 配置 ssr config # 详细配置见ssr配置文件说明 # 启动 ssr start # 停止 ssr stop # 重启 ssr restart # 卸载 ssr uninstall  2、v2ray 使用局域网的代理 手机上的vpn代理有的可以设置开启本地代理，开启后其他设备链接到该设备的ip及开放端口即可使用代理。 应用没有内置本地代理，可以通过Every Proxy 软件实现转发。\n","date":"2022-12-26","permalink":"/post/manjaro_proxy/","tags":["Linux"],"title":"Manjaro_Proxy"},{"content":"！！！ 如何选择manajro 现在有 Xfce 、GNOME 和 KDE Plasma 作为默认桌面环境的3个版本，桌面环境差异不是很大，主要差异为如果按照桌面的轻量级排序 Xfce \u0026gt; GNOME \u0026gt; KDE Plasma 。 Xfce 是最轻量的，比较老的电脑可以选择这个。 GNOME 是以前使用比较多的桌面环境，但随着硬件设备的发展，基本都有在向着 KDE 发展。 KDE 版本 最明显的当然是它的华丽了，很多的桌面修改不用额外去安装、直接在设置里面修改就有很惊艳的效果。\n以下就以我自己的配置为例 此次安装的为 KDE 版本，正常使用的话没有问题，但是在后期设置了不少软件自启，以及桌面装饰 dock 和其他一些美化后，整个使用体验就大幅下降，时不时就卡顿一下，去掉了不必要的配置能够保证正常的使用了，而且虽然 dock 的软件只有几M，但是在运行后是很占空间的。\n一、安装 1.启动盘制作使用的是官方网站推荐的写入软件 rufus 写入镜像。 分区和之前 ubuntu 的差不多，个人觉得几乎所有的 linux 分区都可以这样做。 以我自己的100G为例：\n   boot/efi分区 300M fat32格式     交换分区(swap) 8G ext4格式   主分区(/) 40G ext4格式   home others ext4格式    然后按照提示一步一步正常走完程序然后重启进行一些配置：\n二、配置 1.更新源 1.配置国内源 sudo pacman-mirrors -i -c China -m rank选择目前网络环境下最快的一个\n2.增加 archlinuxcn 源 sudo vim /etc/pacman.conf添加如下内容：\n[archlinuxcn] Server = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/$arch  3.安装签名秘钥和升级系统\nsudo pacman -S archlinuxcn-keyring sudo pacman -Syyu  2.安装 yay yay 和 pacman 差不多，但是其软件资源要更丰富更新也更快。\nsudo pacman -S yay  3.安装软件 (1)输入法 typewriting\n(3)命令行利器 yay -S htop # 进程监控,top 命令的美化版 yay -S screen # 命令行终端切换 yay -S tmux # 和 screen 类似 yay -S net-tools # ifconfig、route、arp 和 netstat 等命令行工具 yay -S iproute2 # 由于 manjaro 是基于 arch 的，所以自带了，但是没有 net-tools yay -S tree # 以树状图列出目录的内容。执行 tree 指令，它会列出指定目录下的所有文件，包括子目录里的文件。  (4)日常软件 音乐 yay -S netease-cloud-music # 网易云音乐 yay -S spotify # 资源较少 yay -S clementine # 可以用做本地播放器，也可以对接 spotify,目前在用的 yay -S cmus # 一个命令行播放音乐的小工具  浏览器 yay -S google-chrome # 谷歌浏览器 yay -S firefox # 火狐浏览器(不过好像有自带，升级一下吧) # 注重隐私的可以自行搜索安装 Tor 浏览器  视频 1、VLC yay -S vlc\n不太习惯，已弃 1、首先是它的文件列表通过快捷键呼出后会将原画面缩小，跳转到其另一个界面，习惯了 potplayer 那样的悬挂式，不影响观看，且又能了解到具体的播放内容，在看课程的时候很有用。 2、用的时候打开多个文件后会卡住，无法打开新的文件，已经打开的窗口也无法在图形化的退出，只能通过命令行强行杀死进程才能结束。 3、而且打开一些视频播放不流畅，有花屏等等。\n2、MPV yay -S mpv # 简单版本的视频播放器,简单播放的话功能够用  仅仅是作为 vlc 容易卡死的一个临时替代品，能播放的视频种类不是很清楚，但是只有很简单的功能，没有列表等其他的，打开一个视频后正常的播放，暂停，快进仅此而已。\n3、Qmplay2 界面设计很好看，功能也符合其要求，目前在使用的。\n项目地址： QMPlay2 \n下载： yay -S uget # bt 下载工具 yay -S xdman # 类似于 idm，多线程下载工具，大大加速下载速度，有时候因为有些需要代理下载的，而代理速度不够好，或者本身源给的下载速度就不快的，比如用 sublime 的包测试，用xdm可以400-500k，而默认使用浏览器下载为10-20k yay -S axel ## 另外一个多线程下载工具，命令行的，相当于wget加强版， ## 使用wget命令下载容易断线，因为wget是单线程的 ## 使用方法 ## axel -n 100 http://download.sublimetext.com/sublime-text-3211-1-x86_64.pkg.tar.xz ## 注：-n 100 表示开100个线程数下载 ## 还有其他参数可选，想要了解更多的自行搜索   yay -S aria2  下载利器,需要进行配置，配置方法见新篇,详情aria2配置 轻型的，多协议支持的下载工具，而且还拥有很多实用的扩展工具\n编辑工具： word、pdf、excel yay -S wps-office ttf-wps-fonts # WPS # 后面的为中文字体 sudo pacman -S wps-office-mui-zh-cn # 中文字体包  笔记： yay -S foxitreader # pdf阅读器，但是感觉自带的Okular挺不错够用了，这个看个人喜好吧。 yay -S mindmaster # 思维导图工具  其他 yay -S deepin.com.qq.im # QQ 安装后不能启动看后续问题归纳 yay -S deepin.com.qq.im.light # qq 轻聊版 （感觉就是老版的 qq，不过喜欢） yay -S electronic-wechat # 微信 yay -S telegram-desktop # Telegram，电报需要做小小的配置才能使用，详见代理配置  qq 和 qq-lite 的对比图 yay -S simplescreenrecorder # 录屏软件：SimpleScreenRecorder yay -S flameshot # 截图工具：flameshot，截图后还有多种方式对图片简单处理 yay -S peek # 简单录制工具，可录制 gif yay -S deepin-screen-recorder # 另一个简单录制工具，可录制 gif 后面的美化效果的 gif 就是分别用其录制的，效果都差不多，需要注意的是保存的文件名是有空格的，有些不太方便  区别： 左为 peek，右为 deepin-screen-recorder peek 是先打开一个窗口，通过拖动边缘可调节大小 deepin-screen-recorder 为打开像截图那样给出截选框先选择大小再录制\n(5)开发工具 [ 1 ] 、JetBrains 全家桶系列 以下是基于版本2019.3 (1)、去官网 ( https://www.jetbrains.com/ ) 下载自己所需的 linux 的 tar,gz 压缩包，命令行移动到自己安装的目录，个人习惯选择 /opt/\n(2)、解压包 解压全部命令参考：\ntar –xvf file.tar 解压 tar tar -xzvf file.tar.gz 解压tar.gz tar -xjvf file.tar.bz2 解压 tar.bz2 tar –xZvf file.tar.Z 解压tar.Z unrar e file.rar 解压rar unzip file.zip 解压zip 可能因为权限不够导致解压失败，建议以超级用户解压  (3)、把 jetbrains-agent.jar 破解包移动到其bin目录下。\n(4)、修改* .vmoptions 和 *64.vmoptions 这两个文件， 在文件最后加入。-javaagent:安装目录/jetbrains-agent.jar\n(5)、启动 切换到 bin 目录下或通过绝对路径启动相应的 sh 脚本。\n(6)、进入正常配置界面，选择 Activation code 找到能用的从code添加注册即可破解。\n(7)、可能会没有快捷方式： 附上快捷方式制作： 创建快捷方式：sudo vim /usr/share/applications/Pycharm.desktop\n粘贴模板： 第一个是最基本的能够获取到工作目录以及添加图标的，以前在 ubuntu 可以正常使用，但是在 manjaro 的时候缺少一些东西，不能对图标进行编辑，而且用的是 svg 那个图，不能被识别，添加到桌面后图标为灰色，所以用的是第二个，\n[Desktop Entry] Version=1.0 Type=Application Name=Pycharm Icon=/opt/pycharm-2019.3.1/bin/pycharm.svg Exec=\u0026quot;/opt/pycharm-2019.3.1/bin/pycharm.sh\u0026quot; %f Comment=Lightning-smart Python IDE Categories=Development;IDE; Terminal=false StartupWMClass=jetbrains-pycharm   [Desktop Entry] Categories=Development;IDE; Comment[en_US]=The smartest Python IDE Comment=The smartest Python IDE Exec=\u0026quot;/opt/pycharm-anaconda-2019.3.1/bin/pycharm.sh\u0026quot; %f GenericName[en_US]= GenericName= Icon=/opt/pycharm-anaconda-2019.3.1/bin/pycharm.png MimeType= Name[en_US]=Pycharm Name=Pycharm Path= StartupNotify=true StartupWMClass=jetbrains-pycharm Terminal=false TerminalOptions= Type=Application Version=1.0 X-DBUS-ServiceName= X-DBUS-StartupType= X-KDE-RunOnDiscreteGpu=false X-KDE-SubstituteUID=false X-KDE-Username=  我们需要替换掉两个地方：Exec=\u0026quot;xx\u0026quot; 和 Icon=,这里要替换掉我们的 pycharm 解压的目录， 然后保存退出之后 打开 搜索 找到图标 pycharm 然后将其拖到需要放置的位置即可。\n后期有了更方便的方法，直接免费试用后将破解插件拖拽进去安装完成重启即可破解。\n[ 2 ] Python 开发环境 Python_Development_Environment\n[ 3 ]、Sublime sublime 基础 sublime LSP sublime go sublime python sublime markdown sublime 前端\n[ 4 ] 、Mysql yay -Si mysql # 查看软件仓库版本 yay -S mysql # 安装mysql  初始化数据库:sudo mysqld --initialize --user=mysql --basedir=/usr --datadir=/var/lib/mysql\n这时会输出密码，记下: 需要留意安装过程中的警告和报错，这次运气好没有遇到，不过看别人的有，留个链接吧，以备不时之需：https://blog.csdn.net/uniondong/article/details/98392738\nsudo systemctl status mysqld # 查看MySql状态 sudo systemctl enable mysqld # 开机自启 sudo systemctl start mysqld # 启动MySql服务   mysql -u root -p # 登录mysql # 如果报错：ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES) 请再三检查是不是密码不对，或者用户名不对   alter user 'root'@'localhost' identified with mysql_native_password by '新密码'; # 修改密码，密码必改，不改初始密码是随机的很难记，而且进入后不改密码也不让进行操作   yay -S mysql-workbench # 安装mysql可视化管理平台 # 然后在连接的时候报错：Could not store password: The name org.freedesktop.secrets was not provided by any .service files，解决方案是安装 gnome-keyring包。 yay -S gnome-keyring  [ 5 ] 、Vim \u0026amp; Neovim yay -S vim # 编码利器 yay -S neovim # 和 vim 差不多的命令行编辑器\n[ 6 ] 、SSH yay -S openssh   ssh username@server_ip # 正常连接服务器 ssh server_ip # 如果客户机的用户名和服务器的用户名相同，登录时可以省略用户名。 \u0026amp;\u0026amp; SSH服务的默认端口是22，上面的都认为22，如果你不设置端口的话登录请求会自动送到远程主机的22端口。我们可以使用 -p 选项来修改端口号。 ssh -p port ldz@192.168.0.1  第一次连接时会出现下面的情况，输入 yes 即可 公钥登录(免密登录)  每次登录远程主机都需要输入密码是很不方便的，如果想要省去这一步骤，可以利用密钥对进行连接，还可以提高安全性。\n1、在本机生成密钥对\n使用 ssh-keygen 命令生成密钥对：\nssh-keygen -t rsa [[-t表示类型选项，这里采用rsa]]加密算法 然后根据提示一步步的按enter键即可（其中有一个提示是要求设置私钥口令passphrase，不设置则为空，这里看心情吧，如果不放心私钥的安全可以设置一下），执行结束以后会在 /home/username/ 目录下生成一个 .ssh 文件夹,其中包含私钥文件 id_rsa 和公钥文件 id_rsa.pub。\n2、将公钥复制到远程主机中\n使用 ssh-copy-id 命令将公钥复制到远程主机。ssh-copy-id 会将公钥写到远程主机的 ~/ .ssh/authorized_key 文件中\nssh-copy-id userbname@server_ip 经过以上两个步骤，以后再登录这个远程主机就不用再输入密码了\n [ 7 ] 、Redis yay -S redis yay -S redis-desktop-maganer # 数据可视化工具 # 支持: Windows 7+, Mac OS X 10.10+, Ubuntu 14+ # 特点： C++ 编写，响应迅速，性能好。但不支持数据库备份与恢复。   [ 8 ] 、Dbeaver 企业版@破解  多平台数据库可视化工具，目前的 redis，mysql，mongodb等等都有 不要使用yay或者pacman安装，安装的是社区版，社区版的功能有限，比如不支持 mnongodb，redis DBeaverEE 7.0.0及以下版本（理论上适用于目前所有新老版本）的破解 下载相应的包并安装，这里就不给出地址了，网上应该能找到。 破解方法: (1). 解压 DBeaverEE 到自己想要安装的位置。 (2). 将下载压缩包解压后得到 dbeaver-agent.jar，把它放到你认为合适的文件夹内。为了方便管理，就放在解压的 /opt/dbeaver/ 目录下 (3). 在 DBeaverEE 安装目录下找到 Eclipse\\dbeaver.ini文件 (4). 在打开的 dbeaver.ini 编辑窗口末行添加：\u0026quot;-javaagent:/opt/dbeaver/dbeaver-agent.jar\u0026quot;，一定要自己确认好路径，填错会导致 DBeaverEE打不开！！！最好使用绝对路径。 (5). 启动 DBeaverEE 即可。注意快捷方式里面的默认指定位置是不对的，所以打不开，自己去修改下复制到快捷方式所在的目录即可。 (6). 如果提示错误:\u0026ldquo;Error opening zip file or JAR manifest missing : dbeaver-agent.jar\u0026rdquo;，这种情况请试着填上jar文件的绝对路径.\n  [ 9 ] 、Git \u0026amp; Gitkraken破解 yay -S git yay -S gitkraken # github 桌面版（有说能破解，但我没破解成功，后面也几乎没用上这个软件）  [ 10 ] 、Docker \u0026amp; Docker-compose yay -S docker yay -S docker-compose  三、美化 1、dock栏 yay -S latte-dock (1) 去掉多余的图标 (2) 背调节阴影效果（我觉得没有好看些） (3) 图标调节大小（可据显示器大小调节） (4) 光标移动到相应图标上时的放大效果 (5) 设置自动隐藏  2、状态栏 （1）原来的状态栏是通栏且在下面，调节为在左边，因为有 dock 栏之后，dock 栏我是放在下面的，尝试把状态栏放在右边，但是鼠标滑动滚动条的时候会带出来，影响较大 （2）调节大小不为通栏且居中 3、文件管理器半透明 win + e 打开文件管理器--定位到顶栏右键--more actions--configure special application setting--appearance fixes--active opacity--force,我调到79%，感觉还不错\n4、终端(konsole)半透明 ctrl + alt + t 打开终端--定位到顶栏右键--more actions--configure special application setting--appearance fixes--active opacity--force 同样调到79\n5、yakuake下拉终端 第一种是直接换一个透明的终端主题就可以了，比如 KDE-story 第二种是选中主题后点击编辑，去添加透明度 6、终端内容美化(zsh) echo $SHELL # 查看当前使用的shell cat /etc/shells # 查看有哪些自带的shell，发现有了，没有的话下面命令安装，manjaro 自带了所以可以不用安装 yay -S zsh sudo chsh -s /bin/zsh # 修改默认shell，这个是修改当前用户的终端，如果要修改root账户，需要切换到root用户 # 这里遇到一个问题，在执行切换shell命令后显示失败，重启终端尝试也是失败，但是在重启后显示用的是zsh，也就是说切换成功了的  安装 oh-my-zsh\ncurl https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh | bash # 下载包并且完成安装  如果终端没有搞定代理的话上面的命令无法下载包，链接被墙。可以手动安装，方法下\ngit clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc  上面的直接在浏览器打开那个链接去下载也行，然后再复制过去\n查看可用的Theme：ls ~/.oh-my-zsh/themes\n不确定安装的时候是否包含了所有的主题，但是查看还是有挺多的。主题效果预览\n修改zsh主题，编辑~/.zshrc文件，将ZSH_THEME=\u0026quot;jonathan\u0026quot;,即将主题修改为jonathan。\n安装插件：zsh不仅可以美观还很强大，\nsudo pacman -S autojump git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions sudo vim ~/.zshrc # 编辑该文件添加入插件名称启用 plugins=(其他的插件 zsh-autosuggestions zsh-syntax-highlighting) source ～/.zshrc # 重启  效果展示： 关闭自动更新 在使用的过程中，如果最新的版本发生变更在新开的命令行窗口中会提示更新，但是往往因为网络问题（未使用代理）而导致更新失败，可以选择关闭自动更新。cat ～/.zshrc 查看配置文件，在文件中有# DISABLE_AUTO_UPDATE=\u0026quot;true\u0026quot;，默认是注释的，取消注释DISABLE_AUTO_UPDATE=\u0026quot;true\u0026quot;即可，如果想要体验新版可以开启代理后omz update更新即可\n 7、壁纸 进入图片目录将图片拖拽到桌面然后会有提示，设置即可，但当壁纸文件移动或者删除就会恢复为默认的壁纸。\n预览： 或者使用本地的壁纸其实也挺不错的。\n8、字体 由于是用做开发环境，对于各个字符的区分就格外重要，像一些字体里面的 [ I (大写的i) 、1、 l (小写的L) ]、[ 0、O (大写的o) ] 等一度让人崩溃。 JetBrainsMono,下载后解压到 /usr/share/fonts 下重启即可。 下载地址：JetBrainsMono-2.001.zip\n9、其他 还有如放大缩小的标改为mac的样子、启动后加载等待的动画、登录页面的样式、图标的更换、整体系统的颜色样式等等、由于精力有限且电脑不太允许，喜欢的可以自行摸索。\n四、时间设置 硬件时钟和系统时钟 系统用两个时钟保存时间：硬件时钟和系统时钟。\n硬件时钟(即实时时钟 RTC 或 CMOS 时钟)仅能保存：年、月、日、时、分、秒这些时间数值，无法保存时间标准(UTC 或 localtime)和是否使用夏令时调节。\n系统时钟(即软件时间) 与硬件时间分别维护，保存了：时间、时区和夏令时设置。Linux 内核保存为自 UTC 时间 1970 年1月1日经过的秒数。初始系统时钟是从硬件时间计算得来，计算时会考虑/etc/adjtime的设置。系统启动之后，系统时钟与硬件时钟独立运行，Linux 通过时钟中断计数维护系统时钟。\n大部分操作系统的时间管理包括如下方面：启动时根据硬件时钟设置系统时间,运行时通过时间同步联网校正时间,关机时根据系统时间设置硬件时间,\n时间标准 时间表示有两个标准：localtime 和 UTC(Coordinated Universal Time) 。UTC 是与时区无关的全球时间标准。尽管概念上有差别，UTC 和 GMT (格林威治时间) 是一样的。localtime 标准则依赖于当前时区。\n时间标准由操作系统设定，Windows 默认使用 localtime，Mac OS 默认使用 UTC 而 UNIX 系列的操作系统两者都有。使用 Linux 时，最好将硬件时钟设置为 UTC 标准，并在所有操作系统中使用。这样 Linux 系统就可以自动调整夏令时设置，而如果使用 localtime 标准那么系统时间不会根据夏令时自动调整。\nLinux方案：\ntimedatectl set-local-rtc 1 --adjust-system-clock timedatectl set-ntp 0  五、启动项设置 1、refind（Win + Mac + Linux） windows10 + manjaro 双系统，先有的 win10 后安装 manjaro，manjaro 的 grub 启动项会替代 win10 原来的并且能识别 win10,界面也不错，不挑剔的话改一下选择系统的等待时间就可以了\n自己的笔记本有大神几乎完美安装了黑苹果，自己也学着黑了下，目前除蓝牙不能用以外其他的几乎完美，虽然不怎么用，但毕竟是一次大胆的探索，占用的空间也不是很大，就当做个纪念(主要看着牛啊😁)\n然后就出现了很尴尬的情况，manjaro 的引导只能识别 windows 和 manjaro，黑苹果的 Clover 只能识别 Mac 和 windows，这时候 refind 就完美的解决了这个问题，把 manjaro 的启动选项固定时间设为0,就可以跳过选择进入manjaro，win10 本来就没有等待直接进入，Clover 也是一样跳过等待就可以。\n2、grub主题修改（Win + Linux） 现在有很多好看的主题可以修改了，而且修改相对简单，如果没有 Mac 的话建议使用这个,下载的时候注意选择合适的分辨率(1080P、2K、4K)\n下载： 在 https://www.gnome-look.org/browse/cat/109/ord/rating/ 下载自己喜欢的主题，已经测试，墙内可以使用，但可能会慢些\n现在比较热门的两个主题: Tela grub themeOriginal Grub-theme-vimixOriginal 安装、 解压下载的文件，切换到解压的目录里面，查看文件会发现有一个 install.sh 文件，运行该文件即可\ntar -xvJf Tela-1080p.tar.gz cd Tela-1080p sudo ./install.sh  六、问题： 1、改快捷方式 去 system setting 里面修改 move to trash 的快捷方式时，一直报错： [ The key sequence Ctrl+D is ambiguous. Use \u0026lsquo;Configure Shortcuts\u0026rsquo; from the \u0026lsquo;Settings\u0026rsquo; menu to solve the ambiguity. ] 解决方法：系统里面设置的和在文件管理器里面的不是同一个快捷键，可用于桌面，文件管理器里面的在文件管理器里面设置这样就不冲突了。\n2、Yakuake 下拉命令行的快捷键原来是 f12,和浏览器的有冲突，记得修改。\n3、kde-connect 中间在使用 kde-connect 传东西的时候传的不完整，会缺少文件，想着换一个类似的，在搜索后下载了一个 sendanywhere，然后自动给浏览器也添加了一个插件，然后系统就变得很卡很卡，其实也不确定是不是，明天测试一下。结果证明不是。详细原因看下面。\n4、系统卡顿原因 在开机的时候挺流畅的，运行几分种后就特别卡，点击文件打开需要很长时间，鼠标移动也特别卡，去 htop 看了一下，最开始只关注 cpu 的情况，但是占用不是很大，就怀疑是不是硬盘的问题(硬盘是二手的)，尝试删除了一些新安装的软件，也没有用，去看了驱动的情况，也没有问题，后来再去仔细看了下 htop 的结果，结果发现了一个叫 baloo_file_extractor 的进程竟然占用着256G的磁盘空间，自己当初给 manjaro 的总空间都不足100G，kill 后速度明显恢复了，但是很快又卡了，再看，这个进程竟然又自己启动了，去网上查了下资料，没有的到一个很好的结果，有一个说法说这是 KDE 桌面留下来的bug，但是一直没有给解决，他会在你开机后疯狂的往磁盘里面写入，从而造成系统卡顿。网上解决的方法挺多的，但我采取了最简单的一种，立刻停止并禁止启用，不知道禁止启用会不会在一些方面造成影响，有的话后续再补充。\nbalooctl suspend # 立即停止 balooctl disable # 禁止启用  5、mysql问题 mysql服务没有启动。\n6、qq安装后无法启动（deein-wine-tim） Manjaro-KDE 桌面安装 TIM/QQ 的时候经常出现无法启动，其主要原因是 deein-wine-tim 打包了 Gnome 桌面部分内容，因此在 KDE 桌面环境下需要安装相应的 Gnome 桌面设置环境\n安装gnome-settings-daemon：yay -S gnome-settings-daemon\n依次进入system settings\u0026ndash;\u0026raquo;autostar\u0026mdash;\u0026raquo;add program 选择文件目录输入/usr/lib/gsd-xsettings 回车确定添加然后开启就可以了，如果没有 gsd-xsettings 就先重启一下就有了，然后再重启一下就可以打开了，第一次打开可能会提示更新wine，稍微等待一下就可以了\n7、使用ping命令无法验证自己的socks代理是否正确启用 最常用的 ping 网络检测命令即是使用的 ICMP 协议，在 OSI 七层网络模型中，ICMP 协议工作在第三层 - 网络层，SOCKS（非套接字 socket）协议工作在第五层 - 会话层，HTTP/HTTPS 协议工作在第七层 - 应用层，SSH 工作在第七层 - 应用层，它们在模型中不同的层次位置，决定了 HTTP/HTTPS 和 SSH 协议可以直接走 SOCKS 代理，而 ICMP 无法直接走 SOCKS 代理。\n模型中不同协议的位置如下图： 8、deb包的安装 有时候去获得的软件源的得到的包不是 manajro 可以直接安装的而是 deb 的 法一： 、、、、 我在使用本方法的时候没有成功 、、、、\nyay -S debtap # 安装第三方转换软件 sudo debtap -u # 更新，这步无法解决，挂代理还是不挂都失败 debtap package_name.deb # 对包进行转换 sudo pacman -U packagename # 安装  法二： 直接解压 deb 文件，然后再解压包到根目录下，或/opt下，快捷方式在正常快捷方式的/usr/share/applications/下，主要看包内部的结构。 后期实际使用情有的可行，有的不行，涉及到一些系统的依赖吧估计。\n9、pycharm 的快捷键问题 设置无法通过以往习惯的（ctrl+alt+s）呼出，估计有其他应用占用了这个快捷键，尝试去查找具体是哪一个占用了，目前没有找到，先改为alt+s将就用着吧。\n10、更新启动导致 grub 设置失效 前期把 manjaro 的启动时间设置为 0 了，然后一段时间又恢复成默认的10s了，以为是系统bug导致的，今天有一个大版本的更新，包括内核，更新完成之后没有重启，电脑特别卡，重启后发现启动时间又变回去了，大版本的更新会重置之前的设置。\n11、启动器其找不到应用快捷方式 有些自己通过解压安装的软件在启动器找不到相应的快捷方式，有点类似于解压安装的没有去写入注册表那样， 大多数快捷方式是在‘/usr/share/applications/’下，第三方解压安装的也会解压到这，如果找不到，复制其快捷方式到‘/home/fish/.local/share/applications/’即可\n12、大版本更新启动等待时间变为默认 这个没法解决，更新时会默认强制把boot/grub里面的文件更新，不能随便删除替换，只能更新一次去更改一次 /boot/grub/grub.cfg文件，而且是重启后更改。\n13、Appimages 软件创建桌面图标 AppimagesLauncher 可以自动将 Appimages 应用程序快捷方式添加到桌面环境的应用程序中启动器/菜单。 下载地址：https://github.com/TheAssassin/AppimagesLauncher/releases\n14、无法连接到 Tor 开始直接使用的是本地的网络，但是无法进入，后切换使用代理后成功进入，再根据查找的资料，大概原因可能是所用的网络可能存在封锁。如果本地网络没法进入请使用代理。\n15 、.run文件的运行 下载后检查有没有权限，没有的话为其添加权限chmod 755 test.run然后双击或者命令行./test.run运行，避免后期权限问题报错，加 sudo 吧。\n16、无意切换到终端模式 之前有在使用快捷键的时候摁错了导致进入了全命令行的终端模式，当时一点头绪都没有，只有强行关机重启。\n今天在使用 ctrl + alt + f5 的时候进入了，就去查询了一下相关的：\n 终端模式又称作命令行模式或者字符模式，默认情况下linux提供六个终端，使用组合键ctrl+alt+F1进入第一个终端，使用组合键ctrl+alt+F2进入第二个终端，其他终端的组合键以此类推.\n  终端又叫做tty，linux定义了六个tty，分别从tty1到tty6，tty是teletype的简写。从tty1到tty6被称为虚拟终端，如果想要切换回桌面，只需要使用组合键ctrl+alt+F7即可。\n  注意，如果系统设定默认启动的时候不启动图形界面，则tty7是不可用的。 此时，若想从终端字符界面进入图形界面就需要使用命令startx 命令如下# startx \u0026lsquo;#\u0026lsquo;是一个提示符，表示当前登陆的用户是root用户，也就是超级用户 $ 也是一个提示符，表示当前登陆的用户是一个普通用户。 超级用户拥有对系统的全部权限，可实现所有的功能。普通用户只有部分功能。这就是区别，看词就可以定义。\n 17、开机自动挂载windows分区 没有任何配置的话 linux 开机默认是不会挂载 windows 分区的，只有比如进入文件管理器点击相应的分区后才会挂载，并且只会在本次有效，重启后恢复默认，这就造成了例如上一次导入的 windows 分区内的歌曲，视频，文件等等都会显示文件不存在或者无法找到等情况。\nKDE 暂时不清楚，在 XFCE 下无论是通过在 /etc/fstab 里写入挂载的盘还是通过开机即执行挂载命令均为成功。先这样吧。\n18、Chrome 浏览器强制使用安全搜索模式 不清楚是自己使用的代理的问题还是说最近 google 在上次更新后加入了这项机制，在设置账户为中文的情况下会默认打开安全搜索模式且无法关闭（关闭保存退出后也会自动打开），解决方法是切换到其他语言如英语就可以关闭了。\n19、Chrome更新后的Reading List 取消 自上次无意更新 Chrome 后在标签栏的位置增加了一个新条目（功能）Reading List，功能感觉像是之前使用的插件 onetab，在添加书签会多一个选择，在打开书签目录的时候由于最右边的位置被占用了，使用习惯就很别扭。\n取消方法：地址栏输入：chrome://flags/#read-later,把 default 改为 disabled 后 relaunch 后即可\n20、libcrypt.so.1 问题 进行了一次大更新，原来能使用的软件不同使用了，libcrypt.so.1 包也找不着，通过安装libcrypt.so.1可以了，多加一次，如果有libcrypt.so.1 libcrypt.so.1.1.0文件，可直接制作一个软连接即可，sudo ln -s libcrypt.so.1 libcrypt.so.1.1.0\n21、zsh-autosuggestion 颜色变为白色 同样是更新后的问题，应该是更新到了zsh 问题是配置了zsh-autosuggestion插件，根据历史命令自动提示出要补全的命令，要补全的为灰色。但是现在全为白色，完全失去了效果。查了下本地的颜色配置没有问题，去修改zsh-autosuggestion插件底层的文件无效，在.zshrc中写入OZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=\u0026lsquo;fg=yellow\u0026rsquo;也无效。\n最后看到是 zsh-syntax-highlighting and zsh-autosuggestions 叠加使用产生的bug，都更新到最新版本可解除这个问题\n","date":"2022-12-26","permalink":"/post/manjaro_kde/","tags":["Linux"],"title":"Manjaro_KDE"},{"content":"使用变量 使用一个定义过的变量，只要在变量名前面加美元符号$即可.\n变量名外面的花括号{ }是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界.\n单引号和双引号的区别 以单引号\u0026rsquo; \u0026lsquo;包围变量的值时，单引号里面是什么就输出什么，即使内容中有变量和命令（命令需要反引起来）也会把它们原样输出。这种方式比较适合定义显示纯字符串的情况，即不希望解析变量、命令等的场景。\n以双引号\u0026quot; \u0026ldquo;包围变量的值时，输出时会先解析里面的变量和命令，而不是把双引号中的变量名和命令原样输出。这种方式比较适合字符串中附带有变量和命令并且想将其解析后再输出的变量定义。\n我的建议：如果变量的内容是数字，那么可以不加引号；如果真的需要原样输出就加单引号；其他没有特别要求的字符串等最好都加上双引号，定义变量时加双引号是最常见的使用场景。\n将命令的结果赋值给变量 Shell 也支持将命令的执行结果赋值给变量，常见的有以下两种方式：\nvariable=`command` variable=$(command)  第一种方式把命令用反引号 （位于 Esc 键的下方）包围起来，反引号和单引号非常相似，容易产生混淆，所以不推荐使用这种方式；第二种方式把命令用$()包围起来，区分更加明显，所以推荐使用这种方式。\n只读变量 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。\n删除变量 使用 unset 命令可以删除变量。语法：unset variable_name,变量被删除后不能再次使用；unset 命令不能删除只读变量。\n特殊变量 Shell 特殊变量及其含义\n   变量 含义     $0 当前脚本的文件名。   $n （n≥1） 传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是 $1，第二个参数是 $2。   $# 传递给脚本或函数的参数个数。   $* 传递给脚本或函数的所有参数。   $@ 传递给脚本或函数的所有参数。当被双引号\u0026rdquo; \u0026ldquo;包含时，$@ 与 $* 稍有不同，我们将在《Shell $*和$@的区别》一节中详细讲解。   $? 上个命令的退出状态，或函数的返回值，我们将在《Shell $?》一节中详细讲解。   $$ 当前 Shell 进程 ID。对于 Shell 脚本，就是这些脚本所在的进程 ID。    环境变量 Linux 环境变量\n简介：Linux的一个重要概念是环境变量，环境变量需要进行定义。有些是由系统设置的，有些是由用户自定义的，还有一些是由shell或加载执行程序时，由程序设置的。环境变量是一个赋值给它的字符串。分配的值可以是数字、文本、文件名、设备或任何其他类型的数据。 1、环境变量介绍 Linux中环境变量包括系统级和用户级，系统级的环境变量是每个登录到系统的用户都要读取的系统变量，而用户级的环境变量则是该用户使用系统时加载的环境变量。所以管理环境变量的文件也分为系统级和用户级的. 1）系统级 /etc/environment：系统在登录时读取的第一个文件，用于为所有进程设置环境变量。系统使用此文件时并不是执行此文件中的命令，而是根据KEY=VALUE模式的代码，对KEY赋值以VALUE，因此文件中如果要定义PATH环境变量，只需加入类似如PATH=$PATH:/xxx/bin的代码即可。 /etc/profile：是系统登录时执行的第二个文件，可以用于设定针对全系统所有用户的环境变量。该文件一般是调用/etc/bash.bashrc文件。 /etc/bash.bashrc：系统级的bashrc文件，为每一个运行bash shell的用户执行此文件。此文件会在用户每次打开shell时执行一次。 注意：/etc/environment是设置整个系统的环境，而/etc/profile是设置所有用户的环境，前者与登录用户无关，后者与登录用户有关。 这两个文件修改后一般都要重启系统才能生效。 2）用户级 ~/.profile: 是对应当前登录用户的profile文件，用于定制当前用户的个人工作环境。 每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次。默认情况下，会设置一些环境变量，执行用户的.bashrc文件。 ~/.bashrc: 是对应当前登录用户的bash初始化文件，当用户每次打开shell时，系统都会执行此文件一次。通常设置环境变量修改这个文件。 上述配置文件执行先后顺序如下： /etc/enviroment –\u0026gt; /etc/profile –\u0026gt; ~/.profile –\u0026gt; /etc/bash.bashrc –\u0026gt; ~/.bashrc 2、环境变量的作用 环境变量相当于给系统或用户应用程序设置的一些参数，具体起什么作用这当然和具体的环境变量相关。比如PATH，是告诉系统，当要求系统运行一个程序而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还应到哪些目录下去寻找；再如tc或vc++中，set include=path1;path2; 是告诉编译程序到哪里去找.h类型的文件；当然不仅仅是指定什么路径，还有其它的作用的，如set dircmd=/4 设置一个环境变量的作用是在使用dir命令时会把/4作为缺省的参数添加到你的dir命令之后，就像你的每个命令都加了/4参数，它实际上是给命令解释程序command设置的一个环境变量，并且是给dir这个内部命令。 3、配置环境变量的方法 1）临时环境变量 linux下设定环境变量时，如果只是临时用一下，可以直接在shell下用set或export命令设定环境变量。但是只能在当前shell环境下可以用，切换或关闭重新进入就会失效。具体配置方法，如下， #终端输入： export MYSQLPATH=/home/mysql #MYSQLPATH设置为该路径 #终端查看一个特定环境变量包含的内容，比如，MYSQLPATH，PATH echo $PATH echo $MYSQLPATH 2）永久环境变量 设置的环境变量，需要经常使用的，而不是临时使用，把上面的设置环境变量命令写到上面提到的相应配置文件中即可，则可以每次开机或打开shell时自动设置， 例如， 只需要当前用户生效的环境变量： 终端中输入：sudo vi ~/.bashrc，编辑这个文件，在其末尾添加： export MYSQLPATH=/home/mysql:$MYSQLPATH # path采用:来分隔,冒号左右不需要空格. # :$MYSQLPATH在后面新添加的优先搜索，$MYSQLPATH:在前面说明新添加的最后搜索，不加代表新路径设置为MYSQLPATH路径。 注意：在终端执行，source ~/.bashrc ，使其立即生效，或者重启电脑即可。 设置所有用户生效的环境变更： 终端中输入：sudo vi /etc/profile，编辑这个文件，在其末尾添加： export MYSQLPATH=/home/mysql:$MYSQLPATH # path采用:来分隔,冒号左右不需要空格. # :$MYSQLPATH在后面新添加的优先搜索，$MYSQLPATH:在前面说明新添加的最后搜索，不加代表新路径设置为MYSQLPATH路径。 注意：在终端执行，source /etc/profile ，使其立即生效，或者重启电脑即可。 4、PATH环境变量 Linux命令其实是一个个的命令行程序，这些程序是分布在不同的众多目录中的。当命令行中输入一个命令的时，Linux需要到指定目录去查找命令对应的程序，而在PATH环境变量就记录这些目录。所以PATH环境变量的作用就是记录命令的查找路径，多个路径之间用英文冒号分割的（和Windows系统的PATH变量不同，Windows的PATH变量的路径是用英文分号分割的），有需要时也可以加入自己的路径。具体配置方法，如下， #比如添加搜索路径/home/cjavapy/python和/home/cjavapy/java 路径到PATH中,采用:来分隔,冒号左右不需要空格 export PATH=$PATH:/home/cjavapy/python:/home/cjavapy/java #若需要将路径放在优先搜索位置，将$PATH放在后面 export PATH=/home/cjavapy/python:/home/cjavapy/java:$PATH 注意：配置PATH环境变量可以永久生效，也可以临时生效，具体可以参考上面介绍的配置环境变量的方法。 5、常用环境变量 Linux系统有一些重要常用的环境变量，具体如下， $HOME：用户家目录。 $SHELL：用户在使用的Shell解释器名称。 $HISTSIZE：输出的历史命令记录条数。 $HISTFILESIZE：保存的历史命令记录条数。 $MAIL：邮件保存路径。 $LANG：系统语言、语系名称。 $RANDOM：生成一个随机数字。 $PS1 ：Bash解释器的提示符。 $PATH：定义解释器搜索用户执行命令的路径。 $EDITOR：用户默认的文本编辑器。 $TERM：表示显示类型。 $RANDOM：每次引用时生成一个0到32,767之间的随机整数。 $PWD：指示由cd命令设置的当前工作目录。 $TZ：指时区。它可以使用GMT、AST等值。 $UID：显示当前用户的数字用户 ID，在shell 启动时初始化。 可以使用echo查看变量信息， 例如， $ echo $HOME /root $ echo $TERM xterm $ echo $PATH /usr/local/bin:/bin:/usr/bin:/home/amrood/bin:/usr/local/bin 6、PS环境变量 PS即是Prompt String,命令提示符的意思。在bash中一共有四个。分为表示为PS1,PS2,PS3,PS4。 1）PS1 PS1是用来控制默认提示符显示格式。下面方括号中的内容便是PS1。 例如， $ echo $PS1 \\h:\\W \\u\\$ PS1的常用参数以及含义: \\d 代表日期，格式为weekday month date，例如：”Mon Aug 1″ \\H 完整的主机名称 \\h 仅取主机名中的第一个名字 \\t 显示时间为24小时格式，如HHMMSS \\T 显示时间为12小时格式 \\A 显示时间为24小时格式HHMM \\@显示时间，为12小时格式am/pm \\u 当前用户的账号名称 \\v BASH的版本信息 \\w 完整的工作目录名称 \\W 利用basename取得工作目录名称，只显示最后一个目录名 \\# 下达的第几个命令 \\$ 提示字符，如果是root用户，提示符为 # ，普通用户则为 $ 2）PS2 一个非常长的命令可以通过在末尾加\\使其分行显示。多行命令的默认提示符是\u0026gt;。 我们可以通过修改PS2 ，将提示符修改为-\u0026gt;。 例如， $ PS2='-\u0026gt;' $ ls \\ -\u0026gt;/etc \\ -\u0026gt;/boot 3）PS3 shell脚本中使用select循环时的提示符。 例如， #!/usr/bin/bash PS3=\u0026quot;Select a program to exectue: \u0026quot; select program in 'ls -F' pwd date do $program done (The Command Line) Select a program to exectue: 2 1) ls -F 2) pwd 3) date # /home/yang # 在执行脚本的时候，PS3里面的字符串会显示在菜单的底部 4）PS4 PS4 是Prompt String 4的缩写，它是Linux/Unix下的一个用于控制脚本调试显示信息的环境变量。 用来修改set -x跟踪输出的前缀 。 例如， $ export PS4=\u0026quot;+Debug Info: \u0026quot; $ set -x test_syntax.sh +Debug Info: GREETINGS= ++Debug Info: pwd +Debug Info: CURRENT_DIR=/Users/liumiao +Debug Info: '[' _HELLO = _ ']'  PATH 环境变量导入的优先级，echo $PATH 查看/home/fiki/.local/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl，自己安装的 python 在/home/fiki/.local/bin中，所以覆盖掉了系统自身的python3，.bashrc时最后加载的，所以如果在里面定义的路径的内容会覆盖掉之前的，换言之拥有最高优先级\nShell $*和$@的区别 当 $* 和 $@ 不被双引号\u0026rdquo; \u0026ldquo;包围时，它们之间没有任何区别，都是将接收到的每个参数看做一份数据，彼此之间以空格来分隔。\n但是当它们被双引号\u0026rdquo; \u0026ldquo;包含时，就会有区别了： \u0026ldquo;$*\u0026ldquo;会将所有的参数从整体上看做一份数据，而不是把每个参数都看做一份数据。 \u0026ldquo;$@\u0026ldquo;仍然将每个参数都看作一份数据，彼此之间是独立的。\n比如传递了 5 个参数，那么对于\u0026rdquo;$*\u0026ldquo;来说，这 5 个参数会合并到一起形成一份数据，它们之间是无法分割的；而对于\u0026rdquo;$@\u0026ldquo;来说，这 5 个参数是相互独立的，它们是 5 份数据。\nshell 命令替换 Shell 命令替换是指将命令的输出结果赋值给某个变量。比如，在某个目录中输入 ls 命令可查看当前目录中所有的文件，但如何将输出内容存入某个变量中呢？这就需要使用命令替换了，这也是 Shell 编程中使用非常频繁的功能。\nShell 中有两种方式可以完成命令替换，一种是反引号 ，一种是$()，使用方法如下：\nvariable=`commands` variable=$(commands)  其中，variable 是变量名，commands 是要执行的命令。commands 可以只有一个命令，也可以有多个命令，多个命令之间以分号;分隔。\n原则上讲，上面提到的两种变量替换的形式是等价的，可以随意使用；但是，反引号毕竟看起来像单引号，有时候会对查看代码造成困扰，而使用 $() 就相对清晰，能有效避免这种混乱。而且有些情况必须使用 $()：$() 支持嵌套，反引号不行。\nFir_File_Lines=$(wc -l $(ls | sed -n '1p')) echo \u0026quot;$Fir_File_Lines\u0026quot;  要注意的是，$() 仅在 Bash Shell 中有效，而反引号可在多种 Shell 中使用。\ndeclare 和 typeset 命令：设置变量属性 declare 命令的用法如下所示： declare [+/-] [aAfFgilprtux] [变量名=变量值]\n其中，-表示设置属性，+表示取消属性，aAfFgilprtux 都是具体的选项，它们的含义如下表所示：\n选项 含义 -f [name] 列出之前由用户在脚本中定义的函数名称和函数体。 -F [name] 仅列出自定义函数名称。 -g name 在 Shell 函数内部创建全局变量。 -p [name] 显示指定变量的属性和值。 -a name 声明变量为普通数组。 -A name 声明变量为关联数组（支持索引下标为字符串）。 -i name 将变量定义为整数型。 -r name[=value] 将变量定义为只读（不可修改和删除），等价于 readonly name。 -x name[=value] 将变量设置为环境变量，等价于 export name[=value]。\n变量高级 指定方式 说明 ${parameter-default}\t如果变量 parameter 没被声明，那么就使用默认值。 ${parameter:-default} 如果变量 parameter 没被设置，那么就使用默认值。 ${parameter=default}\t如果变量parameter没声明，那么就把它的值设为default。 ${parameter:=default} 如果变量 parameter 没设置，那么就把它的值设为 default。 ${parameter+alt_value}\t如果变量parameter被声明了，那么就使用alt_value，否则就使用null字符串。 ${parameter:+alt_value} 如果变量 parameter 被设置了，那么就使用 alt_value，否则就使用 null 字符串。 ${parameter?err_msg}\t如果parameter已经被声明，那么就使用设置的值，否则打印err_msg错误消息。 ${parameter:?err_msg} 如果 parameter 已经被设置，那么就使用设置的值，否则打印 err_msg 错误消息。\n${var#Pattern}, ${var##Pattern} 从变量$var 的开头删除最短或最长匹配$Pattern 的子串。 “#”表示匹配最短，“##”表示匹配最长。\n${var%Pattern}, ${var%%Pattern} 从变量$var 的结尾删除最短或最长匹配$Pattern 的子串。 “%”表示匹配最短，“%%”表示匹配最长。\n${var:pos} 变量var从位置pos开始扩展， 也就是pos之前的字符都丢弃。 ${var:pos:len} 变量 var 从位置 pos 开始，并扩展 len 个字符。 ${var/Pattern/Replacement} 使用Replacement来替换变量var中第一个匹配Pattern的字符串。 ${var//Pattern/Replacement} 全局替换。所有在变量 var 匹配 Pattern 的字符串，都会被替换为 Replacement。 ${var/#Pattern/Replacement} 如果变量var的前缀匹配Pattern，那么就使用Replacement来替换匹配到Pattern的字符串。 ${var/%Pattern/Replacement} 如果变量 var 的后缀匹配 Pattern，那么就使用 Replacement 来替换匹配到 Pattern 的字符串。\n变量的间接引用 假设一个变量的值是第二个变量的名字。如果 a=letter_of_alphabet 并且 letter_of_alphabet=z，\n它被称为间接引用。我们能够通过引用变量 a 来获得 z，它使用 eval var1=$$var2 这种不平常的形式。\nt=table_cell_3 table_cell_3=24 echo \u0026quot;\\\u0026quot;table_cell_3\\\u0026quot; = $table_cell_3\u0026quot; # \u0026quot;table_cell_3\u0026quot; = 24 echo -n \u0026quot;dereferenced \\\u0026quot;t\\\u0026quot; = ${t}\u0026quot; # dereferenced \u0026quot;t\u0026quot; = table_cell_3% echo '------------' echo -n \u0026quot;dereferenced \\\u0026quot;t\\\u0026quot; = \u0026quot;; eval echo \\$$t # dereferenced \u0026quot;t\u0026quot; = 24 echo '------------' # eval echo \\$$t 可以理解为下面的 eval t=\\$$t; echo \u0026quot;\\\u0026quot;t\\\u0026quot; = $t\u0026quot; 的简化 echo -n \u0026quot;dereferenced \u0026quot; ;eval t=\\$$t; echo \u0026quot;\\\u0026quot;t\\\u0026quot; = $t\u0026quot;  输入、输出 read:用来从标准输入中读取数据并赋值给变量。如果没有进行重定向，默认就是从键盘读取用户输入的数据；如果进行了重定向，那么可以从文件中读取数据。\n$REPLY:当没有参数变量提供给 read 命令的时候，这个变量会作为默认变量提供给 read 命令。\nread 命令用于从标准输入读取数值。\nread 内部命令被用来从标准输入读取单行数据。这个命令可以用来读取键盘输入，当使用重定向的时候，可以读取文件中的一行数据。\n语法:read [-ers] [-a aname] [-d delim] [-i text] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name ...]\n   参数 说明     -a 后跟一个变量，该变量会被认为是个数组，然后给其赋值，默认是以空格为分割符。    -d 后面跟一个标志符，其实只有其后的第一个字符有用，作为结束的标志。    -p 后面跟提示信息，即在输入前打印提示信息。    -e 在输入的时候可以使用命令补全功能。    -n 后跟一个数字，定义输入文本的长度，很实用。    -r 屏蔽\\，如果没有该选项，则\\作为一个转义字符，有的话 \\就是个正常的字符了。    -s 安静模式，在输入字符时不再屏幕上显示，例如 login 时输入密码。    -t 后面跟秒数，定义输入字符的等待时间。    -u 后面跟 fd，从文件描述符中读入，该文件描述符可以是 exec 新开启的。     读取文件的几种方法 while read line do echo $line done \u0026lt; filename  cat filename | while read line do echo $line done  IFS 分隔符；cat 逐行读取文件 for line in `cat filename` do echo $line done  使用上面的方法读取文件时，当行内有空白符(空格、tab、换行)时就不会按行输出了。\nfor i in `cat test.txt` do echo $i done 除了更换之前的 while 方法外，还可以通过指定分隔符来实现。  IFS=$'\\n' # 定义分割符 # for i in $(cat file) # better # for i in $(\u0026lt;file\u0026gt;) # in bash for i in `cat file` do echo \u0026quot;$i\u0026quot; done   IFS=\u0026quot;\\n\u0026rdquo; # 将字符 n 作为 IFS 的换行符。 IFS=$\u0026quot;\\n\u0026rdquo; # 这里\\n确实通过$转化为了换行符，但仅当被解释时（或被执行时）才被转化为换行符;第一个和第二个是等价的 IFS=$\u0026rsquo;\\n\u0026rsquo; # 这才是真正的换行符。\n Shell 脚本中有个变量叫 IFS(Internal Field Seprator) ，内部域分隔符。 Shell 的环境变量分为 set, env 两种，其中 set 变量可以通过 export 工具导入到 env 变量中。 其中，set 是显示设置 shell 变量，仅在本 shell 中有效；env 是显示设置用户环境变量 ，仅在当前会话中有效。\nIFS 是一种 set 变量，当 shell 处理\u0026quot;命令替换\u0026quot;和\u0026quot;参数替换\u0026quot;时，shell 根据 IFS 的值，默认是 space, tab, newline 来拆解读入的变量，然后对特殊字符进行处理，最后重新组合赋值给该变量.\necho:用来在终端输出字符串，并在最后默认加上换行符。\nprintf:用来在终端输出。\n字符串 字符串可以由单引号\u0026rsquo; \u0026lsquo;包围，也可以由双引号\u0026rdquo; \u0026ldquo;包围，也可以不用引号。它们之间是有区别的\n三种形式的区别：\n  由单引号\u0026rsquo; \u0026lsquo;包围的字符串： 任何字符都会原样输出，在其中使用变量是无效的。 字符串中不能出现单引号，即使对单引号进行转义也不行。\n  由双引号\u0026rdquo; \u0026ldquo;包围的字符串： 如果其中包含了某个变量，那么该变量会被解析（得到该变量的值），而不是原样输出。 字符串中可以出现双引号，只要它被转义了就行。\n  不被引号包围的字符串 不被引号包围的字符串中出现变量时也会被解析，这一点和双引号\u0026rdquo; \u0026ldquo;包围的字符串一样。 字符串中不能出现空格，否则空格后边的字符串会作为其他变量或者命令解析。\n  获取字符串长度 ${#string_name}:string_name 表示字符串名字。\n字符串拼接（连接、合并） 在 Shell 中你不需要使用任何运算符，将两个字符串并排放在一起就能实现拼接\nShell字符串截取 数组 在 Shell 中，用括号( )来表示数组，数组元素之间用空格来分隔。由此，定义数组的一般形式为： array_name=(ele1 ele2 ele3 \u0026hellip; elen)\n注意，赋值号=两边不能有空格，必须紧挨着数组名和数组元素。\n数组是可变的，在定义之后可以进行增加、删除、修改等操作\n获取数组元素 ${array_name[index]}:其中，array_name 是数组名，index 是下标。\n使用@或*可以获取数组中的所有元素:\n${nums[*]} ${nums[@]}  获取数组长度 ${#array_name[@]} ${#array_name[*]}  其中 array_name 表示数组名。两种形式是等价的，选择其一即可。\n数组合并 array_new=(${array1[@]} ${array2[@]}) array_new=(${array1[*]} ${array2[*]})  两种方式是等价的，选择其一即可。其中，array1 和 array2 是需要拼接的数组，array_new 是拼接后形成的新数组。\n删除数组元素 unset array_name[index] # 删除数组元素 unset array_name # 删除数组  关联数组(“键值对（key-value）”数组) 最新的 Bash Shell 已经支持关联数组了。关联数组使用字符串作为下标，而不是整数，这样可以做到见名知意。\n关联数组也称为“键值对（key-value）”数组，键（key）也即字符串形式的数组下标，值（value）也即元素值。\n例如，我们可以创建一个叫做 color 的关联数组，并用颜色名字作为下标。\ndeclare -A color color[\u0026quot;red\u0026quot;]=\u0026quot;#ff0000\u0026quot; color[\u0026quot;green\u0026quot;]=\u0026quot;#00ff00\u0026quot; color[\u0026quot;blue\u0026quot;]=\u0026quot;#0000ff\u0026quot;  也可以在定义的同时赋值：\ndeclare -A color=([\u0026quot;red\u0026quot;]=\u0026quot;#ff0000\u0026quot;, [\u0026quot;green\u0026quot;]=\u0026quot;#00ff00\u0026quot;, [\u0026quot;blue\u0026quot;]=\u0026quot;#0000ff\u0026quot;)  不同于普通数组，关联数组必须使用带有-A选项的 declare 命令创建。\n访问关联数组元素 访问关联数组元素的方式几乎与普通数组相同array_name[\u0026quot;index\u0026quot;]\n获取所有元素的下标和值 # 获得关联数组的所有元素值： ${array_name[@]} ${array_name[*]}  # 获取关联数组的所有下标值： ${!array_name[@]} ${!array_name[*]}  数学计算 |Shell 中常用的六种数学计算方式| |运算操作符/运算命令\t|说明| |(( ))\t|用于整数运算，效率很高，推荐使用。| |let\t|用于整数运算，和 (()) 类似。| |$[]\t|用于整数运算，不如 (()) 灵活。| |expr\t|可用于整数运算，也可以处理字符串。比较麻烦，需要注意各种细节，不推荐使用。| |bc\t|Linux下的一个计算器程序，可以处理整数和小数。Shell 本身只支持整数运算，想计算小数就得使用 bc 这个外部的计算器。| |declare -i\t|将变量定义为整数，然后再进行数学运算时就不会被当做字符串了。功能有限，仅支持最基本的数学运算（加减乘除和取余），不支持逻辑运算、自增自减等，所以在实际开发中很少使用。| 如果大家时间有限，只学习 (()) 和 bc 即可，不用学习其它的了：(()) 可以用于整数计算，bc 可以小数计算。\n(()) (( )) 只能进行整数运算，不能对小数（浮点数）或者字符串进行运算。 Shell (( )) 的用法\n双小括号 (( )) 的语法格式为：((表达式))\n通俗地讲，就是将数学运算表达式放在((和))之间。\n表达式可以只有一个，也可以有多个，多个表达式之间以逗号,分隔。对于多个表达式的情况，以最后一个表达式的值作为整个 (( )) 命令的执行结果。\n可以使用$获取 (( )) 命令的结果，这和使用$获得变量值是类似的。\nbc Bash Shell 内置了对整数运算的支持，但是并不支持浮点运算，而 Linux bc 命令可以很方便的进行浮点运算，当然整数运算也不再话下。\nbc 甚至可以称得上是一种编程语言了，它支持变量、数组、输入输出、分支结构、循环结构、函数等基本的编程元素，所以 Linux 手册中是这样来描述 bc 的： An arbitrary precision calculator language\n翻译过来就是“一个任意精度的计算器语言”。\n在终端输入bc命令，然后回车即可进入 bc 进行交互式的数学计算。在 Shell 编程中，我们也可以通过管道和输入重定向来使用 bc。\n运算符 关系运算符 基于整数的判断\n   选 项 作 用     num1 -eq num2 判断 num1 是否和 num2 相等。   num1 -ne num2 判断 num1 是否和 num2 不相等。   num1 -gt num2 判断 num1 是否大于 num2 。   num1 -lt num2 判断 num1 是否小于 num2。   num1 -ge num2 判断 num1 是否大于等于 num2。   num1 -le num2 判断 num1 是否小于等于 num2。    布尔运算符 | 运算符 | 说明 | 举例 | | \u0026mdash; | \u0026mdash; | | | ! | 非运算，表达式为 true 则返回 false，否则返回 true。 | [ ! false ] 返回 true。 | | -o | 或运算，有一个表达式为 true 则返回 true。 | [ $a -lt 20 -o $b -gt 100 ] 返回 true。 | | -a | 与运算，两个表达式都为 true 才返回 true。 | [ $a -lt 20 -a $b -gt 100 ] 返回 false。 |\n字符串运算符    选 项 作 用     -z str 判断字符串 str 是否为空。   -n str 判断宇符串 str 是否为非空。   str1 = str2 判断 str1 是否和 str2 相等。   str1 == str2 =和==是等价的，都用来判断 str1 是否和 str2 相等。   str1 != str2 判断 str1 是否和 str2 不相等。   str1 \u0026gt; str2 判断 str1 是否大于 str2。\u0026gt;是\u0026gt;的转义字符，这样写是为了防止\u0026gt;被误认为成重定向运算符。   str1 \u0026lt; str2 判断 str1 是否小于 str2。同样，\u0026lt;也是转义字符。   $str 检测字符串是否为空，不为空返回 true. [ $a ] 返回 true。    文件测试运算符 文件测试运算符用于检测 Unix 文件的各种属性。\n文件类型判断\n   选 项 作 用     -b filename 判断文件是否存在，并且是否为块设备文件。   -c filename 判断文件是否存在，并且是否为字符设备文件。   -d filename 判断文件是否存在，并且是否为目录文件。   -e filename 判断文件是否存在。   -f filename 判断文件是否存在，井且是否为普通文件。   -L filename 判断文件是否存在，并且是否为符号链接文件。   -p filename 判断文件是否存在，并且是否为管道文件。   -s filename 判断文件是否存在，并且是否为非空。   -S filename 判断该文件是否存在，并且是否为套接字文件。    文件权限判断\n   选 项 作 用     -r filename 判断文件是否存在，并且是否拥有读权限。   -w filename 判断文件是否存在，并且是否拥有写权限。   -x filename 判断文件是否存在，并且是否拥有执行权限。   -u filename 判断文件是否存在，并且是否拥有 SUID 权限。   -g filename 判断文件是否存在，并且是否拥有 SGID 权限。   -k filename 判断该文件是否存在，并且是否拥有 SBIT 权限。    文件比较\n   选 项 作 用     filename1 -nt filename2 判断 filename1 的修改时间是否比 filename2 的新。   filename -ot filename2 判断 filename1 的修改时间是否比 filename2 的旧。   filename1 -ef filename2 判断 filename1 是否和 filename2 的 inode 号一致，可以理解为两个文件是否为同一个文件。这个判断用于判断硬链接是很好的方法    条件判断 if 语句中的 condition 用法都是一样的，你可以使用 test 或 [] 命令，也可以使用 (()) 或 [[]]，\n语法格式为：\nif condition then statement(s) fi  condition是判断条件，如果 condition 成立（返回“真”），那么 then 后边的语句将会被执行；如果 condition 不成立（返回“假”），那么不会执行任何语句。\n也可以将 then 和 if 写在一行：\nif condition; then statement(s) fi  请注意 condition 后边的分号;，当 if 和 then 位于同一行的时候，这个分号是必须的，否则会有语法错误。\nif else 格式为：\nif condition then statement1 else statement2 fi  if elif else 格式为：\nif condition1 then statement1 elif condition2 then statement2 elif condition3 then statement3 …… else statementn fi  case in 格式如下：\ncase expression in pattern1) statement1 ;; pattern2) statement2 ;; pattern3) statement3 ;; …… \\*) statementn esac  case、in 和 esac 都是 Shell 关键字，expression 表示表达式，pattern 表示匹配模式。 expression 既可以是一个变量、一个数字、一个字符串，还可以是一个数学计算表达式，或者是命令的执行结果，只要能够得到 expression 的值就可以。 pattern 可以是一个数字、一个字符串，甚至是一个简单的正则表达式。\ncase 会将 expression 的值与 pattern1、pattern2、pattern3 逐个进行匹配： 如果 expression 和某个模式（比如 pattern2）匹配成功，就会执行这模式（比如 pattern2）后面对应的所有语句（该语句可以有一条，也可以有多条），直到遇见双分号;;才停止；然后整个 case 语句就执行完了，程序会跳出整个 case 语句，执行 esac 后面的其它语句。 如果 expression 没有匹配到任何一个模式，那么就执行*)后面的语句（表示其它所有值），直到遇见双分号;;或者esac才结束。)相当于多个 if 分支语句中最后的 else 部分。\ncase in 的 pattern 部分支持简单的正则表达式，具体来说，可以使用以下几种格式：\n   格式 说明     * 表示任意字符串。   [abc] 表示 a、b、c 三个字符中的任意一个。比如，[15ZH] 表示 1、5、Z、H 四个字符中的任意一个。   [m-n] 表示从 m 到 n 的任意一个字符。比如，[0-9] 表示任意一个数字，[0-9a-zA-Z] 表示字母或数字。   \\ 表示多重选择，类似逻辑运算中的或运算。比如，abc   xyz 表示匹配字符串 \u0026ldquo;abc\u0026rdquo; 或者 \u0026ldquo;xyz\u0026rdquo;。    循环 while 语句中的 condition 用法都是一样的，你可以使用 test 或 [] 命令，也可以使用 (()) 或 [[]]，\n用法如下：\nwhile condition do statements done  condition表示判断条件，statements表示要执行的语句（可以只有一条，也可以有多条），do和done都是 Shell 中的关键字。\nwhile 循环的执行流程为： 先对 condition 进行判断，如果该条件成立，就进入循环，执行 while 循环体中的语句，也就是 do 和 done 之间的语句。这样就完成了一次循环。 每一次执行到 done 的时候都会重新判断 condition 是否成立，如果成立，就进入下一次循环，继续执行 do 和 done 之间的语句，如果不成立，就结束整个 while 循环，执行 done 后面的其它 Shell 代码。 如果一开始 condition 就不成立，那么程序就不会进入循环体，do 和 done 之间的语句就没有执行的机会。\nuntil unti 循环和 while 循环恰好相反，当判断条件不成立时才进行循环，一旦判断条件成立，就终止循环。\nuntil 的使用场景很少，一般使用 while 即可。\nShell until 循环的用法如下：\nuntil condition do statements done  condition表示判断条件，statements表示要执行的语句（可以只有一条，也可以有多条），do和done都是 Shell 中的关键字。\nuntil 循环的执行流程为： 先对 condition 进行判断，如果该条件不成立，就进入循环，执行 until 循环体中的语句（do 和 done 之间的语句），这样就完成了一次循环。 每一次执行到 done 的时候都会重新判断 condition 是否成立，如果不成立，就进入下一次循环，继续执行循环体中的语句，如果成立，就结束整个 until 循环，执行 done 后面的其它 Shell 代码。 如果一开始 condition 就成立，那么程序就不会进入循环体，do 和 done 之间的语句就没有执行的机会。\nfor C语言风格的 for 循环 用法如下：\nfor((exp1; exp2; exp3)) do statements done  几点说明： exp1、exp2、exp3 是三个表达式，其中 exp2 是判断条件，for 循环根据 exp2 的结果来决定是否继续下一次循环； statements 是循环体语句，可以有一条，也可以有多条； do 和 done 是 Shell 中的关键字。\n它的运行过程为：\n  先执行 exp1。\n  再执行 exp2，如果它的判断结果是成立的，则执行循环体中的语句，否则结束整个 for 循环。\n  执行完循环体后再执行 exp3。\n  重复执行步骤 2) 和 3)，直到 exp2 的判断结果不成立，就结束循环。\n  上面的步骤中，2) 和 3) 合并在一起算作一次循环，会重复执行，for 语句的主要作用就是不断执行步骤 2) 和 3)。\nexp1 仅在第一次循环时执行，以后都不会再执行，可以认为这是一个初始化语句。exp2 一般是一个关系表达式，决定了是否还要继续下次循环，称为“循环条件”。exp3 很多情况下是一个带有自增或自减运算的表达式，以使循环条件逐渐变得“不成立”。\nfor 循环中的 exp1（初始化语句）、exp2（判断条件）和 exp3（自增或自减）都是可选项，都可以省略（但分号;必须保留）.\nPython 风格的 for in 循环 用法如下：\nfor variable in value_list do statements done  variable 表示变量，value_list 表示取值列表，in 是 Shell 中的关键字。 in value_list 部分可以省略，省略后的效果相当于 in $@，\n每次循环都会从 value_list 中取出一个值赋给变量 variable，然后进入循环体（do 和 done 之间的部分），执行循环体中的 statements。直到取完 value_list 中的所有值，循环就结束了。\nvalue_list的说明 直接给出具体的值 可以在 in 关键字后面直接给出具体的值，多个值之间以空格分隔，比如1 2 3 4 5、\u0026ldquo;abc\u0026rdquo; \u0026ldquo;390\u0026rdquo; \u0026ldquo;tom\u0026quot;等。\n给出一个取值范围 给出一个取值范围的具体格式为：{start..end}\nstart 表示起始值，end 表示终止值；注意中间用两个点号相连，而不是三个点号。这种形式只支持数字和字母。\n使用命令的执行结果 使用 Shell 通配符 shell 通配符 / glob 模式通常用来匹配目录以及文件，而不是文本！！！\n   字符 解释     * 匹配任意长度任意字符   ? 匹配任意单个字符   [list] 匹配指定范围内（list）任意单个字符，也可以是单个字符组成的集合   [^list] 匹配指定范围外的任意单个字符或字符集合   [!list] 同[^list]   {str1,str2,\u0026hellip;} 匹配 srt1 或者 srt2 或者更多字符串，也可以是集合       字符 意义     [:alnum:] 任意数字或者字母   [:alpha:] 任意字母   [:space:] 空格   [:lower:] 小写字母   [:digit:] 任意数字   [:upper:] 任意大写字母   [:cntrl:] 控制符   [:graph:] 图形   [:print:] 可打印字符   [:punct:] 标点符号   [:xdigit:] 十六进制数   [:blank:] 空白字符（未验证）    select in select in 循环用来增强交互性，它可以显示出带编号的菜单，用户输入不同的编号就可以选择不同的菜单，并执行不同的功能。\n用法如下：\nselect variable in value_list do statements done  例子：\n#!/bin/bash echo \u0026quot;What is your favourite OS?\u0026quot; select name in \u0026quot;Linux\u0026quot; \u0026quot;Windows\u0026quot; \u0026quot;Mac OS\u0026quot; \u0026quot;UNIX\u0026quot; \u0026quot;Android\u0026quot; do echo $name done echo \u0026quot;You have selected $name\u0026quot;  运行结果：\nWhat is your favourite OS? 1. Linux 2. Windows 3. Mac OS 4. UNIX 5. Android #? 4↙ You have selected UNIX #? 1↙ You have selected Linux #? 9↙ You have selected #? 2↙ You have selected Windows #?^D   #?用来提示用户输入菜单编号；^D表示按下 Ctrl+D 组合键，它的作用是结束 select in 循环。 运行到 select 语句后，取值列表 value_list 中的内容会以菜单的形式显示出来，用户输入菜单编号，就表示选中了某个值，这个值就会赋给变量 variable，然后再执行循环体中的 statements（do 和 done 之间的部分）。 每次循环时 select 都会要求用户输入菜单编号，并使用环境变量 PS3 的值作为提示符，PS3 的默认值为#?，修改 PS3 的值就可以修改提示符。 如果用户输入的菜单编号不在范围之内，例如上面我们输入的 9，那么就会给 variable 赋一个空值；如果用户输入一个空值（什么也不输入，直接回车），会重新显示一遍菜单。\n 注意，select 是无限循环（死循环），输入空值，或者输入的值无效，都不会结束循环，只有遇到 break 语句，或者按下 Ctrl+D 组合键才能结束循环。\nselect in 通常和 case in 一起使用，在用户输入不同的编号时可以做出不同的反应。\n#!/bin/bash echo \u0026quot;What is your favourite OS?\u0026quot; select name in \u0026quot;Linux\u0026quot; \u0026quot;Windows\u0026quot; \u0026quot;Mac OS\u0026quot; \u0026quot;UNIX\u0026quot; \u0026quot;Android\u0026quot; do case $name in \u0026quot;Linux\u0026quot;) echo \u0026quot;Linux 是一个类 UNIX 操作系统，它开源免费，运行在各种服务器设备和嵌入式设备。\u0026quot; break ;; \u0026quot;Windows\u0026quot;) echo \u0026quot;Windows 是微软开发的个人电脑操作系统，它是闭源收费的。\u0026quot; break ;; \u0026quot;Mac OS\u0026quot;) echo \u0026quot;Mac OS 是苹果公司基于 UNIX 开发的一款图形界面操作系统，只能运行与苹果提供的硬件之上。\u0026quot; break ;; \u0026quot;UNIX\u0026quot;) echo \u0026quot;UNIX 是操作系统的开山鼻祖，现在已经逐渐退出历史舞台，只应用在特殊场合。\u0026quot; break ;; \u0026quot;Android\u0026quot;) echo \u0026quot;Android 是由 Google 开发的手机操作系统，目前已经占据了 70%的市场份额。\u0026quot; break ;; \\*) echo \u0026quot;输入错误，请重新输入\u0026quot; esac done  break和continue跳出循环详解 使用 while、until、for、select 循环时，如果想提前结束循环（在不满足结束条件的情况下结束循环），可以使用 break 或者 continue 关键字。\n在C语言、C++、C#、Python、Java 等大部分编程语言中，break 和 continue 只能跳出当前层次的循环，内层循环中的 break 和 continue 对外层循环不起作用；但是 Shell 中的 break 和 continue 却能够跳出多层循环，也就是说，内层循环中的 break 和 continue 能够跳出外层循环。\n在实际开发中，break 和 continue 一般只用来跳出当前层次的循环，很少有需要跳出多层循环的情况。\nbreak 关键字 Shell break 关键字的用法为：break n\nn 表示跳出循环的层数，如果省略 n，则表示跳出当前的整个循环。break 关键字通常和 if 语句一起使用，即满足条件时便跳出循环。\ncontinue 关键字 Shell continue 关键字的用法为： continue n\nn 表示循环的层数： 如果省略 n，则表示 continue 只对当前层次的循环语句有效，遇到 continue 会跳过本次循环，忽略本次循环的剩余代码，直接进入下一次循环。 如果带上 n，比如 n 的值为 2，那么 continue 对内层和外层循环语句都有效，不但内层会跳过本次循环，外层也会跳过本次循环，其效果相当于内层循环和外层循环同时执行了不带 n 的 continue。这么说可能有点难以理解，稍后我们通过代码来演示。\ncontinue 关键字也通常和 if 语句一起使用，即满足条件时便跳出循环。\n函数 Shell 函数必须先定义后使用 Shell 函数定义的语法格式如下：\nfunction name() { statements [return value] }  对各个部分的说明： function是 Shell 中的关键字，专门用来定义函数，可以省略，如果写了 function 关键字，也可以省略函数名后面的小括号； name是函数名； statements是函数要执行的代码，也就是一组语句； return value表示函数的返回值，其中 return 是 Shell 关键字，专门用在函数中返回一个值；这一部分可以写也可以不写。\n由{ }包围的部分称为函数体，调用一个函数，实际上就是执行函数体中的代码。\n函数的调用、返回值 调用函数只需要给出函数名，不需要加括号。\n函数返回值 1）可以显式增加return语句；如果不加，会将最后一条命令运行结果作为返回值。\nShell 函数返回值只能是整数，一般用来表示函数执行成功与否，0表示成功，其他值表示失败。如果 return 其他数据，比如一个字符串，往往会得到错误提示：“numeric argument required”。\n2）使用全局变量 如果一定要让函数返回字符串，那么可以先定义一个变量，用来接收函数的计算结果，脚本在需要的时候访问这个变量来获得函数返回值。但是有bug\n3）echo 返回值 安全的返回方式，即通过输出到标准输出返回。因为子进程会继承父进程的标准输出，因此，子进程的输出也就直接反应到父进程。因此不存在上面提到的由于管道导致返回值失效的情况。在外边只需要获取函数的返回值即可。\n但是有一点一定要注意，不能向标准输出一些不是结果的东西（也就是说，不能随便echo一些不需要的信息），比如调试信息，这些信息可以重定向到一个文件中解决，特别要注意的是，脚本中用到其它类似grep这样的命令的时候，一定要记得1\u0026gt;/dev/null 2\u0026gt;\u0026amp;1来空这些输出信息输出到空设备，避免这些命令的输出。\n总结： 1）用变量接收函数返回值，函数用echo等标准输出将要返回的东西打印出来。\n2）用$?来接收函数的执行状态，但是$?要紧跟在函数调用处的后面。\n重定向 输出重定向 |类 型|符 号|作 用| |标准输出重定向|command \u0026gt;file|以覆盖的方式，把 command 的正确输出结果输出到 file 文件中。| |command \u0026raquo;file|以追加的方式，把 command 的正确输出结果输出到 file 文件中。| |标准错误输出重定向|command 2\u0026gt;file|以覆盖的方式，把 command 的错误信息输出到 file 文件中。| |command 2\u0026raquo;file|以追加的方式，把 command 的错误信息输出到 file 文件中。| |正确输出和错误信息同时保存|command \u0026gt;file 2\u0026gt;\u0026amp;1|以覆盖的方式，把正确输出和错误信息同时保存到同一个文件（file）中。| |command \u0026raquo;file 2\u0026gt;\u0026amp;1|以追加的方式，把正确输出和错误信息同时保存到同一个文件（file）中。| |command \u0026gt;file1 2\u0026gt;file2|以覆盖的方式，把正确的输出结果输出到 file1 文件中，把错误信息输出到 file2 文件中。| |command \u0026raquo;file1 2\u0026raquo;file2|以追加的方式，把正确的输出结果输出到 file1 文件中，把错误信息输出到 file2 文件中。| |command \u0026gt;file 2\u0026gt;file|【不推荐】这两种写法会导致 file 被打开两次，引起资源竞争，所以 stdout 和 stderr 会互相覆盖，|\n输入重定向 |符号|说明| |command \u0026lt;file|将 file 文件中的内容作为 command 的输入。| |command \u0026laquo;END|从标准输入（键盘）中读取数据，直到遇见分界符 END 才停止（分界符可以是任意的字符串，用户自己定义）。| |command file2|将 file1 作为 command 的输入，并将 command 的处理结果输出到 file2。|\n文件描述符    分类  用法  说明   输出  n\u0026gt;filename  以输出的方式打开文件 filename，并绑定到文件描述符 n。n 可以不写，默认为 1，也即标准输出文件。    n\u0026gt;\u0026amp;m  用文件描述符 m 修改文件描述符 n，或者说用文件描述符 m 的内容覆盖文件描述符 n，结果就是 n 和 m 都代表了同一个文件，因为 n 和 m 的文件指针都指向了同一个文件。\n因为使用的是\u0026gt;，所以 n 和 m 只能用作命令的输出文件。n 可以不写，默认为 1。    n\u0026gt;\u0026amp;-  关闭文件描述符 n 及其代表的文件。n 可以不写，默认为 1。    \u0026amp;\u0026gt;filename  将正确输出结果和错误信息全部重定向到 filename。   输入  n\u0026lt;filename  以输入的方式打开文件 filename，并绑定到文件描述符 n。n 可以不写，默认为 0，也即标准输入文件。    n\u0026lt;\u0026amp;m  类似于 n\u0026gt;\u0026amp;m，但是因为使用的是\u0026lt;，所以 n 和 m 只能用作命令的输入文件。n 可以不写，默认为 0。    n\u0026lt;\u0026amp;-  关闭文件描述符 n 及其代表的文件。n 可以不写，默认为 0。    输入和输出  n\u0026lt;\u0026gt;filename  同时以输入和输出的方式打开文件 filename，并绑定到文件描述符 n，相当于\u0026nbsp;n\u0026gt;filename 和\u0026nbsp;n\u0026lt;filename 的总和。。n 可以不写，默认为 0。   **还有点疑惑，留个链接http://c.biancheng.net/view/3075.html** 使用exec命令操作文件描述符 exec 是 Shell 内置命令，它有两种用法，一种是执行 Shell 命令，一种是操作文件描述符。本节只讲解后面一种，前面一种请大家自行学习。\n使用 exec 命令可以永久性地重定向，后续命令的输入输出方向也被确定了，直到再次遇到 exec 命令才会改变重定向的方向；换句话说，一次重定向，永久有效。\n请看下面的例子：\n [mozhiyan@localhost ~]$ echo \u0026quot;c.biancheng.net\u0026quot; \u0026gt; log.txt [mozhiyan@localhost ~]$ echo \u0026quot;C 语言中文网\u0026quot; C 语言中文网 [mozhiyan@localhost ~]$ cat log.txt c.biancheng.net  第一个 echo 命令使用了重定向，将内容输出到 log.txt 文件；第二个 echo 命令没有再次使用重定向，内容就直接输出到显示器上了。很明显，重定向只对第一个 echo 有效，对第二个 echo 无效。\n有些脚本文件的输出内容很多，我们不希望直接输出到显示器上，或者我们需要把输出内容备份到文件中，方便以后检索，按照以前的思路，必须在每个命令后面都使用一次重定向，写起来非常麻烦。如果以后想修改重定向的方向，那工作量也是不小的。\nexec 命令就是为解决这种困境而生的，它可以让重定向对当前 Shell 进程中的所有命令有效，它的用法为：exec 文件描述符操作\n在《结合Linux文件描述符谈重定向，彻底理解重定向的本质》一节讲到的所有对文件描述符的操作方式 exec 都支持，请看下面的例子：\n [mozhiyan@localhost ~]$ echo \u0026quot;重定向未发生\u0026quot; 重定向未发生 [mozhiyan@localhost ~]$ exec \u0026gt;log.txt [mozhiyan@localhost ~]$ echo \u0026quot;c.biancheng.net\u0026quot; [mozhiyan@localhost ~]$ echo \u0026quot;C 语言中文网\u0026quot; [mozhiyan@localhost ~]$ exec \u0026gt;\u0026amp;2 [mozhiyan@localhost ~]$ echo \u0026quot;重定向已恢复\u0026quot; 重定向已恢复 [mozhiyan@localhost ~]$ cat log.txt c.biancheng.net C 语言中文网  对代码的说明： exec \u0026gt;log.txt将当前 Shell 进程的所有标准输出重定向到 log.txt 文件，它等价于exec 1\u0026gt;log.txt。 后面的两个 echo 命令都没有在显示器上输出，而是输出到了 log.txt 文件。 exec \u0026gt;\u0026amp;2用来恢复重定向，让标准输出重新回到显示器，它等价于exec 1\u0026gt;\u0026amp;2。2 是标准错误输出的文件描述符，它也是输出到显示器，并且没有遭到破坏，我们用 2 来覆盖 1，就能修复 1，让 1 重新指向显示器。 接下来的 echo 命令将结果输出到显示器上，证明exec \u0026gt;\u0026amp;2奏效了。 最后我们用 cat 命令来查看 log.txt 文件的内容，发现就是中间两个 echo 命令的输出。 重定向的恢复 类似echo \u0026ldquo;1234\u0026rdquo; \u0026gt;log.txt这样的重定向只是临时的，当前命名执行完毕后会自动恢复到显示器，我们不用担心。但是诸如exec \u0026gt;log.txt这种使用 exec 命令的重定向都是持久的，如果我们想再次回到显示器，就必须手动恢复。\n以输出重定向为例，手动恢复的方法有两种： /dev/tty 文件代表的就是显示器，将标准输出重定向到 /dev/tty 即可，也就是 exec \u0026gt;/dev/tty。 如果还有别的文件描述符指向了显示器，那么也可以别的文件描述符来恢复标号为 1 的文件描述符，例如 exec \u0026gt;\u0026amp;2。注意，如果文件描述符 2 也被重定向了，那么这种方式就无效了。\nShell代码块重定向 所谓代码块，就是由多条语句组成的一个整体；for、while、until 循环，或者 if\u0026hellip;else、case\u0026hellip;in 选择结构，或者由{ }包围的命令都可以称为代码块。\n #!/bin/bash sum=0 while read n; do ((sum += n)) echo \u0026quot;this number: $n\u0026quot; done \u0026lt;nums.txt \u0026gt;log.txt #同时使用输入输出重定向 echo \u0026quot;sum=$sum\u0026quot;   #!/bin/bash { echo \u0026quot;C 语言中文网\u0026quot;; echo \u0026quot;http://c.biancheng.net\u0026quot;; echo \u0026quot;7\u0026quot; } \u0026gt;log.txt #输出重定向 { read name; read url; read age } \u0026lt;log.txt #输入重定向 echo \u0026quot;$name已经$age 岁了，它的网址是 $url\u0026quot;  组命令 所谓组命令，就是将多个命令划分为一组，或者看成一个整体。\nShell 组命令的写法有两种：\n { command1; command2; command3; . . . } (command1; command2; command3;. . . )  两种写法的区别在于：由花括号{}包围起来的组命名在当前 Shell 进程中执行，而由小括号()包围起来的组命令会创建一个子 Shell，所有命令都在子 Shell 中执行。\n对于第一种写法，花括号和命令之间必须有一个空格，并且最后一个命令必须用一个分号或者一个换行符结束。\n子 Shell 就是一个子进程，是通过当前 Shell 进程创建的一个新进程。\n例如，下面的代码将多个命令的输出重定向到 out.txt：\n ls -l \u0026gt; out.txt #\u0026gt;表示覆盖 echo \u0026quot;http://c.biancheng.net/shell/\u0026quot; \u0026gt;\u0026gt; out.txt #\u0026gt;\u0026gt;表示追加 cat readme.txt \u0026gt;\u0026gt; out.txt  本段代码共使用了三次重定向。\n借助组命令，我们可以将以上三条命令合并在一起，简化成一次重定向：\n { ls -l; echo \u0026quot;http://c.biancheng.net/shell/\u0026quot;; cat readme.txt; } \u0026gt; out.txt  或者写作：\n (ls -l; echo \u0026quot;http://c.biancheng.net/shell/\u0026quot;; cat readme.txt) \u0026gt; out.txt  使用组命令技术，我们节省了一些打字时间。\n类似的道理，我们也可以将组命令和管道结合起来： { ls -l; echo \u0026ldquo;http://c.biancheng.net/shell/\"; cat readme.txt; } | lpr\n这里我们把三个命令的输出结果合并在一起，并把它们用管道输送给命令 lpr 的输入，以便产生一个打印报告。 两种组命令形式的对比 虽然两种 Shell 组命令形式看起来相似，它们都能用在重定向中合并输出结果，但两者之间有一个很重要的不同：由{}包围的组命令在当前 Shell 进程中执行，由()包围的组命令会创建一个子Shell，所有命令都会在这个子 Shell 中执行。\n在子 Shell 中执行意味着，运行环境被复制给了一个新的 shell 进程，当这个子 Shell 退出时，新的进程也会被销毁，环境副本也会消失，所以在子 Shell 环境中的任何更改都会消失（包括给变量赋值）。因此，在大多数情况下，除非脚本要求一个子 Shell，否则使用{}比使用()更受欢迎，并且{}的进行速度更快，占用的内存更少。\n进程替换 http://c.biancheng.net/view/3025.html\n子Shell和子进程 \n 对于 Shell 来说，以新进程的方式运行脚本文件，比如bash ./test.sh、chmod +x ./test.sh; ./test.sh，或者在当前 Shell 中使用 bash 命令启动新的 Shell，它们都属于第二种创建子进程的方式，所以子进程除了能继承父进程的环境变量外，基本上也不能使用父进程的什么东西了，比如，父进程的全局变量、局部变量、文件描述符、别名等在子进程中都无效。\n但是，组命令、命令替换、管道这几种语法都使用第一种方式创建进程，所以子进程可以使用父进程的一切，包括全局变量、局部变量、别名等。我们将这种子进程称为子 Shell（sub shell）。\n子 Shell 虽然能使用父 Shell 的的一切，但是如果子 Shell 对数据做了修改，比如修改了全局变量，那么这种修改只能停留在子 Shell，无法传递给父 Shell。不管是子进程还是子 Shell，都是“传子不传父”。\n管道 Shell 还有一种功能，就是可以将两个或者多个命令（程序或者进程）连接到一起，把一个命令的输出作为下一个命令的输入，以这种方式连接的两个或者多个命令就形成了管道（pipe）。\nLinux 管道使用竖线|连接多个命令，这被称为管道符。Linux 管道的具体语法格式如下：\n command1 | command2 command1 | command2 [ | commandN... ]  当在两个命令之间设置管道时，管道符|左边命令的输出就变成了右边命令的输入。只要第一个命令向标准输出写入，而第二个命令是从标准输入读取，那么这两个命令就可以形成一个管道。大部分的 Linux 命令都可以用来形成管道。 这里需要注意，command1 必须有正确输出，而 command2 必须可以处理 command2 的输出结果；而且 command2 只能处理 command1 的正确输出结果，不能处理 command1 的错误信息。\n过滤器 我们己经知道，将几个命令通过管道符组合在一起就形成一个管道。通常，通过这种方式使用的命令就被称为过滤器。过滤器会获取输入，通过某种方式修改其内容，然后将其输出。\n简单地说，过滤器可以概括为以下两点： 如果一个 Linux 命令是从标准输入接收它的输入数据，并在标准输出上产生它的输出数据（结果），那么这个命令就被称为过滤器。 过滤器通常与 Linux 管道一起使用。\n常用的被作为过滤器使用的命令如下所示： |命令|说明| |awk|用于文本处理的解释性程序设计语言，通常被作为数据提取和报告的工具。| |cut|用于将每个输入文件（如果没有指定文件则为标准输入）的每行的指定部分输出到标准输出。| |grep|用于搜索一个或多个文件中匹配指定模式的行。| |tar|用于归档文件的应用程序。| |head|用于读取文件的开头部分（默认是 10 行）。如果没有指定文件，则从标准输入读取。| |paste|用于合并文件的行。| |sed|用于过滤和转换文本的流编辑器。| |sort|用于对文本文件的行进行排序。| |split|用于将文件分割成块。| |strings|用于打印文件中可打印的字符串。| |tac|与 cat 命令的功能相反，用于倒序地显示文件或连接文件。| |tail|用于显示文件的结尾部分。| |tee|用于从标准输入读取内容并写入到标准输出和文件。| |tr|用于转换或删除字符。| |uniq|用于报告或忽略重复的行。| |wc|用于打印文件中的总行数、单词数或字节数。|\nshell 模块化 所谓模块化，就是把代码分散到多个文件或者文件夹。对于大中型项目，模块化是必须的，否则会在一个文件中堆积成千上万行代码，这简直是一种灾难。\nsource 命令的用法为：source filename\n也可以简写为：. filename\n两种写法的效果相同。对于第二种写法，注意点号.和文件名中间有一个空格。\nsource 是 Shell 内置命令的一种，它会读取 filename 文件中的代码，并依次执行所有语句。你也可以理解为，source 命令会强制执行脚本文件中的全部命令，而忽略脚本文件的权限。\nsource 后边可以使用相对路径，也可以使用绝对路径，这里我们使用的是相对路径。\n避免重复引入 熟悉 C/C++ 的读者都知道，C/C++ 中的头文件可以避免被重复引入；换句话说，即使被多次引入，效果也相当于一次引入。这并不是 #include 的功劳，而是我们在头文件中进行了特殊处理。\nShell source 命令和 C/C++ 中的 #include 类似，都没有避免重复引入的功能，只要你使用一次 source，它就引入一次脚本文件中的代码。\n那么，在 Shell 中究竟该如何避免重复引入呢？\n我们可以在模块中额外设置一个变量，使用 if 语句来检测这个变量是否存在，如果发现这个变量存在，就 return 出去。\n这里需要强调一下 return 关键字。return 在 C++、C#、Java 等大部分编程语言中只能退出函数，除此以外再无他用；但是在 Shell 中，return 除了可以退出函数，还能退出由 source 命令引入的脚本文件。\n所谓退出脚本文件，就是在被 source 引入的脚本文件（子文件）中，一旦遇到 return 关键字，后面的代码都不会再执行了，而是回到父脚本文件中继续执行 source 命令后面的代码。\nreturn 只能退出由 source 命令引入的脚本文件，对其它引入脚本的方式无效。\n下面我们通过一个实例来演示如何避免脚本文件被重复引入。本例会涉及到两个脚本文件，分别是主文件 main.sh 和 模块文件 module.sh。\n模块文件 module.sh：\n if [ -n \u0026quot;$__MODULE_SH__\u0026quot; ]; then return fi **MODULE_SH**='module.sh' echo \u0026quot;http://c.biancheng.net/shell/\u0026quot;  注意第一行代码，一定要是使用双引号把$__MODULE_SH__包围起来，具体原因已经在《Shell test》一节中讲到。\n主文件 main.sh：\n #!/bin/bash source module.sh source module.sh echo \u0026quot;here executed\u0026quot;  ./表示当前文件，你也可以直接写作source module.sh。\n运行 main.sh，输出结果为：\n http://c.biancheng.net/shell/ here executed  我们在 main.sh 中两次引入 module.sh，但是只执行了一次，说明第二次引入是无效的。\nmain.sh 中的最后一条 echo 语句产生了输出结果，说明 return 只是退出了子文件，对父文件没有影响。\n ","date":"2022-12-26","permalink":"/post/linux_shelldeep/","tags":["Linux"],"title":"linux_ShellDeep"},{"content":"grep grep （global search regular expression(RE) and print out the line,全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。\nUnix 的 grep 家族包括 grep、egrep 和 fgrep。egrep 和 fgrep 的命令只跟 grep 有很小不同。egrep 是 grep 的扩展，支持更多的 re 元字符， fgrep 就是 fixed grep 或 fast grep，它们把所有的字母都看作单词，也就是说，正则表达式中的元字符表示回其自身的字面意义，不再特殊。Linux 使用 GNU 版本的 grep。它功能更强，可以通过-G、-E、-F 命令行选项来使用 egrep 和 fgrep 的功能。\ngrep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到屏幕，不影响原文件内容。grep 可用于 shell 脚本，因为 grep 通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回 0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回 2。我们利用这些返回值就可进行一些自动化的文本处理工作。\n匹配模式选择:\n   参数 意义     -E, \u0026ndash;extended-regexp 扩展正则表达式 egrep   -F, \u0026ndash;fixed-strings 一个换行符分隔的字符串的集合 fgrep   -G, \u0026ndash;basic-regexp 基本正则   -P, \u0026ndash;perl-regexp 调用的 perl 正则   -e, \u0026ndash;regexp=PATTERN 后面根正则模式，默认无   -f, \u0026ndash;file=FILE 从文件中获得匹配模式   -i, \u0026ndash;ignore-case 不区分大小写   -w, \u0026ndash;word-regexp 匹配整个单词   -x, \u0026ndash;line-regexp 匹配整行   -z, \u0026ndash;null-data 一个 0 字节的数据行，但不是空行    杂项:\n   参数 意义     -s, \u0026ndash;no-messages 不显示错误信息   -v, \u0026ndash;invert-match 显示不匹配的行   -V, \u0026ndash;version 显示版本号   \u0026ndash;help 显示帮助信息   \u0026ndash;mmap use memory-mapped input if possible    输入控制:\n   参数 意义     -m, \u0026ndash;max-count=NUM 匹配的最大数   -b, \u0026ndash;byte-offset 打印匹配行前面打印该行所在的块号码。   -n, \u0026ndash;line-number 显示的加上匹配所在的行号   \u0026ndash;line-buffered 刷新输出每一行   -H, \u0026ndash;with-filename 当搜索多个文件时，显示匹配文件名前缀   -h, \u0026ndash;no-filename 当搜索多个文件时，不显示匹配文件名前缀   \u0026ndash;label=LABEL print LABEL as filename for standard input   -o, \u0026ndash;only-matching 只显示一行中匹配 PATTERN 的部分   -q, \u0026ndash;quiet, \u0026ndash;silent 不显示任何东西   \u0026ndash;binary-files=TYPE 假定二进制文件的 TYPE 类型； TYPE 可以是 \u0026lsquo;binary\u0026rsquo;, \u0026rsquo;text\u0026rsquo;, 或\u0026rsquo;without-match\u0026rsquo;   -a, \u0026ndash;text 匹配二进制的东西   -I 不匹配二进制的东西   -d, \u0026ndash;directories=ACTION 目录操作，读取，递归，跳过   -D, \u0026ndash;devices=ACTION 设置对设备，FIFO,管道的操作，读取，跳过   -R, -r, \u0026ndash;recursive 递归调用   \u0026ndash;include=PATTERN 只查找匹配 FILE_PATTERN 的文件   \u0026ndash;exclude=PATTERN 跳过匹配 FILE_PATTERN 的文件和目录   \u0026ndash;exclude-from=FILE 跳过所有除 FILE 以外的文件   -L, \u0026ndash;files-without-match 匹配多个文件时，显示不匹配的文件名   -l, \u0026ndash;files-with-matches 匹配多个文件时，显示匹配的文件名   -c, \u0026ndash;count 显示匹配的行数   -Z, \u0026ndash;null 在 FILE 文件最后打印空字符    文件控制:\n   参数 意义     -B, \u0026ndash;before-context=NUM 打印匹配本身以及前面的几个行由 NUM 控制   -A, \u0026ndash;after-context=NUM 打印匹配本身以及随后的几个行由 NUM 控制   -C, \u0026ndash;context=NUM 打印匹配本身以及随后，前面的几个行由 NUM 控制   -NUM 和-C 的用法一样的   \u0026ndash;color[=WHEN],    \u0026ndash;colour[=WHEN] 使用标志高亮匹配字串；   -U, \u0026ndash;binary 使用标志高亮匹配字串；   -u, \u0026ndash;unix-byte-offsets 当 CR 字符不存在，报告字节偏移(MSDOS 模式)    egrep、fgrep  ‘egrep’ is the same as ‘grep -E’.\u0026gt; ‘fgrep’ is the same as ‘grep -F’.Direct invocation as either ‘egrep’ or ‘fgrep’ is deprecated, but is Traditional ‘egrep’ did not support interval expressions and some\n  ‘egrep’ implementations use ‘{’ and ‘}’ instead, so portable scripts\n head head 命令可用于查看文件的开头部分的内容，有一个常用的参数 -n 用于显示行数，默认为 10，即显示 10 行的内容。\n命令格式：head [参数] [文件]\n   参数 意义     -q 隐藏文件名   -v 显示文件名   -c\u0026lt;数目\u0026gt; 显示的字节数。   -n\u0026lt;行数\u0026gt; 显示的行数。    tail tail 命令和head类似，但用于查看文件的末尾部分的内容\n命令格式tail [参数] [文件] \n   参数 意义     -f 循环读取,如日志文件有更新时不断输出更新的内容   -q 不显示处理信息   -v 显示详细的处理信息   -c\u0026lt;数目\u0026gt; 显示的字节数   -n\u0026lt;行数\u0026gt; 显示文件的尾部 n 行内容   \u0026ndash;pid=PID 与-f 合用,表示在进程 ID,PID 死掉之后结束   -q, \u0026ndash;quiet, \u0026ndash;silent 从不输出给出文件名的首部   -s, \u0026ndash;sleep-interval=S 与-f 合用,表示在每次反复的间隔休眠 S 秒    cat cat（英文全拼：concatenate）命令用于连接文件并打印到标准输出设备上,可输出多个文件\n语法格式:cat [-AbeEnstTuv] [--help] [--version] fileName1 fileName2\n   参数 意义     -b 或 \u0026ndash;number-nonblank 和 -n 相似，只不过对于空白行不编号。   -s 或 \u0026ndash;squeeze-blank 当遇到有连续两行以上的空白行，就代换为一行的空白行。   -v 或 \u0026ndash;show-nonprinting 使用 ^ 和 M- 符号，除了 LFD 和 TAB 之外。   -E 或 \u0026ndash;show-ends 在每行结束处显示 $。   -T 或 \u0026ndash;show-tabs 将 TAB 字符显示为 ^I。   -A, \u0026ndash;show-all 等价于 -vET。   -e 等价于\u0026quot;-vE\u0026quot;选项；    tac tac按行为单位反向显示文件内容，如果没有文件或文件为-则读取标准输入。 处理多个文件时，依次将每个文件反向显示，而不是将所有文件连在一起再反向显示。\n   参数 意义     -b, \u0026ndash;before 在之前而不是之后连接分隔符。   -r, \u0026ndash;regex 将分隔符作为基础正则表达式（BRE）处理。   -s, \u0026ndash;separator=STRING 使用 STRING 作为分隔符代替默认的换行符。   \u0026ndash;help 显示帮助信息并退出。   \u0026ndash;version 显示版本信息并退出。    rev rev:命令 将文件中的每行内容以字符为单位反序输出，即第一个字符最后输出，最后一个字符最先输出，依次类推。\nless less 可以随意浏览文件，支持翻页和搜索，支持向上翻页和向下翻页。\n   参数 意义     -b \u0026lt;缓冲区大小\u0026gt; 设置缓冲区的大小   -e 当文件显示结束后，自动离开   -f 强迫打开特殊文件，例如外围设备代号、目录和二进制文件   -g 只标志最后搜索的关键词   -i 忽略搜索时的大小写   -m 显示类似 more 命令的百分比   -N 显示每行的行号   -o \u0026lt;文件名\u0026gt; 将 less 输出的内容在指定文件中保存起来   -Q 不使用警告音   -s 显示连续空行为一行   -S 行过长时间将超出部分舍弃   -x \u0026lt;数字\u0026gt; 将\u0026quot;tab\u0026quot;键显示为规定的数字空格    进入文件浏览页面后可以进一步操作的键\n   参数 意义     /字符串 向下搜索\u0026quot;字符串\u0026quot;的功能   ?字符串 向上搜索\u0026quot;字符串\u0026quot;的功能   n 重复前一个搜索（与 / 或 ? 有关）   N 反向重复前一个搜索（与 / 或 ? 有关）   b 向上翻一页   d 向后翻半页   h 显示帮助界面   Q 退出 less 命令   u 向前滚动半页   y 向前滚动一行   空格键 滚动一页   回车键 滚动一行   [pagedown] 向下翻动一页   [pageup] 向上翻动一页    浏览多个文件:less log1.log log2.log 说明：输入 ：n后，切换到 log2.log输入 ：p 后，切换到 log1.log\nmore more 命令类似 cat ，不过会以一页一页的形式显示，更方便使用者逐页阅读语法：more [-dlfpcsu] [-num] [+/pattern] [+linenum] [fileNames..]\n   参数 意义     -num 一次显示的行数   -d 提示使用者，在画面下方显示 [Press space to continue, \u0026lsquo;q\u0026rsquo; to quit.] ，如果使用者按错键，则会显示 [Press \u0026lsquo;h\u0026rsquo; for instructions.] 而不是 \u0026lsquo;哔\u0026rsquo; 声   -l 取消遇见特殊字元 ^L（送纸字元）时会暂停的功能   -f 计算行数时，以实际上的行数，而非自动换行过后的行数（有些单行字数太长的会被扩展为两行或两行以上）   -p 不以卷动的方式显示每一页，而是先清除萤幕后再显示内容   -c 跟 -p 相似，不同的是先显示内容再清除其他旧资料   -s 当遇到有连续两行以上的空白行，就代换为一行的空白行   -u 不显示下引号 （根据环境变数 TERM 指定的 terminal 而有所不同）    +/pattern 在每个文档显示前搜寻该字串（pattern），然后从该字串之后开始显示 +linenum 从第 num 行开始显示 fileNames 欲显示内容的文档，可为复数个数\n   常用操作命令 意义     Enter 向下 n 行，需要定义。默认为 1 行   Ctrl+F 向下滚动一屏   空格键 向下滚动一屏   Ctrl+B 返回上一屏   = 输出当前行的行号   ：f 输出文件名和当前行的行号   V 调用配置的编辑器   !命令 调用 Shell，并执行命令   q 退出 more    find 语法:find path -option [ -print ] [ -exec -ok command ] {} \\; path 为查找的起始目录，如果 path 是空字串则使用目前路径，path 后的部分为 expression，如果 expression 是空字串则使用 -print 为预设 expression。\n   参数 意义     -amin\u0026lt;分钟\u0026gt; 查找在指定时间曾被存取过的文件或目录，单位以分钟计算；   -anewer\u0026lt;参考文件或目录\u0026gt; 查找其存取时间较指定文件或目录的存取时间更接近现在的文件或目录；   -atime\u0026lt;24 小时数\u0026gt; 查找在指定时间曾被存取过的文件或目录，单位以 24 小时计算；   -cmin\u0026lt;分钟\u0026gt; 查找在指定时间之时被更改过的文件或目录；   -cnewer\u0026lt;参考文件或目录\u0026gt; 查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；   -ctime\u0026lt;24 小时数\u0026gt; 查找在指定时间之时被更改的文件或目录，单位以 24 小时计算；   -daystart 从本日开始计算时间；   -depth 从指定目录下最深层的子目录开始查找；   -empty 寻找文件大小为 0 Byte 的文件，或目录下没有任何子目录或文件的空目录；   -exec\u0026lt;执行指令\u0026gt; 假设 find 指令的回传值为 True，就执行该指令；   -false 将 find 指令的回传值皆设为 False；   -fls\u0026lt;列表文件\u0026gt; 此参数的效果和指定“-ls”参数类似，但会把结果保存为指定的列表文件；   -follow 排除符号连接；   -fprint\u0026lt;列表文件\u0026gt; 此参数的效果和指定“-print”参数类似，但会把结果保存成指定的列表文件；   -fprint0\u0026lt;列表文件\u0026gt; 此参数的效果和指定“-print0”参数类似，但会把结果保存成指定的列表文件；   -fprintf\u0026lt;列表文件\u0026gt;\u0026lt;输出格式\u0026gt; 此参数的效果和指定“-printf”参数类似，但会把结果保存成指定的列表文件；   -fstype\u0026lt;文件系统类型\u0026gt; 只寻找该文件系统类型下的文件或目录；   -gid\u0026lt;群组识别码\u0026gt; 查找符合指定之群组识别码的文件或目录；   -group\u0026lt;群组名称\u0026gt; 查找符合指定之群组名称的文件或目录；   -help 或\u0026ndash;help 在线帮助；   -ilname\u0026lt;范本样式\u0026gt; 此参数的效果和指定“-lname”参数类似，但忽略字符大小写的差别；   -iname\u0026lt;范本样式\u0026gt; 此参数的效果和指定“-name”参数类似，但忽略字符大小写的差别；   -inum\u0026lt;inode 编号\u0026gt; 查找符合指定的 inode 编号的文件或目录；   -ipath\u0026lt;范本样式\u0026gt; 此参数的效果和指定“-path”参数类似，但忽略字符大小写的差别；   -iregex\u0026lt;范本样式\u0026gt; 此参数的效果和指定“-regexe”参数类似，但忽略字符大小写的差别；   -links\u0026lt;连接数目\u0026gt; 查找符合指定的硬连接数目的文件或目录；   -lname\u0026lt;范本样式\u0026gt; 指定字符串作为寻找符号连接的范本样式；   -ls 假设 find 指令的回传值为 Ture，就将文件或目录名称列出到标准输出；   -maxdepth\u0026lt;目录层级\u0026gt; 设置最大目录层级；   -mindepth\u0026lt;目录层级\u0026gt; 设置最小目录层级；   -mmin\u0026lt;分钟\u0026gt; 查找在指定时间曾被更改过的文件或目录，单位以分钟计算；   -mount 此参数的效果和指定“-xdev”相同；   -mtime\u0026lt;24 小时数\u0026gt; 查找在指定时间曾被更改过的文件或目录，单位以 24 小时计算；   -name\u0026lt;范本样式\u0026gt; 指定字符串作为寻找文件或目录的范本样式；   -newer\u0026lt;参考文件或目录\u0026gt; 查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录；   -nogroup 找出不属于本地主机群组识别码的文件或目录；   -noleaf 不去考虑目录至少需拥有两个硬连接存在；   -nouser 找出不属于本地主机用户识别码的文件或目录；   -ok\u0026lt;执行指令\u0026gt; 此参数的效果和指定“-exec”类似，但在执行指令之前会先询问用户，若回答“y”或“Y”，则放弃执行命令；   -path\u0026lt;范本样式\u0026gt; 指定字符串作为寻找目录的范本样式；   -perm\u0026lt;权限数值\u0026gt; 查找符合指定的权限数值的文件或目录；   -print 假设 find 指令的回传值为 Ture，就将文件或目录名称列出到标准输出。格式为每列一个名称，每个名称前皆有“./”字符串；   -print0 假设 find 指令的回传值为 Ture，就将文件或目录名称列出到标准输出。格式为全部的名称皆在同一行；   -printf\u0026lt;输出格式\u0026gt; 假设 find 指令的回传值为 Ture，就将文件或目录名称列出到标准输出。格式可以自行指定；   -prune 不寻找字符串作为寻找文件或目录的范本样式;   -regex\u0026lt;范本样式\u0026gt; 指定字符串作为寻找文件或目录的范本样式；   -size\u0026lt;文件大小\u0026gt; 查找符合指定的文件大小的文件；   -true 将 find 指令的回传值皆设为 True；   -type\u0026lt;文件类型\u0026gt; 只寻找符合指定的文件类型的文件；   -uid\u0026lt;用户识别码\u0026gt; 查找符合指定的用户识别码的文件或目录；   -used\u0026lt;日数\u0026gt; 查找文件或目录被更改之后在指定时间曾被存取过的文件或目录，单位以日计算；   -user\u0026lt;拥有者名称\u0026gt; 查找符和指定的拥有者名称的文件或目录；   -version 显示版本信息；   -xdev 将范围局限在先行的文件系统中；   -xtype\u0026lt;文件类型\u0026gt; 此参数的效果和指定“-type”参数类似，差别在于它针对符号连接检查。    类型参数列表：\n   参数 意义     f 普通文件   l 符号连接   d 目录   c 字符设备   b 块设备   s 套接字   p Fifo    你可以使用 ( ) 将运算式分隔，并使用下列运算。\nexp1 -and exp2 ! expr -not expr exp1 -or/-o exp2 exp1, exp2  例子：\n在/home 目录下查找以.txt 结尾的文件名：find /home -name \u0026quot;*.txt\u0026quot;\n查找上周新增的图片：find ~ \\( -iname '*jpeg' -o -iname '*jpg' \\) -type f -mtime -7\n借助-exec 选项与其他命令结合使用；删除 mac 下自动生成的文件：find ./ -name '__MACOSX' -depth -exec rm -rf {} \\;\n查找 /var/log 目录中更改时间在 7 日以前的普通文件，并在删除之前询问它们：find /var/log -type f -mtime +7 -ok rm {} \\;\nslocate/locate + updatedb 在 manjaro 中并没有自带 locate 这个命令，需要自行安装sudo pacman -Sy mlocate,初次使用需要updatedb建立初始库\nlocate命令用于查找符合条件的文档，他会去保存文档和目录名称的数据库内，查找合乎范本样式条件的文档或目录。\n一般情况我们只需要输入 locate your_file_name 即可查找指定文件。与 find 不同，find 是去硬盘找，locate 只在 \u0026lsquo;/var/lib/slocate\u0026rsquo; 资料库中找。\nlocate 的速度比 find 快，它并不是真的查找，而是查数据库，一般文件数据库在 \u0026lsquo;/var/lib/slocate/slocate.db\u0026rsquo; 中，\n所以 locate 的查找并不是实时的，而是以数据库的更新为准，一般是系统自己维护，也可以手工升级数据库 ，命令为：updatedb,可能需要权限，以及更新需要较长的时间语法locate [-d ][--help][--version][范本样式...]\n后面了解到 manjaro 有slocate这个命令，功能和locate差不多，且共用库，\n   参数 意义     -b, \u0026ndash;basename 仅匹配路径名的基本名称   -c, \u0026ndash;count 只输出找到的数量   -d, \u0026ndash;database DBPATH 使用 DBPATH 指定的数据库，而不是默认数据库 \u0026lsquo;/var/lib/mlocate/mlocate.db\u0026rsquo;   -e, \u0026ndash;existing 仅打印当前现有文件的条目   -1 如果 是 1．则启动安全模式。在安全模式下，使用者不会看到权限无法看到 的档案。这会始速度减慢，因为 locate 必须至实际的档案系统中取得档案的 权限资料。   -0, \u0026ndash;null 在输出上带有 NUL 的单独条目   -S, \u0026ndash;statistics 不搜索条目，打印有关每个数据库的统计信息   -q 安静模式，不会显示任何错误讯息。   -P, \u0026ndash;nofollow, -H 检查文件存在时不要遵循尾随的符号链接   -l, \u0026ndash;limit, -n LIMIT 将输出（或计数）限制为 LIMIT 个条目   -n 至多显示 n 个输出。   -m, \u0026ndash;mmap 被忽略，为了向后兼容   -r, \u0026ndash;regexp REGEXP 使用基本正则表达式   \u0026ndash;regex 使用扩展正则表达式   -q, \u0026ndash;quiet 安静模式，不会显示任何错误讯息   -s, \u0026ndash;stdio 被忽略，为了向后兼容   -o 指定资料库存的名称。   -h, \u0026ndash;help 显示帮助   -i, \u0026ndash;ignore-case 忽略大小写   -V, \u0026ndash;version 显示版本信息    不同的 locate 有对应的 updatedb\n updatedb - update a database for mlocate\n    参数 意义     -U、 \u0026ndash;database-root PATH 仅存储扫描以生成的数据库路径为根的文件系统子树的结果。默认情况下，会扫描整个文件系统。   -o、 \u0026ndash;output FILE 将数据库写入文件，而不是使用默认数据库。    touch touch命令用于修改文件或者目录的时间属性，包括存取时间和更改时间。若文件不存在，系统会建立一个新的文件。\n   参数 意义     -a 改变档案的读取时间记录。   -m 改变档案的修改时间记录。   -c 假如目的档案不存在，不会建立新的档案。与 \u0026ndash;no-create 的效果一样。   -f 不使用，是为了与其他 unix 系统的相容性而保留。   -r 使用参考档的时间记录，与 \u0026ndash;file 的效果一样。   -d 设定时间与日期，可以使用各种不同的格式。   -t 设定档案的时间记录，格式与 date 指令相同。   \u0026ndash;no-create 不会建立新档案。   \u0026ndash;help 列出指令格式。   \u0026ndash;version 列出版本讯息。    mkdir mkdir:英文全拼,make directory,命令用于创建目录\n   参数 意义     -m, \u0026ndash;mode=MODE 创建目录并且设定权限   -p, \u0026ndash;parents 创建多级目录时确保没一级目录都会创建 与-m 参数使用时，作用在最后一级目录   -v, \u0026ndash;verbose 为每个创建的目录打印一条消息   \u0026ndash;help 显示帮助之后退出    rm 、rmdir rm:（英文全拼:remove）命令用于删除一个文件或者目录。\n   参数 意义     -d 直接把欲删除的目录的硬连接数据删除成 0，删除该目录；   -f 强制删除文件或目录；   -i 删除已有文件或目录之前先询问用户；   -r 或-R 递归处理，将指定目录下的所有文件与子目录一并处理；   \u0026ndash;preserve-root 不对根目录进行递归操作；   -v 显示指令的详细执行过程。    文件一旦通过rm命令删除，则无法恢复，所以必须格外小心地使用该命令。\nrmdir:（英文全拼:remove directory）命令删除空的目录。 语法: rmdir [-p] dirName dirName：要删除的空目录列表。当删除多个空目录时，目录名之间使用空格隔开。\n   参数 意义     -p 或 \u0026ndash;parents 删除指定目录后，若该目录的上层目录已变成空目录，则将其一并删除；   \u0026ndash;ignore-fail-on-non-empty 此选项使 rmdir 命令忽略由于删除非空目录时导致的错误信息；   -v 或-verboes 显示命令的详细执行过程；   \u0026ndash;help 显示命令的帮助信息；   \u0026ndash;version 显示命令的版本信息。    rmdir -p www/Test：在工作目录下的 www 目录中，删除名为 Test 的子目录。若 Test 删除后，www 目录成为空目录，则 www 亦予删除。\npwd pwd：（英文全拼：print work directory） 命令用于显示工作目录。执行 pwd 指令可立刻得知您目前所在的工作目录的绝对路径名称。\n   参数 意义     -L, \u0026ndash;logical 打印环境变量\u0026quot;$PWD\u0026quot;的值，可能为符号链接。   -P, \u0026ndash;physical （默认值）打印当前工作目录的物理位置。   \u0026ndash;help 显示帮助信息并退出。   \u0026ndash;version 显示版本信息并退出。    ls ls（英文全拼：list files）命令用于显示指定工作目录下之内容（列出目前工作目录所含之文件及子目录)。\n语法:\nls [选项] [文件名...] [-1abcdfgiklmnopqrstuxABCDFGLNQRSUX] [-w cols] [-T cols] [-I pattern] [--full-time] [--format={long,verbose,commas,across,vertical,single-col‐umn}] [--sort={none,time,size,extension}] [--time={atime,access,use,ctime,status}] [--color[={none,auto,always}]] [--help] [--version] [--]     参数 意义:     -C 多列输出，纵向排序。   -F 每个目录名加 \u0026ldquo;/\u0026rdquo; 后缀，每个 FIFO 名加 \u0026quot;   -R 递归列出遇到的子目录。   -a 列出所有文件，包括以 \u0026ldquo;.\u0026rdquo; 开头的隐含文件。   -c 使用“状态改变时间”代替“文件修改时间”为依据来排序（使用“-t”选项时）或列出（使用“-l”选项时）。   -d 将目录名像其它文件一样列出，而不是列出它们的内容。   -i 输出文件前先输出文件系列号（即 i 节点号: i-node number）。   -l 列出(以单列格式)文件模式(file mode)，文件的链接数，所有者名，组名，文件大小(以字节为单位)，时间信息，及文件名。 缺省时，时间信息显示最近修改时间；可以以选项“-c”和“-u”选择显示其它两种时间信息。对于设备文件，原先显示文件大小的区域通常显示的是主要和次要的信号(majorand minor device numbers)。   -q 将文件名中的非打印字符输出为问号。（对于到终端的输出这是缺省的。）   -r 逆序排列。   -t 按时间信息排序。   -u 使用最近访问时间代替最近修改时间为依据来排序（使用“-t”选项时）或列出（使用“-l”选项时）。   -1 单列输出。   -1,\u0026ndash;format=single-column 一行输出一个文件（单列输出）。如标准输出不是到终端，此选项就是缺省选项。   -a,\u0026ndash;all 列出目录中所有文件，包括以“.”开头的文件。   -b \u0026ndash;escape # 把文件名中不可输出的字符用反斜杠加字符编号(就像在 C 语言里一样)的形式列出。   -c, \u0026ndash;time=ctime, \u0026ndash;time=status 按文件状态改变时间（i节点中的ctime）排序并输出目录内 容。如采用长格式输出（选项“-l”），使用文件的状态改变时间取代文件修改时间。【译注：所谓文件状态改变（i节 点中以ctime标志），既包括文件被修改，又包括文件属性（ 如所有者、组、链接数等等）的变化】   -d, \u0026ndash;directory 将目录名像其它文件一样列出，而不是列出它们的内容。   -f 不排序目录内容；按它们在磁盘上存储的顺序列出。同时启动\u0026rsquo;-a\u0026rsquo;选项，如果在\u0026rsquo;-f\u0026rsquo;之前存在\u0026rsquo;-l\u0026rsquo;、\u0026rsquo;\u0026ndash;color\u0026rsquo;或\u0026rsquo;-s\u0026rsquo;，则禁止它们。   -g 忽略，为兼容UNIX用。   -i, \u0026ndash;inode 在每个文件左边打印i节点号（也叫文件序列号和索引号:fileserialnumber and index num‐ber）。i节点号在每个特定的文件系统中是唯一的。   -k, \u0026ndash;kilobytes 如列出文件大小，则以千字节KB为单位。   -l, \u0026ndash;format=long,\u0026ndash;format=verbose 输出的信息从左到右依次包括文件名、文件类型、权限、硬链接数、所有者名、组名、大小（byte）、及时间信息（如未指明是其它时间即指修改时间）。对于6个月以上的文件或超出未来1小时的文件，时间信息中的时分将被年代取代。每个目录列出前，有一行“总块数”显示目录下全部文件所占的磁盘空间。块默认是1024字节； # 如果设置了 POSIXLY_CORRECT 的环境变量，除非用“-k”选项，则默认块大小是512字节。每一个硬链接都计入总块数（因此可能重复计数），这无疑是个缺点。列出的权限类似于以符号表示（文件）模式的规范。但是 ls 在每套权限的第三个字符中结合了多位（ multiple bits ） 的信息，如下： s 如果设置了setuid 位或 setgid位，而且也设置了相应的可执行位。 S 如果设置了 setuid 位或 setgid位，但是没有设置相应的可执行位。 t如果设置了sticky位，而且也设置了相应的可执行位。T如果设置了sticky位，但是没有设置相应的可执行位。x如果仅仅设置了可执行位而非以上四种情况。 - 其它情况（即可执行位未设置）。   -m, \u0026ndash;format=commas 水平列出文件，每行尽可能多，相互用逗号和一个空格分隔。   -n, \u0026ndash;numeric-uid-gid 列出数字化的 UID 和 GID 而不是用户名和组名。   -o 以长格式列出目录内容，但是不显示组信息。等于使用“\u0026ndash;format=long \u0026ndash;no-group ”选项。提供此选项是为了与其它版本的 ls 兼容。   -p 在每个文件名后附上一个字符以说明该文件的类型。类似“ -F ”选项但是不 标示可执行文件。     | q， \u0026ndash;hide-control-chars | 用问号代替文件名中非打印的字符。这是缺省选项。 | | -r, \u0026ndash;reverse | 逆序排列目录内容。 | | -s, \u0026ndash;size | 在每个文件名左侧输出该文件的大小，以1024字节的块为单位。如果设置了POSIXLY_CORRECT的环境变量，除非用“-k”选项，块大小是 512 字节。 | | -t, \u0026ndash;sort=time | 按文件最近修改时间（ i 节点中的 mtime ）而不是按文件名字典序排序，新文件 靠前。 | | -u, \u0026ndash;time=atime, \u0026ndash;time=access, \u0026ndash;time=use | 类似选项“-t”，但是用文件最近访问时间（i节点中的atime）取代文件修改时间。如果使用长格式列出，打印的时间是最近访问时间。 | | -w, \u0026ndash;width cols | 假定屏幕宽度是cols（cols以实际数字取代）列。如未用此选项，缺省值是这样获得的：如可能先尝试取自终端驱动，否则尝试取自环境变量COLUMNS （如果设# 置了的话），都不行则取 80 。 | | -x, \u0026ndash;format=across, \u0026ndash;format=horizontal多列输出，横向排序。 | | -A, \u0026ndash;almost-all | 显示除 \u0026ldquo;.\u0026rdquo; 和 \u0026ldquo;..\u0026rdquo; 外的所有文件。 | | -B, \u0026ndash;ignore-backups | 不输出以“ ~ ”结尾的备份文件，除非已经在命令行中给出。 | | -C, \u0026ndash;format=vertical | 多列输出，纵向排序。当标准输出是终端时这是缺省项。使用命令名 dir 和 d 时， 则总是缺省的。 | | -G, \u0026ndash;no-group | 以长格式列目录时不显示组信息。 | | -I, \u0026ndash;ignorepattern | 除非在命令行中给定，不要列出匹配shell文件名匹配式（pattern，不是指一般表达式）的文件。在shell中，文 | 件名以\u0026quot;.\u0026ldquo;起始的不与在文件名匹配式(pattern)# 开头的通配符匹配。 | | -L, \u0026ndash;dereference | 列出符号链接指向的文件的信息，而不是符号链接本身。 | | -N, \u0026ndash;literal | 不要用引号引起文件名。 | | -Q, \u0026ndash;quote-name | 用双引号引起文件名，非打印字符以 C 语言的方法表示。 | | -R, \u0026ndash;recursive | 递归列出全部目录的内容。 | | -S, \u0026ndash;sort=size | 按文件大小而不是字典序排序目录内容，大文件靠前。 | | -T, \u0026ndash;tabsize cols | 假定每个制表符宽度是 cols 。缺省为8。为求效率，ls可能在输出中使用制表符。若cols为0，则不使用制表符。 | | -U, \u0026ndash;sort=none | 不排序目录内容；按它们在磁盘上存储的顺序列出。（选项“-U”和“-f”的不同是前者不启动或禁止相关的选项。）这在列很大的目录时特别有用，因为不加排序# 能显著地加快速度。 | | -X, \u0026ndash;sort=extension | 按文件扩展名（由最后的 \u0026ldquo;.\u0026rdquo; 之后的字符组成）的字典序排序。没有扩展名的先列出。 |  dir、vdir dir:和ls 类似，会显示目录内容列表。若罗列出的文件名中有特殊字符，比如空格，在返回结果上这些特殊字符前将会显示一个反斜杠 \\。 dir 命令和下面这条命令功能是一样的：ls -C -b\n用法：dir [选项]... [文件]...\nvdir 命令将使用长列表的形式罗列目录下的内容（除了以 . 开始的隐藏文件与目录），而且在特殊字符前还会加反斜杠 \\ 。长列表中还会显示该文件或者目录的权限信息，连接数，所有者，组所有者，文件大小，上次修改时间和名称等等，和 ll 命令一样。\nvdir 命令和下面这条命令功能是一样的：ls -l -b\nstat stat:用于显示文件的状态信息。stat 命令的输出信息比 ls 命令的输出信息要更详细。用于显示 inode 内容。以文字的格式来显示 inode 的内容。\n语法：stat [文件或目录]\n| 选项 | 说明 | | -L | 支持符号连接； | | -f | 显示文件系统状态而非文件状态； | | -t | 以简洁方式输出信息； | | \u0026ndash;help | 显示指令的帮助信息； | | \u0026ndash;version | 显示指令的版本信息。 |\n inode 的内容 inode 包含文件的元信息，具体来说有以下内容： 文件的字节数 文件拥有者的 User ID 文件的 Group ID 文件的读、写、执行权限 文件的时间戳，共有三个：ctime 指 inode 上一次变动的时间，mtime 指文件内容上一次变动的时间，atime 指文件上一次打开的时间。 链接数，即有多少文件名指向这个 inode 文件数据 block 的位置\n man man用于查看 Linux 中的指令帮助\n语法：man(选项)(参数)\n   选项 意义     -a 在所有的 man 帮助手册中搜索；   -f 等价于 whatis 指令，显示给定关键字的简短描述信息；   -P 指定内容时使用分页程序；   -M 指定 man 手册搜索的路径。       参数 意义     1 用户在 shell 环境可操作的命令或执行文件；   2 系统内核可调用的函数与工具等   3 一些常用的函数(function)与函数库(library)，大部分为 C 的函数库(libc)   4 设备文件说明，通常在/dev 下的文件   5 配置文件或某些文件格式   6 游戏(games)   7 惯例与协议等，如 Linux 文件系统，网络协议，ASCII code 等说明   8 系统管理员可用的管理命令   9 跟 kernel 有关的文件    info语法： info(选项)(参数)\n   选项 意义     -d 添加包含 info 格式帮助文档的目录；   -f 指定要读取的 info 格式的帮助文档；   -n 指定首先访问的 info 帮助文件的节点；   -o 输出被选择的节点内容到指定文件。    file file 命令用于辨识文件类型。通过 file 指令，我们得以辨识该文件的类型。\n语法:file(选项)(参数)\n   选项 意义     -b 列出辨识结果时，不显示文件名称；   -c 详细显示指令执行过程，便于排错或分析程序执行的情形；   -f\u0026lt;名称文件\u0026gt; 指定名称文件，其内容有一个或多个文件名称时，让 file 依序辨识这些文件，格式为每列一个文件名称；   -L 直接显示符号连接所指向的文件类别；   -m\u0026lt;魔法数字文件\u0026gt; 指定魔法数字文件；   -v 显示版本信息；   -z 尝试去解读压缩文件的内容。    参数,文件：要确定类型的文件列表，多个文件之间使用空格分开，可以使用 shell 通配符匹配多个文件。\ncp cp:（英文全拼：copy file）命令主要用于复制文件或目录。语法：cp [options] source dest\n   参数 说明     -a 此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于 dpR 参数组合。   -d 复制时保留链接。这里所说的链接相当于 Windows 系统中的快捷方式。   -f 覆盖已经存在的目标文件而不给出提示。   -i 与 -f 选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答 y 时目标文件将被覆盖。   -p 除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。   -r 若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。   -l 不复制文件，只是生成链接文件。    实例：使用指令 cp 将当前目录 test/ 下的所有文件复制到新目录 newtest 下，输入如下命令：cp –r test/ newtest注意：用户使用该指令复制目录时，必须使用参数 -r 或者 -R 。\nmv mv：（英文全拼：move file）命令用来为文件或目录改名、或将文件或目录移入其它位置。\n语法：mv [options] source dest\n   参数 说明     -b 当目标文件或目录存在时，在执行覆盖前，会为其创建一个备份。   -i 如果指定移动的源目录或文件与目标的目录或文件同名，则会先询问是否覆盖旧文件，输入 y 表示直接覆盖，输入 n 表示取消该操作。   -f 如果指定移动的源目录或文件与目标的目录或文件同名，不会询问，直接覆盖旧文件。   -n 不要覆盖任何已存在的文件或目录。   -u 当源文件比目标文件新或者目标文件不存在时，才执行移动操作。    alias 、unalias alias命令用于设置指令的别名。用户可利用 alias，自定指令的别名。若仅输入 alias，则可列出目前所有的别名设置。\nalias 的效力仅及于该次登入的操作。若要每次登入是即自动设好别名，可在.profile 或.cshrc 中设定指令的别名。\n语法：alias[别名]=[指令名称]\n参数说明：若不加任何参数，则列出目前所有的别名设置。unliasunalias 命令用于删除别名。\n语法：unalias [-a][别名]参数：-a删除全部的别名。\nsort sort： 命令用于将文本文件内容加以排序。\n语法：\nsort [OPTION]... [FILE]... sort [OPTION]... --files0-from=F  主要用途：可针对文本文件的内容，以行为单位来排序。将所有输入文件的内容排序后并输出。当没有文件或文件为-时，读取标准输入。\n   排序选项 意义     -b, \u0026ndash;ignore-leading-blanks 忽略开头的空白。   -d, \u0026ndash;dictionary-order 仅考虑空白、字母、数字。   -f, \u0026ndash;ignore-case 将小写字母作为大写字母考虑。   -g, \u0026ndash;general-numeric-sort 根据数字排序。   -i, \u0026ndash;ignore-nonprinting 排除不可打印字符。   -M, \u0026ndash;month-sort 按照非月份、一月、十二月的顺序排序。   -h, \u0026ndash;human-numeric-sort 根据存储容量排序(注意使用大写字母，例如：2K 1G)。   -n, \u0026ndash;numeric-sort 根据数字排序。   -R, \u0026ndash;random-sort 随机排序，但分组相同的行。   \u0026ndash;random-source=FILE 从 FILE 中获取随机长度的字节。   -r, \u0026ndash;reverse 将结果倒序排列。   \u0026ndash;sort=WORD 根据 WORD 排序。   -V, \u0026ndash;version-sort 文本中(版本)数字的自然排序。       其他选项 意义     \u0026ndash;batch-size=NMERGE 一次合并最多 NMERGE 个输入；超过部分使用临时文件。   -c, \u0026ndash;check, \u0026ndash;check=diagnose-first 检查输入是否已排序，该操作不会执行排序。   -C, \u0026ndash;check=quiet, \u0026ndash;check=silent 类似于 -c 选项，但不输出第一个未排序的行。   \u0026ndash;compress-program=PROG 使用 PROG 压缩临时文件；使用 PROG -d 解压缩。   \u0026ndash;debug 注释用于排序的行，发送可疑用法的警报到 stderr。   \u0026ndash;files0-from=F 从文件 F 中读取以 NUL 结尾的所有文件名称；如果 F 是 - ，那么从标准输入中读取名字。   -k, \u0026ndash;key=位置 1[,位置 2] 通过一个 key 排序；KEYDEF 给出位置和类型。 在位置 1 开始一个 key，在位置 2 终止(默认为行尾) 参看 POS 语法。   -m, \u0026ndash;merge 合并已排序文件，之后不再排序。   -o, \u0026ndash;output=FILE 将结果写入 FILE 而不是标准输出。   -s, \u0026ndash;stable 通过禁用最后的比较来稳定排序。   -S, \u0026ndash;buffer-size=SIZE 使用 SIZE 作为内存缓存大小。   -t, \u0026ndash;field-separator=SEP 使用 SEP 作为列的分隔符。   -T, \u0026ndash;temporary-directory=DIR 使用 DIR 作为临时目录，而不是 $TMPDIR 或 /tmp；多次使用该选项指定多个临时目录。   \u0026ndash;parallel=N 将并发运行的排序数更改为 N。   -u, \u0026ndash;unique 同时使用-c，严格检查排序；不同时使用-c，输出排序后去重的结果。   -z, \u0026ndash;zero-terminated 设置行终止符为 NUL（空），而不是换行符。   \u0026ndash;help 显示帮助信息并退出。   \u0026ndash;version 显示版本信息并退出。    POS 是 F[.C][opts]，F 代表域编号，C 是域中字母的位置，F 和 C 均从 1 开始计数。如果没有有效的-t 或-b 选项存在，则从前导空格后开始计数字符。OPTS 是一个或多个由单个字母表示的顺序选项，以此覆盖此 key 的全局顺序设置。如果没有指定 key， 则将其整个行。指定的大小可以使用以下单位之一： 内存使用率% 1%，b 1、K 1024 (默认)，M、G、T、P、E、Z、Y 等依此类推。如果不指定文件，或者文件为\u0026rdquo;-\u0026quot;，则从标准输入读取数据。参数：FILE（可选）：要处理的文件，可以为任意数量。返回值：返回 0 表示成功，返回非 0 值表示失败。\n例子：\ncat sort.txt AAA:BB:CC aaa:30:1.6 ccc:50:3.3 ddd:20:4.2 bbb:10:2.5 eee:40:5.4 eee:60:5.1 # 将BB列按照数字从小到大顺序排列：sort -nk 2 -t: sort.txt AAA:BB:CC bbb:10:2.5 ddd:20:4.2 aaa:30:1.6 eee:40:5.4 ccc:50:3.3 eee:60:5.1 # 将CC列数字从大到小顺序排列： # -n是按照数字大小排序，-r是以相反顺序，-k是指定需要排序的栏位，-t指定栏位分隔符为冒号 sort -nrk 3 -t: sort.txt eee:40:5.4 eee:60:5.1 ddd:20:4.2 ccc:50:3.3 bbb:10:2.5 aaa:30:1.6 AAA:BB:CC  关于-k 选项的解读和例子：-k 选项深度解读：FStart.CStart Modifier,FEnd.CEnd Modifier -\u0026ndash;Start\u0026mdash;,\u0026mdash;End\u0026mdash; FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分，Start 部分和 End 部分。 Start 部分由三部分组成，其中的 Modifier 部分就是我们之前说过的选项部分； 我们重点说说 Start 部分的 FStart 和 C.Start；C.Start 是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中 FStart 就是表示使用的域，而 CStart 则表示在 FStart 域中从第几个字符开始算排序首字符。 同理，在 End 部分中，你可以设定 FEnd.CEnd，如果你省略.CEnd 或将它设定为 0，则表示结尾到本域的最后一个字符。\n例子：从公司英文名称的第二个字母开始排序： $ sort -t ' ' -k 1.2 facebook.txt baidu 100 5000 sohu 100 4500 google 110 5000 guge 50 3000 解读：使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 例子：只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： $ sort -t ' ' -k 1.2,1.2 -nrk 3,3 facebook.txt baidu 100 5000 google 110 5000 sohu 100 4500 guge 50 3000 解读：由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们只对第二个字母进行排序（如果你问我使用-k 1.2怎么不行？当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。 对员工工资进行排序，我们也使用了-k 3,3，这是最准确的表述，表示我们只对本域进行排序，因为如果你省略了后面的3，就变成了我们对第3个域开始到最后一个域位置的内容进行排序了。  uniq uniq：显示或忽略重复的行。\n语法：uniq [OPTION]... [INPUT [OUTPUT]]主要用途：将输入文件（或标准输入）中邻近的重复行写入到输出文件（或标准输出）中。当没有选项时，邻近的重复行将合并为一个。\n   选项 意义     -c, \u0026ndash;count 在每行开头增加重复次数。   -d, \u0026ndash;repeated 所有邻近的重复行只被打印一次。   -D 所有邻近的重复行将全部打印。   \u0026ndash;all-repeated[=METHOD] 类似于 -D，但允许每组之间以空行分割。METHOD 取值范围{none(默认)，prepend，separate}。   -f, \u0026ndash;skip-fields=N 跳过对前 N 个列的比较。   \u0026ndash;group[=METHOD] 显示所有行，允许每组之间以空行分割。METHOD 取值范围：{separate(默认)，prepend，append，both}。   -i, \u0026ndash;ignore-case 忽略大小写的差异。   -s, \u0026ndash;skip-chars=N 跳过对前 N 个字符的比较。   -u, \u0026ndash;unique 只打印非邻近的重复行。   -z, \u0026ndash;zero-terminated 设置行终止符为 NUL（空），而不是换行符。   -w, \u0026ndash;check-chars=N 只对每行前 N 个字符进行比较。   \u0026ndash;help 显示帮助信息并退出。   \u0026ndash;version 显示版本信息并退出。    参数: INPUT（可选）：输入文件，不提供时为标准输入。 OUTPUT（可选）：输出文件，不提供时为标准输出。返回值: 返回 0 表示成功，返回非 0 值表示失败。\n例子：\nuniq -i -c uniqtest#检查的时候，不区分大小写 uniq -s 4 -c uniqtest #检查的时候，不考虑前4个字符，这样whom have a try 就和 you have a try 就一样了。 uniq -w 2 -c uniqtest#对每行第2个字符以后的内容不作检查，所以i am tank 根 i love tank就一样了。  wc wc：统计文件的字节数、字数（英文单词数）、行数。统计指定文件中的字节数、字数、行数，并将统计结果显示输出。利用 wc 指令我们可以计算文件的 Byte 数、字数或是列数，若不指定文件名称，或是所给予的文件名为“-”，则 wc 指令会从标准输入设备读取数据。wc 同时也给出所指定文件的总统计数。\n语法：\nwc(选项)(参数) wc [选项]... [文件]... wc [选项]... --files0-from=F     选项 意义     -c # 统计字节数，或\u0026ndash;bytes：显示 Bytes 数。   -l # 统计行数，或\u0026ndash;lines：显示列数。   -m # 统计字符数，或\u0026ndash;chars：显示字符数。   -w # 统计字数，或\u0026ndash;words：显示字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。   -L # 打印最长行的长度，或\u0026ndash;max-line-length。   -help 显示帮助信息。   \u0026ndash;version 显示版本信息。    tee tee:从标准输入读取数据并重定向到标准输出和文件。替代重定向可检查输入结果。\n语法：tee [OPTION]... [FILE]...\n主要用途:需要同时查看数据内容并输出到文件时使用。参数:FILE（可选）：要输出的文件，可以为一或多个。\n   选项 意义     -a, \u0026ndash;append 追加到文件中而不是覆盖。   -i, \u0026ndash;ignore-interrupts 忽略中断信号（Ctrl+c 中断操作无效）。   -p 诊断写入非管道的错误。   \u0026ndash;output-error[=MODE] 设置写错误时的行为，请查看下方的 MODE 部分。   \u0026ndash;help 显示帮助信息并退出。   \u0026ndash;version 显示版本信息并退出。    MODE 决定了当出现写错误时的输出行为，可用的 MODE 如下：\n'warn'当写入到任何输出报错时诊断。 'warn-nopipe' 当写入到任何输出（而不是管道）报错时诊断。 'exit'当写入到任何输出报错时退出。 'exit-nopipe' 当写入到任何输出（而不是管道）报错时退出。  -p 选项的指定的默认 MODE 为\u0026rsquo;warn-nopipe\u0026rsquo;。当\u0026rsquo;\u0026ndash;output-error\u0026rsquo;没有在选项中时，默认的操作是当写入到管道报错时立刻退出，诊断错误信息并写入到非管道输出。\nps 、pstree ps:报告当前系统的进程状态\nps [-aAcdefHjlmNVwy][acefghLnrsSTuvxX][-C \u0026lt;指令名称\u0026gt;][-g \u0026lt;群组名称\u0026gt;][-G \u0026lt;群组识别码\u0026gt;][-p \u0026lt;进程识别码\u0026gt;][p \u0026lt;进程识别码\u0026gt;][-s \u0026lt;阶段作业\u0026gt;][-t \u0026lt;终端机编号\u0026gt;][t \u0026lt;终端机编号\u0026gt;][-u \u0026lt;用户识别码\u0026gt;][-U \u0026lt;用户识别码\u0026gt;][U \u0026lt;用户名称\u0026gt;][-\u0026lt;进程识别码\u0026gt;][--cols \u0026lt;每列字符数\u0026gt;][--columns \u0026lt;每列字符数\u0026gt;][--cumulative][--deselect][--forest][--headers][--help][-- info][--lines \u0026lt;显示列数\u0026gt;][--no-headers][--group \u0026lt;群组名称\u0026gt;][-Group \u0026lt;群组识别码\u0026gt;][--pid \u0026lt;进程识别码\u0026gt;][--rows \u0026lt;显示列数\u0026gt;][--sid \u0026lt;阶段作业\u0026gt;][--tty \u0026lt;终端机编号\u0026gt;][--user \u0026lt;用户名称\u0026gt;][--User \u0026lt;用户识别码\u0026gt;][--version][--width \u0026lt;每列字符数\u0026gt;]     参数 意义     -a 显示所有终端机下执行的程序，除了阶段作业领导者之外。   a 显示现行终端机下的所有程序，包括其他用户的程序。   -A 显示所有程序。   -c 显示 CLS 和 PRI 栏位。   c 列出程序时，显示每个程序真正的指令名称，而不包含路径，选项或常驻服务的标示。   -C\u0026lt;指令名称\u0026gt; 指定执行指令的名称，并列出该指令的程序的状况。   -d 显示所有程序，但不包括阶段作业领导者的程序。   -e 此选项的效果和指定\u0026quot;A\u0026quot;选项相同。   e 列出程序时，显示每个程序所使用的环境变量。   -f 显示 UID,PPIP,C 与 STIME 栏位。   f 用 ASCII 字符显示树状结构，表达程序间的相互关系。   -g\u0026lt;群组名称\u0026gt; 此选项的效果和指定\u0026quot;-G\u0026quot;选项相同，当亦能使用阶段作业领导者的名称来指定。   g 显示现行终端机下的所有程序，包括群组领导者的程序。   -G\u0026lt;群组识别码\u0026gt; 列出属于该群组的程序的状况，也可使用群组名称来指定。   h 不显示标题列。   -H 显示树状结构，表示程序间的相互关系。   -j 或 j 采用工作控制的格式显示程序状况。   -l 或 l 采用详细的格式来显示程序状况。   L 列出栏位的相关信息。   -m 或 m 显示所有的执行绪。   n 以数字来表示 USER 和 WCHAN 栏位。   -N 显示所有的程序，除了执行 ps 指令终端机下的程序之外。   -p\u0026lt;程序识别码\u0026gt; 指定程序识别码，并列出该程序的状况。   p\u0026lt;程序识别码\u0026gt; 此选项的效果和指定\u0026quot;-p\u0026quot;选项相同，只在列表格式方面稍有差异。   r 只列出现行终端机正在执行中的程序。   -s\u0026lt;阶段作业\u0026gt; 指定阶段作业的程序识别码，并列出隶属该阶段作业的程序的状况。   s 采用程序信号的格式显示程序状况。   S 列出程序时，包括已中断的子程序资料。   -t\u0026lt;终端机编号\u0026gt; 指定终端机编号，并列出属于该终端机的程序的状况。   t\u0026lt;终端机编号\u0026gt; 此选项的效果和指定\u0026quot;-t\u0026quot;选项相同，只在列表格式方面稍有差异。   -T 显示现行终端机下的所有程序。   -u\u0026lt;用户识别码\u0026gt; 此选项的效果和指定\u0026quot;-U\u0026quot;选项相同。   u 以用户为主的格式来显示程序状况。   -U\u0026lt;用户识别码\u0026gt; 列出属于该用户的程序的状况，也可使用用户名称来指定。   U\u0026lt;用户名称\u0026gt; 列出属于该用户的程序的状况。   v 采用虚拟内存的格式显示程序状况。   -V 或 V 显示版本信息。   -w 或 w 采用宽阔的格式来显示程序状况。   x 显示所有程序，不以终端机来区分。   X 采用旧式的 Linux i386 登陆格式显示程序状况。   -y 配合选项\u0026quot;-l\u0026quot;使用时，不显示 F(flag)栏位，并以 RSS 栏位取代 ADDR 栏位　。   -\u0026lt;程序识别码\u0026gt; 此选项的效果和指定\u0026quot;p\u0026quot;选项相同。   \u0026ndash;cols\u0026lt;每列字符数\u0026gt; 设置每列的最大字符数。   \u0026ndash;columns\u0026lt;每列字符数\u0026gt; 此选项的效果和指定\u0026quot;\u0026ndash;cols\u0026quot;选项相同。   \u0026ndash;cumulative 此选项的效果和指定\u0026quot;S\u0026quot;选项相同。   \u0026ndash;deselect 此选项的效果和指定\u0026quot;-N\u0026quot;选项相同。   \u0026ndash;forest 此选项的效果和指定\u0026quot;f\u0026quot;选项相同。   \u0026ndash;headers 重复显示标题列。   \u0026ndash;help 在线帮助。   \u0026ndash;info 显示排错信息。   \u0026ndash;lines\u0026lt;显示列数\u0026gt; 设置显示画面的列数。   \u0026ndash;no-headers 此选项的效果和指定\u0026quot;h\u0026quot;选项相同，只在列表格式方面稍有差异。   \u0026ndash;group\u0026lt;群组名称\u0026gt; 此选项的效果和指定\u0026quot;-G\u0026quot;选项相同。   \u0026ndash;Group\u0026lt;群组识别码\u0026gt; 此选项的效果和指定\u0026quot;-G\u0026quot;选项相同。   \u0026ndash;pid\u0026lt;程序识别码\u0026gt; 此选项的效果和指定\u0026quot;-p\u0026quot;选项相同。   \u0026ndash;rows\u0026lt;显示列数\u0026gt; 此选项的效果和指定\u0026quot;\u0026ndash;lines\u0026quot;选项相同。   \u0026ndash;sid\u0026lt;阶段作业\u0026gt; 此选项的效果和指定\u0026quot;-s\u0026quot;选项相同。   \u0026ndash;tty\u0026lt;终端机编号\u0026gt; 此选项的效果和指定\u0026quot;-t\u0026quot;选项相同。   \u0026ndash;user\u0026lt;用户名称\u0026gt; 此选项的效果和指定\u0026quot;-U\u0026quot;选项相同。   \u0026ndash;User\u0026lt;用户识别码\u0026gt; 此选项的效果和指定\u0026quot;-U\u0026quot;选项相同。   \u0026ndash;version 此选项的效果和指定\u0026quot;-V\u0026quot;选项相同。   \u0026ndash;widty\u0026lt;每列字符数\u0026gt; 此选项的效果和指定\u0026quot;-cols\u0026quot;选项相同。    au(x) 输出格式 :USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND\nUSER: 行程拥有者 PID: pid %CPU: 占用的 CPU 使用率 %MEM: 占用的记忆体使用率 VSZ: 占用的虚拟记忆体大小 RSS: 占用的记忆体大小 TTY: 终端的次要装置号码 (minor device number of tty) STAT: 该行程的状态:D: 无法中断的休眠状态 (通常 IO 的进程)R: 正在执行中S: 静止状态T: 暂停执行Z: 不存在但暂时无法消除W: 没有足够的记忆体分页可分配 \u0026lt;: 高优先序的行程 N: 低优先序的行程 L: 有记忆体分页分配并锁在记忆体内 (实时系统或捱A I/O) START: 行程开始时间 TIME: 执行的时间 COMMAND:所执行的指令  pstree:命令 以树状图的方式展现进程之间的派生关系，显示效果比较直观。\n   参数 意义     -a 显示每个程序的完整指令，包含路径，参数或是常驻服务的标示；   -c 不使用精简标示法；   -G 使用 VT100 终端机的列绘图字符；   -h 列出树状图时，特别标明现在执行的程序；   -H\u0026lt;程序识别码\u0026gt; 此参数的效果和指定\u0026quot;-h\u0026quot;参数类似，但特别标明指定的程序；   -l 采用长列格式显示树状图；   -n 用程序识别码排序。预设是以程序名称来排序；   -p 显示程序识别码；   -u 显示用户名称；   -U 使用 UTF-8 列绘图字符；   -V 显示版本信息。    kill、killall、pkill kill:发送信号到进程(可以为多个)。\n语法：\nkill [-s sigspec | -n signum | -sigspec] pid | jobspec ... kill -l [sigspec]     选项 意义     -s sig 信号名称。   -n sig 信号名称对应的数字。   -l 列出信号名称。如果在该选项后提供了数字那么假设它是信号名称对应的数字。   -L 等价于-l 选项。    参数: pid：进程 ID; jobspec：作业标识符\n# 列出所有信号名称： [user2@pc] kill -l1) SIGHUP 2) SIGINT 3) SIGQUIT4) SIGILL5) SIGTRAP6) SIGABRT7) SIGBUS 8) SIGFPE9) SIGKILL10) SIGUSR111) SIGSEGV12) SIGUSR2 13) SIGPIPE14) SIGALRM15) SIGTERM16) SIGSTKFLT 17) SIGCHLD18) SIGCONT19) SIGSTOP20) SIGTSTP 21) SIGTTIN22) SIGTTOU23) SIGURG24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM27) SIGPROF28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS34) SIGRTMIN 35) SIGRTMIN+136) SIGRTMIN+237) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+540) SIGRTMIN+641) SIGRTMIN+742) SIGRTMIN+8 43) SIGRTMIN+944) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-956) SIGRTMAX-857) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-560) SIGRTMAX-461) SIGRTMAX-362) SIGRTMAX-2 63) SIGRTMAX-164) SIGRTMAX# 下面是常用的信号。 # 只有第9种信号(SIGKILL)才可以无条件终止进程，其他信号进程都有权利忽略。HUP1 终端挂断 INT2 中断（同 Ctrl + C） QUIT 3 退出（同 Ctrl + \\） KILL 9 强制终止 TERM15 终止 CONT18 继续（与STOP相反，fg/bg命令） STOP19 暂停（同 Ctrl + Z）  killall:用于杀死一个进程，与 kill 不同的是它会杀死指定名字的所有进程。\n语法： killall [选项] name\n   选项 意义     -e \u0026ndash;exact 进程需要和名字完全相符   -I \u0026ndash;ignore-case 忽略大小写   -g \u0026ndash;process-group 结束进程组   -i \u0026ndash;interactive 结束之前询问   -l \u0026ndash;list 列出所有的信号名称   -q \u0026ndash;quite 进程没有结束时，不输出任何信息   -r \u0026ndash;regexp 将进程名模式解释为扩展的正则表达式。   -s \u0026ndash;signal 发送指定信号   -u \u0026ndash;user 结束指定用户的进程   -v \u0026ndash;verbose 显示详细执行过程   -w \u0026ndash;wait 等待所有的进程都结束   -V \u0026ndash;version 显示版本信息   \u0026ndash;help 显示帮助信息    name:进程名称。pkill:用于杀死一个进程，与 kill 不同的是它会杀死指定名字的所有进程，类似于 killall 命令。kill 命令杀死指定进程 PID，需要配合 ps 使用，而 pkill 直接对进程对名字进行操作，更加方便。\njobs jobs:显示作业的状态。主要用途:显示作业的状态。列出活动的作业。列出停止的作业。\n   选项 意义     -l 在作业信息中额外的列出 PID。   -n 只列出最近一次通知以来状态变更的作业。   -p 只列出 PID。   -r 只输出处于运行状态的作业。   -s 只输出处于停止状态的作业。    └─(10:07:54 on master ✖ ✹ ✭)──\u0026gt; sleep 120──(Wed,Mar30)─┘ # 此时按下ctrl+z使得交互停止。 ^Z [1]+ 100640 suspendedsleep 120 ┌─(~/Documents/DEEP_FISH/DEEP)──────────────────────────────────────────────────────────────────────────────────(fiki@FIKI-AI:pts/2)─┐ └─(10:08:01 on master ✖ ✹ ✭)──\u0026gt; sleep 120 \u0026amp; 148 ↵ ──(Wed,Mar30)─┘ [2] 100665 ┌─(~/Documents/DEEP_FISH/DEEP)──────────────────────────────────────────────────────────────────────────────────(fiki@FIKI-AI:pts/2)─┐ └─(10:08:09 on master ✖ ✹ ✭)──\u0026gt; jobs -l──(Wed,Mar30)─┘ [1]+ 100640 suspendedsleep 120 [2]- 100665 running sleep 120  bg bg：将前台终端作业移动到后台运行,后跟指定要移动到后台执行的作业标识符，可以是一到多个。若后台任务中只有一个，则使用该命令时可以省略任务号。\nfg fg：将后台作业移动到前台终端运行，后跟指定要移动到后台执行的作业标识符，可以是一到多个。若后台任务中只有一个，则使用该命令时可以省略任务号。\n└─(10:07:54 on master ✖ ✹ ✭)──\u0026gt; sleep 120──(Wed,Mar30)─┘ # 此时按下ctrl+z使得交互停止。 ^Z [1]+ 100640 suspendedsleep 120 ┌─(~/Documents/DEEP_FISH/DEEP)──────────────────────────────────────────────────────────────────────────────────(fiki@FIKI-AI:pts/2)─┐ └─(10:08:01 on master ✖ ✹ ✭)──\u0026gt; sleep 120 \u0026amp; 148 ↵ ──(Wed,Mar30)─┘ [2] 100665 ┌─(~/Documents/DEEP_FISH/DEEP)──────────────────────────────────────────────────────────────────────────────────(fiki@FIKI-AI:pts/2)─┐ └─(10:08:09 on master ✖ ✹ ✭)──\u0026gt; jobs -l──(Wed,Mar30)─┘ [1]+ 100640 suspendedsleep 120 [2]- 100665 running sleep 120 ┌─(~/Documents/DEEP_FISH/DEEP)──────────────────────────────────────────────────────────────────────────────────(fiki@FIKI-AI:pts/2)─┐ └─(10:10:52 on master ✖ ✹ ✭)──\u0026gt; bg %1 148 ↵ ──(Wed,Mar30)─┘ # 此时后台挂载的程序1在后台恢复运行 [1]- 100640 continuedsleep 120 ┌─(~/Documents/DEEP_FISH/DEEP)──────────────────────────────────────────────────────────────────────────────────(fiki@FIKI-AI:pts/2)─┐ └─(10:11:03 on master ✖ ✹ ✭)──\u0026gt; fg %2 ──(Wed,Mar30)─┘ # 此时后台运行的程序2被调到前台运行 [2]- 100665 running sleep 120  cd cd:（英文全拼：change directory）命令用于切换当前工作目录。其中 dirName 表示法可为绝对路径或相对路径。若目录名称省略，则变换至使用者的 home 目录 (也就是刚 login 时所在的目录)。另外，~ 也表示为 home 目录 的意思， . 则是表示目前所在的目录， .. 则表示目前目录位置的上一层目录。\n语法:cd [dirName] ,dirName：要切换的目标目录。\ntree tree:命令用于以树状图列出目录的内容。执行 tree 指令，它会列出指定目录下的所有文件，包括子目录里的文件。\n语法：tree [-aACdDfFgilnNpqstux][-I \u0026lt;范本样式\u0026gt;][-P \u0026lt;范本样式\u0026gt;][目录...]\n   参数 说明     列表选项    -a 显示所有文件和目录。   -d 显示目录名称而非文件。   -l 如遇到性质为符号连接的目录，直接列出该连接所指向的原始目录。   -f 在每个文件或目录之前，显示完整的相对路径名称。   -x 将范围局限在现行的文件系统中，若指定目录下的某些子目录，其存放于另一个文件系统上，则将该目录予以排除在寻找范围外。   -L level 限制目录显示层级。   -R Rerun tree when max dir level reached.   -P pattern \u0026lt;范本样式\u0026gt; 只显示符合范本样式的文件和目录名称。   -I pattern Do not list files that match the given pattern.   \u0026ndash;ignore-case Ignore case when pattern matching.   \u0026ndash;matchdirs Include directory names in -P pattern matching.   \u0026ndash;noreport Turn off file/directory count at end of tree listing.   \u0026ndash;charset X Use charset X for terminal/HTML and indentation line output.   \u0026ndash;filelimit Do not descend dirs with more than # files in them.   \u0026ndash;timefmt  Print and format time according to the format .   -o filename Output to file instead of stdout.   文件选项    -q 用“？”号取代控制字符，列出文件和目录名称。   -N 直接列出文件和目录名称，包括控制字符。   -Q Quote filenames with double quotes.   -p 列出权限标示。   -u 列出文件或目录的拥有者名称，没有对应的名称时，则显示用户识别码。   -g 列出文件或目录的所属群组名称，没有对应的名称时，则显示群组识别码。   -s 列出文件和目录大小。   -h Print the size in a more human readable way.   \u0026ndash;si Like -h, but use in SI units (powers of 1000).   -D 列出文件或目录的更改时间。   -F 在执行文件，目录，Socket，符号连接，管道名称名称，各自加上\u0026quot;*\u0026quot;，\u0026quot;/\u0026quot;，\u0026quot;@\u0026quot;，\u0026quot;#\u0026ldquo;号。   \u0026ndash;inodes Print inode number of each file.   \u0026ndash;device Print device ID number to which each file belongs.   排序选项    -v Sort files alphanumerically by version.   -t 用文件和目录的更改时间排序。   -c Sort files by last status change time.   -U Leave files unsorted.   -r Reverse the order of the sort.   \u0026ndash;dirsfirst List directories before files (-U disables).   \u0026ndash;sort X Select sort: name,version,size,mtime,ctime.   图形选项    -i 不以阶梯状列出文件和目录名称。   -A 使用 ASNI 绘图字符显示树状图而非以 ASCII 字符组合。   -S Print with CP437 (console) graphics indentation lines.   -n Turn colorization off always (-C overrides).   -C 在文件和目录清单加上色彩，便于区分各种类型。   XML / HTML / JSON 选项    -X Prints out an XML representation of the tree.   -J Prints out an JSON representation of the tree.   -H baseHREF Prints out HTML format with baseHREF as top directory.   -T string Replace the default HTML title and H1 header with string.   \u0026ndash;nolinks Turn off hyperlinks in HTML output.   杂项选项    \u0026ndash;version 输入版本信息。   \u0026ndash;help 打印使用帮助信息。   \u0026ndash; Options processing terminator.    du du:（英文全拼：disk usage）命令用于显示目录或文件的大小。du 会显示指定的目录或文件所占用的磁盘空间。\n语法:du [-abcDhHklmsSx][-L \u0026lt;符号连接\u0026gt;][-X \u0026lt;文件\u0026gt;][--block-size][--exclude=\u0026lt;目录或文件\u0026gt;][--max-depth=\u0026lt;目录层数\u0026gt;][--help][--version][目录或文件]\n   参数 意义     -a, \u0026ndash;all 显示目录中个别文件的大小。   -B, \u0026ndash;block-size= 大小使用指定字节数的块   -b, \u0026ndash;bytes 显示目录或文件大小时，以byte为单位。   -c, \u0026ndash;total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。   -D, \u0026ndash;dereference-args 显示指定符号链接的源文件大小。   -H, \u0026ndash;si 与-h参数相同，但是K，M，G是以1000为换算单位。   -h, \u0026ndash;human-readable 以K，M，G为单位，提高信息的可读性。   -k, \u0026ndash;kilobytes 以KB(1024bytes)为单位输出。   -l, \u0026ndash;count-links 重复计算硬件链接的文件。   -m, \u0026ndash;megabytes 以MB为单位输出。   -L\u0026lt;符号链接\u0026gt;, \u0026ndash;dereference\u0026lt;符号链接\u0026gt; 显示选项中所指定符号链接的源文件大小。   -P, \u0026ndash;no-dereference 不跟随任何符号链接(默认)   -0, \u0026ndash;null 将每个空行视作0 字节而非换行符   -S, \u0026ndash;separate-dirs 显示个别目录的大小时，并不含其子目录的大小。   -s, \u0026ndash;summarize 仅显示总计，只列出最后加总的值。   -x, \u0026ndash;one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。   -X\u0026lt;文件\u0026gt;, \u0026ndash;exclude-from=\u0026lt;文件\u0026gt; 在\u0026lt;文件\u0026gt;指定目录或文件。   \u0026ndash;apparent-size 显示表面用量，而并非是磁盘用量；虽然表面用量通常会小一些，但有时它会因为稀疏文件间的\u0026quot;洞\u0026rdquo;、内部碎片、非直接引   \u0026ndash;files0-from=F 计算文件F中以NUL结尾的文件名对应占用的磁盘空间如果F的值是\u0026quot;-\u0026quot;，则从标准输入读入文件名   \u0026ndash;exclude=\u0026lt;目录或文件\u0026gt; 略过指定的目录或文件。   \u0026ndash;max-depth=N 显示目录总计(与\u0026ndash;all 一起使用计算文件)当N为指定数值时计算深度为N，等于0时等同\u0026ndash;summarize   \u0026ndash;si 类似-h，但在计算时使用1000 为基底而非1024   \u0026ndash;time 显示目录或该目录子目录下所有文件的最后修改时间   \u0026ndash;time=WORD 显示WORD时间，而非修改时间：atime，access，use，ctime 或status   \u0026ndash;time-style= 样式 按照指定样式显示时间(样式解释规则同\u0026quot;date\u0026quot;命令)：full-iso，long-iso，iso，+FORMAT   \u0026ndash;help 显示此帮助信息并退出   \u0026ndash;version 显示版本信息并退出    df df:（英文全拼：disk free） 命令用于显示目前在 Linux 系统上的文件系统磁盘使用情况统计.默认显示单位为 KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。\n语法:df [选项]... [FILE]...\n   选项 意义     -a或\u0026ndash;all 包含全部的文件系统；   \u0026ndash;block-size=\u0026lt;区块大小\u0026gt; 以指定的区块大小来显示区块数目；   -h或\u0026ndash;human-readable 以可读性较高的方式来显示信息；   -H或\u0026ndash;si 与-h参数相同，但在计算时是以1000 Bytes为换算单位而非1024 Bytes；   -i或\u0026ndash;inodes 显示inode的信息；   -k或\u0026ndash;kilobytes 指定区块大小为1024字节；   -l或\u0026ndash;local 仅显示本地端的文件系统；   -m或\u0026ndash;megabytes 指定区块大小为1048576字节；   \u0026ndash;no-sync 在取得磁盘使用信息前，不要执行sync指令，此为预设值；   -P或\u0026ndash;portability 使用POSIX的输出格式；   \u0026ndash;sync 在取得磁盘使用信息前，先执行sync指令；   -t\u0026lt;文件系统类型\u0026gt;或\u0026ndash;type=\u0026lt;文件系统类型\u0026gt; 仅显示指定文件系统类型的磁盘信息；   -T或\u0026ndash;print-type 显示文件系统的类型；   -x\u0026lt;文件系统类型\u0026gt;或\u0026ndash;exclude-type=\u0026lt;文件系统类型\u0026gt; 不要显示指定文件系统类型的磁盘信息；   \u0026ndash;help 显示帮助；   \u0026ndash;version 显示版本信息。    diff 、diff3 diff:命令用于比较文件的差异。diff 以逐行的方式，比较文本文件的异同处。如果指定要比较目录，则 diff 会比较目录中相同文件名的文件，但不会比较其中子目录。\n语法： diff [-abBcdefHilnNpPqrstTuvwy][-\u0026lt;行数\u0026gt;][-C \u0026lt;行数\u0026gt;][-D \u0026lt;巨集名称\u0026gt;][-I \u0026lt;字符或字符串\u0026gt;][-S \u0026lt;文件\u0026gt;][-W \u0026lt;宽度\u0026gt;][-x \u0026lt;文件或目录\u0026gt;][-X \u0026lt;文件\u0026gt;][--help][--left-column][--suppress-common-line][文件或目录1][文件或目录2] \n   选项 意义     -\u0026lt;行数\u0026gt; 指定要显示多少行的文本。此参数必须与-c 或-u 参数一并使用；   -a 或——text diff 预设只会逐行比较文本文件；   -b 或\u0026ndash;ignore-space-change 不检查空格字符的不同；   -B 或\u0026ndash;ignore-blank-lines 不检查空白行；   -c 显示全部内容，并标出不同之处；   -C\u0026lt;行数\u0026gt;或\u0026ndash;context\u0026lt;行数\u0026gt; 与执行“-c-\u0026lt;行数\u0026gt;”指令相同；   -d 或——minimal 使用不同的演算法，以小的单位来做比较；   -D\u0026lt;巨集名称\u0026gt;或 ifdef\u0026lt;巨集名称\u0026gt; 此参数的输出格式可用于前置处理器巨集；   -e 或——ed 此参数的输出格式可用于 ed 的 script 文件；   -f 或-forward-ed 输出的格式类似 ed 的 script 文件，但按照原来文件的顺序来显示不同处；   -H 或\u0026ndash;speed-large-files 比较大文件时，可加快速度；   -l\u0026lt;字符或字符串\u0026gt;或\u0026ndash;ignore-matching-lines\u0026lt;字符或字符串\u0026gt; 若两个文件在某几行有所不同，而之际航同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异；   -i 或\u0026ndash;ignore-case 不检查大小写的不同；   -l 或——paginate 将结果交由 pr 程序来分页；   -n 或——rcs 将比较结果以 RCS 的格式来显示；   -N 或\u0026ndash;new-file 在比较目录时，若文件 A 仅出现在某个目录中，预设会显示 Only in 目录，文件 A 若使用-N 参数，则 diff 会将文件 A 与一个空白的文件比较；   -p 若比较的文件为 C 语言的程序码文件时，显示差异所在的函数名称；   -P 或\u0026ndash;unidirectional-new-file 与-N 类似，但只有当第二个目录包含了第一个目录所没有的文件时，才会将这个文件与空白的文件做比较；   -q 或\u0026ndash;brief 仅显示有无差异，不显示详细的信息；   -r 或——recursive 比较子目录中的文件；   -s 或\u0026ndash;report-identical-files 若没有发现任何差异，仍然显示信息；   -S\u0026lt;文件\u0026gt;或\u0026ndash;starting-file\u0026lt;文件\u0026gt; 在比较目录时，从指定的文件开始比较；   -t 或\u0026ndash;expand-tabs 在输出时，将 tab 字符展开；   -T 或\u0026ndash;initial-tab 在每行前面加上 tab 字符以便对齐；   -u，-U\u0026lt;列数\u0026gt;或\u0026ndash;unified=\u0026lt;列数\u0026gt; 以合并的方式来显示文件内容的不同；   -v 或——version 显示版本信息；   -w 或\u0026ndash;ignore-all-space 忽略全部的空格字符；   -W\u0026lt;宽度\u0026gt;或\u0026ndash;width\u0026lt;宽度\u0026gt; 在使用-y 参数时，指定栏宽；   -x\u0026lt;文件名或目录\u0026gt;或\u0026ndash;exclude\u0026lt;文件名或目录\u0026gt; 不比较选项中所指定的文件或目录；   -X\u0026lt;文件\u0026gt;或\u0026ndash;exclude-from\u0026lt;文件\u0026gt; 您可以将文件或目录类型存成文本文件，然后在=\u0026lt;文件\u0026gt;中指定此文本文件；   -y 或\u0026ndash;side-by-side 以并列的方式显示文件的异同之处；   \u0026ndash;help 显示帮助；   \u0026ndash;left-column 在使用-y 参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容；   \u0026ndash;suppress-common-lines 在使用-y 参数时，仅显示不同之处。    diff3:命令 用于比较 3 个文件，将 3 个文件的不同的地方显示到标准输出。\n   选项 意义     -a 把所有的文件都当做文本文件按照行为单位进行比较，即给定的文件不是文本文件；   -A 合并第 2 个文件和第 3 个文件之间的不同到第 1 个文件中，有冲突内容用括号括起来；   -B 与选项“-A”功能相同，但是不显示冲突的内容；   -e/\u0026ndash;ed 生成一个“-ed”脚本，用于将第 2 个文件和第 3 个文件之间的不同合并到第 1 个文件中；   \u0026ndash;easy-only 除了不显示互相重叠的变化，与选项“-e”的功能相同；   -i 为了和 system V 系统兼容，在“ed”脚本的最后生成“w”和“q”命令。此选项必须和选项“-AeExX3”连用，但是不能和“-m”连用；   \u0026ndash;initial-tab 在正常格式的行的文本前，输出一个 TAB 字符而非两个空白字符。此选项将导致在行中 TAB 字符的对齐方式看上去规范。    cmp cmp：Linux cmp 命令用于比较两个文件是否有差异。当相互比较的两个文件完全一样时，则该指令不会显示任何信息。若发现有所差异，预设会标示出第一个不同之处的字符和列数编号。若不指定任何文件名称或是所给予的文件名为\u0026quot;-\u0026quot;，则 cmp 指令会从标准输入设备读取数据。\n   选项 意义     -c 或\u0026ndash;print-chars 除了标明差异处的十进制字码之外，一并显示该字符所对应字符；   -i\u0026lt;字符数目\u0026gt;或\u0026ndash;ignore-initial=\u0026lt;字符数目\u0026gt; 指定一个数目；   -l 或——verbose 标示出所有不一样的地方；   -s 或\u0026ndash;quiet 或——silent 不显示错误信息；   -v 或——version 显示版本信息；   \u0026ndash;help 在线帮助。    comm comm:Linux comm 命令用于比较两个已排过序的文件。这项指令会一列列地比较两个已排序文件的差异，并将其结果显示出来，如果没有指定任何参数，则会把结果分成 3 列显示：第 1 列仅是在第 1 个文件中出现过的列，第 2 列是仅在第 2 个文件中出现过的列，第 3 列则是在第 1 与第 2 个文件里都出现过的列。若给予的文件名称为 - ，则 comm 指令会从标准输入设备读取数据。\n语法\ncomm [-123][--help][--version][第1个文件][第 2 个文件] 参数：-1 不显示只在第 1 个文件里出现过的列。 -2 不显示只在第 2 个文件里出现过的列。 -3 不显示只在第 1 和第 2 个文件里出现过的列。  who who:显示当前所有登陆用户的信息。当没有给出非选项参数时，按以下字段顺序为每个当前用户打印信息：登录用户名称，终端信息，登录时间，远程主机或 X display。\n当用户执行 whoami 时，只显示运行该命令的用户的信息。\n   选项 说明     -a, \u0026ndash;all 等价于调用 \u0026lsquo;-b -d \u0026ndash;login -p -r -t -T -u\u0026rsquo;。   -b, \u0026ndash;boot 上次系统启动的时间。   -d, \u0026ndash;dead 打印 dead 状态的进程。   -H, \u0026ndash;heading 打印列标题行。   -l, \u0026ndash;login 打印系统登录进程。   \u0026ndash;lookup 尝试通过 DNS 规范主机名。   -m 仅显示和标准输入关联的主机名和用户。   -p, \u0026ndash;process 打印由 init 生成的活动进程。   -q, \u0026ndash;count 列出所有已登录的用户的名称和数量。   -r, \u0026ndash;runlevel 打印当前运行级别。   -s, \u0026ndash;short 仅打印名称、行和时间（默认）。   -t, \u0026ndash;time 打印上次系统时钟更改。   -T, -w, \u0026ndash;mesg, \u0026ndash;message, \u0026ndash;writable 将 \u0026lsquo;+、-、?\u0026rsquo; 中的一个作为用户的消息状态添加到用户名称后面。   -u, \u0026ndash;users 列出登录的用户。   \u0026ndash;help 显示帮助信息并退出。   \u0026ndash;version 显示版本信息并退出。    关于 -T 选项的 '+、-、?'： '+'允许写入信息 '-'禁止写入信息 '?'不能查找到终端设备  whoami whoami:命令用于显示自身用户名称。显示自身的用户名称，本指令相当于执行\u0026quot;id -un\u0026quot;指令。\n   选项 说明     -b, \u0026ndash;boot 上次系统启动的时间。   -d, \u0026ndash;dead 打印 dead 状态的进程。   -H, \u0026ndash;heading 打印列标题行。   -l, \u0026ndash;login 打印系统登录进程。   \u0026ndash;lookup 尝试通过 DNS 规范主机名。   -m 仅显示和标准输入关联的主机名和用户。   -p, \u0026ndash;process 打印由 init 生成的活动进程。   -q, \u0026ndash;count 列出所有已登录的用户的名称和数量。   -r, \u0026ndash;runlevel 打印当前运行级别。   -s, \u0026ndash;short 仅打印名称、行和时间（默认）。   -t, \u0026ndash;time 打印上次系统时钟更改。   -T, -w, \u0026ndash;mesg, \u0026ndash;message, \u0026ndash;writable 将 \u0026lsquo;+、-、?\u0026rsquo; 中的一个作为用户的消息状态添加到用户名称后面。   -u, \u0026ndash;users 列出登录的用户。   \u0026ndash;help 显示帮助信息并退出。   \u0026ndash;version 显示版本信息并退出。    关于 -T 选项的 '+、-、?'： '+'允许写入信息 '-'禁止写入信息 '?'不能查找到终端设备  logname logname:打印当前终端登录用户的名称。whoami、logname区别，在正常用户的输出都是一样的，但如果切入超级用户，whoami会输出 root\nw显示目前登入系统的用户信息补充说明 w 命令 用于显示已经登陆系统的用户列表，并显示用户正在执行的指令。执行这个命令可得知目前登入系统的用户有那些人，以及他们正在执行的程序。单独执行 w 命令会显示所有的用户，您也可指定用户名称，仅显示某位用户的相关信息。\nenv、export env:命令 用于显示系统中已存在的环境变量，以及在定义的环境中执行指令。该命令只使用\u0026quot;-\u0026ldquo;作为参数选项时，隐藏了选项\u0026rdquo;-i\u0026quot;的功能。若没有设置任何选项和参数时，则直接显示当前的环境变量。如果使用 env 命令在新环境中执行指令时，会因为没有定义环境变量\u0026quot;PATH\u0026quot;而提示错误信息\u0026quot;such file or directory\u0026quot;。此时，用户可以重新定义一个新的\u0026quot;PATH\u0026quot;或者使用绝对路径。\nexport:为 shell 变量或函数设置导出属性。\n概要:\nexport [-fn] [name[=word]]... export -p  主要用途:定义一到多个变量并设置导出属性。 修改一到多个变量的值并设置导出属性。删除一到多个变量的导出属性。显示全部拥有导出属性的变量。为一到多个已定义函数新增导出属性。删除一到多个函数的导出属性。显示全部拥有导出属性的函数\n| 选项 | 说明 | |-f|指向函数。| |-n|删除变量的导出属性。| |-p|显示全部拥有导出属性的变量。| |-pf|显示全部拥有导出属性的函数。| |-nf|删除函数的导出属性。| |\u0026ndash;|在它之后的选项无效|\nfree free:可以显示当前系统未使用的和已使用的内存数目，还可以显示被内核使用的内存缓冲区。\n   选项 说明     -b 以 Byte 为单位显示内存使用情况；   -k 以 KB 为单位显示内存使用情况；   -m 以 MB 为单位显示内存使用情况；   -g 以 GB 为单位显示内存使用情况。   -o 不显示缓冲区调节列；   -s\u0026lt;间隔秒数\u0026gt; 持续观察内存使用状况；   -t 显示内存总和列；   -V 显示版本信息。    clear clear:清除当前屏幕终端上的任何信息\nhalt 、poweroff、shutdown、reboot、init halt:关闭正在运行的 Linux 操作系统,用来关闭正在运行的 Linux 操作系统。halt 命令会先检测系统的 runlevel，若 runlevel 为 0 或 6，则关闭系统，否则即调用 shutdown 来关闭系统。\n   选项 说明     -d 不要在 wtmp 中记录；   -f 不论目前的 runlevel 为何，不调用 shutdown 即强制关闭系统；   -i 在 halt 之前，关闭全部的网络界面；   -n halt 前，不用先执行 sync；   -p halt 之后，执行 poweroff；   -w 仅在 wtmp 中记录，而不实际结束系统。    poweroff:关闭 Linux 系统，关闭记录会被写入到/var/log/wtmp 日志文件中\n   选项 说明     -n 关闭之前不同步   -p 当被称为 halt 时关闭电源   -v 增加输出，包括消息   -q 降低输出错误唯一的消息   -w 并不实际关闭系统，只是写入/var/log/wtmp 文件中   -f 强制关机，不调用 shutdown    shhutdown:命令 用来系统关机命令。shutdown 指令可以关闭所有程序，并依用户的需要，进行重新开机或关机的动作。\n语法：shutdown(选项)(参数)\n   选项 说明     -c 当执行“shutdown -h 11:50”指令时，只要按+键就可以中断关机的指令；   -f 重新启动时不执行 fsck；   -F 重新启动时执行 fsck；   -h 将系统关机；   -k 只是送出信息给所有用户，但不会实际关机；   -n 不调用 init 程序进行关机，而由 shutdown 自己进行；   -r shutdown 之后重新启动；   -t\u0026lt;秒数\u0026gt; 送出警告信息和删除信息之间要延迟多少秒。    参数: [时间]：设置多久时间后执行 shutdown 指令； [警告信息]：要传送给所有登入用户的信息。\nreboot:重新启动正在运行的 Linux 操作系统\n   选项 说明     -d 重新开机时不把数据写入记录文件/var/tmp/wtmp。本参数具有“-n”参数效果；   -f 强制重新开机，不调用 shutdown 指令的功能；   -i 在重开机之前，先关闭所有网络界面；   -n 重开机之前不检查是否有未结束的程序；   -w 仅做测试，并不真正将系统重新开机，只会把重开机的数据写入/var/log 目录下的 wtmp 记录文件。    init:命令 是 Linux 下的进程初始化工具，init 进程是所有 Linux 进程的父进程，它的进程号为 1。init 命令是 Linux 操作系统中不可缺少的程序之一，init 进程是 Linux 内核引导运行的，是系统中的第一个进程。\n语法：init(选项)(参数)\n   选项 说明     -b 不执行相关脚本而直接进入单用户模式；   -s 切换到单用户模式。    参数:运行等级：指定 Linux 系统要切换到的运行等级什么是运行级呢？简单的说，运行级就是操作系统当前正在运行的功能级别。这个级别从 0 到 6 ，具有不同的功能。你也可以在/etc/inittab 中查看它的英文介绍。\n   级别 说明     #0 停机（千万不能把 initdefault 设置为 0）   #1 单用户模式   #2 多用户，没有 NFS(和级别 3 相似，会停止部分服务)   #3 完全多用户模式   #4 没有用到   #5 x11(Xwindow)   #6 重新启动（千万不要把 initdefault 设置为 6）    init 0可以实现关机，也就是调用系统的 0 级别，init 6可以实现重启，也就是调用系统的 6 级别.\n 在早期的 Linux 系统中，应该尽量使用 shutdown 命令来进行关机和重启。因为在那时的 Linux 中，只有 shutdown 命令在关机或重启之前会正确地中止进程及服务，所以我们一直认为 shutdown 才是最安全的关机与重启命令。 而在现在的系统中，一些其他的命令（如 reboot）也会正确地中止进程及服务，但我们仍建议使用 shutdown 命令来进行关机和重启。 在现在的系统中，reboot 命令也是安全的，而且不需要加入过多的选项。\n tr tr:命令 可以对来自标准输入的字符进行替换、压缩和删除。它可以将一组字符变成另一组字符，经常用来编写优美的单行命令，作用很强大。\n   选项 说明     -c ——complerment 取代所有不属于第一字符集的字符；   -d ——delete 删除所有属于第一字符集的字符；   -s \u0026ndash;squeeze-repeats 把连续重复的字符以单独一个字符表示；   -t \u0026ndash;truncate-set1 先删除第一字符集较第二字符集多出的字符。    语法:tr(选项)(参数)\n参数:\n字符集 1：指定要转换或删除的原字符集。当执行转换操作时，必须使用参数“字符集2”指定转换的目标字符集。但执行删除操作时，不需要参数“字符集 2”； 字符集 2：指定要转换成的目标字符集。\n例子：\necho \u0026quot;HELLO WORLD\u0026quot; | tr 'A-Z' 'a-z'# 将输入字符由大写转换为小写 echo \u0026quot;HELLOWORLD\u0026quot; | tr -s ' '# 将空格压缩为一个  tr 可以使用的字符类：\n[:alnum:]：字母和数字 [:alpha:]：字母 [:cntrl:]：控制（非打印）字符 [:digit:]：数字 [:graph:]：图形字符 [:lower:]：小写字母 [:print:]：可打印字符 [:punct:]：标点符号 [:space:]：空白字符 [:upper:]：大写字母 [:xdigit:]：十六进制字符  使用方式：tr \u0026lsquo;[:lower:]\u0026rsquo; \u0026lsquo;[:upper:]\u0026rsquo;\ncol col:命令 是一个标准输入文本过滤器，它从标注输入设备读取文本内容，并把内容显示到标注输出设备。在许多 UNIX 说明文件里，都有 RLF 控制字符。当我们运用 shell 特殊字符\u0026gt;和\u0026raquo;，把说明文件的内容输出成纯文本文件时，控制字符会变成乱码，col 命令则能有效滤除这些控制字符。RLF 字符(reverse line feed)是反向换行符，HRLF 字符（half-reverse line feed）是半反向换行符。\n   选项 说明     -b, \u0026ndash;no-backspaces 不输出任何退格符，只打印写入每个列位置的最后一个字符   -f, \u0026ndash;fine 允许正向半换行符（half-forward line feeds）。通常，处于半行分界线上的字符打印在下一行   -h, \u0026ndash;tabs 将多个空格转换为 Tab，一般 4 个 空格转为 1 个 Tab   -l, \u0026ndash;lines NUMBER 设置缓冲行为 NUMBER，默认为 128   -p, \u0026ndash;pass 不转换未识别的控制符   -x, \u0026ndash;spaces 将 Tab 转为多个空格，一般 1 一个 Tab 转为 4 个空格   -H, \u0026ndash;help 显示帮助信息并退出   -V, \u0026ndash;version 显示版本信息并退出    例子:\necho -e \u0026quot;123\\t456\u0026quot; | col -x# 将 Tab 替换为空格，一般 1 个 Tab 转为 4 个空格。 echo -e \u0026quot;123 456\u0026quot; | col -h# 将空格替换为 Tab，一般 4 个 空格转为 1 个 Tab。 man col | col -b \u0026gt; newFile# 将帮助文档内的控制符删除。以 col 命令的 manual 为例。  join join:用来将两个文件中，制定栏位内容相同的行连接起来。找出两个文件中，指定栏位内容相同的行，并加以合并，再输出到标准输出设备。\n语法：join(选项)(参数)\n   选项 说明     -a\u0026lt;1 或 2\u0026gt; 除了显示原来的输出内容之外，还显示指令文件中没有相同栏位的行；   -e\u0026lt;字符串\u0026gt; 若[文件 1]与[文件 2]中找不到指定的栏位，则在输出中填入选项中的字符串；   -i 或\u0026ndash;ignore-case 比较栏位内容时，忽略大小写的差异；   -j FIELD 等同于 -1 FIELD -2 FIELD,-j 指定一个域作为匹配字段   -o\u0026lt;格式\u0026gt; 按照指定的格式来显示结果；   -t\u0026lt;字符\u0026gt; 使用栏位的分割字符；   -v\u0026lt;1 或 2\u0026gt; 更-a 相同，但是只显示文件中没有相同栏位的行；   -1\u0026lt;栏位\u0026gt; 连接[文件 1]指定的栏位；   -2\u0026lt;栏位\u0026gt; 连接[文件 2]指定的栏位。    参数：文件 1：要进行合并操作的第 1 个文件参数；文件 2：要进行合并操作的第 2 个文件参数。\n例子：\ncat 1.txt aa 1 2 bb 2 3 cc 4 6 dd 3 3cat 2.txt aa 2 1 bb 8 2 ff 2 4 cc 4 4 dd 5 5join 1.txt 2.txt aa 1 2 2 1 bb 2 3 8 2 join: 2:4: is not sorted: cc 4 4 join: input is not in sorted order 具有相同顺序列首的行被组合在一起了，而第三行两个文件的不同，下面的就不会拼接在一起了join -1 2 -2 3 file1.txt file2.txt#以第一个文件的第二列和第二个文件的第三列做匹配字  cut cut:用来显示行中的指定部分，删除文件中指定字段。cut 经常用来显示文件的内容，类似于 type 命令。\n语法：cut（选项）（参数）\n   选项 说明     -b 仅显示行中指定直接范围的内容；   -c 仅显示行中指定范围的字符；   -d 指定字段的分隔符，默认的字段分隔符为“TAB”；   -f 显示指定字段的内容；   -n 与“-b”选项连用，不分割多字节字符；   \u0026ndash;complement 补足被选择的字节、字符或字段；   \u0026ndash;out-delimiter= 字段分隔符 指定输出内容是的字段分割符；   \u0026ndash;help 显示指令的帮助信息；   \u0026ndash;version 显示指令的版本信息。    例子：\ncat 1.txt aa 1 2 bb 2 3 cc 4 6 dd 3 3cut -d ' ' -f 1 1.txt # 以空格为分割符提取第一列 aa bb cc dd如果文件的分隔符为tab则可以省略-d  exit exit:执行 exit 可使 shell 以指定的状态值退出。若不设置参数，则以最后一条命令的返回值作为 exit 的返回值退出。\ntype type:显示指定命令的类型。\n语法：type [-afptP] name [name ...]\n主要用途： 显示要查找的命令的信息。 控制查找范围和行为。 显示要查找的命令优先级最高的类型。\n   选项 说明     -a 在环境变量 PATH 中查找并显示所有包含 name 的可执行文件路径；当\u0026rsquo;-p\u0026rsquo;选项没有同时给出时，如果在别名、关键字，函数，内建的信息中存在 name，则一并显示。   -f 排除对 shell 函数的查找。   -p 如果 name 在执行\u0026rsquo;type -t name\u0026rsquo;返回的不是\u0026rsquo;file\u0026rsquo;，那么什么也不返回；否则会在环境变量 PATH 中查找并返回可执行文件路径。   -P 即使要查找的 name 是别名、内建、函数中的一个，仍然会在环境变量 PATH 中查找并返回可执行文件路径。   -t 根据 name 的类型返回一个单词（别名，关键字，函数，内建，文件），否则返回空值。    参数：name：要查找的命令，可以为多个。\npaste paste:命令用于合并文件的列。会把每个文件以列对列的方式，一列列地加以合并。\n语法:paste [-s][-d \u0026lt;间隔字符\u0026gt;][--help][--version][文件...]\n   选项 说明     -d\u0026lt;间隔字符\u0026gt;或\u0026ndash;delimiters=\u0026lt;间隔字符\u0026gt; 用指定的间隔字符取代跳格字符。   -s 或\u0026ndash;serial 串列进行而非平行处理。   \u0026ndash;help 在线帮助。   \u0026ndash;version 显示帮助信息。   [文件…] 指定操作的文件路径    与join的区别：会无差别的把两文件的第一行合并在一起，而不会根据栏位相同选择合并。\n例子：\ncat 1.txt aa 1 2 bb 2 3 cc 4 6 dd 3 3cat 2.txt aa 2 1 bb 8 2 ff 2 4 cc 4 4 dd 5 5paste 1.txt 2.txt aa 1 2\taa 2 1 bb 2 3\tbb 8 2 cc 4 6\tff 2 4 dd 3 3\tcc 4 4dd 5 5  expand、unexpand expand:命令 用于将文件的制表符（TAB）转换为空白字符（space），将结果显示到标准输出设备。\n语法:expand(选项)(参数)选项:-t\u0026lt;数字\u0026gt;：指定制表符所代表的空白字符的个数，而不使用默认的 8。参数:文件,指定要转换制表符为空白的文件unexpand:命令 用于将给定文件中的空白字符（space）转换为制表符（TAB），并把转换结果显示在标准输出设备（显示终端）。\n语法:unexpand(选项)(参数)\n   选项 说明     -a 或\u0026ndash;all 转换文件中所有的空白字符；   \u0026ndash;first-only 仅转换开头的空白字符；   -t 指定 TAB 所代表的 N 个（N 为整数）字符数，默认 N 值是 8。    参数:文件,指定要转换空白为 TAB 的文件列表。\nbasename 、dirname basename:显示指定路径除了文件名。\n例子：\npwd# /opt/go/bin/ basename /opt/go/bin/go# /opt/go/bin basename /opt/go/bin/# /opt/go basename /opt/go/bin# /opt/go basename go# go  dirname:显示指定路径除了文件名之外的路径前缀。\n例子：\npwd # /opt/go/bin/ dirname /opt/go/bin/go# /opt/go/bin dirname /opt/go/bin/# /opt/go dirname /opt/go/bin# /opt/go dirname go# .  hostname、hostnamectl hostname：命令 用于显示和设置系统的主机名称。在使用 hostname 命令设置主机名后，系统并不会永久保存新的主机名，重启之后还是原来的主机名。\n如果需要永久修改主机名，需要修改/etc/hosts 和 /etc/sysconfig/network 的相关内容并进行重启；也可以使用 hostnamectl 命令进行永久修改。\nhostnamectl：可用于查询和更改系统主机名和相关设置。\n语法：hostnamectl [选项...] 指令 ...\n   指令 说明     status 显示当前主机名设置   set-hostname NAME 设置系统主机名   set-icon-name NAME 设置主机的图标名称   set-chassis NAME 设置主机的机箱类型   set-deployment NAME 设置主机的部署环境   set-location NAME 设置主机位置    lsattr lsattr:命令 用于查看文件的第二扩展文件系统属性。\n语法：lsattr(选项)(参数)\n   选项 说明     -E 可显示设备属性的当前值，但这个当前值是从用户设备数据库中获得的，而不是从设备直接获得的。   -D 显示属性的名称，属性的默认值，描述和用户是否可以修改属性值的标志。   -R 递归的操作方式；   -V 显示指令的版本信息；   -a 列出目录中的所有文件，包括隐藏文件。    lsattr 经常使用的几个选项-D，-E，-R 这三个选项不可以一起使用，它们是互斥的，经常使用的还有-l,-H，使用 lsattr 时，必须指出具体的设备名，用-l 选项指出要显示设备的逻辑名称，否则要用-c，-s，-t 等选项唯一的确定某个已存在的设备。|参数:文件：指定显示文件系统属性的文件名。\nchattr chattr:命令 用来改变文件属性。这项指令可改变存放在 ext2 文件系统上的文件或目录属性，这些属性共有以下 8 种模式：\n语法:chattr(选项)\n   选项 说明     a 让文件或目录仅供附加用途；   b 不更新文件或目录的最后存取时间；   c 将文件或目录压缩后存放；   d 将文件或目录排除在倾倒操作之外；   i 不得任意更动文件或目录；   s 保密性删除文件或目录；   S 即时更新文件或目录；   u 预防意外删除。   -R 递归处理，将指令目录下的所有文件及子目录一并处理   -v\u0026lt;版本编号\u0026gt; 设置文件或目录版本；   -V 显示指令执行过程；   +\u0026lt;属性\u0026gt; 开启文件或目录的该项属性；   -\u0026lt;属性\u0026gt; 关闭文件或目录的该项属性；   =\u0026lt;属性\u0026gt; 指定文件或目录的该项属性。    cal、date、timedatectl、hwclock cal：显示当前日历或指定日期的日历cal 命令 用于显示当前日历，或者指定日期的日历，如果没有指定参数，则显示当前月份。 一个单一的参数指定要显示的年份 (1 - 9999) ; 注意年份必须被完全地指定: cal 89 不会 显示 1989 年的日历. 两个参数表示月份 (1 - 12) 和年份. 如果没有指定参数, 则显示当前月份的日历. 一年从 Jan 1 (1 月 1 日) 开始.\n语法:cal [ -mjy ] [ 月份 ] [ 年份 ]\n   选项 说明     -l 显示单月输出；   -3 显示临近三个月的日历；   -s 将星期日作为月的第一天；   -m 显示星期一作为一周的第一天.. (缺省为星期日.)   -j 显示儒略历的(Julian)日期 (以 1 为基的天数, 从 1 月 1 日开始计数) .   -y 显示当前年份的日历    参数:月：指定月份；年：指定年份。\ndate：显示或设置系统时间与日期\n概要:\ndate [OPTION]... [+FORMAT] date [-u|--utc|--universal] [MMDDhhmm[[CC]YY][.ss]]  主要用途: 转换时间到选定的格式，默认为当前。 设置系统时间。参数:format：输出的时间格式。format 可用的转义序列如下：\n%%百分号 %a当地缩写的工作日名称（例如，Sun） %A当地完整的工作日名称（例如，Sunday） %b当地缩写的月份名称（例如，Jan） %B当地完整的月份名称（例如，January） %c当地的日期和时间（例如，Thu Mar3 23:05:25 2005） %C世纪，和%Y类似，但是省略后两位（例如，20） %d一月中的一天（例如，01） %D日期，等价于%m/%d/%y %e一月中的一天，格式使用空格填充，等价于%_d %F完整的日期；等价于%+4Y-%m-%d %gISO标准计数周的年份的最后两位数字 %GISO标准计数周的年份，通常只对%V有用 %h等价于%b %H小时，范围（00..23） %I小时，范围（00..23） %j一年中的一天，范围（001..366） %k小时，使用空格填充，范围（0..23），等价于%_H %l小时，使用空格填充，范围（1..12），等价于%_I %m月，范围（01..12） %M分钟，范围（00..59） %n换行符 %N纳秒，范围（000000000..000000000） %p用于表示当地的AM或PM，如果未知则为空白 %P类似于%p，但用小写表示 %q季度，范围（1..4） %r当地以12小时表示的时钟时间（例如，11:11:04 PM） %R24小时每分钟；等价于%H:%M %s自协调世界时1970年01月01日00时00分以来的秒数 %S秒数，范围（00..60） %t水平制表符 %T时间；等价于%H:%M:%S %u一周中的一天（1..7），1代表星期一 %U一年中的第几周，周日作为一周的起始（00..53） %VISO标准计数周，该方法将周一作为一周的起始（01..53） %w一周中的一天（0..6），0代表星期天 %W一年中的第几周，周一作为一周的起始（00..53） %x当地的日期表示（例如，12/31/99） %X当地的时间表示（例如，23:13:48） %y年份后两位数字，范围（00..99） %Y年份 %z+hhmm格式的数值化时区格式（例如，-0400） %:z+hh:mm格式的数值化时区格式（例如，-04:00） %::z +hh:mm:ss格式的数值化时区格式（例如，-04:00:00） %:::z数值化时区格式，相比上一个格式增加':'以显示必要的精度（例如，-04，+05:30） %Z时区缩写（如EDT）  默认情况下，日期用零填充数字字段；以下可选的符号可以跟在\u0026rsquo;%\u0026lsquo;后面:\n-(连字符) 不要填充相应的字段。 _(下划线) 使用空格填充相应的字段。 0(数字0) 使用数字0填充相应的字段。 +用数字0填充，未来年份大于4位数字则在前面加上'+'号。 ^允许的情况下使用大写。 #允许的情况下将默认的大写转换为小写，默认的小写转换为大写。  在任何标志之后都有一个可选的字段宽度，如小数；然后是一个可选的修饰符，在可用的情况下，使用 E 来使用当地语言环境的替代表示， 使用 O 来使用当地语言环境的替代数字符号。\n   选项 说明     -d, \u0026ndash;date=STRING 解析字符串并按照指定格式输出，字符串不能是\u0026rsquo;now\u0026rsquo;。   \u0026ndash;debug 注释已解析的日期，并将有疑问的用法发送到标准错误。   -f, \u0026ndash;file=DATEFILE 类似于\u0026ndash;date; 一次从 DATEFILE 处理一行。   -I[FMT], \u0026ndash;iso-8601[=FMT] 按照 ISO 8601 格式输出，FMT 可以为\u0026rsquo;date\u0026rsquo;(默认)，\u0026lsquo;hours\u0026rsquo;，\u0026lsquo;minutes\u0026rsquo;，\u0026lsquo;seconds\u0026rsquo;，\u0026rsquo;ns\u0026rsquo;。 例如：2006-08-14T02:34:56-06:00   -R, \u0026ndash;rfc-email 按照 RFC 5322 格式输出，例如: Mon, 14 Aug 2006 02:34:56 -0600   \u0026ndash;rfc-3339=FMT 按照 RFC 3339 格式输出，FMT 可以为\u0026rsquo;date\u0026rsquo;, \u0026lsquo;seconds\u0026rsquo;,\u0026rsquo;ns\u0026rsquo;中的一个， 例如：2006-08-14 02:34:56-06:00   -r, \u0026ndash;reference=FILE 显示文件的上次修改时间。   -s, \u0026ndash;set=STRING 根据字符串设置系统时间。   -u, \u0026ndash;utc, \u0026ndash;universal 显示或设置世界协调时(UTC)。   \u0026ndash;help 显示帮助信息并退出。   \u0026ndash;version 显示版本信息并退出。    timedatectl：用于在 Linux 中设置或查询系统时间、日期和时区等配置。在 Linux 运维中，通常使用此命令来设置或更改当前的日期、时间和时区，或启用自动系统时钟与远程 NTP 服务器同步，以确保 Linux 系统始终保持正确的时间。\n概要:timedatectl [OPTIONS...] COMMAND ...\n主要用途:转换时间到选定的格式，默认为当前。设置系统时间。\n   选项 说明     -h \u0026ndash;help 显示帮助信息。   \u0026ndash;version 显示软件包版本。   \u0026ndash;no-pager 不用将输出通过管道传输到寻呼机（pager）。   \u0026ndash;no-ask-password 不提示输入密码。   -H \u0026ndash;host=[USER@]HOST 在远程主机上操作   -M \u0026ndash;machine=CONTAINER 在本地容器上操作。   \u0026ndash;adjust-system-clock 更改本地 RTC 模式时调整系统时钟。    Commands:status 显示当前的时间设置。set-time TIME设置系统时间。set-timezone ZONE设置系统时区。list-timezones显示已知时区。set-local-rtc BOOL 控制 RTC 是否在当地时间。（BOOL 的值可以是 1 / true 或 0 / false）set-ntp BOOL 启用或禁用网络时间同步。（BOOL 的值可以是 1 / true 或 0 / false）timesync-status 显示 systemd-timesyncd 的状态。show-timesync显示 systemd-timesyncd 的属性。  hwclock:是一个硬件时钟访问工具，它可以显示当前时间、设置硬件时钟的时间和设置硬件时钟为系统时间，也可设置系统时间为硬件时钟的时间。 在 Linux 中有硬件时钟与系统时钟等两种时钟。硬件时钟是指主机板上的时钟设备，也就是通常可在 BIOS 画面设定的时钟。系统时钟则是指 kernel 中的时钟。当 Linux 启动时，系统时钟会去读取硬件时钟的设定，之后系统时钟即独立运作。所有 Linux 相关指令与函数都是读取系统时钟的设定。\n语法:hwclock(选项)\n   选项 说明     \u0026ndash;adjust hwclock 每次更改硬件时钟时，都会记录在/etc/adjtime 文件中。使用\u0026ndash;adjust 参数，可使 hwclock 根据先前的记录来估算硬件时钟的偏差，并用来校正目前的硬件时钟；   \u0026ndash;debug 显示 hwclock 执行时详细的信息；   \u0026ndash;directisa hwclock 预设从/dev/rtc 设备来存取硬件时钟。若无法存取时，可用此参数直接以 I/O 指令来存取硬件时钟；   \u0026ndash;hctosys 将系统时钟调整为与目前的硬件时钟一致；   \u0026ndash;set \u0026ndash;date=\u0026lt;日期与时间\u0026gt; 设定硬件时钟；   \u0026ndash;show 显示硬件时钟的时间与日期；   \u0026ndash;systohc 将硬件时钟调整为与目前的系统时钟一致；   \u0026ndash;test 仅测试程序，而不会实际更改硬件时钟；   \u0026ndash;utc 若要使用格林威治时间，请加入此参数，hwclock 会执行转换的工作；   \u0026ndash;version 显示版本信息。    split split：可以将一个大文件按大小分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。\n   选项 说明     -b 值为每一输出档案的大小，单位为 byte。   -C 每一输出档中，单行的最大 byte 数。   -d 使用数字作为后缀。   -l 值为每一输出档的行数大小。   -a 指定后缀长度(默认为 2)。    csplit csplit:命令 用于将一个大文件以模式分割成小的碎片（例如时间段），并且将分割后的每个碎片保存成一个文件。碎片文件的命名类似“xx00”，“xx01”。csplit 命令是 split 的一个变体，split 只能够根据文件大小或行数来分割，但 csplit 能够根据文件本身特点来分割文件。\n语法:csplit(选项)(参数)\n   选项 说明     -b\u0026lt;输出格式\u0026gt;或\u0026ndash;suffix-format=\u0026lt;输出格式\u0026gt; 预设的输出格式其文件名称为 xx00，xx01 等，用户可以通过改变\u0026lt;输出格式\u0026gt;来改变输出的文件名；   -f\u0026lt;输出字首字符串\u0026gt;或\u0026ndash;prefix=\u0026lt;输出字首字符串\u0026gt; 预设的输出字首字符串其文件名为 xx00，xx01 等，如果制定输出字首字符串为“hello”，则输出的文件名称会变成 hello00，hello、01\u0026hellip;\u0026hellip;   -k 或\u0026ndash;keep-files 保留文件，就算发生错误或中断执行，与不能删除已经输出保存的文件；   -n\u0026lt;输出文件名位数\u0026gt;或\u0026ndash;digits=\u0026lt;输出文件名位数\u0026gt; 预设的输出文件名位数其文件名称为 xx00，xx01\u0026hellip;\u0026hellip;如果用户指定输出文件名位数为“3”，则输出的文件名称会变成 xx000，xx001 等；   -q 或-s 或\u0026ndash;quiet 或——silent 不显示指令执行过程；   -z 或\u0026ndash;elide-empty-files 删除长度为 0 Byte 文件。    参数: 文件：指定要分割的原文件； 模式：指定要分割文件时的匹配模式。\n命令详细说明：\n/[正则表达式]/#匹配文本样式，比如/SERVER/，从第一行到包含SERVER的匹配行。 {*}#表示根据匹配重复执行分割，直到文件尾停止，使用{整数}的形式指定分割执行的次数。 -s#静默模式，不打印其他信息。 -n#指定分割后的文件名后缀的数字个数。比如01、02、03等。 -f#指定分割后的文件名前缀。 -b#指定后缀格式。比如%02d.log，类似于C语言中的printf参数格式。 rm server00.log #是删除第一个文件，因为分割后的的第一个文件没有内容，匹配的单词就位于文件的第一行中。  uname uname:印机器和操作系统的信息。 当没有选项时，默认启用 -s 选项。 如果给出多个选项或 -a 选项时，输出信息按以下字段排序：内核名称 主机名称 内核 release 内核版本 机器名称 处理器 硬件平台 操作系统。\n   选项 说明     -a, \u0026ndash;all 按顺序打印全部信息，如果 -p 和 -i 的信息是未知，那么省略。   -s, \u0026ndash;kernel-name 打印内核名称。   -n, \u0026ndash;nodename 打印网络节点主机名称。   -r, \u0026ndash;kernel-release 打印内核 release。   -v, \u0026ndash;kernel-version 打印内核版本。   -m, \u0026ndash;machine 打印机器名称。   -p, \u0026ndash;processor 打印处理器名称。   -i, \u0026ndash;hardware-platform 打印硬件平台名称。   -o, \u0026ndash;operating-system 打印操作系统名称。   \u0026ndash;help 显示帮助信息并退出。   \u0026ndash;version 显示版本信息并退出。    md5sum md5sum:计算和校验文件报文摘要的工具程序补充说明:md5sum 命令 采用 MD5 报文摘要算法（128 位）计算和检查文件的校验和。一般来说，安装了 Linux 后，就会有 md5sum 这个工具，直接在命令行终端直接运行。 MD5 算法常常被用来验证网络文件传输的完整性，防止文件被人篡改。MD5 全称是报文摘要算法（Message-Digest Algorithm 5），此算法对任意长度的信息逐位进行计算，产生一个二进制长度为 128 位（十六进制长度就是 32 位）的“指纹”（或称“报文摘要”），不同的文件产生相同的报文摘要的可能性是非常非常之小的。\n语法:md5sum(选项)(参数)\n   选项 说明     -b 二进制模式读取文件；   -t 或\u0026ndash;text 把输入的文件作为文本文件看待；   -c 从指定文件中读取 MD5 校验和，并进行校验；   \u0026ndash;status 验证成功时不输出任何信息；   -w 当校验不正确时给出警告信息。    参数:文件：指定保存着文件名和校验和的文本文件。\nuptime uptime:查看 Linux 系统负载信息\necho、printf echo:用于在 shell 中打印 shell 变量的值，或者直接输出指定的字符串。\nprintf:格式化并输出结果。\n语法：printf [-v var] format [arguments]\n选项:-v var：将结果输出到变量 var 中而不是输出到标准输出。\n参数:format：输出格式。arguments：一到多个参数。转义序列：除了支持 printf(1)和 printf(3)的转义序列，内建 printf 还支持以下转义序列：\n%b 展开参数中的反斜杠转义字符。 %q 将参数扩起以用作 shell 输入。 %(fmt)T 根据 strftime(3)中的转义字符来输出日期时间字符串。  返回值:返回状态为成功除非给出了非法选项、写错误、赋值错误。\nread read 命令用于从标准输入读取数值。read 内部命令被用来从标准输入读取单行数据。这个命令可以用来读取键盘输入，当使用重定向的时候，可以读取文件中的一行数据。\n语法:read [-ers] [-a aname] [-d delim] [-i text] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name ...]\n参数说明:-a 后跟一个变量，该变量会被认为是个数组，然后给其赋值，默认是以空格为分割符,如果没有指定变量名，读取的数据将被自动赋值给特定的变量 REPLY-p\n   选项 意义     -d 后面跟一个标志符，其实只有其后的第一个字符有用，作为结束的标志。   -p 后面跟提示信息，即在输入前打印提示信息。   -e 在输入的时候可以使用命令补全功能。   -n 后跟一个数字，定义输入文本的长度，很实用。   -r 屏蔽\\，如果没有该选项，则\\作为一个转义字符，有的话 \\就是个正常的字符了。   -s 安静模式，在输入字符时不再屏幕上显示，例如 login 时输入密码。   -t 后面跟秒数，定义输入字符的等待时间。   -u 后面跟 fd，从文件描述符中读入，该文件描述符可以是 exec 新开启的。     read first last 从标准输入读取输入到第一个空格或者回车，将输入的第一个单词放到变量first中，并将该行其他的输入放在变量last中。read first last 从标准输入读取输入到第一个空格或者回车，将输入的第一个单词放到变量first中，并将该行其他的输入放在变量last中。read 从标准输入读取一行并赋值给特定变量REPLY。read -a arrayname 把单词清单读入arrayname的数组里。read -p \u0026quot;text\u0026quot; 打印提示（text），等待输入，并将输入存储在REPLY中。read -r line 允许输入包含反斜杠。 \u0026lt;!----\u0026gt; 补充一个终端输入密码时候，不让密码显示出来的例子。方法1：#!/bin/bash read -p \u0026quot;输入密码：\u0026quot; -s pwd echo echo password read, is \u0026quot;$pwd\u0026quot; 方法2：#!/bin/bash stty -echo read -p \u0026quot;输入密码：\u0026quot; pwd stty echo echo echo 输入完毕。 其中，选项-echo禁止将输出发送到终端，而选项echo则允许发送输出。  let let 命令是 BASH 中用于计算的工具，用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量。如果表达式中包含了空格或其他特殊字符，则必须引起来。let 命令和双小括号 (( )) 的用法是类似的，它们都是用来对整数进行运算，\nexprexpr 命令是一个手工命令行计数器，用于在 UNIX/LINUX 下求表达式变量的值，一般用于整数值，也可用于字符串。\n语法:expr 表达式\n表达式说明:用空格隔开每个项； 用反斜杠 \\ 放在 shell 特定的字符前面； 对包含空格和其他特殊字符的字符串要用引号括起来\n实例 1、计算字串长度\u0026gt; expr length “this is a test”14 2、抓取字串\u0026gt; expr substr “this is a test” 3 5 3、抓取第一个字符数字串出现的位置\u0026gt; expr index \u0026quot;sarasara\u0026quot;a2 4、整数运算\u0026gt; expr 14 % 95\u0026gt; expr 10 + 1020\u0026gt; expr 1000 + 9001900\u0026gt; expr 30 / 3 / 25\u0026gt; expr 30 \\* 3 (使用乘号时，必须用反斜线屏蔽其特定含义。因为shell可能会误解显示星号的意义)90\u0026gt; expr 30 * 3expr: Syntax error  mount、umount用于挂载 Linux 系统外的文件补充说明 mount 命令 Linux mount 命令是经常会使用到的命令，它用于挂载 Linux 系统外的文件。\n   选项 意义     -V 显示程序版本   -h 显示辅助讯息   -v 显示较讯息，通常和 -f 用来除错。   -a 将 /etc/fstab 中定义的所有档案系统挂上。   -F 这个命令通常和 -a 一起使用，它会为每一个 mount 的动作产生一个行程负责执行。在系统需要挂上大量 NFS   -f 通常用在除错的用途。它会使 mount 并不执行实际挂上的动作，而是模拟整个挂上的过程。通常会和 -v 一起使用。   -n 一般而言，mount 在挂上后会在 /etc/mtab   -s-r 等于 -o ro   -w 等于 -o rw   -L 将含有特定标签的硬盘分割挂上。   -U 将档案分割序号为 的档案系统挂下。-L 和 -U 必须在/proc/partition 这种档案存在时才有意义。   -t 指定档案系统的型态，通常不必指定。mount 会自动选择正确的型态。   -o async 打开非同步模式，所有的档案读写动作都会用非同步模式执行。   -o sync 在同步模式下执行。   -o atime、-o noatime 当 atime 打开时，系统会在每次读取档案时更新档案的『上一次调用时间』。当我们使用 flash   -o auto、-o noauto 打开/关闭自动挂上模式。   -o defaults 使用预设的选项 rw, suid, dev, exec, auto, nouser, and async.   -o dev、-o nodev-o exec、-o noexec 允许执行档被执行。   -o suid、-o nosuid 允许执行档在 root 权限下执行。   -o user、-o nouser 使用者可以执行 mount/umount 的动作。   -o remount 将一个已经挂下的档案系统重新用不同的方式挂上。例如原先是唯读的系统，现在用可读写的模式重新挂上。   -o ro 用唯读模式挂上。   -o rw 用可读写模式挂上。   -o loop= 使用 loop 模式用来将一个档案当成硬盘分割挂上系统。    umount命令 用于卸载已经加载的文件系统。利用设备名或挂载点都能 umount 文件系统，不过最好还是通过挂载点卸载，以免使用绑定挂载（一个设备，多个挂载点）时产生混乱。\n语法:umount(选项)(参数)\n   选项 意义     -a 卸除/etc/mtab 中记录的所有文件系统；   -h 显示帮助；   -n 卸除时不要将信息存入/etc/mtab 文件中；   -r 若无法成功卸除，则尝试以只读的方式重新挂入文件系统；   -t\u0026lt;文件系统类型\u0026gt; 仅卸除选项中所指定的文件系统；   -v 执行时显示详细的信息；   -V 显示版本信息。    参数:文件系统：指定要卸载的文件系统或者其对应的设备文件名。\nchown chmod chmod（英文全拼：change mode）命令是控制用户对文件的权限的命令\nLinux/Unix 的文件调用权限分为三级 : 文件所有者（Owner）、用户组（Group）、其它用户（Other Users）。\n语法：chmod [-cfvR] [--help] [--version] mode file...\n参数说明:mode : 权限设定字串，格式如下 :[ugoa...][[+-=][rwxX]...][,...] 其中：u 表示该文件的拥有者，g 表示与该文件的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。 + 表示增加权限、- 表示取消权限、= 表示唯一设定权限。 r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该文件是个子目录或者该文件已经被设定过为可执行。 其他参数说明： -c : 若该文件权限确实已经更改，才显示其更改动作 -f : 若该文件权限无法被更改也不要显示错误讯息 -v : 显示权限变更的详细资料 -R : 对目前目录下的所有文件与子目录进行相同的权限变更(即以递归的方式逐个变更) --help : 显示辅助说明 --version : 显示版本  符号模式 使用符号模式可以设置多个项目：who（用户类型），operator（操作符）和 permission（权限），每个项目的设置可以用逗号隔开。 命令 chmod 将修改 who 指定的用户类型对文件的访问权限，用户类型由一个或者多个字母在 who 的位置来说明，如 who 的符号模式表所示:\n   who 用户类型 说明     u user 文件所有者   g group 文件所有者所在组   o others 所有其他用户   a all 所用用户, 相当于 ugo    operator 的符号模式表:\n   Operator 说明     + 为指定的用户类型增加权限   - 去除指定用户类型的权限   = 设置指定用户权限的设置，即将用户类型的所有权限重新设置    permission 的符号模式表:\n   模式 名字 说明     r 读 设置为可读权限   w 写 设置为可写权限   x 执行权限 设置为可执行权限   X 特殊执行权限 只有当文件为目录文件，或者其他类型的用户有可执行权限时，才将文件权限设置可执行   s setuid/gid 当文件被执行时，根据who参数指定的用户类型设置文件的setuid或者setgid权限   t 粘贴位 设置粘贴位，只有超级用户可以设置该位，只有文件所有者u可以使用该位    八进制语法 chmod命令可以使用八进制数来指定权限。文件或目录的权限位是由9个权限位来控制，每三位为一组，它们分别是文件所有者（User）的读、写、执行，用户组（Group）的读、写、执行以及其它用户（Other）的读、写、执行。历史上，文件权限被放在一个比特掩码中，掩码中指定的比特位设为1，用来说明一个类具有相应的优先级。\n   # 权限 rwx 二进制     7 读 + 写 + 执行 rwx 111   6 读 + 写 rw- 110   5 读 + 执行 r-x 101   4 只读 r\u0026ndash; 100   3 写 + 执行 -wx 011   2 只写 -w- 010   1 只执行 \u0026ndash;x 001   0 无 \u0026mdash; 000    例如\n765 将这样解释：所有者的权限用数字表达：属主的那三个权限位的数字加起来的总和。如 rwx ，也就是 4+2+1 ，应该是 7。 用户组的权限用数字表达：属组的那个权限位数字的相加的总和。如 rw- ，也就是 4+2+0 ，应该是 6。 其它用户的权限数字表达：其它用户权限位的数字相加的总和。如 r-x ，也就是 4+0+1 ，应该是 5。  chgrp ping ping命令 用来测试主机之间网络的连通性。执行 ping 指令会使用 ICMP 传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。\n语法：ping(选项)(参数)\n   选项 参数     -d 使用Socket的SO_DEBUG功能；   -c\u0026lt;完成次数\u0026gt; 设置完成要求回应的次数；   -f 极限检测；   -i\u0026lt;间隔秒数\u0026gt; 指定收发信息的间隔时间；   -I\u0026lt;网络界面\u0026gt; 使用指定的网络界面送出数据包；   -l\u0026lt;前置载入\u0026gt; 设置在送出要求信息之前，先行发出的数据包；   -n 只输出数值；   -p\u0026lt;范本样式\u0026gt; 设置填满数据包的范本样式；   -q 不显示指令执行过程，开头和结尾的相关信息除外；   -r 忽略普通的Routing Table，直接将数据包送到远端主机上；   -R 记录路由过程；   -s\u0026lt;数据包大小\u0026gt; 设置数据包的大小；   -t\u0026lt;存活数值\u0026gt; 设置存活数值TTL的大小；   -v：详细显示指令的执行过程。     参数：目的主机：指定发送 ICMP 报文的目的主机。\nnetstat netstat 命令 用来打印 Linux 中网络系统的状态信息，可让你得知整个 Linux 系统的网络情况。\n语法:netstat(选项)\n   选项 参数     -a 或\u0026ndash;all 显示所有连线中的 Socket；   -A\u0026lt;网络类型\u0026gt;或\u0026ndash;\u0026lt;网络类型\u0026gt; 列出该网络类型连线中的相关地址；   -c 或\u0026ndash;continuous 持续列出网络状态；   -C 或\u0026ndash;cache 显示路由器配置的快取信息；   -e 或\u0026ndash;extend 显示网络其他相关信息；   -F 或\u0026ndash;fib 显示 FIB；   -g 或\u0026ndash;groups 显示多重广播功能群组组员名单；   -h 或\u0026ndash;help 在线帮助；   -i 或\u0026ndash;interfaces 显示网络界面信息表单；   -l 或\u0026ndash;listening 显示监控中的服务器的 Socket；   -M 或\u0026ndash;masquerade 显示伪装的网络连线；   -n 或\u0026ndash;numeric 直接使用 ip 地址，而不通过域名服务器；   -N 或\u0026ndash;netlink 或\u0026ndash;symbolic 显示网络硬件外围设备的符号连接名称；   -o 或\u0026ndash;timers 显示计时器；   -p 或\u0026ndash;programs 显示正在使用 Socket 的程序识别码和程序名称；   -r 或\u0026ndash;route 显示 Routing Table；   -s 或\u0026ndash;statistice 显示网络工作信息统计表；   -t 或\u0026ndash;tcp 显示 TCP 传输协议的连线状况；   -u 或\u0026ndash;udp 显示 UDP 传输协议的连线状况；   -v 或\u0026ndash;verbose 显示指令执行过程；   -V 或\u0026ndash;version 显示版本信息；   -w 或\u0026ndash;raw 显示 RAW 传输协议的连线状况；   -x 或\u0026ndash;unix 此参数的效果和指定\u0026quot;-A unix\u0026quot;参数相同；   \u0026ndash;ip 或\u0026ndash;inet 此参数的效果和指定\u0026quot;-A inet\u0026quot;参数相同。    ss  比 netstat 好用的 socket 统计信息，iproute2 包附带的另一个工具，允许你查询 socket 的有关统计信息补充说明\nss 命令 用来显示处于活动状态的套接字信息。ss 命令可以用来获取 socket 统计信息，它可以显示和 netstat 类似的内容。\n但 ss 的优势在于它能够显示更多更详细的有关 TCP 和连接状态的信息，而且比 netstat 更快速更高效。当服务器的 socket 连接数量变得非常大时，无论是使用 netstat 命令还是直接 cat /proc/net/tcp，执行速度都会很慢。可能你不会有切身的感受，但请相信我，当服务器维持的连接达到上万个的时候，使用 netstat 等于浪费 生命，而用 ss 才是节省时间。但天下武功唯快不破。ss 快的秘诀在于，它利用到了 TCP 协议栈中 tcp_diag。\ntcp_diag 是一个用于分析统计的模块，可以获得 Linux 内核中第一手的信息，这就确保了 ss 的快捷高效。当然，如果你的系统中没有 tcp_diag，ss 也可以正常运行，只是效率会变得稍慢。\n 语法:\n ss [参数] ss [参数] [过滤]     选项 意义     -h, \u0026ndash;help 帮助信息   -V, \u0026ndash;version 程序版本信息   -n, \u0026ndash;numeric 不解析服务名称   -r, \u0026ndash;resolve 解析主机名   -a, \u0026ndash;all 显示所有套接字（sockets）   -l, \u0026ndash;listening 显示监听状态的套接字（sockets）   -o, \u0026ndash;options 显示计时器信息   -e, \u0026ndash;extended 显示详细的套接字（sockets）信息   -m, \u0026ndash;memory 显示套接字（socket）的内存使用情况   -p, \u0026ndash;processes 显示使用套接字（socket）的进程   -i, \u0026ndash;info 显示 TCP 内部信息   -s, \u0026ndash;summary 显示套接字（socket）使用概况   -4, \u0026ndash;ipv4 仅显示 IPv4 的套接字（sockets）   -6, \u0026ndash;ipv6 仅显示 IPv6 的套接字（sockets）   -0, \u0026ndash;packet 显示 PACKET 套接字（socket）   -t, \u0026ndash;tcp 仅显示 TCP 套接字（sockets）   -u, \u0026ndash;udp 仅显示 UCP 套接字（sockets）   -d, \u0026ndash;dccp 仅显示 DCCP 套接字（sockets）   -w, \u0026ndash;raw 仅显示 RAW 套接字（sockets）   -x, \u0026ndash;unix 仅显示 Unix 套接字（sockets）   -f, \u0026ndash;family=FAMILY 显示 FAMILY 类型的套接字（sockets），FAMILY 可选，支持 unix, inet, inet6, link, netlink   -D, \u0026ndash;diag=FILE 将原始 TCP 套接字（sockets）信息转储到文件   -F, \u0026ndash;filter=FILE 从文件中都去过滤器信息 FILTER := [ state TCP-STATE ] [ EXPRESSION ]    lscpu 显示有关 CPU 架构的信息\n   选项 意义     -a, \u0026ndash;all 打印在线和离线 CPU（默认为-e）   -b, \u0026ndash;online 仅打印在线 CPU（-p 的默认值）   -c, \u0026ndash;offline 打印离线 CPU   -e, \u0026ndash;extended[=] 打印出一个扩展的可读格式   -p, \u0026ndash;parse[=] 打印出可解析的格式   -s, \u0026ndash;sysroot  将指定的目录用作系统根目录   -x, \u0026ndash;hex 打印十六进制掩码，而不是 CPU 列表-h, \u0026ndash;help # 显示此帮助并退出   -V, \u0026ndash;version 输出版本信息并退出    lsmod 显示已载入系统的模块\nlspci 显示当前主机的所有 PCI 总线信息\nuseradd+passwd、adduser useradd 创建的新的系统用户\nuseradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。\n语法：useradd(选项)(参数)\n   选项 意义     -b, \u0026ndash;base-dir BASE_DIR 如果未指定 -d HOME_DIR，则系统的默认基本目录。如果未指定此选项，useradd 将使用 /etc/default/useradd 中的   -c, \u0026ndash;comment COMMENT 加上备注文字。任何文本字符串。它通常是对登录名的简短描述，目前用作用户全名的字段。   -d, \u0026ndash;home HOME_DIR 将使用 HOME_DIR 作为用户登录目录的值来创建新用户。   -D, \u0026ndash;defaults 变更预设值。   -e, \u0026ndash;expiredate EXPIRE_DATE 用户帐户将被禁用的日期。 日期以 YYYY-MM-DD   -f, \u0026ndash;inactive INACTIVE 密码过期后到帐户被永久禁用的天数。   -g, \u0026ndash;gid GROUP 用户初始登录组的组名或编号。组名必须存在。组号必须引用已经存在的组。   -G, \u0026ndash;groups GROUP1[,GROUP2,\u0026hellip;[,GROUPN]]] 用户也是其成员的补充组列表。每个组用逗号隔开，中间没有空格。   -h, \u0026ndash;help 显示帮助信息并退出。   -k, \u0026ndash;skel SKEL_DIR 骨架目录，其中包含要在用户的主目录中复制的文件和目录，当主目录由   -K, \u0026ndash;key KEY=VALUE 覆盖 /etc/login.defs 默认值（UID_MIN、UID_MAX、UMASK、PASS_MAX_DAYS 等）。   -l, \u0026ndash;no-log-init 不要将用户添加到 lastlog 和 faillog 数据库。   -m, \u0026ndash;create-home 如果用户的主目录不存在，则创建它。   -M 不要创建用户的主目录，即使 /etc/login.defs (CREATE_HOME) 中的系统范围设置设置为 yes。   -N, \u0026ndash;no-user-group 不要创建与用户同名的组，而是将用户添加到由 -g 选项或 /etc/default/useradd 中的 GROUP 变量指定的组中。   -o, \u0026ndash;non-unique 允许创建具有重复（非唯一）UID 的用户帐户。 此选项仅在与 -o 选项结合使用时有效。   -p, \u0026ndash;password PASSWORD crypt(3) 返回的加密密码。 默认是禁用密码。   -r, \u0026ndash;system 创建一个系统帐户。   -s, \u0026ndash;shell SHELL 用户登录 shell 的名称。   -u, \u0026ndash;uid UID 用户 ID 的数值。   -U, \u0026ndash;user-group 创建一个与用户同名的组，并将用户添加到该组。   -Z, \u0026ndash;selinux-user SEUSER 用户登录的 SELinux 用户。 默认情况下将此字段留空，这会导致系统选择默认的 SELinux 用户。    参数 用户名：要创建的用户名。\n退出值 useradd 命令以以下值退出：\n   选项 意义     0 成功   1 无法更新密码文件   2 无效的命令语法   3 选项的无效参数   4 UID 已经在使用（并且没有 -o）   6 指定的组不存在   9 用户名已被使用   10 无法更新组文件   12 无法创建主目录   13 无法创建邮件假脱机   14 无法更新 SELinux 用户映射    新建一个系统用户,系统用户一般用于管理服务，无需登录，所以分配nologin，限制其登录系统,简单来说就是不会主动创建/home目录，不能用于登陆。\npasswd:用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。\n语法:passwd(选项)(参数)\n   选项 意义     -d 删除密码，仅有系统管理者才能使用；   -f 强制执行；   -k 设置只有在密码过期失效后，方能更新；   -l 锁住密码；   -s 列出密码的相关信息，仅有系统管理者才能使用；   -u 解开已上锁的帐号。    adduser命令是useradd包装后的程序，能够交互式创建用户。两者区别不大。\nuserdel userdel用于删除给定的用户以及与用户相关的文件.\n语法userdel(选项)(参数)\n   选项 意义     -f 强制删除用户，即使用户当前已登录；   -r 删除用户的同时，删除与用户相关的所有文件。    参数:用户名：要删除的用户名。\nusermod usermod用于修改用户的基本信息.\n语法:usermod(选项)(参数)\n   选项 意义     -c\u0026lt;备注\u0026gt; 修改用户帐号的备注文字；   -d\u0026lt;登入目录\u0026gt; 修改用户登入时的目录，只是修改/etc/passwd中用户的家目录配置信息，不会自动创建新的家目录，通常和-m一起使用；   -m\u0026lt;移动用户家目录\u0026gt; 移动用户家目录到新的位置，不能单独使用，一般与-d一起使用。   -e\u0026lt;有效期限\u0026gt; 修改帐号的有效期限；   -f\u0026lt;缓冲天数\u0026gt; 修改在密码过期后多少天即关闭该帐号；   -g\u0026lt;群组\u0026gt; 修改用户所属的群组；   -G\u0026lt;群组\u0026gt; 修改用户所属的附加群组；   -l\u0026lt;帐号名称\u0026gt; 修改用户帐号名称；   -L 锁定用户密码，使密码无效；   -s 修改用户登入后所使用的shell；   -u 修改用户ID；   -U 解除密码锁定。    参数:登录名：指定要修改信息的用户登录名。\nusers user打印当前主机所有登陆用户的名称。\n每个显示的用户名对应一个登录会话；如果一个用户有不止一个登录会话，那他的用户名将显示相同的次数。\ngroupadd 注：添加用户组;\ngroupdel 注：删除用户组;\ngroupmod 注：修改用户组信息\ngroups 注：显示用户所属的用户组\nchkconfig 检查、设置系统的各种服务。这是Red Hat公司遵循GPL规则所开发的程序，它可查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。谨记chkconfig不是立即自动禁止或激活一个服务，它只是简单的改变了符号连接。\nservice 用来控制系统服务的实用工具，它以启动、停止、重新启动和关闭系统服务，还可以显示所有系统服务的当前状态。\nsystemd systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。\nsystemctl 是系统服务管理器指令，它实际上将 service 和 chkconfig 这两个命令组合到一起。\nhostnamectl 命令用于查看当前主机的信息。\nlocalectl 命令用于查看本地化设置。\ntimedatectl 命令用于查看当前时区设置。\n","date":"2022-12-26","permalink":"/post/linux_command/","tags":["Linux"],"title":"linux_command"},{"content":"kali的一些默认账户密码 https://www.kali.org/docs/introduction/default-credentials/ Kali changed to a non-root user policy by default since the release of 2020.1. This means: During the installation of amd64 and i386 images, it will prompt you for a standard user account to be created. Any default operating system credentials used during Live Boot, or pre-created image (like Virtual Machines \u0026amp; ARM) will be: User: kali Password: kali Vagrant image (based on their policy): Username: vagrant Password: vagrant Amazon EC2: User: kali Password: \u0026lt;ssh key\u0026gt; Default Tool Credentials Some tools shipped with Kali, will use their own default hardcoded credentials (others will generate a new password the first time its used). The following tools have the default values: BeEF-XSS Username: beef Password: beef Configuration File: /etc/beef-xss/config.yaml MySQL User: root Password: (blank) Setup Program: mysql_secure_installation OpenVAS Username: admin Password: \u0026lt;Generated during setup\u0026gt; Setup Program: openvas-setup Metasploit-Framework Username: postgres Password: postgres Configuration File: /usr/share/metasploit-framework/config/database.yml PowerShell-Empire/Starkiller Username: empireadmin Password: password123  ","date":"2022-12-26","permalink":"/post/kali/","tags":["Linux"],"title":"Kali"},{"content":"有兴趣的可以去查下NeoVim项目的创作初衷，实际使用和Vim几乎是一样的，但NeoVim在某些方面是有独到之处的，发布的时间并没有很长，但是以及积累了大量用户，甚至从Vim转的也不少，插件系统几乎兼容了Vim的，而且放宽了插件创作的空间，现在已经有很多优秀的插件，完整的生态链已经建立起来了，而且不少插件的开发者没有对Vim进行适配，插件只能在NeoVim中使用。\n并没有长时间的去体验，体验了一些插件包系统，使用一些大家都认可的插件，更友好的快捷键方案打造的开箱即用的配置系统。只需要些许的配置就可以完成对自己使用的一个或多个语言的环境配置。\nAstroNvim LunarVim NvChad SpaceVim\nSpaceVim 的话是考虑了对 Vim 的兼容，也就吸收了两者的用户，文档相对来说最为齐全，对于一些热门语言有独立的篇章进行介绍及基本环境的配置，其中包括中文文档，对于 Vim 多年积累下来的多套补全方案（例如YouCompleteMe 、COC、asyncomplete等，SpaceVim autocomplete）都兼容，可以自行选择，插件的下载并不是全安装，会根据配置进行下载安装，但是默认是对Vim、NeoVim都进行配置的。\n优点是几乎包含了所有的内容，但缺点也是，内容太多太重，虽然有详细的使用文档，但是项目的层级结构相对复杂，在安装、配置过程中如果遇到异常，如果在网上找不到解决方案，自己根本无从下手，插件太多，甚至官方对 Issues 进行了删除，如果想要进行一些对 Vim 的调整也不容易。\n","date":"2022-12-14","permalink":"/post/neovim/","tags":["Tools","Editor","NeoVim"],"title":"NeoVim"},{"content":"itertools 标准化了一个快速、高效利用内存的核心工具集，这些工具本身或组合都很有用。它们一起形成了“迭代器代数”，这使得在纯 Python 中有可能创建简洁又高效的专用工具。\n无穷迭代器： count() itertools.count(start=0, step=1) 可以设置两个参数，第一个参数为起始点，且包含在内，第二个参数为步长，如果不设置第二个参数则默认步长为1\n创建一个迭代器，它从 start 值开始，返回均匀间隔的值。常用于 map() 中的实参来生成连续的数据点。此外，还用于 zip() 来添加序列号。\n# count(10) --\u0026gt; 10 11 12 13 14 ... # count(2.5, 0.5) -\u0026gt; 2.5 3.0 3.5 ...  cycle() itertools.cycle(iterable) 可以设置一个参数，且只接受可以迭代的参数，如列表，元组，字符串。。。，该函数会对可迭代的所有元素进行循环\n创建一个迭代器，返回 iterable 中所有元素并保存一个副本。当取完 iterable 中所有元素，返回副本中的所有元素。无限重复。\n# cycle('ABCD') --\u0026gt; A B C D A B C D A B C D ...  repeat() itertools.repeat(object[, times]) 可以设置两个参数，其中第一个参数要求可迭代，第二个参数为重复次数，第二个参数如不设置则无限循环，一般来说使用时都会设置第二个参数，用来满足预期重复次数后终止： 创建一个迭代器，不断重复 object 。除非设定参数 times ，否则将无限重复。可用于 map() 函数中的参数，被调用函数可得到一个不变参数。也可用于 zip() 的参数以在元组记录中创建一个不变的部分。\n提供与map或zip一起使用的恒定值流\n\u0026gt;\u0026gt;\u0026gt; list(map(pow, range(10), repeat(2))) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]  根据最短输入序列长度停止的迭代器 islice() ：迭代器切片 函数 itertools.islice() 正好适用于在迭代器和生成器上做切片操作。\nimport itertools def count(n): while True: yield n n += 1 c = count(0) for x in itertools.islice(c, 10, 13): print(x) # 10 # 11 # 12  dropwhile() ： TODO 太多了，日后补上\npython3-cookbook Documentation » Python 标准库 » 函数式编程模块 \nhttps://docs.python.org/zh-tw/3/library/itertools.html#itertools.repeat http://www.cppcns.com/jiaoben/python/242169.html 一个网上看到的讨论，解释的非常好，收藏一下。\n问题： \u0026gt;\u0026gt;\u0026gt; [i for i in itertools.repeat('example', 5)] ['example', 'example', 'example', 'example', 'example'] \u0026gt;\u0026gt;\u0026gt; ['example'] * 5 ['example', 'example', 'example', 'example', 'example'] \u0026gt;\u0026gt;\u0026gt; list(map(str.upper, itertools.repeat('example', 5))) ['EXAMPLE', 'EXAMPLE', 'EXAMPLE', 'EXAMPLE', 'EXAMPLE'] \u0026gt;\u0026gt;\u0026gt; ['example'.upper()] * 5 ['EXAMPLE', 'EXAMPLE', 'EXAMPLE', 'EXAMPLE', 'EXAMPLE']  大意是不使用 itertools.repeat() 也可以得到相类似的结果，那么使用 itertools.repeat() 是必要的吗？itertools.repeat() 实现会更好吗？为什么？\n解答： one itertools.repeat的主要目的是提供与map或zip一起使用的恒定值流：\n\u0026gt;\u0026gt;\u0026gt; list(map(pow, range(10), repeat(2))) # list of squares [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]  第二个目的是，它提供了一种非常快速的循环方式，循环次数固定如下：\nfor _ in itertools.repeat(None, 10000): do_something()  for i in range(10000): do_something()  两者相比，前者更好，因为它只需要更新现有 None 对象的引用计数。后者则需要 range() 制造 10000 个不同的整数对象。\n注意，Guido自己在timeit()模块中使用了快速循环技术。请参阅https://hg.python.org/cpython/file/2.7/Lib/timeit.py#l195上的源代码：\nif itertools: it = itertools.repeat(None, number) else: it = [None] * number gcold = gc.isenabled() gc.disable() try: timing = self.inner(it, self.timer)  two itertools.repeat函数是惰性的；它只使用一个项所需的内存。另一方面，(a,) * n和[a] * n习惯用法在内存中创建对象的n个副本。对于五个项目，乘法习惯用法可能更好，但是如果必须重复某些内容（比如说，一百万次），您可能会注意到资源问题。\n然而，很难想象许多静态用于itertools.repeat。然而，事实上itertools.repeat是一个函数允许您在许多函数应用程序中使用它。例如，您可能有一些库函数func，它对一个iterable输入进行操作。有时，您可能已经预先构建了各种项的列表。其他时候，你可能只想在统一的名单上做手术。如果列表很大，itertools.repeat将为您节省内存。\n最后，repeat使itertools文档中描述的所谓“迭代器代数”成为可能。甚至itertools模块本身也使用repeat函数。例如，下面的代码是作为itertools.izip_longest的等价实现给出的（即使真正的代码可能是用C编写的）。请注意下面使用的repeat七行：\nclass ZipExhausted(Exception): pass def izip_longest(*args, **kwds): # izip_longest('ABCD', 'xy', fillvalue='-') --\u0026gt; Ax By C- D- fillvalue = kwds.get('fillvalue') counter = [len(args) - 1] def sentinel(): if not counter[0]: raise ZipExhausted counter[0] -= 1 yield fillvalue fillers = repeat(fillvalue) iterators = [chain(it, sentinel(), fillers) for it in args] try: while iterators: yield tuple(map(next, iterators)) except ZipExhausted: pass  three 你的例子foo * 5表面上看起来与itertools.repeat(foo, 5)很相似，但实际上却完全不同。\n如果你写foo * 100000，解释器必须创建100000份foo，然后才能给你答案。因此，这是一个非常昂贵且不利于记忆的操作。\n但是，如果您编写itertools.repeat(foo, 100000)，解释器可以返回一个为同一个函数服务的迭代器，并且在您需要结果之前不需要计算结果——例如，通过在一个函数中使用它来知道序列中的每个结果。\n这是迭代器的主要优点：它们可以将列表的一部分（或全部）的计算推迟到您真正需要答案的时候。\nPython的itertools.repeat的目的是什么？\n","date":"2022-11-18","permalink":"/post/model_itertools/","tags":["Python"],"title":"model_itertools"},{"content":"第三章 前端  01 什么是防抖和节流，他们的应用场景有哪些? 在 Issue 中交流与讨论: 01 什么是防抖和节流，他们的应用场景有哪些\n防抖 (debounce) 防抖，顾名思义，防止抖动，以免把一次事件误认为多次，敲键盘就是一个每天都会接触到的防抖操作。\n想要了解一个概念，必先了解概念所应用的场景。在 JS 这个世界中，有哪些防抖的场景呢?\n登录、发短信等按钮避免用户点击太快，以致于发送了多次请求，需要防抖\n调整浏览器窗口大小时，resize 次数过于频繁，造成计算过多，此时需要一次到位，就用到了防抖\n文本编辑器实时保存，当无任何更改操作一秒后进行保存\n代码如下，可以看出来防抖重在清零 clearTimeout(timer)\nfunction debounce (f, wait) { let timer return (\u0026hellip;args) =\u0026gt; { clearTimeout(timer) timer = setTimeout(() =\u0026gt; { f(\u0026hellip;args) }, wait) } }\n节流 (throttle) 节流，顾名思义，控制水的流量。控制事件发生的频率，如控制为1s发生一次，甚至1分钟发生一次。与服务端(server)及网关(gateway)控制的限流 (Rate Limit) 类似。\nscroll 事件，每隔一秒计算一次位置信息等\n浏览器播放事件，每隔一秒计算一次进度信息等\ninput 框实时搜索并发送请求展示下拉列表，每隔一秒发送一次请求 (也可做防抖)\n代码如下，可以看出来节流重在加锁 timer=timeout\nfunction throttle (f, wait) { let timer return (\u0026hellip;args) =\u0026gt; { if (timer) { return } timer = setTimeout(() =\u0026gt; { f(\u0026hellip;args) timer = null }, wait) } }\n总结 (简要答案) 防抖：防止抖动，单位时间内事件触发会被重置，避免事件被误伤触发多次。代码实现重在清零clearTimeout。防抖可以比作等电梯，只要有一个人进来，就需要再等一会儿。业务场景有避免登录按钮多次点击的重复提交。 节流：控制流量，单位时间内事件只能触发一次，与服务器端的限流 (Rate Limit) 类似。代码实现重在开锁关锁 timer=timeout; timer=null。节流可以比作过红绿灯，每等一个红灯时间就可以过一次。\n02 在前端开发中，如何获取浏览器的唯一标识 更多描述: 如何获取浏览器的唯一标识，原理是什么?\n在 Issue 中交流与讨论: 02 在前端开发中，如何获取浏览器的唯一标识\n由于不同的系统显卡绘制 canvas 时渲染参数、抗锯齿等算法不同，因此绘制成图片数据的 CRC 校验也不一样。\nfunction getCanvasFp () { const canvas = document.getElementById(\u0026lsquo;canvas\u0026rsquo;) const ctx = canvas.getContext(\u0026lsquo;2d\u0026rsquo;) ctx.font = \u0026lsquo;14px Arial\u0026rsquo; ctx.fillStyle = \u0026lsquo;#ccc\u0026rsquo; ctx.fillText(\u0026lsquo;hello, shanyue\u0026rsquo;, 2, 2) return canvas.toDataURL(\u0026lsquo;image/jpeg\u0026rsquo;) } 因此根据 canvas 可以获取浏览器指纹信息。\n绘制 canvas，获取 base64 的 dataurl\n对 dataurl 这个字符串进行 md5 摘要计算，得到指纹信息\n但是对于常见的需求就有成熟的解决方案，若在生产环境使用，可以使用以下库\nfingerprintjs2 它依据以下信息，获取到浏览器指纹信息，而这些信息，则成为 component\ncanvas\nwebgl\nUserAgent\nAudioContext\n对新式 API 的支持程度等\nrequestIdleCallback(function () { Fingerprint2.get((components) =\u0026gt; { const values = components.map((component) =\u0026gt; component.value) const fp = Fingerprint2.x64hash128(values.join(\u0026rsquo;\u0026rsquo;), 31) }) })\n在 fingerprintjs2 中，对于 component 也有分类\nbrowser independent component：有些 component 同一设备跨浏览器也可以得到相同的值，有些独立浏览器，得到不同的值\nstable component: 有些 component 刷新后值就会发生变化，称为不稳定组件\n在实际业务中，可根据业务选择合适的组件\nconst options = { excludes: {userAgent: true, language: true} } 简答 根据 canvas 可以获取浏览器指纹信息\n绘制 canvas，获取 base64 的 dataurl\n对 dataurl 这个字符串进行 md5 摘要计算，得到指纹信息\n若在生产环境使用，可以使用 fingerprintjs2，根据业务需求，如单设备是否可跨浏览器，以此选择合适的 component\n03 在服务端应用中如何获得客户端 IP 在 Issue 中交流与讨论: 03 在服务端应用中如何获得客户端 IP\n如果有 x-forwarded-for 的请求头，则取其中的第一个 IP，否则取建立连接 socket 的 remoteAddr。\n而 x-forwarded-for 基本已成为了基于 proxy 的标准HTTP头，格式如下，可见第一个 IP 代表其真实的 IP，可以参考 MDN X-Forwarded-For\nX-Forwarded-For: 203.0.113.195, 70.41.3.18, 150.172.238.178 X-Forwarded-For: , ,  以下是 koa 获取 IP 的方法\nget ips() { const proxy = this.app.proxy; const val = this.get(this.app.proxyIpHeader); let ips = proxy \u0026amp;\u0026amp; val ? val.split(/\\s*,\\s*/) : []; if (this.app.maxIpsCount \u0026gt; 0) { ips = ips.slice(-this.app.maxIpsCount); } return ips; },\nget ip() { if (!this[IP]) { this[IP] = this.ips[0] || this.socket.remoteAddress || \u0026lsquo;\u0026rsquo;; } return this[IP]; }, 参见源码: https://github.com/koajs/koa/…\n04 js 如何全部替代一个子串为另一个子串 更多描述: 假设有一个字符串 hello. hello. hello. 需要替换为 AAA，即把 hello. 替换为 A\n在 Issue 中交流与讨论: 04 js 如何全部替代一个子串为另一个子串\n如果需要全量替换字符串，可以使用 String.prototype.replace(re, replacer)，其中正则表达式需要开启 global flag\nconst s = \u0026lsquo;foo foo foo\u0026rsquo; s.replce(/foo/g, \u0026lsquo;bar\u0026rsquo;) 那如题中，是否可以使用正则表达式来替代子串\n答：不可以，因为使用子串构建正则时，有可能有特殊字符，就有可能出现问题，如下\n// 期待结果: \u0026lsquo;AhelloX hello3 '\n \u0026lsquo;hello. helloX hello3 \u0026lsquo;.replace(new RegExp(\u0026lsquo;hello. \u0026lsquo;, \u0026lsquo;g\u0026rsquo;), \u0026lsquo;A\u0026rsquo;) \u0026lt; \u0026ldquo;AAA\u0026rdquo; 而在 javascript 中替换子串只能使用一种巧妙的办法：str.split(‘foo’).join(‘bar’)\n  \u0026lsquo;hello. hello. hello. \u0026lsquo;.split(\u0026lsquo;hello. \u0026lsquo;).join(\u0026lsquo;A\u0026rsquo;) \u0026lt; \u0026ldquo;AAA\u0026rdquo; 真是一个巧(笨)妙(拙)的办法啊！！！！！大概 TC39 也意识到了一个问题，于是出了一个新的 API，在 ESNext 中\n String.prototype.replaceAll()\n\u0026lsquo;aabbcc\u0026rsquo;.replaceAll(\u0026lsquo;b\u0026rsquo;, \u0026lsquo;.\u0026rsquo;); // \u0026lsquo;aa..cc\u0026rsquo; 详细文档在 String.prototype.replaceAll\n总结(及直接答案) 两种办法\nstr.split(‘foo’).join(‘bar’)\nstr.replaceAll(‘foo’, ‘bar’)，在 ESNext 中，目前支持性不好\n05 如何获取一个进程的内存并监控 更多描述: 在编写脚本时，有时会出现内存过大发生 OOM 的事情，那我们如何得知某个进程的内存？另外又如何监控它\n在 Issue 中交流与讨论: 05 如何获取一个进程的内存并监控\n通过 ps 可以获知一个进程所占用的内存\n$ ps -O rss -p 3506 PID RSS S TTY TIME COMMAND 3506 6984 S pts/1 00:00:00 vim 如果要监控内存，肯定使用对进程万能的命令 pidstat （PS: 这名字一听就知道是干嘛的）\n-r 显示内存信息 -p 指定 pid 1: 每个一秒打印一次 $ pidstat -r -p 3506 1\nLinux 3.10.0-957.21.3.el7.x86_64 (shanyue) 11/04/19 x86_64 (2 CPU)\n20:47:35 UID PID minflt/s majflt/s VSZ RSS %MEM Command 20:47:36 0 3506 0.00 0.00 139940 6984 0.18 vim 20:47:37 0 3506 0.00 0.00 139940 6984 0.18 vim 20:47:38 0 3506 0.00 0.00 139940 6984 0.18 vim 20:47:39 0 3506 0.00 0.00 139940 6984 0.18 vim 20:47:40 0 3506 0.00 0.00 139940 6984 0.18 vim 20:47:41 0 3506 0.00 0.00 139940 6984 0.18 vim pidstat 是属于 sysstat 下的 linux 性能工具，但在 mac 中，如何定位内存的变化？此时可以使用万能的 top/htop\n$ htop -p 31796 1\n总结 简而言之，有以下三个命令\npidstat -r\nhtop/top -p\nps -O rss -p\n关于更多指标的监控可以参考我的文章: linux 各项监控指标小记\n06 CORS 如果需要指定多个域名怎么办? 在 Issue 中交流与讨论: 06 CORS 如果需要指定多个域名怎么办\nCORS 通过控制 Access-Control-Allow-Origin 控制哪些域名可以共享资源，取值如下\nAccess-Control-Allow-Origin:  | * 其中 * 代表所有域名，origin 代表指定特定域名，那如何设置多个域名了？\n此时需要通过代码实现，根据请求头中的 Origin 来设置响应头 Access-Control-Allow-Origin，那 Origin 又是什么东西？\n请求头: Origin 并不是所有请求都会自动带上 Origin，在浏览器中带 Origin 的逻辑如下\n如果存在跨域，则带上 Origin，值为当前域名\n如果不存在跨域，则不带 Origin\n逻辑理清楚后，关于服务器中对于 Access-Control-Allow-Origin 设置多域名的逻辑也很清晰了\n如果请求头不带有 Origin，证明未跨域，则不作任何处理\n如果请求头带有 Origin，证明跨域，根据 Origin 设置相应的 Access-Control-Allow-Origin:\n使用伪代码实现如下:\n// 获取 Origin 请求头 const requestOrigin = ctx.get(\u0026lsquo;Origin\u0026rsquo;);\n// 如果没有，则跳过 if (!requestOrigin) { return await next(); }\n// 设置响应头 ctx.set(\u0026lsquo;Access-Control-Allow-Origin\u0026rsquo;, requestOrigin) Vary: Origin 此时可以给多个域名控制 CORS，但此时假设有两个域名访问 static.shanyue.tech 的跨域资源\nfoo.shanyue.tech，响应头中返回 Access-Control-Allow-Origin: foo.shanyue.tech\nbar.shanyue.tech，响应头中返回 Access-Control-Allow-Origin: bar.shanyue.tech\n看起来一切正常，但如果中间有缓存怎么办？\nfoo.shanyue.tech，响应头中返回 Access-Control-Allow-Origin: foo.shanyue.tech，被 CDN 缓存\nbar.shanyue.tech，因由缓存，响应头中返回 Access-Control-Allow-Origin: foo.shanyue.tech，跨域出现问题\n此时，Vary: Origin 就上场了，代表为不同的 Origin 缓存不同的资源\n总结 (简要答案) CORS 如何指定多个域名？\n根据请求头中的 Origin 来设置响应头 Access-Control-Allow-Origin，思路如下\n总是设置 Vary: Origin，避免 CDN 缓存破坏 CORS 配置\n如果请求头不带有 Origin，证明未跨域，则不作任何处理\n如果请求头带有 Origin，证明浏览器访问跨域，根据 Origin 设置相应的 Access-Control-Allow-Origin: \n使用伪代码实现如下\n// 获取 Origin 请求头 const requestOrigin = ctx.get(\u0026lsquo;Origin\u0026rsquo;);\nctx.set(\u0026lsquo;Vary\u0026rsquo;, \u0026lsquo;Origin\u0026rsquo;)\n// 如果没有，则跳过 if (!requestOrigin) { return await next(); }\n// 设置响应头 ctx.set(\u0026lsquo;Access-Control-Allow-Origin\u0026rsquo;, requestOrigin) 相关问题：如何避免 CDN 为 PC 端缓存移动端页面\n07 既然 cors 配置可以做跨域控制，那可以防止 CSRF 攻击吗? 在 Issue 中交流与讨论: 07 既然 cors 配置可以做跨域控制，那可以防止 CSRF 攻击吗\n对 CORS 一点用也没有\nform 提交不通过 CORS 检测，你可以在本地进行测试\n即使通过 xhr 及 fetch 进行提交被 CORS 拦住，但是对于简单请求而言，请求仍被发送，已造成了攻击\nhttps://www.toutiao.com/i6855150148276716039/\n一、HTML head 内常用标签:\nhttps://www.cnblogs.com/Dominic-Ji/p/9085037.html\nbody 内常用标签:\nhttps://www.cnblogs.com/Dominic-Ji/p/9085099.html\n二、CSS CSS 选择器:\nhttps://www.cnblogs.com/Dominic-Ji/p/9091130.html\n属性相关:\nhttps://www.cnblogs.com/Dominic-Ji/p/9100443.html\n1. 什么是 CSS 初始化？有什么好处？  CSS 初始化是指重设浏览器的样式。不同的浏览器默认的样式可能不尽相同，如果没对 CSS 初始化往往会出现浏览器之间的页面差异。 好处：能够统一标签在各大主流浏览器中的默认样式，使得我们开发网页内容时更加方便简洁，同时减少 CSS 代码量，节约网页下载时间。\n 2. 简述浮动的特征和清除浮动的方法？  浮动的特征： 浮动元素有左浮动 (float:left) 和右浮动 (float:right) 两种。 浮动的元素会向左或向右浮动，碰到父元素边界、其他元素才停下来。 相邻浮动的块元素可以并在一行，超出父级宽度就换行。 浮动让行内元素或块元素转化为有浮动特性的行内块元素 (此时不会有行内块元素间隙问题)。 父元素如果没有设置尺寸 (一般是高度不设置)，父元素内整体浮动的子元素无法撑开父元素，父元素需要清除浮动。\n  清除浮动的方法： 父级上增加属性 overflow：hidden。 在最后一个子元素的后面加一个空的 div，给它样式属性 clear:both。 使用成熟的清浮动样式类，clearfix。\n .clearfix:after,.clearfix:before {content:\u0026quot;\u0026quot;;display: table;} .clearfix:after {clear:both;} .clearfix {zoom:1;}  三、JavaScript 1. AJAX 试什么？如何使用 AJAX？  ajax(异步的 javascript 和 xml) 能够刷新局部网页数据而不是重新加载整个网页。\n  第一步，创建 xmlhttprequest 对象，var xmlhttp =new XMLHttpRequest（);XMLHttpRequest 对象用来和服务器交换数据。 第二步，使用 xmlhttprequest 对象的 open () 和 send () 方法发送资源请求给服务器。 第三步，使用 xmlhttprequest 对象的 responseText 或 responseXML 属性获得服务器的响应。 第四步，onreadystatechange 函数，当发送请求到服务器，我们想要服务器响应执行一些功能就需要使用 onreadystatechange 函数，每次 xmlhttprequest 对象的 readyState 发生改变都会触发 onreadystatechange 函数。\n 四、jQuery 五、Vue.js 六、数据结构与算法 (扩充) 知识点梳理 数据结构  数据结构决定了数据存储的空间和时间效率问题，数据的写入和提取速度要求也决定了应该选择怎样的数据结构。 根据对场景需求的不同，我们设计不同的数据结构，比如： 读得多的数据结构，应该想办法提高数据的读取效率，比如 IP 数据库，只需要写一次，剩下的都是读取； 读写都多的数据结构，要兼顾两者的需求平衡，比如 LRU Cache 算法。 算法是数据加工处理的方式，一定的算法会提升数据的处理效率。 比如有序数组的二分查找，要比普通的顺序查找快很多，尤其是在处理大量数据的时候。 数据结构和算法是程序开发的通用技能，所以在任何面试中都可能会遇见。随着近几年 AI、大数据、小游戏越来越火，Web 前端职位难免会跟数据结构和算法打交道，面试中也会出现越来越多的算法题目。学习数据结构和算法也能够帮助我们打开思路，突破技能瓶颈。\n 前端常遇⻅的数据结构问题  现在我来梳理下前端常遇见的数据结构： 简单数据结构（必须理解掌握） 有序数据结构：栈、队列、链表，有序数据结构省空间（存储空间小） 无序数据结构：集合、字典、散列表，无序数据结构省时间（读取时间快） 复杂数据结构 树、堆 图\n  对于简单数据结构，在 ES 中对应的是数组（Array）和对象（Object）。可以想一下，数组的存储是有序的，对象的存储是无序的，但是我要在对象中根据 key 找到一个值是立即返回的，数组则需要查找的过程。\n  这里我通过一个真实面试题目来说明介绍下数据结构设计。\n  题目：使用 ECMAScript（JS）代码实现一个事件类 Event，包含下面功能：绑定事件、解绑事件和派发事件。\n  在稍微复杂点的页面中，比如组件化开发的页面，同一个页面由两三个人来开发，为了保证组件的独立性和降低组件间耦合度，我们往往使用「订阅发布模式」，即组件间通信使用事件监听和派发的方式，而不是直接相互调用组件方法，这就是题目要求写的 Event 类。\n  这个题目的核心是一个事件类型对应回调函数的数据设计。为了实现绑定事件，我们需要一个 _cache 对象 来记录绑定了哪些事件。而事件发生的时候，我们需要从 _cache 中读取出来事件回调，依次执行它们。一般页面中事件派发（读）要比事件绑定（写）多。所以我们设计的数据结构应该尽量地能够在事件发生时，更加快速地找到对应事件的回调函数们，然后执行。\n  经过这样一番考虑，我简单写了下代码实现：\n class Event {constructor () { // 存储事件的数据结构 // 为了查找迅速，使⽤了对象（字典） this._cache = {};} // 绑定 on (type, callback) { // 为了按类查找⽅便和节省空间， // 将同⼀类型事件放到⼀个数组中 // 这⾥的数组是队列，遵循先进先出 // 即先绑定的事件先触发 let fns = (this._cache [type] = this._cache [type] || []); if (fns.indexOf (callback) === -1) {fns.push (callback); } return this; } // 触发 trigger (type, data) {let fns = this._cache [type]; if (Array.isArray (fns)) {fns.forEach ((fn) =\u0026gt; {fn (data); }); } return this; } // 解绑 off (type, callback) {let fns = this._cache [type]; if (Array.isArray (fns)) {if (callback) {let index = fns.indexOf (callback); if (index !== -1) {fns.splice (index, 1); } } else { // 全部清空 fns.length = 0; } } return this; } } // 测试⽤例 const event = new Event (); event.on ('test', (a) =\u0026gt; {console.log (a); }); event.trigger ('test', 'hello world'); event.off ('test'); event.trigger ('test', 'hello world');   类似于树、堆、图这些高级数据结构，前端一般也不会考查太多，但是它们的查找方法却常考，后面介绍。高级数据应该平时多积累，好好理解，比如理解了堆是什么样的数据结构，在面试中遇见的「查找最大的 K 个数」这类算法问题，就会迎刃而解。\n  算法的效率是通过算法复杂度来衡量的\n  算法的好坏可以通过算法复杂度来衡量，算法复杂度包括时间复杂度和空间复杂度两个。时间复杂度由于好估算、好评估等特点，是面试中考查的重点。空间复杂度在面试中考查得不多。\n 常见的时间复杂度有  常数阶 O (1) 对数阶 O (logN) 线性阶 O (n) 线性对数阶 O (nlogN) 平方阶 O (n^2) 立方阶 O (n^3) !k 次方阶 O (n^k) 指数阶 O (2^n)   随着问题规模 n 的不断增大，上述时间复杂度不断增大，算法的执行效率越低。\n  一般做算法复杂度分析的时候，遵循下面的技巧：\n     看看有几重循环，一般来说一重就是 O (n)，两重就是 O (n^2)，以此类推      如果有二分，则为 O (logN)      保留最高项，去除常数项      题目：分析下面代码的算法复杂度（为了方便，我已经在注释中加了代码分析）\n let i = 0; // 语句执⾏⼀次 while (i \u0026lt; n) { // 语句执⾏ n 次 console.log (`Current i is ${i}`); // 语句执⾏ n 次 i++; // 语句执⾏ n 次 } ```\u0026gt; 根据注释可以得到，算法复杂度为 1 + n + n + n = 1 + 3n，去除常数项，为 O (n)。``` let number = 1; // 语句执⾏⼀次 while (number \u0026lt; n) { // 语句执⾏ logN 次 number *= 2; // 语句执⾏ logN 次 } ```\u0026gt; 上面代码`while`的跳出判断条件是`number\u0026lt;n`，而循环体内 number 增长速度是 (2^n)，所以循环代码实际执行 logN 次，复杂度为：1 + 2 * logN = O (logN)``` for (let i = 0; i \u0026lt; n; i++) {// 语句执⾏ n 次 for (let j = 0; j \u0026lt; n; j++) {// 语句执⾏ n^2 次 console.log ('I am here!'); // 语句执⾏ n^2 次 } }   上面代码是两个 for 循环嵌套，很容易得出复杂度为：O (n^2)\n ⼈⼈都要掌握的基础算法  枚举和递归是最最简单的算法，也是复杂算法的基础，人人都应该掌握！枚举相对比较简单，我们重点说下递归。\n  递归由下面两部分组成：\n     递归主体，就是要循环解决问题的代码      递归的跳出条件，递归不能一直递归下去，需要完成一定条件后跳出      关于递归有个经典的面试题目是：\n  实现 JS 对象的深拷贝\n 什么是深拷⻉？  「深拷贝」就是在拷贝数据的时候，将数据的所有引用结构都拷贝一份。简单的说就是，在内存中存在两个数据结构完全相同又相互独立的数据，将引用型类型进行复制，而不是只复制其引用关系。\n  分析下怎么做「深拷贝」：\n     首先假设深拷贝这个方法已经完成，为 deepClone      要拷贝一个数据，我们肯定要去遍历它的属性，如果这个对象的属性仍是对象，继续使用这个方法，如此往复     function deepClone (o1, o2) {for (let k in o2) {if (typeof o2 [k] === 'object') {o1 [k] = {}; deepClone (o1 [k], o2 [k]); } else {o1 [k] = o2 [k]; } } } // 测试⽤例 let obj = { a: 1, b: [1, 2, 3], c: {}}; let emptyObj = Object.create (null); deepClone (emptyObj, obj); console.log (emptyObj.a == obj.a); console.log (emptyObj.b == obj.b); ```\u0026gt;` 递归容易造成爆栈 `，尾部调用可以解决递归的这个问题，Chrome 的 V8 引擎做了尾部调用优化，我们在写代码的时候也要注意尾部调用写法。 \u0026gt; 递归的爆栈问题可以通过将递归改写成枚举的方式来解决，就是通过 `for` 或者 `while` 来代替 `递归`。 \u0026gt; 我们在使用递归的时候，要注意做优化，比如下面的题目。 \u0026gt; `题目：求斐波那契数列（兔子数列） 1,1,2,3,5,8,13,21,34,55,89… 中的第 n 项下面的代码中 count 记录递归的次数`，我们看下两种差异性的代码中的 count 的值：  let count = 0;\nfunction fn (n) {let cache = {};\nfunction _fn (n) {if (cache [n]) {return cache [n]; } count++; if (n == 1 || n == 2) {return 1;} let prev = _fn (n - 1); cache [n - 1] = prev; let next = _fn (n - 2); cache [n - 2] = next; return prev + next; } return _fn (n);  }\nlet count2 = 0;\nfunction fn2 (n) { count2++; if (n == 1 || n == 2) {return 1;} return fn2 (n - 1) + fn2 (n - 2); }\nconsole.log (fn (20), count); // 6765 20 console.log (fn2 (20), count2); // 6765 13529\n #### 快排和⼆分查找 \u0026gt; 前端中面试排序和查找的可能性比较小，因为 JS 引擎已经把这些常用操作优化得很好了，可能项目中你费劲写的一个排序方法，都不如 Array.sort 速度快且代码少。因此，掌握快排和二分查找就可以了。 \u0026gt; 快排和二分查找都基于一种叫做「分治」的算法思想，通过对数据进行分类处理，不断降低数量级，实现 O (logN)（对数级别，比 O (n) 这种线性复杂度更低的一种）的复杂度。 #### 快速排序 \u0026gt; `快排大概的流程是`： * \u0026gt; 1. 随机选择数组中的一个数 A，以这个数为基准 * \u0026gt; 2. 其他数字跟这个数进行比较，比这个数小的放在其左边，大的放到其右边 * \u0026gt; 3. 经过一次循环之后，A 左边为小于 A 的，右边为大于 A 的 * \u0026gt; 4. 这时候将左边和右边的数再递归上面的过程 \u0026gt; `具体代码如下`：  const Arr = [85, 24, 63, 45, 17, 31, 96, 50];\nfunction quickSort (arr) {if (arr.length \u0026lt;= 1) {return arr;} let pivotIndex = Math.floor (arr.length/ 2); let pivot = arr.splice (pivotIndex, 1)[0]; let left = []; let right = []; for (let i = 0; i \u0026lt; arr.length; i++) {if (arr [i] \u0026lt;pivot) {left.push (arr [i]); } else {right.push (arr [i]); } } // 递归 return quickSort (left).concat ([pivot], quickSort (right)); }\nconsole.log (quickSort (Arr));\n #### ⼆分查找 \u0026gt; 二分查找法主要是解决「在一堆有序的数中找出指定的数」这类问题，不管这些数是一维数组还是多维数组，只要有序，就可以用二分查找来优化。 \u0026gt; 二分查找是一种「分治」思想的算法，`大概流程` 如下： * \u0026gt; 1. 数组中排在中间的数字 A，与要找的数字比较大小 * \u0026gt; 2. 因为数组是有序的，所以： a) A 较大则说明要查找的数字应该从前半部分查找 b) A 较小则说明应该从查找数字的后半部分查找 * \u0026gt; 3. 这样不断查找缩小数量级（扔掉一半数据），直到找完数组为止 \u0026gt; `题目：在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。` \u0026gt; 另外笔者在面试中遇见过下面的问题： \u0026gt; `题目：现在我有一个 1~1000 区间中的正整数，需要你猜下这个数字是几，你只能问一个问题：大了还是小了？问需要猜几次才能猜对？` \u0026gt; 拿到这个题目，笔者想到的就是电视上面有个「猜价格」的购物节目，在规定时间内猜对价格就可以把实物抱回家。所以问题就是让面试官不停地回答我猜的数字比这个数字大了还是小了。这就是二分查找！ \u0026gt; 猜几次呢？其实这个问题就是个二分查找的算法时间复杂度问题，二分查找的时间复杂度是 O (logN)，所以求 log1000 的解就是猜的次数。我们知道 2^10=1024，所以可以快速估算出： log1000 约等于 10，最多问 10 次就能得到这个数！ #### ⾯试遇⻅不会的算法问题怎么办 \u0026gt; 面试的时候，在遇见算法题目的时候，应该揣摩面试官的意图，听好关键词，比如：有序的数列做查找、要求算法复杂度是 O (logN) 这类一般就是用二分的思想。 \u0026gt; 一般来说算法题目的解题思路分以下四步： * \u0026gt; 1. 先降低数量级，拿可以计算出来的情况（数据）来构思解题步骤 * \u0026gt; 2. 根据解题步骤编写程序，优先将特殊情况做好判断处理，比如一个大数组的问题，如果数组为两个数长度的情况 * \u0026gt; 3. 检验程序正确性 * \u0026gt; 4. 是否可以优化（由浅到深），有能力的话可以故意预留优化点，这样可以体现个人技术能力 #### 正则匹配解题 \u0026gt; 很多算法题目利用 ES 语法的特性来回答更加简单，比如正则匹配就是常用的一种方式。笔者简单通过几个真题来汇总下正则的知识点。 \u0026gt; `题目：字符串中第一个出现一次的字符` \u0026gt; 请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符「go」时，第一个只出现一次的字符是「g」。当从该字符流中读出前六个字符「google」时，第一个只出现一次的字符是「l」。 \u0026gt; 这个如果用纯算法来解答需要遍历字符串，统计每个字符出现的次数，然后按照字符串的顺序来找出第一次出现一次的字符，整个过程比较繁琐，如果用正则就简单多了。  function find (str) {for (var i = 0; i \u0026lt; str.length; i++) {let char = str [i] let reg = new RegExp (char, \u0026lsquo;g\u0026rsquo;); let l = str.match (reg).length if (l === 1) {return char} } }\n \u0026gt; 当然，使用 indexOf/lastIndexOf 也是一个取巧的方式。再来看一个千分位问题。 \u0026gt; `题目：将 1234567 变成 1,234,567，即千分位标注` \u0026gt; 这个题目可以用算法直接来解，如果候选人使用正则来回答，这样主动展现了自己其他方面的优势，即使不是算法解答出来的，面试官一般也不会太难为他。这道题目可以利用正则的 `「零宽断言」(?=exp)`，意思是它断言自身出现的位置的后面能匹配表达式 exp。 \u0026gt; 数字千分位的特点是，第一个逗号后面数字的个数是 3 的倍数，正则：`/(\\d {3})+$/`；第一个逗号前最多可以有 `1~3` 个数字，正则：`/\\d {1,3}/`。加起来就是 `/\\d {1,3}(\\d {3})+$/`，分隔符要从前往后加。  function exchange (num) { num += \u0026lsquo;\u0026rsquo;; // 转成字符串 if (num.length \u0026lt;= 3) {return num;}\nnum = num.replace (/\\\\d {1,3}(?=(\\\\d {3})+$)/g, (v) =\u0026gt; {console.log (v) return v + ','; }); return num;  }\nconsole.log (exchange (1234567));\n \u0026gt; 当然上面讲到的多数是算法题目取巧的方式，下面这个题目是纯正则考查，笔者在面试的过程中碰见过，这里顺便提一下。 \u0026gt; `题目，请写出下面的代码执行结果`  var str = \u0026lsquo;google\u0026rsquo;; var reg = /o/g; console.log (reg.test (str)) console.log (reg.test (str)) console.log (reg.test (str)) \u0026gt; 代码执行后，会发现，最后一个不是为 true，而是 false，这是因为 reg 这个正则有个 g，即 global 全局的属性，这种情况下`lastIndex`就发挥作用了，可以看下面的代码执行结果就明白了。 console.log (reg.test (str), reg.lastIndex) console.log (reg.test (str), reg.lastIndex) console.log (reg.test (str), reg.lastIndex) \u0026gt; 实际开发中也会犯这样的错误，比如为了减少变量每次都重新定义，会把用到的变量提前定义好，这样在使用的时候容易掉进坑里，比如下面代码： (function () { const reg = /o/g;\nfunction isHasO (str) { //reg.lastIndex = 0; 这样就可以避免这种情况 return reg.test (str) } var str = 'google'; console.log (isHasO (str)) console.log (isHasO (str)) console.log (isHasO (str))  }())\n #### ⼩结 \u0026gt; 本小节介绍了数据结构和算法的关系，作为普通的前端也应该学习数据结构和算法知识，并且顺带介绍了下正则匹配。具体来说，本小节梳理了以下几部分数据结构和算法知识点： * \u0026gt; 1. 经常用到的数据结构有哪些，它们的特点有哪些 * \u0026gt; 2. 递归和枚举是最基础的算法，必须牢牢掌握 * \u0026gt; 3. 排序里面理解并掌握快速排序算法，其他排序算法可以根据个人实际情况大概了解 * \u0026gt; 4. 有序查找用二分查找 * \u0026gt; 5. 遇见不会的算法问题，先缩小数量级，然后分析推导 \u0026gt; 当然算法部分还有很多知识，比如动态规划这些算法思想，还有图和树常用到的广度优先搜索和深度优先搜索。这些知识在前端面试和项目中遇见得不多，`感兴趣的同学可以在梳理知识点的时候根据个人情况自行决定是否复习`。 ```None.K.Sun 整理 2020-11-09 10:33``` ### 七、《京程一灯》精英班考试题 * 1. 请写出弹出值，并解释为什么。(5 分) * 2. 请写出如下输出值，并写出把注释掉的代码取消注释的值，并解释为什么？(8 分) * 3. 请写出如下点击 li 的输出值，并用三种办法正确输出 li 里的数字。(12 分) * 4. 写出输出值，并解释为什么。(5 分) * 5. 请在下面写出 JavaScript 面向对象编程的混合式继承。并写出 ES6 版本的继承。（20 分） * 6. 请用一句话算出 0-100 之间学生的学生等级，如 90-100 输出为 1 等生、80-90 为 2 等生以此类推。不允许使用 if switch 等。（10 分） * 7. 请你写出如何利用 EcmaScript6/7（小 Demo）优化多步异步嵌套的代码？(15 分) * 8. 请问点击 ```\u0026lt;buttion id=“test”\u0026gt;\u0026lt;/button\u0026gt;``` 会有反应么？为什么？能解决么？（5 分） * 9. 请用一句话遍历变量 a。(禁止用 for 已知 var a = “abc”)(10 分) * 10. 请写出如下输出值，并解释为什么。(12 分) * 【附加题】. 请描述你理解的函数式编程，并书写如下代码结果，最后请问如何具体的将函数式编程应用到你的项目（10 分） * 题目：类型判断用到哪些方法？  以下内容来自 《京程一灯》精英班大前端学习班考试题\n申明：内容仅用于个人学习使用，非商业用途. ##### 1. 请写出弹出值，并解释为什么。(5 分) +function () {alert (a); a (); var a = function () {console.log (1); };\nfunction a () {console.log (2); } alert (a); a (); var c = d = a;  }(); alert (d); alert (c); ##### 2. 请写出如下输出值，并写出把注释掉的代码取消注释的值，并解释为什么？(8 分) this.a = 20; var test = { a: 40, init: () =\u0026gt; {console.log (this.a);\n function go () { //this.a = 60; console.log (this.a); } go.prototype.a = 50; return go; }  }; //var p = test.init (); //p (); new (test.init ())(); ##### 3. 请写出如下点击 li 的输出值，并用三种办法正确输出 li 里的数字。(12 分)\n 1 2 3 4 5 6  var list_li = document.getElementsByTagName (\"li\"); for (var i = 0; i ##### 4. 写出输出值，并解释为什么。(5 分) function test (m) {m = {v: 5} }\nvar m = {k: 30}; test (m); alert (m.v); ##### 5. 请在下面写出 JavaScript 面向对象编程的混合式继承。并写出 ES6 版本的继承。（20 分） 要求：\n汽车是父类，Cruze 是子类。\n父类有颜色、价格属性，有售卖的方法。\nCruze 子类实现父类颜色是红色，价格是 140000, 售卖方法实现输出如下语句：\n将 红色的 Cruze 买给了小王价格是 14 万。\n ##### 6. 请用一句话算出 0-100 之间学生的学生等级，如 90-100 输出为 1 等生、80-90 为 2 等生以此类推。不允许使用 if switch 等。（10 分） ##### 7. 请你写出如何利用 EcmaScript6/7（小 Demo）优化多步异步嵌套的代码？(15 分) ##### 8. 请问点击 ```\u0026lt;buttion id=“test”\u0026gt;\u0026lt;/button\u0026gt;``` 会有反应么？为什么？能解决么？（5 分）  $(\u0026rsquo;#test\u0026rsquo;).click (function (argument) {console.log (1); }); setTimeout (function () {console.log (2); }, 0); while (true) {console.log (3); }\n #### 9. 请用一句话遍历变量 a。(禁止用 for 已知 var a = “abc”)(10 分) #### 10. 请写出如下输出值，并解释为什么。(12 分)  var s = []; var arr = s; for (var i = 0; i \u0026lt; 3; i++) { var pusher = {value: \u0026ldquo;item\u0026rdquo; + i}, tmp; if (i !== 2) {tmp = [] pusher.children = tmp } arr.push (pusher); arr = tmp; } console.log (s [0]); ```\u0026gt;10 答案 \n 这道题可能稍微有点难，考的是 JS 的模拟指针移动问题。\n  本题考点分为如下\n  (http://qiniu.cdn.python87.com/ 微信截图_20201109110123.png \u0026ldquo;null\u0026rdquo;)\n   (http://qiniu.cdn.python87.com/ 微信截图_20201109110133.png \u0026ldquo;null\u0026rdquo;)\n  【附加题】. 请描述你理解的函数式编程，并书写如下代码结果，最后请问如何具体的将函数式编程应用到你的项目（10 分） var Container = function (x) {this.__value = x;} Container.of = x =\u0026gt; new Container (x); Container.prototype.map = function (f) {return Container.of (f (this.__value)) } Container.of (3) .map (x =\u0026gt; x + 1) .map (x =\u0026gt; 'Result is ' + x); ```\u0026gt;` 附加题答案 `： \u0026gt; 函数式编程是很复杂的话题但是我教给大家的东西足够了。 \u0026gt; 本题考点分为如下 \u0026gt; 1. 基础函子的概念。首先创建一个基础的容器 Container，当一个容器具有 map 方法的时候她也 可以叫一个函子。of 是为了解决函数式编程更不像面向对象编程。接下来我们传入了新的 3 ， x+1 等，每一次都是一个变形关系。产生一个新的函子，也就是一个新的范畴。函数式编程讲 的是函数要切忌纯！同时柯里化（curry）？代码组合（compose）？等等你是否还有印象。 \u0026gt; 2. 项目实战。我去掉了一些模块化的判断。  (function () {var _ = function (obj) {if (obj instanceof _) return obj; if (!(this instanceof _)) return new _(obj); }; _.VERSION = \u0026lsquo;0\u0026rsquo;; _.isNaN = function (obj) {return _.isNumber (obj) \u0026amp;\u0026amp; obj !== +obj; }; return _; }.call (this));\n * * * \u0026gt; `写到最后` \u0026gt; 其实 JS 的基础知识还不止这些，比如 ECMAScript 中定义了 6 种原始类型：Boolean、String、Number、Null、Undefined、Symbol（ES6 新定义）`注意：原始类型不包含 Object`。 ##### 题目：类型判断用到哪些方法？ \u0026gt; typeof xxx 得到的值有以下几种类型：undefined boolean number string object function、symbol 。 \u0026gt; 这里需要注意的有三点： * \u0026gt; (1) typeof null 结果是 object ，因为 null 也是一种引用类型，不是值类型 * \u0026gt; (2) typeof [1, 2] 结果是 object，结果中没有 array 这一项，引用类型除了 function 其他的全部都是 object、typeof symbol 用 typeof 获取 symbol 类型的值得到的是 symbol，这是 ES6 新增的知识点 * \u0026gt; (3) instanceof 用于实例和构造函数的对应。例如判断一个变量是否是数组，使用 typeof 无法判断，但可以使用 [1, 2] instanceof Array 来判断。因为，[1, 2] 是数组，它的构造函数就是 Array。 \u0026gt; 同理  function Foo (name) {this.name = name}\nvar foo = new Foo (\u0026lsquo;bar\u0026rsquo;); console.log (foo instanceof Foo); //true\u0026hellip; \u0026gt; 最后问下为什么 Object.prototype.toString.call ([])== \u0026ldquo;[object Array]\u0026rdquo;;\n 最正确？ ### No1: 答案 \u0026gt; 本题考点分为如下： \u0026gt; 1.IIFE 第一行考点用了一个 + 号 ，其实就是把函数变成函数表达式，直接执行 function () {}(); 会报错。常用的写法是 ( function (){})(), 此时其实创建了闭包。\u0026gt; 2. 变量与函数提升，但此时函数的名字也是 a 变量也是 a, 所以会造成 function a (){}，var a。此时 var a 因为未被定义所以被忽略了。所以顶部的输出值是 2 和 2. 接下来输出是 1 和 1，是因为函数的提升要比变量提升的更前。局面就是 var a,function a (){},a=function (){};alert (a) \u0026gt; 3. 作用域和连等问题 此时的 var c = d= a。实际是 d = a,var c=d; 所以 c 是 undefined，但是要千 万注意是不是严格模式下。扩展的题为 var a = {n:1}; var b = a; a.x = a = {n:2}; alert (a.x);// –\u0026gt; undefined alert (b.x);// –\u0026gt; {n:2}  ","date":"2022-11-18","permalink":"/post/front_end/","tags":["InterviewQuestions","FrontEnd"],"title":"front_end"},{"content":"第十章 数据结构与算法 1. 算法的特征？ 1） 有穷性： 一个算法必须保证执行有限步骤之后结束；\n2） 确切性： 算法的每一步骤必须有确切的定义；\n3） 输入：一个算法有 0 个或多个输入,以刻画运算对象的初始情况,所谓 0 个输入是指算法本身给出了初始条件；\n4） 输出：一个算法有一个或多个输出,以反映对输入数据加工后的结果。没有输出的算法是毫无意义的；\n5） 可行性： 算法原则上能够精确地运行,而且人们用笔和纸做有限次数运算后即可完成。\n2、排序 3. 如何判断单向链表中是否有环？ 首先遍历链表,寻找是否有相同地址,借此判断链表中是否有环。如果程序进入死循环,则需要一块空间来存储指针,遍历新指针时将其和储存的旧指针比对,若有相同指针,则该链表有环,否则将这个新指针存下来后继续往下读取,直到遇见 NULL,这说明这个链表无环。\n4. 基础的数据结构有哪些？ 基本的算法有： 排序算(冒泡排序,插入排序,快速排序,归并排序), 查(二分查找), 搜索（（DFS）深度优先搜索,（BFS）广度优先搜索),（Dijkstra 算法）,] 动态规划算法, 分类（朴素贝叶斯分类算法等）。\n评价算法的好坏一般有两种： 时间复杂度和空间复杂度。 时间复杂度：同样的输入规(问题规模) 花费多少时间。空间复杂度：同样的输入规模花费多少空(主要是内存)。以上两点越小越好。\n稳定性：不会因为输入的不同而导致不稳定的情况发生。\n算法的思路是否简单：越简单越容易实现的越好。\n5. 哪种数据结构可以实现递归？ 栈可以实现,递归需要保存正在计算的上下文, 等待当前计算完成后弹出,再继续计算, 只有栈先进后出的特性才能实现。\n6. 斐波那契数列 斐波那契数列：简单地说,起始两项为 0 和 1,此后的项分别为它的前两项之和。\ndef fib(num): numList = [0,1] for i in range(num - 2): numList.append(numList[-2] + numList[-1]) return numList  7. 二叉树如何求两个叶节点的最近公共祖先？ 二叉树是搜索二叉树：\n1、原理：二叉搜索树是排序过的,位于左子树的结点都比父结点小,位于右子树的结点都比父结点大,我们只需从根节点开始和两个输入的结点进行比较,如果当前节点的值比两个结点的值都大,那么最低的公共祖先结点在该结点的左子树中,下一步开遍历当前结点的左子树。如果当前节点的两个结点的值都小,那么最低的公共祖先结点一定在该结点的右子树中,步开遍历当前结点的右子树。这样从上到下找到第一个在两个输入结点的间的结点。\n2、实现代码：\nclass TreeNod(object）： def __init__(self, left=None, right=None, data=None): self.data = data self.left = left self.right = right def getCommonAncesto(root, node1, node2): while root: if root.data \u0026gt; nodedata and root.data \u0026gt; nodedata: root = root.left elif root.data \u0026lt; nodedata and root.data \u0026lt; nodedata: root = root.right else: return root return None  8. 两个字符串,如何求公共字符串？ def getLCStrin(str1,str2): maxlenn = len(str1) if len(str1) \u0026lt; len(str2) else len(str2) examplen = str1 if len(str1) \u0026lt; len(str2) else str2 other = str2 if str1 == examplen else str1 for i in range(maxlenn): for j in range(maxlenn, 0,-1): if other.find(examplen[i:j]) != -1: print(examplen[i:j])  9. 找出二叉树中最远结点的距离？  计算一个二叉树的最大距离有两个情况。\n情况 A: 路径经过左子树的最深节点,通过根节点,再到右子树的最深节点。 情况 B: 路径不穿过根节点,而是左子树或右子树的最大距离路径,取其大者。只需要计算这两个情况的路径距离,并取其大者,就是该二叉树的最大距离。\n 10. 写一个二叉树 class TreeNode(object): def __init__(self, left=None, right=None, data=None): self.data = data self.left = left self.right = right def preorder(root): # 前序遍历 if root is None: return else: print root.data preorder(root.left) preorder(root.right) def inorder(root): # 中序遍历 if root is None: return else: inorder(root.left) print root.data inorder(root.right) def postorder(root): # 后序遍历 if root is None: return postorder(root.left) postorder(root.right) print root.data  11. 写一个霍夫曼数 12. 写一个二分查找 13. set 用 in 时间复杂度是多少,为什么？ O(1),因为 set 是键值相同的一个数据结构,键做了 hash 处理\n14. 深度优先遍历和广度优先遍历的区别？ 1） 二叉树的深度优先遍历的非递归的通用做法是采用栈,广度优先遍历的非递归的通用做法是采用队列。\n2） 深度优先遍历：对每一个可能的分支路径深入到不能再深入为止,而且每个结点只能访问一次。要特别注意的是,二叉树的深度优先遍历比较特殊,可以细分为先序遍历、中序遍历、后序遍历。具体说明\n如下：\n先序遍历：对任一子树,先访问根,然后遍历其左子树,最后遍历其右子树。\n中序遍历：对任一子树,先遍历其左子树,然后访问根,最后遍历其右子树。\n后序遍历：对任一子树,先遍历其左子树,然后遍历其右子树,最后访问根。\n广度优先遍历：又叫层次遍历,从上往下对每一层依次访问,在每一层中,从左往右（也可以从右往左）访问结点,访问完一层就进入下一层,直到没有结点可以访问为止。\n3）深度优先搜素算法：不全部保留结点,占用空间少；有回溯操(即有入栈、出栈操作),运行速度慢。广度优先搜索算法：保留全部结点,占用空间大； 无回溯操(即无入栈、出栈操作),运行速度快。通常,深度优先搜索法不全部保留结点,扩展完的结点从数据库中弹出删去,这样,一般在数据库中存储的结点数就是深度值,因此它占用空间较少。所以,当搜索树的结点较多,用其它方法易产生内存溢出时,深度优先搜索不失为一种有效的求解方法。\n广度优先搜索算法,一般需存储产生的所有结点,占用的存储空间要比深度优先搜索大得多,因此,程序设计中,必须考虑溢出和节省内存空间的问题。但广度优先搜索法一般无回溯操作,即入栈和出栈的操作,所以运行速度比深度优先搜索要快些。\n15. 写程序把一个单向链表顺序倒过来） 第一种方法：迭代\nclass ListNode(object): def __init__(self, x): self.val = x self.next = None class Solution(object): def reverseList(self, head): \u0026quot;\u0026quot;\u0026quot; :type head: ListNode :rtype: ListNode \u0026quot;\u0026quot;\u0026quot; pre = cur = None if head: pre = head cur = head.next pre.next = None else: return None while cur: p = cur cur = cur.next p.next = pre pre = p return pre  第二种方法：递归\nclass ListNode(object): def __init__(self, x): self.val = x self.next = None class Solution: def reverseList(self, head): return reverse(head) def reverse(self, node, prev=None): if not node: return prev n = node.next node.next = prev return reverse(n, node)  16. 青蛙跳台阶问题  一只青蛙要跳上 n 层高的台阶,一次能跳一级,也可以跳两级,请问这只青蛙有多少种跳上这个 n\n层高台阶的方法？\n思路分析：\n这个问题有三种方法来解决,并在下面给出三处方法的 python 实现。\n  方法 1: 递归\n设青蛙跳上 n 级台阶有 (n) 种方法,把这 n 种方法分为两大类,第一种最后一次跳了一级台阶,这\n类方法共有 (n-1) 种, 第二种最后一次跳了两级台阶,这种方法共有 (n-2) 种, 则得出递推公式\n(n)=(n-1)+(n-2),显然,(1)=1,(2)=2,递推公式如下：\n  这种方法虽然代码简单,但效率低,会超出时间上限 *\n代码实现如下：  class Solution: # @param {integer} n # @return {integer} def climbStair(self, n): if n==1: return 1 elif n==2: return 2 else: return self.climbStair(n-1)+self.climbStair(n-2)   方法 2: 用循环来代替递归\n这种方法的原理仍然基于上面的公式,但是用循环代替了递归,比上面的代码效率上有较大的提升,可以 AC。\n代码实现如下：\n class Solution: # @param {integer} n # @return {integer} def climbStair(self, n): if n==1 or n==2: return n a=1;b=2;c=3 for i in range(3,n+1): c=a+b;a=b;b=c return c   方法三：建立简单数学模型,利用组合数公式\n  设青蛙跳上这 n 级台阶一共跳了 z 次,其中有 x 次是一次跳了两级,y 次是一次跳了一级,则有 z=x+y ,2x+y=n,对一个固定的 x,利用组合可求出跳上这 n 级台阶的方法共有种方法又因为 x 在区间 [0,n/2] 内,所以我们只需要遍历这个区间内所有的整数,求出每个 x 对应的组合数累加到最后的结果即可。\n python 代码实现如下：\nclass Solution: # @param {integer} n # @return {integer} def climbStair(self, n): def fac(n): result=1 for i in range(1,n+1): result*=i return result total=0 for i in range(n/2+1): total+=fac(i+n-2*i)/fac(i)/fac(n-2*i) return total  17. 用两个队列如何实现一个栈,用两个栈如何实现一个队列？ 两个栈实现一个队列：\n栈的特性是先进后出（FILO）,队列的特性是先进先出（FIFO）,在实现 delete 时,将一个栈中的数据依次拿出来压入到另一个为空的栈,另一个栈中数据的顺序恰好是先压入栈 1 的元素此时在栈2的上面,为了实现效率的提升,在 delete 时,判断栈 2 是否有数据,如果有的话,直接删除栈顶元素,在栈 2 为空时才将栈 1 的数据压入到栈 2 中,从而提高程序的运行效率,实现过程可以分为下面 几个步骤：\n1、push 操作时,一直将数据压入到栈 2 中\n2、delete 操作时,首先判断栈 2 是否为空,不为空的情况直接删除栈 2 栈顶元素,为空的话将栈 1 的数据压入到栈 2 中,再将栈 2 栈顶元素删除。\n两个队列实现一个栈：\n因为队列是先进先出,所以要拿到队列中最后压入的数据,只能每次将队列中数据 pop 到只剩一个,此时这个数据为最后压入队列的数据,在每次 pop 时,将数据压入到另一个队列中。每次执行 delete 操作时,循环往复。\n18. 爬楼梯 Climbing Stairs 假设你正在爬楼梯,需要 n 步你才能到达顶部。但每次你只能爬一步或者两步,你能有多少种不同的方法爬到楼顶部？\n样例：比如 n=3,1+1+1=1+2=2+1=3,共有 3 中不同的方法返回 3\n解题思路：\n如果按照从右至左的逆序递归求解,其实就相当于搜索算法了,会造成子搜索过程的重复计算。搜索算法一般都可以用动态规划来替代,因此这里就用 1D 动态规划。然后可以发现,(x) 的求解只依赖于 (x-1) 和 (x-2),因此可以将空间复杂度缩小到 int [3]。于是你就会发现,这其实就是一个裴波拉契数列问题。\nclass Solution: # @param n, an integer # @return an integer def climbStairs(self, n): if n \u0026lt;= 1: return 1 arr = [1, 1, 0] # look here, arr[0] = 1, arr[1] = 2 for i in range(2, n + 1): arr[2] = arr[0] + arr[1] arr[0], arr[1] = arr[1], arr[2] return arr[2]  ","date":"2022-11-18","permalink":"/post/datastruct/","tags":["InterviewQuestions","DateStructure_Algorithm"],"title":"dataStruct"},{"content":"第十一章 企业真题实战 一、360 面试题 1. 请拿出 B 表中的 accd，(A 表中和 B 表中的一样的数据)？ select * from B inner join on B.name = A.name\n2. a = \u0026lsquo;abbbccc\u0026rsquo;，用正则匹配为 abccc, 不管有多少 b，就出现一次？ 思路：不管有多少个 b 替换成一个 re.sub(r'b+', 'b', a)  3. xpath 使用的什么库？ lxml\n####5. Redis 里面 list 内容的长度？ len(key_name)\n6. 多线程交互，访问数据，如果访问到了就不访问了，怎么避免重读？ 创建一个已访问数据列表，用于存储已经访问过的数据，并加上互斥锁，在多线程访问数据的时候先查看数据是否已经在已访问的列表中，若已存在就直接跳过。\n7. Mysql 怎么限制 IP 访问？ grant all privileges on . to 数据库中用户名'@'ip 地址' identified by 数据库密码';\n8. 带参数的装饰器？ 带定长参数的装饰器。\ndef new_func(func): def wrappedfun(username,passwd): if username == 'root' and passwd == '123456789': print(' 通过认证！') print(' 开始执行附加功能 ') return func() else: print(' 用户名或密码错误 ') return return wrappedfun @new_func def orign(): print(' 开始执行函数 ') orign('root','123456789')  带不定长参数的装饰器。\ndef new_func(func): def wrappedfun(*parts): if parts: counts = len(parts) print(' 本系统包含 ', end='') for part in parts: print(part, ' ', end='') print(' 等 ', counts, ' 部分 ') return func() else: print(' 用户名或密码错误 ') return func() return wrappedfun @new_func def orign(): print(' 开始执行函数 ') orign(' 硬件 ', ' 软件 ', ' 用户数据 ')  同时带不定长、关键字参数的装饰器。\ndef new_func(func): def wrappedfun(*args,**kwargs): if args: counts = len(args) print(' 本系统包含 ',end='') for arg in args: print(arg,' ',end='') print(' 等 ',counts,' 部分 ') if kwargs: for k in kwargs: v= kwargs [k] print(k,' 为：',v) return func() else: if kwargs: for kwarg in kwargs: print(kwarg) k,v = kwarg print(k,' 为：',v) return func() return wrappedfun @new_func def orign(): print(' 开始执行函数 ') orign(' 硬件 ',' 软件 ',' 用户数据 ', 总用户数 = 5, 系统版本 ='CentOS 4')  二、妙计旅行面试题 1. Python 主要的内置数据类型有哪些？ Python 主要的内置数据类型有：str，int，float，tuple，list，dict，set。\n2. print(dir(‘a\u0026rsquo;)) 输出的是什么？ 会打印出字符型的所有的内置方法。\n['__add__', '__class__', '__contains__', '__delattr__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getslice__', '__gt__', '__hash__', '__init__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_formatter_field_name_split', '_formatter_parser', 'capitalize', 'center', 'count', 'decode', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'index', 'isalnum', 'isalpha', 'isdigit', 'islower', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']  3. 给定两个 list，A 和 B，找出相同元素和不同元素？ A、B 中相同元素：print(set(A)\u0026amp;set(B)) A、B 中不同元素：print(set(A)^set(B))\n4. 请反转字符串？ new_str = old_str [::-1]\n5. 交换变量 a,b 的值？ a,b = b,a\n6. 用 select 语句输出每个城市中心距离市中心大于 20km 酒店数？ select count（hotel）i from hotel_table where distance \u0026gt;20 group by city\n7. 给定一个有序列表，请输出要插入值 k 所在的索引位置？ def index(list, key): if key \u0026lt;list [0]: position = 0 elif key \u0026gt; list [-1]: position = len(list) else: for i in range(len(list)): if key\u0026gt;list [i] and list [i+1]\u0026gt;key: position = i+1 return position  8. 正则表达式贪婪与非贪婪模式的区别？ 在形式上非贪婪模式有一个？作为该部分的结束标志。 在功能上贪婪模式是尽可能多的匹配当前正则表达式，可能会包含好几个满足正则表达式的字符串，非贪婪模式，在满足所有正则表达式的情况下尽可能少的匹配当前正则表达式。\n9. 写出开头匹配字母和下划线，末尾是数字的正则表达式？ ^[A-Za-z]|_.*\\d$\n10. 请说明 HTTP 状态码的用途，请说明常见的状态码机器意义？ 通过状态码告诉客户端服务器的执行状态，以判断下一步该执行什么操作。 常见的状态机器码有: 100-199：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。 200-299：表示服务器成功接收请求并已完成处理过程，常用 200（OK 请求成功）。 300-399：为完成请求，客户需要进一步细化请求。302（所有请求页面已经临时转移到新的 url）， 304、307（使用缓存资源）。 400-499：客户端请求有错误，常用 404（服务器无法找到被请求页面），403（服务器拒绝访问，权限不够）。 500-599：服务器端出现错误，常用 500（请求未完成，服务器遇到不可预知的情况）。\n11. 当输入 http://www.itheima.com 时，返回页面的过程中发生了什么？  浏览器向 DNS 服务器发送 itheima.com 域名解析请求； DNS 服务器返回解析后的 ip 给客户端浏览器，浏览器想该 ip 发送页面请求； DNS 服务器接收到请求后，查询该页面，并将页面发送给客户端浏览器； 客户端浏览器接收到页面后，解析页面中的引用，并再次向服务器发送引用资源请求； 服务器接收到资源请求后，查找并返回资源给客户端； 客户端浏览器接收到资源后，渲染，输出页面展现给用户。  12. 有一个多层嵌套列表 A=[1,2,[3.4 [\u0026lsquo;434\u0026rsquo;,[…]]]] 请写一段代码遍历 A 中的每一个 元素并打印出来。 思路：就是有几个嵌套链表就用几个 for 循环进行迭代，然后对最后一个结果进行打印。\na= [\u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,[\u0026quot;1\u0026quot;,\u0026quot;3\u0026quot;,[\u0026quot;4\u0026quot;,\u0026quot;haha\u0026quot;]]] for b in a : for c in b : for d in c : print(d)  13. 关系型数据库中，表和表之间有左连接，内连接，外连接，分别解释下他们的 含义和区别？ 内连接查询：查询的结果为两个表匹配到的数据。 右接查询：查询的结果为两个表匹配到的数据，右表特有的数据，对于左表中不存在的数据使用 null 填充。 左连接查询：查询的结果为两个表匹配到的数据，左表特有的数据，对于右表中不存在的数据使用 null 填充。\n14. 如何定时启动你的爬虫项目： 1.最简单的方法：直接使用 Timer 类\nimport time import os while True: os.system(\u0026quot;scrapy crawl News\u0026quot;) time.sleep(86400)#每隔一天运行一次 24*60*60=86400s  2.使用 sched\nimport sched # 初始化 sched 模块的 scheduler 类 # 第一个参数是一个可以返回时间戳的函数，第二个参数可以在定时未到达之前阻塞。 schedule = sched.scheduler(time.time, time.sleep) # 被周期性调度触发的函数 def func(): os.system(\u0026quot;scrapy crawl News\u0026quot;) def perform1(inc): schedule.enter(inc,0,perform1,(inc,)) func()# 需要周期执行的函数 def mymain(): schedule.enter(0,0,perform1,(86400,)) if __name__==\u0026quot;__main__\u0026quot;: mymain() schedule.run()# 开始运行，直到计划时间队列变成空为止  15. 什么是 scrapy-redis 中的指纹，是如何去重的？ 指纹：通过 sha1 加密，把请求体，请求方式，请求 url 放在一起。然后进行 16 进制的转义符字符串生成指纹。生成一个字符串，放到数据库中作为唯一标示。 去重：urll 中按照 url 去重：\n 按照 url 去重，有一个列表，发送请求之前从数据表中看一下这个 url 有没有请求过，请求过了就不用看了 2，内容判断，从数据库中查数据的表示，如果请求过了就在不在请求了。  16. 代码优化从哪些方面考虑？有什么想法？  优化算法时间复杂度。 减少冗余数据。 合理使用 copy 与 deepcopy。 使用 dict 或 set 查找元素。 合理使用生成器（generator）和 yield。 优化循环。 优化包含多个判断表达式的顺序。 使用 join 合并迭代器中的字符串。 选择合适的格式化字符方式。 10 不借助中间变量交换两个变量的值。 使用 if is。 使用级联比较 x \u0026lt; y \u0026lt; z。 13.while 1 比 while True 更快。 使用 ** 而不是 pow。 使用 cProfile, cStringIO 和 cPickle 等用 c 实现相同功能（分别对应 profile, StringIO, pickle）的包。 使用最佳的反序列化方式。 使用 C 扩展(Extension)。 并行编程。 终级大杀器：PyPy。 使用性能分析工具。  17. Django 项目的优化（web 通用） 1.优化数据库查询 1.1 一次提供所有数据 1.2 仅提供相关的数据 2.代码优化 2.1 简化代码 2.2 更新或替代第三方软件包 2.3 重构代码\n三、智慧星光面试题 这家公司主要做舆情分析，问题主要集中在页面解析和 Selenium+Phantom JS 解析复杂页面，ajax 页面请求等，图片识别，机器学习。\n1. 定义 A=(\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;, \u0026lsquo;c\u0026rsquo;, \u0026rsquo;d\u0026rsquo;), 执行 del A[2] 后的结果为： D A: (a, c, d) B: (a, b, c) C: (a, b, d) D: 异常\nTypeError: 'tuple' object doesn't support item deletion\n2. String = \u0026lsquo;{1},{0}\u0026rsquo;; string = string.format(\u0026lsquo;Hello\u0026rsquo;, \u0026lsquo;Python\u0026rsquo;), 请问将 string 打印出来为（C） A: Hello Python B: {1},{0} C:Python,Hello D:Hello,Hello\n3. 定义 A=[1,2,3,4], 使用列表生成式 [i*i for i in A] 生成列表为：B A: [1, 2, 3, 4] B: [1, 4, 9, 16] C: [1, 4, 6, 8] D: [1, 4, 9, 12]\n4. 请对 Python 数据结构 Tuple,List,Dict 进行操作 1.如何让元祖内部可变（叙述或简单定义）? 元祖变成列表，比如：\nA =(1,2,3,4) A = list(A)   如何将 L1 = [1,2,3,4,5],L2 = [6,7,8,9]; 使用列表内置函数变成 L1=[1,2,3,4,5,6,7,8,9]? L1.extend(L2)。\n  如何将字典 D={\u0026lsquo;Adam\u0026rsquo;: 95, \u0026lsquo;Lisa\u0026rsquo;: 85, \u0026lsquo;Bart\u0026rsquo;: 59} 中的值\u0026rsquo;Adam\u0026rsquo;删除？ del D[\u0026lsquo;Adam\u0026rsquo;]。\n  请按照如下格式 K：V 打印出字典？\n  for k,v in D.items(): print(k,\u0026quot;:\u0026quot;,v)  5. 请用 Python 内置函数处理以下问题？  请判断一个字符串是否以 er 结尾？ 使用 endswith 函数，比如：  Str1 =\u0026quot;nihaoer\u0026quot;print(Strendswith(\u0026quot;er\u0026quot;))  请将 #teacher# 两侧的 #去掉  str =\u0026quot;#tea#\u0026quot; b = str.replace(\u0026quot;#\u0026quot;,\u0026quot;\u0026quot;).strip()  请使用 map 函数将 [1,2,3,4] 处理成 [1,0,1,0]  def f(x): if x%2 == 0: return 0 else: return 1 b = map(f,[1,2,3,4]) print(list(b))  请使用 filter 函数将 [1,2,3,4] 处理成 [2,4]?  def f(x): if x%2 == 0: return x b = filter(f,[1,2,3,4]) print(list(b))  5. 请使用 reduce 函数计算 100 的阶乘？\u0026gt; reduce() 函数在库 functools 里，如果要使用它，要从这个库里导入。reduce 函数与 map 函数有不一样地方，map 操作是并行操作，reduce 函数是把多个参数合并的操作，也就是从多个条件简化的结果，在计算机的算法里，大多数情况下，就是为了简单化。比如识别图像是否是一只猫，那么就是从众多的像素里提炼出来一个判断：是或否。可能是几百万个像素，就只出来一个结果。在 google 大规模集群里，就是利用这个思想，把前面并行处理的操作叫做 map，并行处理之后的结果，就需要简化，归类，把这个简化和归类的过程就叫做 reduce。由于 reduce 只能在一台主机上操作，并不能分布式地处理，但是 reduce 处理的是 map 结果，那么意味着这些结果已经非常简单，数据量大大减小，处理起来就非常快。因此可以把 mapreduce 过程叫做分析归纳的过程。 from functools import reduce sum=reduce(lambda x,y:x*y,range(1,101)) print(sum)  ####6. 现在需要从一个简单的登陆网站获取信息，请使用 Python 写出简要的登陆函数的具体实现？（登录信息只包含用户名，密码）\nsession = requests.session() response = session.get(url,headers)  7. 正则表达式操作   匹配手机号 分析： （1）手机号位数为 11 位； （2）开头为 1，第二位为 3 或 4 或 5 或 7 或 8; 表达式为：/1 [3,4,5,7,8][0-9]{9}$/。\n  请匹配出变量 A = json({Adam:95,Lisa:85,Bart:59}) 中的 json 字符串。\n  A = 'json({\u0026quot;Adam\u0026quot;:95,\u0026quot;Lisa\u0026quot;:85,\u0026quot;Bart\u0026quot;:59})' b = re.search(r'json.*?({.*?}).*',A,re.S) print(b.group(1))  怎么过滤评论中的表情？  co = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]') co.sub('',text)  四、壹讯面试题 1. Python 中 pass 语句的作用是什么？ 在编写代码时只写框架思路，具体实现还未编写就可以用 pass 进行占位，使程序不报错，不会进行任何操作。\n2. 生成一个斐波那契数列？ # [] 列表实现 def fibonacci(num): fibs = [0, 1] for i in range(num - 2): fibs.append(fibs [-2] + fibs [-1])# 倒数第二个 + 倒数第一个数的结果，追加到列表 print(fibs) # yield 实现 def fab_demo4(max): a,n,b = 0,0,1 while n \u0026lt; max: yieldb #print b a,b = b,a+b n+=1 print(next(fab_demo4(5))) for i in fab_demo4(8): print(i)  3. 说明一下 os.path 和 sys.path 分别代表什么？ os.path 主要是用于用户对系统路径文件的操作。 sys.path 主要用户对 Python 解释器的系统环境参数的操作。\n4. 什么是 lambda 函数？ 有什么好处？ lambda 函数是一个可以接收任意多个参数(包括可选参数) 并且返回单个表达式值的函数。 1、lambda 函数比较轻便，即用即仍，很适合需要完成一项功能，但是此功能只在此一处使用，连名字都很随意的情况下； 2、匿名函数，一般用来给 filter， map 这样的函数式编程服务；3、作为回调函数，传递给某些应用，比如消息处理。\n五、H3C 面试题 1. 下列哪个语句在 Python 中是非法的？(B)  A、x=y=z=1 B、x=(y=z+1) C、x,y=y,x D、x +=y\n 2. 关于 Python 内存管理，下列说法错误的是(B) A、变量不必事先声明 B、变量无须先创建和赋值而直接使用 C、变量无须指定类型 D、可以使用 del 释放资源\n3. 下面哪个不是 Python 合法的标识符(b) A、int32 B、40XL C、self D、name\n第一个字符必须是字母或是下划线\n4. 下列哪种说法是错误的（A） A、除字典类型外，所有标准对象均可以用于布尔测试 B、空字符串的布尔值是 False C、空列表对象的布尔值是 Fale D、值为 0 的任何数字对象的布尔值是 False\n5. 下列表达式的值为 True 的是（C） A、5+4j\u0026gt;2-3j B、3\u0026gt;2\u0026gt;2 C、(3,2)\u0026lt;(\u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;) D、\u0026lsquo;abc\u0026rsquo;\u0026gt;xyz\n备注：在 Python3 中 整数和字符不可以使用运算符做比较，在 Python2 中可以。\n6. Python 不支持的数据类型有(A) A、char B、int C、 float D、list\n7. 关于 Python 中的复数，下列说法错误的是 C A、表示复数的语法是 real+ image j B、实部和虚部都是浮点数 C、虚部必须后缀 j, 且必须是小写 D、方法 conjugate 返回复数的共轭\n六、通联数据 1. 说一下你对多线程的看法？\u0026gt; 答案详见第三章→七。系统编程。 2. 多线程和多线程有什么区别？\u0026gt; 答案详见第三章→七。系统编程。 3. 进程间的数据共享和线程间数据共享？ 进程间数据共享： 多进程中，每个进程都是独立的，各自持有一份数据，无法共享。 Queue：\nfrom multiprocessing import queues import multiprocessing def func(i, q): q.put(i) print(\u0026quot;---\u0026gt;\u0026quot;, i, q.qsize()) q = queues.Queue(9, ctx=multiprocessing) for i in range(5): p = multiprocessing.Process(target=func, args=(i, q,)) p.start() p.join()  Queue 是多进程安全的队列，可以使用 Queue 实现多进程之间的数据传递。put 方法用以插入数据到队列中，put 方法还有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，该方法会阻塞 timeout 指定的时间，直到该队列有剩余的空间。如果超时，会抛出 Queue.Full 异常。如果 blocked 为 False，但该 Queue 已满，会立即抛出 Queue.Full 异常。 get 方法可以从队列读取并且删除一个元素。同样，get 方法有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，那么在等待时间内没有取到任何元素，会抛出 Queue.Empty 异常。如果 blocked 为 False，有两种情况存在，如果 Queue 有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出 Queue.Empty 异常。\nimport multiprocessing def func(i, q): q.put(i) print(\u0026quot;---\u0026gt;\u0026quot;, i, q.qsize()) q = multiprocessing.Queue() for i in range(5): p = multiprocessing.Process(target=func, args=(i, q,)) p.start() p.join()  4. Redis 数据库结构有那些？ String（字符串），Hash（哈希），List（列表），Set（集合）及 zset(sortedset：有序集合\n5. MonggoDB 中存入了 100 万条数据，如何提高查询速度？ 索引在很多数据库中是提高性能的标志优化手段，所以在大数据量的情况下索引可以提高数据的查询速度，如果没有索引 MongoDB 会扫描全部数据，才能获取满足条件的内容，在关系数据库中可以使用强制索引方式查询数据库，确保更准确快速的查询到满足条件的数据。 语法： 1、ensureIndex() 基本语法 1 创建升序索引 -1 创建降序索引 2、mongodb 默认所以字段 _id , 创建文档，会自动创建，此索引不能删除由 mongodb 自己维护相关参数： unique 创建唯一索引，默认 false ，true 必须唯一索引，否则报错 实例：1、创建升序索引\ndb.user.ensureIndex({age:1}); db.user.find({age:{$gte:20}});  6. 如何提高并发性能？ 我们常规处理并发的解决方案：\n 动态页面静态化。 制作数据库散列表，即分库分表。 增加缓存。 增加镜像。 部署集群。 负载均衡。 异步读取，异步编程。 创建线程池和自定义连接池，将数据持久化。 把一件事，拆成若干件小事，启用线程，为每个线程分配一定的事做，多个线程同时进行把该事件搞定再合并。  7. 归并排序的时间复杂度？ 合并排序是比较复杂的排序，特别是对于不了解分治法基本思想的同学来说可能难以理解。总时间 = 分解时间 + 解决问题时间 + 合并时间。分解时间就是把一个待排序序列分解成两序列，时间为一常数，时间复杂度 o(1). 解决问题时间是两个递归式，把一个规模为 n 的问题分成两个规模分别为 n/2 的子问题，时间为 2T(n/2). 合并时间复杂度为 o（n）。总时间 T(n)=2T(n/2)+o(n). 这个递归式可以用递归树来解，其解是 o(nlogn). 此外在最坏、最佳、平均情况下归并排序时间复杂度均为 o(nlogn). 从合并过程中可以看出合并排序稳定。 用递归树的方法解递归式 T(n)=2T(n/2)+o(n): 假设解决最后的子问题用时为常数 c，则对于 n 个待排序记录来说整个问题的规模为 cn。 从这个递归树可以看出，第一层时间代价为 cn，第二层时间代价为 cn/2+cn/2=cn…… 每一层代价都是 cn，总共有 logn+1 层。所以总的时间代价为 cn*(logn+1). 时间复杂度是 o(nlogn)。\n七、北京号外科技爬虫面试题 1. 单引号、双引号、三引号的区别？ 这几个符号都是可以表示字符串的，如果是表示一行，则用单引号或者双引号表示，它们的区别是如果内容里有 \u0026quot; 符号，并且你用双引号表示的话则需要转义字符，而单引号则不需要。 三单引号和三双引号也是表示字符串，并且可以表示多行，遵循的是所见即所得的原则。 另外，三双引号和三单引号可以作为多行注释来用，单行注释用 #号。\n2. 如何在一个 function 里面设置一个全局变量？ Global 声明。\n3. 描述 yield 使用场景？ 生成器。 当有多个返回值时，用 return 全部一起返回了，需要单个逐一返回时可以用 yield。\n4. 生成 1~10 之间的整数？ for i in range(1,11) 生成器：(i for i in range(1,10))\n5. Python 如何生成缩略图？ import os import glob from PIL import Image def thumbnail_pic(path): a=glob.glob(r'./*.jpg') for x in a: name=os.path.join(path,x) im=Image.open(name) im.thumbnail((80,80)) print(im.format,im.size,im.mode) im.save(name,'JPEG') print('Done!') if __name__=='__main__': path='.' thumbnail_pic(path）  6. 列出比较熟悉的爬虫框架，并简要说明？ (1) Scrapy 框架：很强大的爬虫框架，可以满足简单的页面爬取（比如可以明确获知 url pattern 的情况）。用这个框架可以轻松爬下来如亚马逊商品信息之类的数据。但是对于稍微复杂一点的页面，如 weibo 的页面信息，这个框架就满足不了需求了。 (2) Crawley: 高速爬取对应网站的内容，支持关系和非关系数据库，数据可以导出为 JSON、XML 等 (3) Portia: 可视化爬取网页内容 (4) newspaper: 提取新闻、文章以及内容分析 (5) python-goose:java 写的文章提取工具 (6) Beautiful Soup: 名气大，整合了一些常用爬虫需求。缺点：不能加载 JS。 (7) mechanize: 优点：可以加载 JS。缺点：文档严重缺失。不过通过官方的 example 以及人肉尝试的方法，还是勉强能用的。 (8) selenium: 这是一个调用浏览器的 driver，通过这个库你可以直接调用浏览器完成某些操作，比如输入验证码。 (9) cola: 一个分布式爬虫框架。项目整体设计有点糟，模块间耦合度较高。\n7. 列举常见的反爬技术，并给出应对方案？ 1. Headers： 从用户的 headers 进行反爬是最常见的反爬虫策略。Headers（上一讲中已经提及） 是一种区分浏览器行为和机器行为中最简单的方法，还有一些网站会对 Referer （上级链接）进行检测（机器行为不太可能通过链接跳转实现）从而实现爬虫。 解决措施：通过审查元素或者开发者工具获取相应的 headers 然后把相应的 headers 传输给 python 的 requests，这样就能很好地绕过。\n2. IP 限制 一些网站会根据你的 IP 地址访问的频率，次数进行反爬。也就是说如果你用单一的 IP 地址访问频率过高，那么服务器会在短时间内禁止这个 IP 访问。 解决措施：构造自己的 IP 代理池，然后每次访问时随机选择代理（但一些 IP 地址不是非常稳定，需要经常检查更新）。\n3. UA 限制 UA 是用户访问网站时候的浏览器标识，其反爬机制与 ip 限制类似。 解决措施：构造自己的 UA 池，每次 python 做 requests 访问时随机挂上 UA 标识，更好地模拟浏览器行为。当然如果反爬对时间还有限制的话，可以在 requests 设置 timeout（最好是随机休眠，这样会更安全稳定，time.sleep()）。\n4. 验证码反爬虫或者模拟登陆 验证码：这个办法也是相当古老并且相当的有效果，如果一个爬虫要解释一个验证码中的内容，这在以前通过简单的图像识别是可以完成的，但是就现在来讲，验证码的干扰线，噪点都很多，甚至还出现了人类都难以认识的验证码。 解决措施：验证码识别的基本方法：截图，二值化、中值滤波去噪、分割、紧缩重排（让高矮统一）、字库特征匹配识别。（python 的 PIL 库或者其他） 模拟登陆（例如知乎等）：用好 python requests 中的 session(下面几行代码实现了最简单的 163 邮箱的登陆，其实原理是类似的～）。\nimport requests s =requests.session() login_data={\u0026quot;account\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;password\u0026quot;:\u0026quot;\u0026quot;} res=s.post(\u0026quot;http://mail.1com/\u0026quot;,login_data)  5.Ajax 动态加载  网页的不希望被爬虫拿到的数据使用 Ajax 动态加载，这样就为爬虫造成了绝大的麻烦，如果一个爬虫不具备 js 引擎，或者具备 js 引擎，但是没有处理 js 返回的方案，或者是具备了 js 引擎，但是没办法让站点显示启用脚本设置。基于这些情况，ajax 动态加载反制爬虫还是相当有效的。 Ajax 动态加载的工作原理是：从网页的 url 加载网页的源代码之后，会在浏览器里执行 JavaScript 程序。这些程序会加载出更多的内容，并把这些内容传输到网页中。这就是为什么有些网页直接爬它的 URL 时却没有数据的原因。 处理方法：若使用审查元素分析请求对应的链接(方法：右键→审查元素→Network→清空，点击加载更多，出现对应的 GET 链接寻找 Type 为 text/html 的，点击，查看 get 参数或者复制 Request URL)，循环过程。如果请求之前有页面，依据上一步的网址进行分析推导第 1 页。以此类推，抓取抓 Ajax 地址的数据。对返回的 json 使用 requests 中的 json 进行解析，使用 eval（）转成字典处理（上一讲中的 fiddler 可以格式化输出 json 数据。\n6.cookie 限制 一次打开网页会生成一个随机 cookie，如果再次打开网页这个 cookie 不存在，那么再次设置，第三次打开仍然不存在，这就非常有可能是爬虫在工作了。 解决措施：在 headers 挂上相应的 cookie 或者根据其方法进行构造（例如从中选取几个字母进行构造）。如果过于复杂，可以考虑使用 selenium 模块（可以完全模拟浏览器行为）。\n8. 网络协议 http 和 https 区别？ HTTP：是互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从 WWW 服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。 HTTPS：是以安全为目标的 HTTP 通道，简单讲是 HTTP 的安全版，即 HTTP 下加入 SSL 层，HTTPS 的安全基础是 SSL，因此加密的详细内容就需要 SSL。 HTTPS 协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。\n9. 什么是 cookie，session 有什么区别？ 1、cookie 数据存放在客户的浏览器上，session 数据放在服务器上。 2、cookie 不是很安全，别人可以分析存放在本地的 cookie 并进行 cookie 欺骗，考虑到安全应当使用 session。 3、session 会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用 cookie。 4、单个 cookie 保存的数据不能超过 4K，很多浏览器都限制一个站点最多保存 20 个 cookie。 5、可以考虑将登陆信息等重要信息存放为 session，其他信息如果需要保留，可以放在 cookie 中。\n10. Mysql 中 myisam 与 innodb 的区别？ 1、 存储结构 MyISAM：每个 MyISAM 在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm 文件存储表定义。数据文件的扩展名为.MYD(MYData)。索引文件的扩展名是.MYI(MYIndex)。 InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB 表的大小只受限于操作系统文件的大小，一般为 2GB。\n2、 存储空间 MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。 InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。\n3、 事务支持 MyISAM：强调的是性能，每次查询具有原子性，其执行数度比 InnoDB 类型更快，但是不提供事务支持。 InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback) 和崩溃修复能力(crash recovery capabilities) 的事务安全(transaction-safe(ACID compliant)) 型表。\n4、 CURD 操作 MyISAM：如果执行大量的 SELECT，MyISAM 是更好的选择。(因为没有支持行级锁)，在增删的时候需要锁定整个表格，效率会低一些。相关的是 innodb 支持行级锁，删除插入的时候只需要锁定改行就行，效率较高 InnoDB：如果你的数据执行大量的 INSERT 或 UPDATE，出于性能方面的考虑，应该使用 InnoDB 表。DELETE 从性能上 InnoDB 更优，但 DELETE FROM table 时，InnoDB 不会重新建立表，而是一行一行的删除，在 innodb 上如果要清空保存有大量数据的表，最好使用 truncate table 这个命令。\n5、 外键 MyISAM：不支持 InnoDB：支持\n八、首信 Python 研发面试 1. Python 中 list、tuple、dict、set 有什么区别，主要应用在什么样的场景？并用 for 语句遍历？ 区别： 1、list、tuple 是有序列表；dict、set 是无序列表； 2、list 元素可变、tuple 元素不可变； 3、dict 和 set 的 key 值不可变，唯一性； 4、set 只有 key 没有 value； 5、set 的用途：去重、并集、交集等； 6、list、tuple：+、*、索引、切片、检查成员等； 7、dict 查询效率高，但是消耗内存多；list、tuple 查询效率低、但是消耗内存少;\n应用场景： list,：简单的数据集合，可以使用索引； tuple：把一些数据当做一个整体去使用，不能修改； dict：使用键值和值进行关联的数据； set：数据只出现一次，只关心数据是否出现，不关心其位置；列表遍历：\na_list = [1, 2, 3, 4, 5] for num in a_list: print(num,end=' ') a_turple = (1, 2, 3, 4, 5) for num in a_turple: print(num,end=\u0026quot; \u0026quot;) a_dict = {'name':'xiaoming','sex':'man'} for key in a_dict.key(): print(key) for value in dict.value(): print(value) for item in dict.items() print(key,value) s = set(['Adam', 'Lisa', 'Bart']) for name in s: print(name)  2. 用 Python 语言写一个函数，输入一个字符串，返回倒序结果？ def test() strA = raw_input(\u0026quot;请输入需要翻转的字符串：\u0026quot;) order = [] for i in strA: order.append(i) order.reverse() #将列表反转 print(''.join(order))# 将 list 转换成字符串 test()  3. 介绍一下 Python 的异常处理机制和自己开发过程中的体会？  默认的异常处理器 代码如下:  s = 'Hello girl!' print(s[100] ) print('continue')  如果我们没有对异常进行任何预防，那么在程序执行的过程中发生异常，就会中断程序，调用 python 默认的异常处理器，并在终端输出异常信息。这种情况下，第 3 行代码不会执行。\ntry…except 代码如下:  s = 'Hello girl!' try: print(s[100]) except IndexError: print('error...') print('continue')  程序执行到第 2 句时发现 try 语句，进入 try 语句块执行，发生异常，回到 try 语句层，寻找后面是否有 except 语句。找到 except 语句后，会调用这个自定义的异常处理器。except 将异常处理完毕后，程序继续往下执行。这种情况下，最后两个 print 语句都会执行。 except 后面也可以为空，表示捕获任何类型的异常。\ntry…finally 代码如下:  s = 'Hello girl!' try: print(s[100]) finally: print('error...' ) print('continue finally' )  语句表示，无论异常发生与否，finally 中的语句都要执行。但是，由于没有 except 处理器，finally 执行完毕后程序便中断。这种情况下，倒第 2 个 print 会执行，到第 1 个不会执行。如果 try 语句中没有异常，三个 print 都会执行。\nassert 代码如下:  assert False,'error..' print('continue')  这个语句，先判断 assert 后面紧跟的语句是 True 还是 False, 如果是 True 则继续执行 print, 如果是 False 则中断程序，调用默认的异常处理器，同时输出 assert 语句逗号后面的提示信息。本例情况下，程序中断，提示 error, 后面的 print 不执行。\nwith…as` 代码如下:  with open('nothing.txt','r') as f: f.read() print(2/0) print('continue')  我们平时在使用类似文件的流对象时，使用完毕后要调用 close 方法关闭，很麻烦。这里 with…as 语句提供了一个非常方便的替代方法:open 打开文件后将返回的文件流对象赋值给 f, 然后在 with 语句块中使用。with 语句块完毕之后，会隐藏地自动关闭文件。 如果 with 语句或语句块中发生异常，会调用默认的异常处理器处理，但文件还是会正常关闭。 这种情况下，会抛出异常，最后的 print 不执行。\n4. jQuery 库中 $() 是什么？网上有 5 个元素，如何使用 jQuery 来选择它们？ () 函 数 是 J Q u e r y 函 数 的 别 称 ， () 函数是 JQuery 函数的别称， () 函数是 JQuery 函数的别称，() 函数用于将任何对象包裹成 jQuery 对象，接着就可以被允许调用定义在 jQuery 对象上的多个不同方法。甚至可以将一个选择器字符串传入 $() 函数，它会返回一个包含所有匹配的 DOM 元素数组的 jQuery 对象。可以用 each() 方法进行遍历里面的对象。 选择元素：这个问题是 jQuery 基于选择器的。jQuery 支持不同类型的选择器，有 ID 选择器、class 选择器、标签选择器。这个问题的答案是使用标签选择器来选择所有的 div 元素。 jQuery 代码： $(\u0026lsquo;div\u0026rsquo;). 其返回值是一个包含 5 个 div 标签的 jQuery 对象。\n5. 写一个 Bash Shell 脚本来得到当前的日期、时间、用户名和当前工作目录？ 输出用户名，当前日期和时间，以及当前工作目录的命令就是 logname，date，who i am 和 pwd。\n6. Django 中使用 memcached 作为缓存的具体方法？有缺点说明？ memcached 是一种缓存技术，基于 c/s 模式，他可以把你的数据放入内存，从而通过内存访问提速，因为内存最快的。\n九、微影时代 1. HTTP 头有什么字段？ 每个 HTTP 请求和响应都会带有相应的头部信息。默认情况下，在发送 XHR 请求的同时，还会发送下列头部信息： Accept: 浏览器能够处理的内容类型 Accept-Charset: 浏览器能够显示的字符集 Accept-Encoding：浏览器能够处理的压缩编码 Accept-Language：浏览器当前设置的语言 Connection：浏览器与服务器之间连接的类型 Cookie：当前页面设置的任何 Cookie Host：发出请求的页面所在的域 Referer：发出请求的页面的 URL User-Agent：浏览器的用户代理字符串 HTTP 响应头部信息: Date：表示消息发送的时间，时间的描述格式由 rfc822 定义 server: 服务器名字。 Connection：浏览器与服务器之间连接的类型 content-type: 表示后面的文档属于什么 MIME 类型 Cache-Control：控制 HTTP 缓存\n2. POST 登录数据方式？ HTTP 协议是以 ASCII 码传输，建立在 TCP/IP 协议之上的应用层规范。规范把 HTTP 请求分为三个部分：状态行、请求头、消息主体。协议规定 POST 提交的数据必须放在消息主体（entity-body）中，但协议并没有规定数据必须使用什么编码方式。\n十、斯沃创智 1. 简述 Python 中 is 和 = = 的区别 Python 中的对象包含三要素：id、type、value。 其中 id 用来唯一标识一个对象，type 标识对象的类型，value 是对象的值。is 判断的是 a 对象是否就是 b 对象，是通过 id 来判断的。== 判断的是 a 对象的值是否和 b 对象的值相等，是通过 value 来判断的。\n2. 简述 read,readline 和 readlines 的区别 1.read() 每次读取整个文件，它通常将读取到底文件内容放到一个字符串变量中，也就是说 .read() 生成文件内容是一个字符串类型； 2.readline() 每只读取文件的一行，通常也是读取到的一行内容放到一个字符串变量中，返回 str 类型； 3.readlines() 每次按行读取整个文件内容，将读取到的内容放到一个列表中，返回 list 类型；\n3. 举例说明创建字典的至少两种方法 1.用 {} 创建字典 2.用内置函数 dict()\n4. 简述 Python 里面 search 和 match 的区别 match() 函数只检测 RE 是不是在 string 的开始位置匹配，search() 会扫描整个 string 查找匹配；也就是说 match() 只有在 0 位置匹配成功的话才有返回，如果不是开始位置匹配成功的话，match() 就返回 none； 例如：print(re.match(super\u0026rsquo;, superstition\u0026rsquo;).span()) 会返回(0, 5) 而 print(re.match(super\u0026rsquo;, insuperable\u0026rsquo;)) 则返回 None\nsearch() 会扫描整个字符串并返回第一个成功的匹配： 例如：print(re.search(super\u0026rsquo;, superstition\u0026rsquo;).span()) 返回(0, 5) print(re.search(super\u0026rsquo;, insuperable\u0026rsquo;).span()) 返回(2, 7) 其中 span 函数定义如下，返回位置信息： span([group]):返回(start(group), end(group))。\n5. Python 代码实现：删除一个 list 里面的重复元素 方法一：是利用 map 的 fromkeys 来自动过滤重复值，map 是基于 hash 的，大数组的时候应该会比排序快点。 方法二：是用 set(),set 是定义集合的，无序，非重复。 方法三：是排序后，倒着扫描，遇到已有的元素删之。\n#!/usr/bin/python #coding=utf-8 ''' Created on 2012-2-22 Q: 给定一个列表，去掉其重复的元素，并输出 ''' def distFunc1(): a=[1,2,4,2,4,5,6,5,7,8,9,0] b={} b=b.fromkeys(a) print b #print b.keys() a=list(b.keys()) print a def distFunc2(): a=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3] a=list(set(a)) # set 是非重复的，无序集合。可以用 list 来的排队对 set 进行排序，list() 转换为列表，a.sort 来排序 print a def distFunc3(): #可以先把 list 重新排序，然后从 list 的最后开始扫描，代码如下： List=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3] if List: List.sort() #print List last = List [-1] #print last for i in range(len(List)-2, -1, -1): if last==List [i]: del List [i] else: last=List [i] if __name__ == '__main__': distFunc1() distFunc2() distFunc3()  6. Python 代码中(*args, **kwargs) 是什么意思 *args 表示任何多个无名参数，它是一个 tuple。 **kwargs 表示关键字参数，它是一个 dict。\n十一、天广汇通 1. 说明 os.path 和 sys.path 分别代表什么？ sys.path 是喜闻乐见的 PATH 环境变量，os.path 是一个 module，提供 split、join、basename 等处理目录、文件名的工具。\n2. 解释一下并行（parallel）和并发（concurrency）的区别 并行（parallel）是指同一时刻，两个或两个以上时间同时发生。 并发（parallel）是指同一时间间隔（同一段时间），两个或两个以上时间同时发生。\n3. 在 Python 中可以实现并发的库有哪些？ 1）线程 2）进程 3）协程 4）threading。\n4. 如果一个程序需要进行大量的 IO 操作，应当使用并行还是并发？ 并发。\n十二、信德数据 1. 网络七层协议是哪几层？HTTP 协议输入是第几层？ 7 层从上到下分别是 应用层 表示层 会话层 传输层 网络层 数据链路层 物理层\n其中高层（即 7、6、5、4 层）定义了应用程序的功能，下面 3 层（即 3、2、1 层）主要面向通过网络的端到端的数据流。 HTTP 属于应用层。\n2. 什么是 HTTP 协议？HTTP 请求有哪几种？ HTTP 是 hypertext transfer protocol（超文本传输协议）的简写，它是 TCP/IP 协议的一个应用层协议，用于定义 WEB 浏览器与 WEB 服务器之间交换数据的过程。客户端连上 web 服务器后，若想获得 web 服务器中的某个 web 资源，需遵守一定的通讯格式，HTTP 协议用于定义客户端与 web 服务器通迅的格式。 HTTP 请求有 8 种: OPTIONS / HEAD / GET / POST / PUT / DELETE / TRACE / CONNECT 。\n3. 什么是 HTTP 代理？作用是什么？ 代理服务器英文全称是 Proxy Server，其功能就是代理网络用户去取得网络信息。形象的说：它是网络信息的中转站。 代理服务器可以实现各种时髦且有用的功能。它们可以改善安全性，提高性能，节省费用。\n4. 什么是反向代理？作用是什么？ 代理可以假扮 Web 服务器。这些被称为替换物(surrogate) 或反向代理(reverse proxy) 的代理接收发送给 Web 服务器的真实请求，但与 Web 服务器不同的是，它们可以发起与其他服务器的通信，以便按需定位所请求的内容。 可以用这些反向代理来提高访问慢速 Web 服务器上公共内容的性能。在这种配置中，通常将这些反向代理称为服务器加速器(server accelerator)。还可以将替换物与内容路由功能配合使用，以创建按需复制内容的分布式网络。\n5. HTTPS 和 HTTP 的区别 1）https 协议需要到 ca 申请证书，一般免费证书很少，需要交费。 2）http 是超文本传输协议，信息是明文传输，https 则是具有安全性的 ssl 加密传输协议。 3）http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。 4）http 的连接很简单，是无状态的；HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、身份认证的网络协议，比 http 协议安全。\n十三、成安 1. Python 的 logging 模块常用的几个等级？ critical \u0026gt; error \u0026gt; warning \u0026gt; info \u0026gt; debug,notset 级别越高打印的日志越少，反之亦然，即 Debug : 打印全部的日志(notset 等同于 debug) info : 打印 info,warning,error,critical 级别的日志 warning : 打印 warning,error,critical 级别的日志 error : 打印 error,critical 级别的日志 critical : 打印 critical 级\n2. 在 HTTP1.1 中常见的状态码有哪些，如何设置状态码？ 1XX Informational 信息性状态码，表示接受的请求正在处理 2XX Success 成功状态码，表示请求正常处理完毕 3XX Redirection 重定向状态码，表示需要客户端需要进行附加操作 4XX Client Error 客户端错误状态码，表示服务器无法处理请求 5XX Server Error 服务器错误状态码，表示服务器处理请求出错\n3. Python 如何处理上传文件？ Python 中使用 GET 方法实现上传文件，下面就是用 Get 上传文件的例子，client 用来发 Get 请求，server 用来收请求。\n请求端代码： import requests #需要安装 requests with open('test.txt', 'rb') as f: requests.get('http:// 服务器 IP 地址：端口 ', data=f) 服务端代码： var http = require('http'); var fs = require('fs'); var server = http.createServer(function(req, res){//console.log(req); var recData = \u0026quot;\u0026quot;; req.on('data', function(data){recData += data;}) req.on('end', function(data){ recData += data; fs.writeFile('recData.txt', recData, function(err){console.log('file received'); }) }) res.end('hello'); }) server.listen(端口);  十四、博派通达 1.请列举你使用过的 Python 代码检测工具 2.移动应用自动化测试 Appium 3.OpenStack 集成测试 Tempest 4.自动化测试框架 STAF 5.自动化测试平台 TestMaker 6.JavaScript 内存泄露检测工具 Leak Finder 7.Python 的 Web 应用验收测试 Splinter 8.即插即用设备调试工具 UPnP-Inspector\n1. 简述 Python 垃圾回收机制和如何解决循环引用 引用计数：是一种垃圾收集机制，而且也是一种最直观，最简单的垃圾收集技术，当一个对象的引用被创建或者复制时，对象的引用计数加 1；当一个对象的引用被销毁时，对象的引用计数减 1；当对象的引用计数减少为 0 时，就意味着对象已经没有被任何人使用了，可以将其所占用的内存释放了。虽然引用计数必须在每次分配和释放内存的时候加入管理引用计数的动作，然而与其他主流的垃圾收集技术相比，引用计数有一个最大的有点，即实时性，任何内存，一旦没有指向它的引用，就会立即被回收。而其他的垃圾收集计数必须在某种特殊条件下（比如内存分配失败）才能进行无效内存的回收。\n引用计数机制执行效率问题：引用计数机制所带来的维护引用计数的额外操作与 Python 运行中所进行的内存分配和释放，引用赋值的次数是成正比的。而这点相比其他主流的垃圾回收机制，比如标记 - 清除，停止 - 复制，是一个弱点，因为这些技术所带来的额外操作基本上只是与待回收的内存数量有关。 如果说执行效率还仅仅是引用计数机制的一个软肋的话，那么很不幸，引用计数机制还存在着一个致命的弱点，正是由于这个弱点，使得侠义的垃圾收集从来没有将引用计数包含在内，能引发出这个致命的弱点就是循环引用（也称交叉引用）。\n问题说明： 循环引用可以使一组对象的引用计数不为 0，然而这些对象实际上并没有被任何外部对象所引用，它们之间只是相互引用。这意味着不会再有人使用这组对象，应该回收这组对象所占用的内存空间，然后由于相互引用的存在，每一个对象的引用计数都不为 0，因此这些对象所占用的内存永远不会被释放。 比如：这一点是致命的，这与手动进行内存管理所产生的内存泄露毫无区别。 要解决这个问题，Python 引入了其他的垃圾收集机制来弥补引用计数的缺陷：标记 - 清除，分代回收两种收集技术。 标记 - 清除：标记 - 清除是为了解决循环引用的问题。可以包含其他对象引用的容器对象（比如：list，set，dict，class，instance）都可能产生循环引用。 我们必须承认一个事实，如果两个对象的引用计数都为 1，但是仅仅存在他们之间的循环引用，那么这两个对象都是需要被回收的，也就是说，它们的引用计数虽然表现为非 0，但实际上有效的引用计数为 0。我们必须先将循环引用摘掉，那么这两个对象的有效计数就现身了。假设两个对象为 A、B，我们从 A 出发，因为它有一个对 B 的引用，则将 B 的引用计数减 1；然后顺着引用达到 B，因为 B 有一个对 A 的引用，同样将 A 的引用减 1，这样，就完成了循环引用对象间环摘除。 但是这样就有一个问题，假设对象 A 有一个对象引用 C，而 C 没有引用 A，如果将 C 计数引用减 1，而最后 A 并没有被回收，显然，我们错误的将 C 的引用计数减 1，这将导致在未来的某个时刻出现一个对 C 的悬空引用。这就要求我们必须在 A 没有被删除的情况下复原 C 的引用计数，如果采用这样的方案，那么维护引用计数的复杂度将成倍增加。 原理：标记 - 清除采用了更好的做法，我们并不改动真实的引用计数，而是将集合中对象的引用计数复制一份副本，改动该对象引用的副本。对于副本做任何的改动，都不会影响到对象生命走起的维护。 这个计数副本的唯一作用是寻找 root object 集合（该集合中的对象是不能被回收的）。当成功寻找到 root object 集合之后，首先将现在的内存链表一分为二，一条链表中维护 root object 集合，成为 root 链表，而另外一条链表中维护剩下的对象，成为 unreachable 链表。之所以要剖成两个链表，是基于这样的一种考虑：现在的 unreachable 可能存在被 root 链表中的对象，直接或间接引用的对象，这些对象是不能被回收的，一旦在标记的过程中，发现这样的对象，就将其从 unreachable 链表中移到 root 链表中；当完成标后，unreachable 链表中剩下的所有对象就是名副其实的垃圾对象了，接下来的垃圾回收只需限制在 unreachable 链表中即可。 分代回收 背景：分代的垃圾收集技术是在上个世纪 80 年代初发展起来的一种垃圾收集机制，一系列的研究表明：无论使用何种语言开发，无论开发的是何种类型，何种规模的程序，都存在这样一点相同之处。即：一定比例的内存块的生存周期都比较短，通常是几百万条机器指令的时间，而剩下的内存块，起生存周期比较长，甚至会从程序开始一直持续到程序结束。 从前面标记 - 清除这样的垃圾收集机制来看，这种垃圾收集机制所带来的额外操作实际上与系统中总的内存块的数量是相关的，当需要回收的内存块越多时，垃圾检测带来的额外操作就越多，而垃圾回收带来的额外操作就越少；反之，当需回收的内存块越少时，垃圾检测就将比垃圾回收带来更少的额外操作。为了提高垃圾收集的效率，采用空间换时间的策略。 原理：将系统中的所有内存块根据其存活时间划分为不同的集合，每一个集合就成为一个代，垃圾收集的频率随着代的存活时间的增大而减小。也就是说，活得越长的对象，就越不可能是垃圾，就应该减少对它的垃圾收集频率。那么如何来衡量这个存活时间：通常是利用几次垃圾收集动作来衡量，如果一个对象经过的垃圾收集次数越多，可以得出：该对象存活时间就越长。\n2. 请简述如何编写清晰可读的的代码 一、写 pythonic 代码 二、理解 Python 和 C 语言的不同之处 三、在代码中适当添加注释 Python 中有三种形式的代码注释：块注释、行注释以及文档注释。 使用块注释或者行注释的时候仅仅注释那些复杂的操作、算法，或者难以理解，不能一目了然的代码给外部可访问的函数和方法（无论简单与否）添加文档注释。注释要清楚的描述方法的功能，并对参数、返回值以及可能发生的异常进行说明，使得外部调用它的人员仅仅看文档注释就能正确使用。较为复杂的内部方法也需要进行注释。 四、通过适当添加空行使代码布局更为优雅、合理。 五、编写函数的 4 个原则 1）函数设计要尽量短小 2）函数声明要做到合理、简单、易于使用 3）函数参数设计应该考虑向下兼容 4）一个函数只做一件事情，尽量保证函数语句粒度的一致性 六、将常量集中到一个文件 在 Python 中如何使用常量呢，一般来说有一下两种方式： 1）通过命名风格来提醒使用者该变量代表的意义为常量。如 TOTAL，MAX_OVERFLOW，然而这种方式并没有实现真正的常量，其对应的值仍然可以改变，这只是一种约定俗成的风格。 2）通过自定义的类实现常量功能，这要求符合命名全部为大写和值一旦绑定便不可再修改这两个条件。\n5. 请列出 MySQL 数据库查询的技巧  对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null 可以在 num 上设置默认值 0，确保表中 num 列没有 null 值，然后这样查询：select id from t where num=0。 应尽量避免在 where 子句中使用！= 或 \u0026lt;\u0026gt; 操作符，否则引擎将放弃使用索引而进行全表扫描。 应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描， 如：select id from t where num=10 or num=20 可以这样查询：select id from t where num=10 union all select id from t where num=20。 5.in 和 not in 也要慎用，否则会导致全表扫描，如：select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了：select id from t where num between 1 and 3。 下面的查询也将导致全表扫描：select id from t where name like % 李 %\u0026lsquo;若要提高效率，可以考虑全文检索。 如果在 where 子句中使用参数，也会导致全表扫描。因为 SQL 只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： select id from t where num=@num 可以改为强制查询使用索引：select id from t with(index(索引名)) where num=@num。 应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。 如：select id from t where num/2=100 应改为:select id from t where num=100_2。 应尽量避免在 where 子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。 如：select id from t where substring(name,1,3)=\u0026lsquo;abc\u0026rsquo; ，name 以 abc 开头的 id 应改为:select id from t where name like abc%\u0026rsquo;。 不要在 where 子句中的 = 左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 不要写一些没有意义的查询，如需要生成一个空表结构： select col1,col2 into #t from t where 1=0； 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：create table #t(…)。 很多时候用 exists 代替 in 是一个好的选择： select num from a where num in(select num from b)； 用下面的语句替换：select num from a where exists(select 1 from b where num=a.num) 并不是所有索引对查询都有效，SQL 是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL 查询可能不会去利用索引，如一表中有字段 sex，male、female 几乎各一半，那么即使在 sex 上建了索引也对查询效率起不了作用。 索引并不是越多越好，索引固然可 以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过 6 个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。 应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 任何地方都不要使用 select * from t ，用具体的字段列表代替_，不要返回用不到的任何字段。 尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 避免频繁创建和删除临时表，以减少系统表资源的消耗。 临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先 create table，然后 insert。 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过 1 万行，那么就应该考虑改写。 使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 与临时表一样，游标并不是不可使 用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括合计的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。 尽量避免大事务操作，提高系统并发能力。 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。  8. 请列出常见的 HTTP 头及其作用 http 请求中的常用头（请求头）的含义： Accept：告诉服务器，客户端支持的数据类型。 Accept-Charset：告诉服务器，客户端采用的编码。 Accept-Encoding：告诉服务器，客户机支持的数据压缩格式。 Accept-Language：告诉服务器，客户机的语言环境。 Host：客户机通过这个头告诉服务器，想访问的主机名。 If-Modified-Since: 客户机通过这个头告诉服务器，资源的缓存时间。 Referer: 客户机通过这个头告诉服务器，它是从哪个资源来访问服务器的。（一般用于防盗链） User-Agent: 客户机通过这个头告诉服务器，客户机的软件环境。 Cookie：客户机通过这个头告诉服务器，可以向服务器带数据。 cookie 是临时文件的意思，保存你浏览网页的痕迹，使得再次上同一页面的时候提高网速，判断你是否登录过这个网站，有些可以帮你自动登录的。 Cookie 就是服务器暂存放在你的电脑里的资料（.txt 格式的文本文件），通过在 HTTP 传输中的状态好让服务器用来辨认你的计算机。当你在浏览网站的时候，Web 服务器会先送一小小资料放在你的计算机上，Cookie 会帮你在网站上所打的文字或是一些选择都记录下来。当下次你再访问同一个网站，Web 服务器会先看看有没有它上次留下的 Cookie 资料，有的话，就会依据 Cookie 里的内容来判断使用者，送出特定的网页内容给你。 http 请求是指从客户端到服务器端的请求消息。包括：消息首行中，对资源的请求方法、资源的标识符及使用的协议。 Connection：客户机通过这个头告诉服务器，请求完后是关闭还是保持链接。 Date：客户机通过这个头告诉服务器，客户机当前请求时间。 http 请求中常用的响应头的含义： Location: 这个头配合 302 状态码使用，告诉用户端找谁。 Server: 服务器通过这个头，告诉浏览器服务器的类型 Content-Encoding: 服务器通过这个头，告诉浏览器数据采用的压缩格式。 Content-Length: 服务器通过这个头，告诉浏览器回送数据的长度。 Content-Language：服务器通过这个头，告诉服务器的语言环境。 Content-Type: 服务器通过这个头，回送数据的类型 Last-Modified: 服务器通过这个头，告诉浏览器当前资源的缓存时间。 Refresh: 服务器通过这个头，告诉浏览器隔多长时间刷新一次。 Content-Disposition: 服务器通过这个头，告诉浏览器以下载的方式打开数据。 Transfer-Encoding: 服务器通过这个头，告诉浏览器数据的传送格式。 ETag: 与缓存相关的头。 Expires: 服务器通过这个头，告诉浏览器把回送的数据缓存多长时间。-1 或 0 不缓存。 Cache-Control 和 Pragma：服务器通过这个头，也可以控制浏览器不缓存数据。 Connection: 服务器通过这个头，响应完是保持链接还是关闭链接。 Date: 告诉客户机，返回响应的时间。\n9. 请列举常见的 HTTP 状态码及其意义 成功 2×× 成功处理了请求的状态码。 200 服务器已成功处理了请求并提供了请求的网页。 204 服务器成功处理了请求，但没有返回任何容。\n重定向 3×× 每次请求中使用重定向不要超过 5 次。 301 请求的网页已永久移动到新位置。当 URLs 发生变化时，使用 301 代码。搜索引擎索引中保存新的 URL。 302 请求的网页临时移动到新位置。搜索引擎索引中保存原来的 URL。 304 如果网页自请求者上次请求后没有更新，则用 304 代码告诉搜索引擎机器人， 可节省带宽和开销。 客户端错误 4×× 表示请求可能出错，妨碍了服务器的处理。 400 服务器不理解请求的语法。 403 服务器拒绝请求。 404 服务器找不到请求的网页。服务器上不存在的网页经常会返回此代码。 410 请求的资源永久删除后，服务器返回此响应。该代码与 404（未找到）代码相似，但在资源以前存在而现在不存在的情况下，有时用来替代 404 代码。如果资源已永久删除，应当使用 301 指定资源的新位置。\n服务器错误 5×× 表示服务器在处理请求时发生内部错误。这些错误可能是服务器本身的错误，而不是请求出错。 500 服务器遇到错误，无法完成请求。 503 服务器目前无法使用（由于超载或停机维护）。通常，这只是暂时状态。\n10. 请简述 RESTfulAPI 设计规范的理解 一、域名 将 api 部署在专用域名下：http://api.example.com。 或者将 api 放在主域名下：http://www.example.com/api/。\n二、版本 将 API 的版本号放在 url 中。http://www.example.com/app/1.0/info。\n三、路径 路径表示 API 的具体网址。每个网址代表一种资源。 资源作为网址，网址中不能有动词只能有名词， 一般名词要与数据库的表名对应。而且名词要使用复数。 错误示例：http://www.example.com/getGoods http://www.example.com/listOrders 正确示例： 获取单个商品 http://www.example.com/app/goods/1 获取所有商品 http://www.example.com/app/goods\n四、使用标准的 HTTP 方法： 对于资源的具体操作类型，由 HTTP 动词表示。 常用的 HTTP 动词有四个。 GET SELECT ：从服务器获取资源。 POST CREATE ：在服务器新建资源。 PUT UPDATE ：在服务器更新资源。 DELETE DELETE ：从服务器删除资源。 示例： 获取指定商品的信息 GET http://www.example.com/goods/ID 新建商品的信息 POST http://www.example.com/goods 更新指定商品的信息 PUT http://www.example.com/goods/ID 删除指定商品的信息 DELETE http://www.example.com/goods/ID\n五、过滤信息 如果资源数据较多，服务器不能将所有数据一次全部返回给客户端。API 应该提供参数，过滤返回结果。 实例： 指定返回数据的数量 http://www.example.com/goods?limit=10 指定返回数据的开始位置 http://www.example.com/goods?offset=10 指定第几页，以及每页数据的数量 http://www.example.com/goods?page=2\u0026per_page=20\n六、状态码 服务器向用户返回的状态码和提示信息，常用的有： 200 OK ：服务器成功返回用户请求的数据 201 CREATED ：用户新建或修改数据成功。 202 Accepted：表示请求已进入后台排队。 400 INVALID REQUEST ：用户发出的请求有错误。 401 Unauthorized ：用户没有权限。 403 Forbidden ：访问被禁止。 404 NOT FOUND ：请求针对的是不存在的记录。 406 Not Acceptable ：用户请求的的格式不正确。 500 INTERNAL SERVER ERROR ：服务器发生错误。\n七、错误信息 一般来说，服务器返回的错误信息，以键值对的形式返回。 {error:Invalid API KEY\u0026rsquo;}\n八、响应结果： 针对不同结果，服务器向客户端返回的结果应符合以下规范。 返回商品列表 GET http://www.example.com/goods 返回单个商品 GET http://www.example.com/goods/cup 返回新生成的商品 POST http://www.example.com/goods 返回一个空文档 DELETE http://www.example.com/goods 九、使用链接关联相关的资源\n在返回响应结果时提供链接其他 API 的方法，使客户端很方便的获取相关联的信息。\n十、其他 服务器返回的数据格式，应该尽量使用 JSON，避免使用 XML。\n11. 请简述标准库中 functools.wraps 的作用 Python 中使用装饰器对在运行期对函数进行一些外部功能的扩展。但是在使用过程中，由于装饰器的加入导致解释器认为函数本身发生了改变，在某些情况下 —— 比如测试时 —— 会导致一些问题。Python 通过 functool.wraps 为我们解决了这个问题：在编写装饰器时，在实现前加入 @functools.wraps(func) 可以保证装饰器不会对被装饰函数造成影响。\n十五、乐飞天下 1. 如何判断一个 python 对象的类型 type() isinstance()\n2. Python 里面如何生成随机数 Python 中的 random 函数，可以生成随机浮点数、整数、字符串，甚至帮助你随机选择列表序列中的一个元素，打乱一组数据等。\n3. 请写出匹配 ip 的 Python 正则表达式 iPv4 的 ip 地址都是（1255）.（0255）.（0255）.（0255）的格式。 下面给出相对应的正则表达式： ^(1\\d {2}|2 [0-4]\\d|25 [0-5]|[1-9]\\d|[1-9]). +\u0026quot;(1\\d {2}|2 [0-4]\\d|25 [0-5]|[1-9]\\d|\\d).\u0026quot; +\u0026quot;(1\\d {2}|2 [0-4]\\d|25 [0-5]|[1-9]\\d|\\d).\u0026quot; +\u0026quot;(1\\d {2}|2 [0-4]\\d|25 [0-5]|[1-9]\\d|\\d)$\u0026quot; 简单的讲解一下： \\d 表示 0~9 的任何一个数字 {2} 表示正好出现两次 [0-4] 表示 0~4 的任何一个数字 | 的意思是或者() 上面的括号不能少，是为了提取匹配的字符串，表达式中有几个() 就表示有几个相应的匹配字符串 1\\d {2} 的意思就是 100~199 之间的任意一个数字 2 [0-4]\\d 的意思是 200~249 之间的任意一个数字 25 [0-5] 的意思是 250~255 之间的任意一个数字 [1-9]\\d 的意思是 10~99 之间的任意一个数字 [1-9]) 的意思是 1~9 之间的任意一个数字 . 的意思是。点要转义（特殊字符类似，@都要加 \\ 转义）\n5. 全局变量和局部变量的区别，如何在 function 里面给一个全局变量赋值 一、局部变量：在函数内部定义的变量，叫局部变量。 当这个函数被调用的时候，这个变量存在，当这个函数执行完成之后，因为函数都已经结束了，所有函数里面定义的变量也就结束了。 在一个函数中定义的局部变量，只能在这个函数中使用，不能再其他的函数中使用。\n二、全局变量：子函数外边定的变量，叫做全局变量。 所有的函数都可以使用它的值，如果函数需要修改全局变量的值，那么需要在这个函数中，使用 global xxx 进行说明。\n6. Tuple 和 list 的区别，有两个 list b1 = [1,2,3] b2=[2,3,4] 写出合并代码 list： Python 内置的一种数据类型是列表 list。list 是一种有序的集合，可以随时添加和删除其中的元素。用 len() 函数可以获得 list 元素的个数，用索引来访问 list 中每一个位置的元素下标从 0 开始，要删除 list 末尾的元素，用 pop() 方法 要删除指定位置的元素，用 pop(i) 方法，其中 i 是索引位置，要把某个元素替换成别的元素，可以直接赋值给对应的索引位置，ist 里面的元素的数据类型也可以不同，list 元素也可以是另一个 list。\ntuple 另一种有序列表叫元组：tuple。tuple 和 list 非常类似，但是 tuple 一旦初始化就不能修改现在，classmates 这个 tuple 不能变了，它也没有 append()，insert() 这样的方法。其他获取元素的方法和 list 是一样的，你可以正常地使用 classmates [0]，classmates [-1]，但不能赋值成另外的元素。 不可变的 tuple 有什么意义？因为 tuple 不可变，所以代码更安全。如果可能，能用 tuple 代替 list 就尽量用 tuple。 tuple 的陷阱：当你定义一个 tuple 时，在定义的时候，tuple 的元素就必须被确定下来。\nb1 = [1,2,3] b2=[2,3,4] bextend(b2) print(b1)  7. 请写出一段代码删除一个 list 里面的重复元素 l =[1,1,2,3,4,5,4] l = [1, 1, 2, 3, 4, 5, 4] l2=set(l) l = list(l2) print(l)  8. 写出 list 的交集与差集的代码 b1 =[1,2,3] b2=[2,3,4] b1 =[1,2,3] b2=[2,3,4] b3 = [] b4 =[] # 方法一 for val in b2 : ifval in b1: bappend(val) print(b3) for val in b2 : if val not in b1: bappend(val) forval2 in b1: if val2 not in b2: bappend(val2) print(b4) # 方法二 t1 = set(b1) t2 = set(b2) print(t1\u0026amp;t2) list1 = list(t1-t2) listextend(list(t2-t1)) print(list1)  9. 写一段 Python 代码实现 list 里排 a=[1,2,4,2,45,7,10,5,5,7,8,9,0,3] a=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3] print(sorted(a,reverse=False))# 返回新的列表 print(a) a.sort()# 注意 sort 没有返回值在原列表上修改 print(a)  10. D= [i**2 for i in range(1,10)] 请写出 D 的输出结果 [1, 4, 9, 16, 25, 36, 49, 64, 81]。\n11. 什么时 lambda 函数，下面这段代码的输出是什么 nums = range(2,20) for i in nums: nums = filter(lambda x :x == i or x % i,nums) print(nums)  nums 为 2 到 19 的数字。注解 filter 函数为用于过滤序列函数。Python3 中 nums 是一个可迭代对象 迭代后的结果是 2 到 19 的数字。\n13. 谈一下对于对线程的理解，对于 cpu 密集性 IO 密集性怎样使用线程，说说线程池， 线程锁的用法，有么有用过 multiprocessing 或者 concurrrent.futures? 线程：可以理解成程序中的一个可以执行的分支，它是 cup 调度 0 的基本单元，python 的 thread 模块是比较底层的模块，python 的 threading 模块是对 thread 做了一些包装的，可以更加方便的被使用。 线程池用法： 1）安装:pip installthreadpool 2）使用 （1）引入 threadpool 模块 （2）定义线程函数 （3）创建线程 池 threadpool.ThreadPool() （4）创建需要线程池处理的任务即 threadpool.makeRequests() （5）将创建的多个任务 put 到线程池中，threadpool.putRequest （6）等到所有任务处理完毕 theadpool.pool()\n 锁的使用： threading 模块中定义了 Lock 类，可以方便的处理锁定： \u0026ndash; 创建锁 mutex = threading.Lock() \u0026ndash; 锁定 mutex.acquire() \u0026ndash; 释放 mutex.release()\n 十六、莉莉丝广告开发工程师初始题目 1. 用递归实现快速排序 quik_sort(A) def sub_sort(array,low,high): pivotkey=array [low] while low\u0026lt;high : while low\u0026lt;high and array [high]\u0026gt;=pivotkey: high -= 1 array [low]=array [high] while low\u0026lt;high and array [low]\u0026lt;=pivotkey: low += 1 array [high]=array [low] array [low]=pivotkey return low def quick_sort(array,low,high): if low \u0026lt; high : pivoloc=sub_sort(array,low,high) quick_sort(array,low,pivoloc-1) quick_sort(array,pivoloc+1,high) if __name__==\u0026quot;__main__\u0026quot;: array=[49,38,65,97,76,13,27] print array quick_sort(array,0,len(array)-1) print array  2. 简述 Python 在异常处理中，else 和 finally 的作用分别是什么？ 如果一个 Try - exception 中，没有发生异常，即 exception 没有执行，那么将会执行 else 语句的内容。反之，如果触发了 Try - exception（异常在 exception 中被定义），那么将会执行 exception 中的内容，而不执行 else 中的内容。如果 try 中的异常没有在 exception 中被指出，那么系统将会抛出 Traceback(默认错误代码） , 并且终止程序，接下来的所有代码都不会被执行，但如果有 Finally 关键字，则会在程序抛出 Traceback 之前（程序最后一口气的时候），执行 finally 中的语句。这个方法在某些必须要结束的操作中颇为有用，如释放文件句柄，或释放内存空间等。\n3. 简述 Python GIL 的概念，以及它对 Python 多线程的影响。如何实现一个抓取网页 的程序，使用多线程是否比单线程有性能提升，并解释原因 GIL 锁 全局解释器锁（只在 cpython 里才有） 作用：限制多线程同时执行，保证同一时间只有一个线程执行，所以 cpython 里的多线程其实是伪多线程！所以 Python 里常常使用协程技术来代替多线程，协程是一种更轻量级的线程， 进程和线程的切换时由系统决定，而协程由我们程序员自己决定，而模块 gevent 下切换是遇到了耗时操作才会切换。 会有所提升，因为下载完图片之后进行存储就是 IO 密集性的操作，线程对 IO 密集性的操作有所提升。\n十七、罗格数据 1. 滑动验证码如何解决 1.selenium 控制鼠标实现，速度太机械化，成功率比较低 2. 计算缺口的偏移量，推荐博客：http://blog.csdn.net/paololiu/article/details/52514504?\u0026gt; 3. 极验滑动验证码需要具体网站具体分析，一般牵扯算法乃至深度学习相关知识。\n2. ajax 请求页面如何加载 ajax 可以实现局部刷新，也叫做无刷新，无刷新指的是整个页面不刷新，只是局部刷新，ajax 可以自己发送 http 请求，不用通过浏览器的地址栏，所以页面整体不会刷新，ajax 获取到后台数据，更新页面显示数据的部分，就做到了页面局部刷新。\n6. HTTPS 网站如何爬取  在使用 requests 前加入：requests.packages.urllib3.disable_warnings()。 为 requests 添加 verify=False 参数，比如： r =requests.get(https://blog.bbzhh.com\u0026rsquo;,verify=False)。  十八、牧游科技 1. 函数参数传递，下面程序运行的结果是？ defadd(a,s_list=[]): s_list.append(a) return s_list print(add(1)) Print(add(2)) print(add(3)) 结果是 [1],[1,2],[1,2,3]。  2. Python 中类方法，静态方法的区别及调用 类方法：是类对象的方法，在定义时需要在上方使用 @classmethod 进行装饰，形参为 cls，表示类对象，类对象和实例对象都可调用。 静态方法：是一个任意函数，在其上方使用 @staticmethod 进行装饰，可以用对象直接调用，静态方法实际上跟该类没有太大关系。\n3. 类变量，实例变量 class Person: name =\u0026quot;aaa\u0026quot; pl =Person() p2 = Person() pl.name= \u0026quot;bbb\u0026quot; print(pl.name,pname) print(Person.name) 结果 bbb aaaaaa  4. 函数式编程与内置函数 a = [1,2,3,4,5,6,7] b =filter(lambda x:x\u0026gt;5,a) for i in b : print(i) a = map(lambda x:x*2,[1,2,3]) print(list(a))   第一个 print 返回的是一个可迭代对象 6,7 第二个返回的是 2,4,6。\n 十九、上海金台灯 1. 什么是 lambda 函数，它有什么好处 lambad 表达式就是一个函数，可以赋值给一个变量，既然是表达式，可以参与运算。lambda x: x *2 这个匿名函数的形参是 x，表达式 x * 2 的值就是这个函数的返回值。 好处： 1.lambda 只是一个表达式，函数体比 def 简单很多。 2.lambda 的主体是一个表达式，而不是一个代码块。仅仅能在 lambda 表达式中封装有限的逻辑进去。 3.lambda 表达式是起到一个函数速写的作用。允许在代码内嵌入一个函数的定义。\n2. 什么是 Python 的 list and dict comprehensions, 写一段代码 List 是 Python 的列表\na = list() a.append(\u0026quot;2\u0026quot;)  dict comprehensions 是 Python 的字典的推导式\nmcase = {'a': 10, 'b': 34} mcase_frequency = {v: k for k, v in mcase.items()}  3. Python 里面如何实现 tuple 和 list 的转换 list2 =[\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;] t = type(list2)  4. Python 里面如何拷贝一个对象 list2 =[\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;] q=listcopy() print(q)  5. 写一段 except 的函数 a=10 b=0 try: c=a/b print(c) except Exception as e: print(e) print(\u0026quot;done\u0026quot;)  6. Pyhon 里面 pass 语句的作用是什么？ Python pass 是空语句，是为了保持程序结构的完整性。pass 不做任何事情，一般用做占位语句。\n7. 如何知道 Python 对象的类型？ isinstance(变量名，类型)； type()；\n8. Python 中 range() 函数的用法 range 函数大多数时常出如今 for 循环中。在 for 循环中可做为索引使用。事实上它也能够出如今不论什么须要整数列表的环境中，在 python 3.0 中 range 函数是一个迭代器。\n9. Python re 模块匹配 HTML tag 的的时候，\u0026lt;.*\u0026gt; 和 \u0026lt;.?\u0026gt; 有什么区别(2018-5-2-xhq) \u0026lt;.*\u0026gt; 匹配前一个字符 0 或多次 \u0026lt;.?\u0026gt; 匹配一个字符 0 次或 1 次\n10. Python 里面如何生成随机数 importrandom random.randint()  11. 如何在 function 里面设置一个全局变量 globals() globals()返回包含当前作用域全局变量的字典。 global 变量 设置使用全局变量  13. Python 的传参是传值还是传址 Python 是传对象引用。\n14. with 语句的作用，写一段代码 with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的清理操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。\nwith open(\u0026quot;a.book\u0026quot;,\u0026quot;w\u0026quot;) as f :  二十、钱方好近 1. 求字符串\u0026rsquo; 是一个 test 字符串\u0026rsquo; 的字符个数字符编码为 utf8 a=' 是一个 test 字符串 ' print(len(a))  2. 一个 list 对象 a = [1,2,4,3,2,2,3,4] 需要去掉里面重复的的值 a = [1,2,4,3,2,2,3,4] t =set(a) print(list(t))  3. 有一个文件 test.txt 里面有数据 1 test 100 2012-04-18 2 aaa 12 2012-04-19 3 bbb 333 2012-04-18 4 ccc 211 2012-04-17 5 ddd 334 2012-04-16 一共有 5 行 4 列数据，最后一列为日期，按日期大小进行排序。\na=[\u0026quot;2012-04-18\u0026quot;,\u0026quot;2012-04-19\u0026quot;,\u0026quot;2012-04-18\u0026quot;,\u0026quot;2012-04-17\u0026quot;,\u0026quot;2012-04-16\u0026quot;] import datetime def date_sort3(x): ls=list(x) #用了冒泡排序来排序，其他方法效果一样 for j in range(len(ls)-1): for i in range(len(ls)-j-1): lower=datetime.datetime.strptime(ls [i], '% Y-% m-% d') upper=datetime.datetime.strptime(ls [i+1], '% Y-% m-% d') if lower\u0026gt;upper: ls [i],ls [i+1]=ls [i+1],ls [i] return tuple(ls) print(date_sort3(a))  二十一、西北莜面村 1. 列举出一些常用的设计模式？ 创建型： 1.Factory Method（工厂方法） 2.Abstract Factory（抽象工厂） 3.Builder（建造者） 4.Prototype（原型） 5.Singleton（单例）\n结构型： 6.Adapter Class/Object（适配器） 7.Bridge（桥接） 8.Composite（组合） 9.Decorator（装饰） 10.Facade（外观） 11.Flyweight（享元） 12.Proxy（代理）\n行为型： 13.Interpreter（解释器） 14.Template Method（模板方法） 15.Chain of Responsibility（责任链） 16.Command（命令） 17.Iterator（迭代器） 18.Mediator（中介者） 19.Memento（备忘录） 20.Observer（观察者） 21.State（状态） 22.Strategy（策略） 23.Visitor（访问者）\n2. Python 关键字 yield 的用法？ yield 就是保存当前程序执行状态。你用 for 循环的时候，每次取一个元素的时候就会计算一次。用 yield 的函数叫 generator，和 iterator 一样，它的好处是不用一次计算所有元素，而是用一次算一次，可以节省很多空间 generator 每次计算需要上一次计算结果，所以用 yield，否则一 return，上次计算结果就没了。\n3. 深拷贝，浅拷贝的区别？ 在 Python 中对象的赋值其实就是对象的引用。当创建一个对象，把它赋值给另一个变量的时候，python 并没有拷贝这个对象，只是拷贝了这个对象的引用而已。 浅拷贝：拷贝了最外围的对象本身，内部的元素都只是拷贝了一个引用而已。也就是，把对象复制一遍，但是该对象中引用的其他对象我不复制 深拷贝：外围和内部元素都进行了拷贝对象本身，而不是引用。也就是，把对象复制一遍，并且该对象中引用的其他对象我也复制。\n4. 简化代码？ l = [] for i in range(10): l.append(i**2) print(l)   简化后的来：\n print([x**2 for x in range(10)])  6. 这两个参数什么意思 *args **kwargs，我们为什么要使用他们？ 缺省参数指在调用函数的时候没有传入参数的情况下，调用默认的参数，在调用函数的同时赋值时，所传入的参数会替代默认参数。 *args 是不定长参数，他可以表示输入参数是不确定的，可以是任意多个。 **kwargs 是关键字参数，赋值的时候是以键 = 值的方式，参数是可以任意多对在定义函数的时候不确定会有多少参数会传入时，就可以使用两个参数。\n7. 数据库连表查询？ 内连接：inner join on ； 左连接：left join on ； 右连接：right join on ;  二十二、浙江从泰 2. 反爬虫措施？ 通过 Headers 反爬虫： 从用户请求的 Headers 反爬虫是最常见的反爬虫策略。很多网站都会对 Headers 的 User-Agent 进行检测，还有一部分网站会对 Referer 进行检测（一些资源网站的防盗链就是检测 Referer）。如果遇到了这类反爬虫机制，可以直接在爬虫中添加 Headers，将浏览器的 User-Agent 复制到爬虫的 Headers 中；或者将 Referer 值修改为目标网站域名。对于检测 Headers 的反爬虫，在爬虫中修改或者添加 Headers 就能很好的绕过。\n基于用户行为反爬虫： 还有一部分网站是通过检测用户行为，例如同一 IP 短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。 大多数网站都是前一种情况，对于这种情况，使用 IP 代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理 ip，检测后全部保存起来。这样的代理 ip 爬虫经常会用到，最好自己准备一个。有了大量代理 ip 后可以每请求几次更换一个 ip，这在 requests 或者 urllib2 中很容易做到，这样就能很容易的绕过第一种反爬虫。 对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限制。\n动态页面的反爬虫： 上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过 ajax 请求得到，或者通过 JavaScript 生成的。首先用 Fiddler 对网络请求进行分析。如果能够找到 ajax 请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用 requests 或者 urllib2 模拟 ajax 请求，对响应的 json 进行分析得到需要的数据。 能够直接模拟 ajax 请求获取数据固然是极好的，但是有些网站把 ajax 请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。这种情况下就用 selenium+phantomJS，调用浏览器内核，并利用 phantomJS 执行 js 来模拟人为操作以及触发页面中的 js 脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。 用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加 Headers 一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS 就是一个没有界面的浏览器，只是操控这个浏览器的不是人。利 selenium+phantomJS 能干很多事情，例如识别点触式（12306）或者滑动式的验证码，对页面表单进行暴力破解等。\n3. 分布式爬虫原理？ scrapy-redis 实现分布式，其实从原理上来说很简单，这里为描述方便，我们把自己的核心服务器称为 master，而把用于跑爬虫程序的机器称为 slave。 我们知道，采用 scrapy 框架抓取网页，我们需要首先给定它一些 start_urls，爬虫首先访问 start_urls 里面的 url，再根据我们的具体逻辑，对里面的元素、或者是其他的二级、三级页面进行抓取。而要实现分布式，我们只需要在这个 starts_urls 里面做文章就行了。 我们在 master 上搭建一个 redis 数据库（注意这个数据库只用作 url 的存储，不关心爬取的具体数据，不要和后面的 mongodb 或者 mysql 混淆），并对每一个需要爬取的网站类型，都开辟一个单独的列表字段。通过设置 slave 上 scrapy-redis 获取 url 的地址为 master 地址。这样的结果就是，尽管有多个 slave，然而大家获取 url 的地方只有一个，那就是服务器 master 上的 redis 数据库。 并且，由于 scrapy-redis 自身的队列机制，slave 获取的链接不会相互冲突。这样各个 slave 在完成抓取任务之后，再把获取的结果汇总到服务器上（这时的数据存储不再在是 redis，而是 mongodb 或者 mysql 等存放具体内容的数据库了） 这种方法的还有好处就是程序移植性强，只要处理好路径问题，把 slave 上的程序移植到另一台机器上运行，基本上就是复制粘贴的事情。\n二十三、tataUFO 1. 将字符串：\u0026lsquo;k:1|k1:2|k2:3|k3:4\u0026rsquo;，处理成 Python 字典：{k:1， k1:2， …} # 字典 里的 K 作为字符串处理 str1 =\u0026quot;k:1|k1:2|k2:3|k3:4\u0026quot; def str2dict(str1): dict1 = {} for iterms in strsplit('|'): key，value = iterms.split(':') dict1 [key] = value return dict1  2. 现有字典 d={‘a\u0026rsquo;:24，\u0026lsquo;g\u0026rsquo;:52，\u0026rsquo;l\u0026rsquo;:12，\u0026lsquo;k\u0026rsquo;:33} 请按字典中的 value 值进 行排序？ sorted(d.items()，key = lambda x:x [1])\n3. 写一个装饰器？ 装饰器经常被用于有切面需求的场景，较为经典的有插入日志、性能测试、事务处理等。装饰器是解决这类问题的绝佳设计。 有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。\n#! coding=utf-8 import time def timeit(func): def wrapper(a): start = time.clock() func(1,2) end =time.clock() prin('used:', end - start) print(a) return wrapper # foo = timeit(foo) 完全等价，# 使用之后，foo 函数就变了，相当于是 wrapper 了 @timeit def foo(a,b): pass # 不带参数的装饰器 # wraper 将 fn 进行装饰，return wraper , 返回的 wraper 就是装饰之后的 fn def test(func): def wraper(): print \u0026quot;test start\u0026quot; func() print \u0026quot;end start\u0026quot; return wraper @test def foo(): print \u0026quot;in foo\u0026quot;  4. Python 中可变类型和不可变类型有哪些？ 可变不可变指的是内存中的值是否可以被改变，不可变类型指的是对象所在内存块里面的值不可以改变，有数值、字符串、元组；可变类型则是可以改变，主要有列表、字典。\n二十四、全品教育 1. Tuple 和 list 区别 Python 内置的一种数据类型是列表：list。list 是一种有序的集合，可以随时添加和删除其中的元素。 另一种有序列表叫元组：tuple。tuple 和 list 非常类似，但是 tuple 一旦初始化就不能修改\n2. 这两个参数 * args **kwargs 是什么意思 缺省参数指在调用函数的时候没有传入参数的情况下，调用默认的参数，在调用函数的同时赋值时， 所传入的参数会替代默认参数。 *args 是不定长参数，他可以表示输入参数是不确定的，可以是任意多个。 **kwargs 是关键字参数，赋值的时候是以键 = 值的方式，参数是可以任意多对在定义函数的时候不确定会有多少参数会传入时，就可以使用两个参数。\n3. Python 里面如何实现 tuple 和 list 的转换 t =(1,5,8) l = list(t)  4. Python 里面 range 和 xrange 的区别 xrange 和 range 的参数和用法是相同的。只是 xrange() 返回的不再是一个数列，而是一个 xrange对象。这个对象可以按需生成参数指定范围内的数字（即元素）。由于 xrange 对象是按需生成单个的元素，而不像 range 那样，首先创建整个 list。所以，在相同的范围内，xrange 占用的内存空间将更小，xrange 也会更快。实际上，xrange 由于是在循环内被调用时才会生成元素，因此无论循环多少次，只有当前一个元素占用了内存空间，且每次循环占用的都是相同的单个元素空间。我们可以粗略的认为，相同 n 个元素的话，range 占用的空间是 xrange 的 n 倍。因此，在循环很大情况下，xrange 的高效率和快速将表现的很明显。 注意：Python3 中已经没有 xrange 了。\n5. Python 里面 classmethod 和 staticmethod 的区别 如果在 @staticmethod 中要调用到这个类的一些属性方法，只能直接类名。属性名或类名。方法名。 而 @classmethod 因为持有 cls 参数，可以来调用类的属性，类的方法，实例化对象等。\n6. 如何反向输出序列比如 [2,6,5,3], 输出为 [3,5,6,2] l =[2,6,5,3] l.reverse() print(l)  7. Python 里面实现删除重复的元素``` lis=[1,1,2,3,1,2,1,3,2,4,5,5,4] t = set(lis) print(t)\n #### 8. Python 里面 copy 和 deepcopy 的区别 deepcopy（深复制），即将被复制对象完全再复制一遍作为独立的新个体单独存在。所以改变原有被复制对象不会对已经复制出来的新对象产生影响。 而等于（=）赋值，并不会产生一个独立的对象单独存在，他只是将原有的数据块打上一个新标签，所以当其中一个标签被改变的时候，数据块就会发生变化，另一个标签也会随之改变。 而 copy（浅复制）要分两种情况进行讨论： 1）当浅复制的值是不可变对象（数值，字符串，元组）时和等于赋值的情况一样，对象的 id 值与浅复制原来的值相同。 2）当浅复制的值是可变对象（列表和元组）时会产生一个不是那么独立的对象存在。有两种情况： 第一种情况：复制的 对象中无 复杂 子对象，原来值的改变并不会影响浅复制的值，同时浅复制的值改变也并不会影响原来的值。原来值的 id 值与浅复制原来的值不同。 第二种情况：复制的对象中有 复杂 子对象 （例如列表中的一个子元素是一个列表），如果不改变其中复杂子对象，浅复制的值改变并不会影响原来的值。 但是改变原来的值 中的复杂子对象的值 会影响浅复制的值。 对于简单的 object，例如不可变对象（数值，字符串，元组），用 shallow copy 和 deep copy 没区别。复杂的 object， 如 list 中套着 list 的情况，shallow copy 中的 子 list，并未从原 object 真的「独立」出来。也就是说，如果你改变原 object 的子 list 中的一个元素，你的 copy 就会跟着一起变。这跟我们直觉上对「复制」的理解不同。 #### 9. Python 里面的 search 和 match 的区别 match（）函数只检测 RE 是不是在 string 的开始位置匹配， search() 会扫描整个 string 查找匹配，也就是说 match（）只有在 0 位置匹配成功的话才有返回，如果不是开始位置匹配成功的话，match() 就返回 none。 #### 10. 输出下列代码的结果  class Parent(object): x=1 class Child1(Parent): pass class Child2(Parent): pass print(Parent.x,Childx,Childx) Childx=2 print(Parent.x,Childx,Childx) Parent.x=3 print(Parent.x,Childx,Childx) 结果1 1 1，1 2 1，3 2 3\n #### 11. Python 代码如何得到列表的交集和差集？  b1 =[1,2,3] b2 =[2,3,4] t1 = set(b1) t2 =set(b2) t3 =t1\u0026amp;t2 t4 = t1-t2 t5 = t2-t1 print(list(t3)) print(list(t4)) print(list(t5))\n #### 12. 请写出一段代码求出 1 到 100 的和？  i =1 su=0 while i \u0026lt;=100: su =su+i i+=1 print(su)\n #### 13. Python 中正则表达式提取出字符串中的数字  s =\u0026lsquo;12j33jk12ksdjfkj23jk4h1k23h\u0026rsquo; import re b=re.findall(\u0026quot;\\d\u0026quot;,s) b=\u0026quot;\u0026quot;.join(b) print(b)\n #### 14. 补全下列代码  def deco(func): pass \u0026lsquo;补全代码\u0026rsquo;\n@deco def myfunc(a,b): print(\u0026ldquo;myfunc(% s,% s) called\u0026rdquo; %(a,b)) return a+b\n\u0026gt; 补充后：  def deco(func): def inner(a,b): return func(a,b) return inner\n@deco def myfunc(a,b): print(\u0026ldquo;myfunc(% s,% s) called\u0026rdquo; %(a,b)) return a+b\n ### 二十五、名企片爬虫面试题 #### 1. 简述一次完整的 http 的通信过程、常用的响应状态码、http 的无状态性、Cookies 等这些概念 `一、http 过程` HTTP 通信机制是在一次完整的 HTTP 通信过程中，Web 浏览器与 Web 服务器之间将完成下列 7 个步骤： `1. 建立 TCP 连接` 在 HTTP 工作开始之前，Web 浏览器首先要通过网络与 Web 服务器建立连接，该连接是通过 TCP 来完成的，该协议与 IP 协议共同构建 Internet，即著名的 TCP/IP 协议族，因此 Internet 又被称作是 TCP/IP 网络。HTTP 是比 TCP 更高层次的应用层协议，根据规则，只有低层协议建立之后才能进行更高层协议的连接，因此，首先要建立 TCP 连接，一般 TCP 连接的端口号是 80。 `2. Web 浏览器向 Web 服务器发送请求命令` 一旦建立了 TCP 连接，Web 浏览器就会向 Web 服务器发送请求命令。例如：GET/sample/hello.jsp HTTP/1.1。 `3. Web 浏览器发送请求头信息` 浏览器发送其请求命令之后，还要以头信息的形式向 Web 服务器发送一些别的信息，之后浏览器发送了一空白行来通知服务器，它已经结束了该头信息的发送。 `4. Web 服务器应答` 客户机向服务器发出请求后，服务器会客户机回送应答， HTTP/1.1 200 OK ，应答的第一部分是协议的版本号和应答状态码。 `5. Web 服务器发送应答头信息` 正如客户端会随同请求发送关于自身的信息一样，服务器也会随同应答向用户发送关于它自己的数据及被请求的文档。 `6. Web 服务器向浏览器发送数据` Web 服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此为结束，接着，它就以 Content-Type 应答头信息所描述的格式发送用户所请求的实际数据。 `7. Web 服务器关闭 TCP 连接` 一般情况下，一旦 Web 服务器向浏览器发送了请求数据，它就要关闭 TCP 连接，然后如果浏览器或者服务器在其头信息加入了这行代码：Connection:keep-alive TCP 连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。 #### 二、常见的响应状态码 1．200 301 302 404 500。 #### 三、无状态 无状态是指协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。即我们给服务器发送 HTTP 请求之后，服务器根据请求，会给我们发送数据过来，但是，发送完，不会记录任何信息。 #### 四、Cookie Cookie 是由 HTTP 服务器设置的，保存在浏览器中，但 HTTP 协议是一种无状态协议，在数据交换完毕后，服务器端和客户端的链接就会关闭，每次交换数据都需要建立新的链接。就像我们去超市买东西，没有积分卡的情况下，我们买完东西之后，超市没有我们的任何消费信息，但我们办了积分卡之后，超市就有了我们的消费信息。cookie 就像是积分卡，可以保存积分，商品就是我 们的信息，超市的系统就像服务器后台，http 协议就是交易的过程。 #### 2. 说说进程和线程和锁之间的关系 `一、进程` 首先进程是指在系统中正在运行的一个应用程序；程序一旦运行就是进程，或者更专业化来说：进程是指程序执行时的一个实例，即它是程序已经执行到课中程度的数据结构的汇集。从内核的观点看，进程的目的就是担当分配系统资源（CPU 时间、内存等）的基本单位，进程有五方面的特点： 第一：动态性：进程的实质是程序的一次执行过程，进程是动态产生，动态消亡的。 第二： 并发性：任何进程都可以同其他进程一起并发执行 第三：独立性：进程是一个能独立运行的基本单位，同时也是系统分配资源和调度的独立单位；第四：异步性：由于进程间的相互制约，使进程具有执行的间断性，即进程按各自独立的、不可预知的速度向前推进 第五：结构特征：进程由程序、数据和进程控制块三部分组成。进程可以使用 fork（）函数来创建子进程也可以使用 vfork（）来实现进程，使用的时候注意不要产生僵尸进程和孤儿进程。 `二、线程` 线程是系统分配处理器时间资源的基本单元，或者说进程之内独立执行的一个单元执行流，线程有四方面特点：第一，线程有独立的堆栈段，共享地址空间，开销较小，切换速度较快。第二，线程间的通信机制比较方便。第三，因为操作系统会保证当线程数不大于 CPU 数目时，不同的线程运行于不同的 CPU 上。线程使 CPU 系统更加有效。第四，线程改善了程序结构，避免了一些嵌套循环。使用 pthread_create() 函数来创建线程，使用线程的时候有两点注意事项：第一，当多线程访问同一全局变量的时候，一定要加互斥量，也就是上锁。当然最后不要忘记了解锁。第二：正确处理线程结束的问题：因为一个线程的终止，线程的资源不会随线程的终止释放，我们需要调用 pthread_join() 来获得另一个线程的终止状态并且释放该线程所占的资源。 一个程序至少有一个进程，一个进程至少有一个线程。线程不能够独立执行，必须依存在进程中。 `三、锁` 当多线程访问同一全局变量的时候，一定要加互斥量，也就是上锁。 #### 3. MySQL 操作：为 person 表的 name 创建普通的索引 ```CREATE INDEX name ON table_name(person)``` #### 4. *args and **kwargs 的区别 在函数定义中使用 * args 和 kwargs 传递可变长参数. *args 用作传递非命名键值可变长参数列表（位置参数）; kwargs 用作传递键值可变长参数列表，并且，*args 必须位于 **kwargs 之前，因为 positional arguments 必须位于 keyword arguments 之前。 #### 5. 写一个匹配 Email 地址的正则表达式  只允许英文字母、数字、下划线、英文句号、以及中划线组成 ^[a-zA-Z0-9_-]+@[a-zA-Z0-9_-]+(.[a-zA-Z0-9_-]+)+$ 名称允许汉字、字母、数字，域名只允许英文域名 ^[A-Za-z0-9\\u4e00-\\u9fa5]+@[a-zA-Z0-9_-]+(.[a-zA-Z0-9_-]+)+$\n #### 6. 常见的反爬虫措施有哪些？一般怎么克服 1）通过 Headers 反爬虫 从用户请求的 Headers 反爬虫是最常见的反爬虫策略。很多网站都会对 Headers 的 User-Agent 进行检测，还有一部分网站会对 Referer 进行检测（一些资源网站的防盗链就是检测 Referer）。如果遇到了这类反爬虫机制，可以直接在爬虫中添加 Headers，将浏览器的 User-Agent 复制到爬虫的 Headers 中；或者将 Referer 值修改为目标网站域名。对于检测 Headers 的反爬虫，在爬虫中修改或者添加 Headers 就能很好的绕过。 2）基于用户行为反爬虫 还有一部分网站是通过检测用户行为，例如同一 IP 短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。 大多数网站都是前一种情况，对于这种情况，使用 IP 代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理 ip，检测后全部保存起来。这样的代理 ip 爬虫经常会用到，最好自己准备一个。有了大量代理 ip 后可以每请求几次更换一个 ip，这在 requests 或者 urllib2 中很容易做到，这样就能很容易的绕过第一种反爬虫。 对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限制。 3）动态页面的反爬虫 上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过 ajax 请求得到，或者通过 JavaScript 生成的。首先用 Firebug 或者 HttpFox 对网络请求进行分析。如果能够找到 ajax 请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用 requests 或者 urllib2 模拟 ajax 请求，对响应的 json 进行分析得到需要的数据。 能够直接模拟 ajax 请求获取数据固然是极好的，但是有些网站把 ajax 请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。我这几天爬的那个网站就是这样，除了加密 ajax 参数，它还把一些基本的功能都封装了，全部都是在调用自己的接口，而接口参数都是加密的。遇到这样的网站，我们就不能用上面的方法了，我用的是 selenium+phantomJS 框架，调用浏览器内核，并利用 phantomJS 执行 js 来模拟人为操作以及触发页面中的 js 脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。 用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加 Headers 一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS 就是一个没有界面的浏览器，只是操控这个浏览器的不是人。利用 selenium+phantomJS 能干很多事情，例如识别点触式（12306）或者滑动式的验证码，对页面表单进行暴力破解等等。 #### 7. 编写爬虫的常用模块或者框架有哪些？请说明一个爬虫的行为步骤 爬虫的行为步骤： 1、获取网页 2、提取数据 3、高效抓取数据 4、持续抓取数据（增量式爬虫） 5. 爬虫和反爬虫和反反爬虫 #### 8. 排序算法有哪些用 Python 写一种排序算法 ### 二十六、欧特咨询 #### 1. 对 Cookie 的理解，遇到没有 Cookie 登陆的问题 Cookie 是由 HTTP 服务器设置的，保存在浏览器中，但 HTTP 协议是一种无状态协议，在数据交换完毕后，服务器端和客户端的链接就会关闭，每次交换数据都需要建立新的链接。就像我们去超市买东西，没有积分卡的情况下，我们买完东西之后，超市没有我们的任何消费信息，但我们办了积分卡之后，超市就有了我们的消费信息。Cookie 就像是积分卡，可以保存积分，商品就是我们的信息，超市的系统就像服务器后台，http 协议就是交易的过程。 如果遇到没有 Cookie 的话可以借助 phantomjs selenium 进行登陆。 #### 2. 如何解决验证码的问题，用什么模块，听过哪些人工打码平台 PIL、pytesser、tesseract。 平台的话有：云打码、答题吧打码平台、挣码、斐斐打码、若快打码。 #### 3. 对于 scrapy_redis 的理解 scrapy-redis 是一个基于 redis 的 scrapy 组件，通过它可以快速实现简单分布式爬虫程序，该组件本质上提供了三大功能： scheduler - 调度器 dupefilter - URL 去重规则（被调度器使用） pipeline - 数据持久化 \u0026gt; 一、scrapy-redis 组件 1.URL 去重 去重规则通过 redis 的集合完成，去重规则中将 url 转换成唯一标示，然后在 redis 中检查是否已经在集合中存在 2.调度器 调度器，调度器使用 PriorityQueue（有序集合）、FifoQueue（列表）、LifoQueue（列表）进行保存请求，并且使用 RFPDupeFilter 对 URL 去重 3.数据持久化 定义持久化，爬虫 yield Item 对象时执行 RedisPipeline 将 item 持久化到 redis 时，指定 key 和序列化函数 REDIS_ITEMS_KEY = ‘%(spider) s:items' REDIS_ITEMS_SERIALIZER = ‘json.dumps' 使用列表保存 item 数据 4.起始 URL 相关 获取起始 URL 时，去集合中获取还是去列表中获取？True，集合；False，列表 REDIS_START_URLS_AS_SET = False # 获取起始 URL 时，如果为 True，则使用 self.server.spop；如果为 False，则使用 self.server.lpop。 编写爬虫时，起始 URL 从 redis 的 Key 中获取。 #### 4. ip 被封了怎么解决，自己做过 ip 池么？\u0026gt; 关于 ip 可以通过 ip 代理池来解决问题 ip 代理池相关的可以在 github 上搜索 ip proxy 自己选一个 去说 https://github.com/awolfly9/IPProxyTool 提供大体思路： 1.获取器 通过 requests 的爬虫爬取免费的 IP 代理网址获取 IP。 2.过滤器通过获取器获取的代理请求网页数据有数据返回的保存进 Redis。 3.定时检测器定时拿出一部分 Proxy 重新的用过滤器进行检测剔除不能用的代理。 4.利用 Flask web 服务器提供 API 方便提取 IP ### 二十七、多来点 #### 1. 请阐述 Python 的特点 1）面向对象 2）免费 3）开源 4）可移植 5）功能强大 6）可混合 7）简单易用 ### 二十八、傲盾 #### 1. Python 线程和进程的区别？ 1) 调度：线程作为调度和分配的基本单位，进程作为拥有资源的基本单位。 2) 并发性：不仅进程之间可以并发执行，同一个进程的多个线程之间也可并发执行。 3）拥有资源：进程是拥有资源的一个独立单位，线程不拥有系统资源，但可以访问隶属于进程的资源. 4）系统开销：在创建或撤消进程时，由于系统都要为之分配和回收资源，导致系统的开销明显大于创建或撤消线程时的开销。 #### 2. 如何保证线程安全？ \u0026gt; 通常加锁也有 2 种不同的粒度的锁： 1.fine-grained(细粒度)，程序员需要自行加 / 解锁来保证线程安全 2.coarse-grained(粗粒度)，语言层面本身维护着一个全局的锁机制用来保证线程安全 前一种方式比较典型的是 Java, Jython 等，后一种方式比较典型的是 CPython(即 Python)。 #### 3. 编程实现 list 转 dict  方法一，使用 zip 函数 a = [\u0026lsquo;hello\u0026rsquo;, \u0026lsquo;world\u0026rsquo;, \u0026lsquo;1\u0026rsquo;, \u0026lsquo;2\u0026rsquo;] b = dict(zip(a[0::2], a[1::2])) print(b)\n方法二，利用循环 b = {} for i in range(0, len(a), 2): b[a [i]] = a[i+1] print(b)\n使用 enumerate 函数生成 index 实现 my_dict = {} for index, item in enumerate(a): if index % 2 == 0: my_dict[item] = a[index+1] print(b)\n ### 二十九、汉迪 #### 1. 在 Python 中，list，tuple，dict，set 有什么区别，主要应用在什么场景？ 区别： list：链表，有序的数据结构，通过索引进行查找，使用方括号 []; tuple：元组，元组将多样的对象集合到一起，不能修改，通过索引进行查找，使用括号(); dict：字典，字典是一组键(key) 和值(value) 的组合，通过键(key) 进行查找，没有顺序，使用大括号{}; set：集合，无序，元素只出现一次，自动去重，使用 set([]) 应用场景: list：简单的数据集合，可以使用索引； tuple：把一些数据当做一个整体去使用，不能修改； dict：使用键值和值进行关联的数据； set：数据只出现一次，只关心数据是否出现，不关心其位置。 #### 2. 说明 Session 和 Cookie 的联系 Session 对 Cookie 的依赖：Cookie 采用客户端存储，Session 采用的服务端存储的机制。Session 是针对每个用户（浏览器端）的，Session 值保存在服务器上，通过 SessionId 来区分哪个用户的 Session。因此 SessionId 需要被绑定在浏览器端。SessionId 通常会默认通过 Cookie 在浏览器端绑定，当浏览器端禁用 cookie 时，可通过 Url 重写（可以在地址栏看到 sessionid=KWJHUG6JJM65HS2K6 之类的字符串）或者表单隐藏字段的方式将 SessionId 传回给服务器，以便服务通过 SessionId 获取客户端对应的 Session。 具体一次的请求流程：当程序需要为客户端创建一个 Session 的时候，服务器首先检测这个客户端请求里面是否已经包含了 Session 的表示（SessionId）, 如果已经包含，则说明已经为客户端创建过一个 Session，服务端根据 SessionId 检索出来 Sesion 并使用。如果客户端请求不包含 SessionId，则为客户端创建一个 Session，并生成一个 SessionId 返回给客户端保存。 #### 3. 说明 Session 和 Cookie 的区别 1、Cookie 数据存放在客户的浏览器上，session 数据放在服务器上。 2、Cookie 不是很安全，别人可以分析存放在本地的 cookie 并进行 cookie 欺骗，考虑到安全应当使用 Session。 3、Session 会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用 Cookie。 4、单个 Cookie 保存的数据不能超过 4K，很多浏览器都限制一个站点最多保存 20 个 Cookie。 5、可以考虑将登陆信息等重要信息存放为 Session，其他信息如果需要保留，可以放在 Cookie 中。 ### 三十、大会信统 Python 工程师 #### 1. 请写出一段 python 代码实现删除一个 list 里面的重复元素 方法一： 利用 Set 集合去重实现。  l1 = [\u0026lsquo;b\u0026rsquo;,\u0026lsquo;c\u0026rsquo;,\u0026rsquo;d\u0026rsquo;,\u0026lsquo;b\u0026rsquo;,\u0026lsquo;c\u0026rsquo;,\u0026lsquo;a\u0026rsquo;,\u0026lsquo;a\u0026rsquo;] l2 = list(set(l1)) lsort(key=lindex) print(l2)\n 方法二：使用字典函数。  a=[1,2,4,2,4,5,6,5,7,8,9,0] b={} b=b.fromkeys(a) c=list(b.keys()) print(\u0026rsquo; 去重后的 list 为：\u0026rsquo;,c)\n 方法三：用 append 方法实现。  def delList(L): L1 = [] for i in L: if i not in L1: L1.append(i) return L1 print(delList([1,2,2,3,3,4,5])) print(delList([1,8,8,3,9,3,3,3,3,3,6,3]))\n 换成列表推导式更简单：  l1 = [\u0026lsquo;b\u0026rsquo;,\u0026lsquo;c\u0026rsquo;,\u0026rsquo;d\u0026rsquo;,\u0026lsquo;b\u0026rsquo;,\u0026lsquo;c\u0026rsquo;,\u0026lsquo;a\u0026rsquo;,\u0026lsquo;a\u0026rsquo;] l2 = [] [l2.append(i) for i in l1 if not i in l2] print(l2)\n 方法四：先对元素进行排序，然后从列表的最后开始扫描。  List=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3] if List: List.sort() last = List[-1] for i in range(len(List)-2, -1, -1): if last==List [i]: del List[i] else: last=List[i] print(List)\n #### 2. 编程用 sort 进行排序，然后从最后一个元素开始判断  a=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3] a.sort() last=a[-1] for i in range(len(a)-2,-1,-1): if last==a[i]: del a[i] else: last=a[i] print(a)\n #### 3. Python 里面如何拷贝一个对象？（赋值，浅拷贝，深拷贝的区别） 1）赋值（=），就是创建了对象的一个新的引用，修改其中任意一个变量都会影响到另一个。 2）浅拷贝：创建一个新的对象，但它包含的是对原始对象中包含项的引用（如果用引用的方式修改其中一个对象，另外一个也会修改改变）{1, 完全切片方法；2，工厂函数，如 list()；3，copy 模块的 copy() 函数}。 3）深拷贝：创建一个新的对象，并且递归的复制它所包含的对象（修改其中一个，另外一个不会改变）{copy 模块的 copy.deepcopy() 函数}。 #### 4. Python 里面 match() 和 search() 的区别？ re 模块中 match(pattern,string [,flags]), 检查 string 的开头是否与 pattern 匹配。 re 模块中 research(pattern,string [,flags]), 在 string 搜索 pattern 的第一个匹配值。     print(re.match(‘super\u0026rsquo;, ‘superstition\u0026rsquo;).span()) (0, 5) print(re.match(‘super\u0026rsquo;, ‘insuperable\u0026rsquo;)) None print(re.search(‘super\u0026rsquo;, ‘superstition\u0026rsquo;).span()) (0, 5) print(re.search(‘super\u0026rsquo;, ‘insuperable\u0026rsquo;).span()) (2, 7)\n    #### 5. 用 Python 匹配 HTML tag 的时候，\u0026lt;._\\\u0026gt; 和 \u0026lt;._?\u0026gt; 有什么区别？ 术语叫贪婪匹配(\u0026lt;.*\u0026gt; ) 和非贪婪匹配(\u0026lt;.*?\u0026gt; )。 例如：test\u0026lt;.*\u0026gt; : test\u0026lt;.*?\u0026gt;： \u0026lt;.*\u0026gt; 是贪婪匹配，会从第一个 \u0026lt;开始匹配，直到最后一个\u0026gt; 中间所有的字符都会匹配到，中间可能会包含 \u0026lt;\u0026gt;。 \u0026lt;.*?\u0026gt; 是非贪婪匹配，从第一个 \u0026lt;开始往后，遇到第一个\u0026gt; 结束匹配，这中间的字符串都会匹配到，但是不会有 \u0026lt;\u0026gt;。 #### 6. Python 里面如何生成随机数？ 使用 random 模块。 1）随机整数：random.randint(a,b)：返回随机整数 x,a\u0026lt;=x\u0026lt;=b random.randrange(start,stop,[,step])：返回一个范围在(start,stop,step) 之间的随机整数，不包括结束值。 2）随机实数：random.random( ): 返回 0 到 1 之间的浮点数 random.uniform(a,b): 返回指定范围内的浮点数。 ### 三十一、倍通供应链 信息 \u0026amp; 数据中心工程师 笔试题 #### 1. OOP 编程三大特点是什么，多态应用的基础是什么？ 1）封装 ：就是将一个类的使用和实现分开，只保留部分接口和方法与外部联系。 2）继承：子类自动继承其父级类中的属性和方法，并可以添加新的属性和方法或者对部分属性和方法进行重写。继承增加了代码的可重用性。 3）多态：多个子类中虽然都具有同一个方法，但是这些子类实例化的对象调用这些相同的方法后却可以获得完全不同的结果，多态性增强了软件的灵活性。（多态的概念依赖于继承）。 #### 2. 请描述抽象类和接口类的区别和联系？ 1）抽象类：规定了一系列的方法，并规定了必须由继承类实现的方法。由于有抽象方法的存在，所以抽象类不能实例化。可以将抽象类理解为毛坯房，门窗、墙面的样式由你自己来定，所以抽象类与作为基类的普通类的区别在于约束性更强。 2）接口类：与抽象类很相似，表现在接口中定义的方法，必须由引用类实现，但他与抽象类的根本区别在于用途：与不同个体间沟通的规则（方法），你要进宿舍需要有钥匙，这个钥匙就是你与宿舍的接口，你的同室也有这个接口，所以他也能进入宿舍，你用手机通话，那么手机就是你与他人交流的接口。 3）区别和关联： 1. 接口是抽象类的变体，接口中所有的方法都是抽象的。而抽象类中可以有非抽象方法。抽象类是声明方法的存在而不去实现它的类。 2. 接口可以继承，抽象类不行。 3. 接口定义方法，没有实现的代码，而抽象类可以实现部分方法。 4. 接口中基本数据类型为 static 而抽类象不是。 5. 接口可以继承，抽象类不行。 6. 可以在一个类中同时实现多个接口。 7. 接口的使用方式通过 implements 关键字进行，抽象类则是通过继承 extends 关键字进行。 #### 3. 请描述方法重载与方法重写？ 1）方法重载：是在一个类里面，方法名字相同，而参数不同。返回类型呢？可以相同也可以不同。重载是让类以统一的方式处理不同类型数据的一种手段。 2）方法重写：子类不想原封不动地继承父类的方法，而是想作一定的修改，这就需要采用方法的重写。方法重写又称方法覆盖。 #### 4. 请用代码实现一个冒泡排序？  def bubbleSort(nums): for i in range(len(nums)-1):# 这个循环负责设置冒泡排序进行的次数 for j in range(len(nums)-i-1):# ｊ为列表下标 if nums[j] \u0026gt; nums[j+1]: nums[j], nums[j+1] = nums[j+1], nums[j] return nums nums = [5,2,45,6,8,2,1] print（bubbleSort(nums)\n #### 5. 请用代码实现输出：1，2，3，5，8，13，21，34，55，89…… 这道题考的是斐波那契数列的实现。 用生成器实现：  def fib(n): \u0026hellip;.: current = 0 \u0026hellip;.: num1, num2 = 0, 1 \u0026hellip;.: while current \u0026lt; n: \u0026hellip;.: num = num1 \u0026hellip;.: num1, num2 = num2, num1+num2 \u0026hellip;.: current += 1 \u0026hellip;.: yield num \u0026hellip;.: return \u0026lsquo;done\u0026rsquo;\n 迭代器实现：  class FibIterator(object): def init(self, n): \u0026quot;\u0026quot;\u0026quot; :param n: int, 指明生成数列的前 n 个数 \u0026quot;\u0026quot;\u0026quot; self.n = n # current 用来保存当前生成到数列中的第几个数了 self.current = 0 # num1 用来保存前前一个数，初始值为数列中的第一个数 0 self.num1 = 0 # num2 用来保存前一个数，初始值为数列中的第二个数 1 self.num2 = 1\ndef __next__(self): \u0026quot;\u0026quot;\u0026quot;被 next() 函数调用来获取下一个数\u0026quot;\u0026quot;\u0026quot; if self.current \u0026lt; self.n: num = self.num1 self.num1, self.num2 = self.num2, self.num1+self.num2 self.current += 1 return num else: raise StopIteration def __iter__(self): \u0026quot;\u0026quot;\u0026quot;迭代器的__iter__返回自身即可\u0026quot;\u0026quot;\u0026quot; return self   #### 6. 请解释下 TCP/IP 协议和 HTTP 协议？ HTTP 协议： HTTP 协议即超文本传送协议(Hypertext Transfer Protocol)，是 Web 联网的基础，也是手机联网常用的协议之一，HTTP 协议是建立在 TCP 协议之上的一种应用。HTTP 连接最显著的特点是客户端发送的每次请求都需要服务器回送响应，在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为一次连接。 TCP/IP 协议： TCP/IP（Transmission Control Protocol/Internet Protocol）协议是传输层协议，主要解决数据如何在网络中传输。HTTP 是应用层协议，主要解决如何包装数据。IP 协议对应于网络层。 详细了解可以看 第三章 Python 高级→八。网路编程 #### 7. Python 里面如何实现 tuple 和 list 的转换？ list 转换成 tuple：t=tuple(l)。 tuple 转换成 list：l=list(t)。 #### 8. 请写出以下 Linux 的 SHELL 命令？ 显示所有文件包括隐藏文件 ls -a 切换到当前目录下的 dir 目录 cd dir 删除某一个文件 rm test 创建一个空文件 touch test 切换到 xiaoming 用户 su xiaoming 设置系统时间为 20:30:30 date -s 20:30:30 ### 三十二、上海行知道教育 Python 程序员笔试题 #### 1. Python 如何实现单例模式？请写出两种实现方法？ 在 Python 中，我们可以用多种方法来实现单例模式： `1. 使用模块；` `2. 使用__new__；` `3. 使用装饰器；` `4. 使用元类（metaclass）。` **1）使用模块：** 其实，Python 的模块就是天然的单例模式，因为模块在第一次导入时，会生成.pyc 文件，当第二次导入时，就会直接加载.pyc 文件，而不会再次执行模块代码。因此我们只需把相关的函数和数据定义在一个模块中，就可以获得一个单例对象了。  mysingle.py class MySingle: def foo(self): pass sinleton = MySingle() 将上面的代码保存在文件 mysingle.py 中，然后这样使用： from mysingle import sinleton singleton.foo()\n **2）使用__new__：** 为了使类只能出现一个实例，我们可以使用__new__来控制实例的创建过程，  class Singleton(object): def new(cls): # 关键在于这，每一次实例化的时候，我们都只会返回这同一个 instance 对象 if not hasattr(cls, \u0026lsquo;instance\u0026rsquo;): cls.instance = super(Singleton, cls).new(cls) return cls.instance obj1 = Singleton() obj2 = Singleton() objattr1 = \u0026lsquo;value1\u0026rsquo; print(objattr1, objattr1 ) print(obj1 is obj2)\n输出结果： value1value1  **3）使用装饰器：** 装饰器可以动态的修改一个类或函数的功能。这里，我们也可以使用装饰器来装饰某个类，使其只能生成一个实例  def singleton(cls): instances = {} def getinstance(*args,**kwargs): if cls not in instances: instances [cls] = cls(*args,**kwargs) return instances [cls] return getinstance\n@singleton class MyClass: a = 1\nc1 = MyClass() c2 = MyClass() print(c1 == c2) # True 在上面，我们定义了一个装饰器 singleton，它返回了一个内部函数 getinstance， 该函数会判断某个类是否在字典 instances 中，如果不存在，则会将 cls 作为 key，cls(*args, **kw) 作为 value 存到 instances 中，否则，直接返回 instances [cls]。\n **4）使用 metaclass（元类）：** 元类可以控制类的创建过程，它主要做三件事： 拦截类的创建 修改类的定义 返回修改后的类  class Singleton2(type): def init(self, *args, **kwargs): self.__instance = None super(Singleton2,self).init(*args, **kwargs)\ndef __call__(self, *args, **kwargs): if self.__instance is None: self.__instance = super(Singleton2,self).__call__(*args, **kwargs) return self.__instance  class Foo(object): metaclass = Singleton2 # 在代码执行到这里的时候，元类中的__new__方法和__init__方法其实已经被执行了，而不 是在 Foo 实例化的时候执行。且仅会执行一次。 foo1 = Foo() foo2 = Foo() print(Foo.dict)\n_Singleton__instance\u0026rsquo;: \u0026lt;main.Foo object at 0x100c52f10\u0026gt; 存在一个私有属性来保存属性，而不会污染 Foo 类（其实还是会污染，只是无法直接通过__instance 属性访问） print(foo1 is foo2)# True\n #### 2. 什么是 lambda 函数？请举例说明？ 匿名函数 lambda：是指一类无需定义标识符（函数名）的函数或子程序。lambda 函数可以接收任意多个参数(包括可选参数) 并且返回单个表达式的值。 例 1:传入多个参数的 lambda 函数  def sum(x,y): return x+y\n用 lambda 来实现：  p = lambda x,y:x+y print(4,6)\n例 2： 传入一个参数的 lambda 函数  a=lambda x:x*x print(a(3))\n注意：这里直接 a(3) 可以执行，但没有输出的，前面的 print 不能少  例 3： 多个参数的 lambda 形式：  a = lambda x,y,z:(x+8)*y-z print(a(5,6,8))\n #### 3. 如何反序地迭代一个序列？ 在列表中，如果我们要将列表反向迭代通常使用 reverse()。但这个方法有个缺陷就是会改变列表。因此，我们推荐使用 reversed()，它会返回一个迭代器。这里，我们可以实现__reversed__() 解决反向迭代问题。  class FloatRange: def init(self, start, end, step): self.start = start self.end = end self.step = step\n# 正向迭代 def __iter__(self): t = self.start while round(t, 2) \u0026lt;= round(self.end, 2): yield t t += self.step # 反向迭代 def __reversed__(self): t = self.end while round(t, 2) \u0026gt;= round(self.start, 2): yield t t -= self.step  if name == \u0026ldquo;main\u0026rdquo;: for x in FloatRange(0, 0, 2): print(x) for x in reversed(FloatRange(0, 0, 2)): print(x)\n ","date":"2022-11-18","permalink":"/post/questions/","tags":["InterviewQuestions","CompanyQuestionSet"],"title":"questions"},{"content":"MySQL 服务器硬件和操作系统调节:  拥有足够的物理内存来把整个InnoDB文件加载到内存中——在内存中访问文件时的速度要比在硬盘中访问时快的多。 不惜一切代价避免使用Swap交换分区 – 交换时是从硬盘读取的，它的速度很慢。 使用电池供电的RAM（注：RAM即随机存储器）。 使用高级的RAID（注：Redundant Arrays of Inexpensive Disks，即磁盘阵列） – 最好是RAID10或更高。 避免RAID5（注：一种存储性能、数据安全和存储成本兼顾的存储解决方案） – 确保数据库完整性的校验是要付出代价的。 将操作系统和数据分区分开，不仅仅是逻辑上，还包括物理上 – 操作系统的读写操作会影响数据库的性能。 把MySQL临时空间和复制日志与数据放到不同的分区 – 当数据库后台从磁盘进行读写操作时会影响数据库的性能。 更多的磁盘空间等于更快的速度。 更好更快的磁盘。 使用SAS（注： Serial Attached SCSI，即串行连接SCSI）代替SATA（注：SATA，即串口硬盘）。 较小的硬盘 比 较大的硬盘快，尤其是在RAID配置的情况下。 使用电池支持的高速缓存RAID控制器。 避免使用软件磁盘阵列。 考虑为数据分区使用固态IO卡 (不是磁盘驱动器) – 这些卡能够为几乎任何数量的数据支持2GB/s的写入速度。 在Linux中设置swappiness的值为0 – 在数据库服务器中没有理由缓存文件，这是一个服务器或台式机的优势。 如果可以的话，使用 noatime 和 nodirtime 挂载文件系统 – 没有理由更新访问数据库文件的修改时间。 使用 XFS 文件系统 – 一种比ext3更快、更小的文件系统，并且有许多日志选项， 而且ext3 已被证实与MySQL有双缓冲问题。 调整 XFS 文件系统日志和缓冲变量 – 为了最高性能标准。 在 Linux 系统中, 使用 NOOP 或者 DEADLINE IO 定时调度程序 – 同 NOOP 和 DEADLINE定时调度程序相比，这个 CFQ 和 ANTICIPATORY 定时调度程序 显得非常慢。 使用64位的操作系统 – 对于MySQL，会有更大的内存支持和使用。 删除服务器上未使用的安装包和守护进程 – 更少的资源占用。 把使用MySQL的host和你的MySQL host放到一个hosts文件中 – 没有DNS查找。 切勿强制杀死一个MySQL进程 – 你会损坏数据库和正在运行备份的程序。 把服务器贡献给MySQL – 后台进程和其他服务能够缩短数据库占用CPU的时间。  MySQL 配置: 当写入时，使用 innodb_flush_method=O_DIRECT 来避免双缓冲。 避免使用 O_DIRECT 和 EXT3 文件系统 – 你将序列化所有要写入的。 分配足够的 innodb_buffer_pool_size 来加载整个 InnoDB 文件到内存中– 少从磁盘中读取。 不要将 innodb_log_file_size 参数设置太大， 这样可以更快同时有更多的磁盘空间 – 丢掉多的日志通常是好的，在数据库崩溃后可以降低恢复数据库的时间。 不要混用 innodb_thread_concurrency 和 thread_concurrency 参数– 这2个值是不兼容的。 分配一个极小的数量给 max_connections 参数 – 太多的连接会用尽RAM并锁定MySQL服务。 保持 thread_cache 在一个相对较高的数字，大约 16 – 防止打开连接时缓慢。 使用skip-name-resolve参数 – 去掉 DNS 查找。 33.如果你的查询都是重复的，并且数据不常常发生变化，那么可以使用查询缓存。但是如果你的数据经常发生变化，那么使用查询缓存会让你感到失望。 34.增大temp_table_size值，以防止写入磁盘 35.增大max_heap_table_size值，以防止写入磁盘 36.不要把sort_buffer_size值设置的太高，否则的话你的内存将会很快耗尽 37.根据key_read_requests和key_reads值来决定key_buffer的大小，一般情况下key_read_requests应该比key_reads值高，否则你不能高效的使用key_buffer 38.将innodb_flush_log_at_trx_commit设置为0将会提高性能，但是如果你要保持默认值（1）的话，那么你就要确保数据的完整性，同时你也要确保复制不会滞后。 39.你要有一个测试环境，来测试你的配置，并且在不影响正常生产的情况下，可以常常进行重启。  MySQL模式优化: 保持你的数据库整理性。 旧数据归档 - 删除多余的行返回或搜索查询。 将您的数据加上索引. 不要过度使用索引，比较与查询. 压缩文字和BLOB数据类型 - 以节省空间和减少磁盘读取次数. UTF 8和UTF16都低于latin1执行效率. 有节制地使用触发器. 冗余数据保持到最低限度 - 不重复不必要的数据. 使用链接表，而不是扩展行. 注意数据类型，在您的真实数据中，尽可能使用最小的一个. 如果其他数据经常被用于查询时，而BLOB / TEXT数据不是，就把BLOB / TEXT数据从其他数据分离出来. 51.检查和经常优化表. 经常重写InnoDB表优化. 有时，当添加列时删除索引，然后在添加回来索引，这样就会更快. 针对不同的需求，使用不同的存储引擎. 使用归档存储引擎日志表或审计表-这是更有效地写道. 会话数据存储在缓存（memcache）的而不是MySQL中 - 缓存允许自动自动填值的，并阻止您创建难以读取和写入到MySQL的时空数据. 57.存储可变长度的字符串时使用VARCHAR而不是CHAR - 节省空间，因为固定长度的CHAR，而VARCHAR长度不固定（UTF8不受此影响）. 逐步进行模式的变化 - 一个小的变化，可以有巨大的影响. 59.在开发环境中测试所有模式，反映生产变化. 不要随意更改你的配置文件中的值，它可以产生灾难性的影响. 有时候，在MySQL的configs少即是多. 62.有疑问时使用一个通用的MySQL配置文件.  查询优化: 使用慢查询日志去发现慢查询。 使用执行计划去判断查询是否正常运行。 总是去测试你的查询看看是否他们运行在最佳状态下 –久而久之性能总会变化。 避免在整个表上使用count(*),它可能锁住整张表。 使查询保持一致以便后续相似的查询可以使用查询缓存。 在适当的情形下使用GROUP BY而不是DISTINCT。 在WHERE, GROUP BY和ORDER BY子句中使用有索引的列。 保持索引简单,不在多个索引中包含同一个列。 有时候MySQL会使用错误的索引,对于这种情况使用USE INDEX。 检查使用SQL_MODE=STRICT的问题。 对于记录数小于5的索引字段，在UNION的时候使用LIMIT不是是用OR. 为了 避免在更新前SELECT，使用INSERT ON DUPLICATE KEY或者INSERT IGNORE ,不要用UPDATE去实现。 不要使用 MAX,使用索引字段和ORDER BY子句。 避免使用ORDER BY RAND(). 77。LIMIT M，N实际上可以减缓查询在某些情况下，有节制地使用。 78。在WHERE子句中使用UNION代替子查询。 79。对于UPDATES（更新），使用 SHARE MODE（共享模式），以防止独占锁。 80。在重新启动的MySQL，记得来温暖你的数据库，以确保您的数据在内存和查询速度快。 81。使用DROP TABLE，CREATE TABLE DELETE FROM从表中删除所有数据。 82。最小化的数据在查询你需要的数据，使用*消耗大量的时间。 83。考虑持久连接，而不是多个连接，以减少开销。 84。基准查询，包括使用服务器上的负载，有时一个简单的查询可以影响其他查询。 85。当负载增加您的服务器上，使用SHOW PROCESSLIST查看慢的和有问题的查询。 86。在开发环境中产生的镜像数据中 测试的所有可疑的查询。  MySQL 备份过程: 从二级复制服务器上进行备份。 在进行备份期间停止复制，以避免在数据依赖和外键约束上出现不一致。 彻底停止MySQL，从数据库文件进行备份。 如果使用 MySQL dump进行备份，请同时备份二进制日志文件 – 确保复制没有中断。 不要信任LVM 快照 – 这很可能产生数据不一致，将来会给你带来麻烦。 为了更容易进行单表恢复，以表为单位导出数据 – 如果数据是与其他表隔离的。 当使用mysqldump时请使用 –opt。 在备份之前检查和优化表。 为了更快的进行导入，在导入时临时禁用外键约束。 为了更快的进行导入，在导入时临时禁用唯一性检测。 在每一次备份后计算数据库，表以及索引的尺寸，以便更够监控数据尺寸的增长。 通过自动调度脚本监控复制实例的错误和延迟。 定期执行备份。 定期测试你的备份。  最后 101: 执行MySQL 监控: Monitis Unveils The World’s First Free On-demand MySQL Monitoring.\n101 个 MySQL 的调节和优化的提示\n","date":"2022-10-25","permalink":"/post/101%E4%B8%AAmysql%E4%BC%98%E5%8C%96%E6%8F%90%E7%A4%BA/","tags":["DataBase"],"title":"101个MySQL优化提示"},{"content":"1. 简介： linux 中拥有大量的下载工具，不过没有一个像迅雷那样在windows中文世界中占有统治地位。 对于一般 linux 用户常用的下载工具，比如gnome和kde等桌面环境中自带的 utorrent 或 ktorrent ， 自身下载速度不快，还有着占用内存资源多，支持下载协议有限的缺点。对于追求简洁高效的系统的人来说，自然是难以接受的。\n而 aria2 作为一个轻型的，多协议支持的下载工具，而且还拥有很多实用的扩展工具， 可以说是 linux 下一个杀手级的下载工具。\n不过 aria2 不像一般的下载工具那样开箱即用； 它的安装和配置自定义的程度高， 并且是需要花点功夫来折腾下的， 可以说是一款偏 geek 的下载工具。\n1.1. aria2是什么？ aria2是一款轻量级的、支持多协议的和多个下载源的下载工具，它能够支持HTTP/HTTPS，FTP，SFTP，BitTorrent以及Metalink这么多协议的下载。 具体介绍可参照aria2官网。\n除了丰富的协议支持，它还有两个非常惊艳的优点：一个是轻量级：占用内存空间十分少，通常只占用4-9MB。 另一个优势就是支持RPC界面远程控制，可以很方便的对被配置为服务器的主机进行下载管理。 可以说对于linux主机来说是一款绝佳的下载工具。\n用了一段时间后，还是非常满意这个小巧易用的下载工具； 唯一的一点遗憾就是不能下载迅雷链接，每次搜老电影都只能找到迅雷链接的我真是快哭了。\n体会到了aria2的强大了之后，就来看看正确安装并配置aria2。\n2. aria2的安装及配置 1、进入 /etc 目录，新建一个文件夹和一个session文件并设置权限\ncd /etc mkdir aria2 touch /aria2/aria2.session sudo chmod 777 aria2/aria2.session  2、新建配置文件并编辑\nvim aria2.conf  3、启动\nsudo aria2c --conf-path=/etc/aria2/aria2.conf  sudo aria2c --conf-path=/etc/aria2/aria2.conf -D # 启动aria2到后台。-D表示运行程序到后台。 # 假如你不确定自己的配置是否正确，你可以命令后不加-D来观察输出是否正确。  3. 问题以及解决方法： 如果跳出什么无法打开aria2.session的错误， 1、就在当前运行命令的目录下建一个aria2.session文件 2、或者切换到/etc/aria2目录下运行命令即可解决 3、相对路径改为绝对路径就好了 input-file=aria2.session save-session=aria2.session 改为： input-file=/etc/aria2/aria2.session save-session=/etc/aria2/aria2.session\n","date":"2022-10-25","permalink":"/post/aria2/","tags":["Tools","Aria2"],"title":"Aria2"},{"content":"map() map()函数语法： map(function,iterable,\u0026hellip;)\n参数: function\u0026ndash;函数 iterable\u0026ndash;一个或多个序列\n返回值 Python2.x返回列表。 Python3.x返回迭代器对象。\n传很多个iterable，如果有额外的iterable参数，并行的从这些参数中取元素，并调用function。如果一个iterable参数比另外的iterable参数要短，将以长度最小的为基准。\ndef add(x): return x+1 list1=[1,2,3] res=map(add,list1) print(list(res)) # [2,3,4]  from collections.abc importIterator defadd(x,y,z): return x,y,z list1=[1,2,3] list2=[1,2,3,4] list3=[1,2,3,4,5] res=map(add,list1,list2,list3) print(list(res)) print(isinstance(res,Iterator)) # [(1,1,1),(2,2,2),(3,3,3)] # True  filter() 语法： filter(function,iterable)\n参数： function\u0026ndash;判断函数。 iterable\u0026ndash;可迭代对象。\n返回值： 返回迭代器对象。\nfilter()函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。\n该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判断，然后返回True或False，最后将返回True的元素放到新列表中。\ndefis_odd(n): returnn%2==1 newlist=filter(is_odd,[1,2,3,4,5,6,7,8,9,10]) print(newlist) # [1,3,5,7,9]  reduce() 函数语法： reduce(function,sequence[,initial])\n参数: function有两个参数的函数，必需参数 sequencetuple，list，dictionary，string等可迭代物，必需参数 initial初始值，可选参数\n返回值： reduce的工作过程是：在迭代sequence(tuple，list，dictionary，string等可迭代物)的过程中，首先把前两个元素传给函数参数，函数加工后，然后把得到的结果和第三个元素作为两个参数传给函数参数，函数加工后得到的结果又和第四个元素作为两个参数传给函数参数，依次类推。如果传入了initial值，那么首先传的就不是sequence的第一个和第二个元素，而是initial值和第一个元素。经过这样的累计计算之后合并序列到一个单一返回值。\ndefadd(x,y): return x+y from functools import reduce print(reduce(add,[1,2,3,4])) # 10  ##zip() 语法： zip([iterable,\u0026hellip;])\n参数说明： iterabl\u0026ndash;一个或多个迭代对象;\n返回值： 返回一个迭代器对象。\nzip()函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象，这样做的好处是节约了不少的内存。\n我们可以使用list()转换来输出列表。\n如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用*号操作符，可以将元组解压为列表。\na=[1,2,3] b=[4,5,6] c=[4,5,6,7,8] print(list(zip(a,b,c))) print(list(zip(a,c)))#元素个数与最短的列表一致 a1,a2=zip(*zip(a,b))#与zip相反，zip(*)可理解为解压，返回二维矩阵式 print(*zip(a,b)) print(list(a2)) # [(1,4,4),(2,5,5),(3,6,6)] # [(1,4),(2,5),(3,6)] # (1,4)(2,5)(3,6) # [4,5,6]  enumrate() 语法： enumerate(sequence,[start=0])\n参数： sequence\u0026ndash;一个序列、迭代器或其他支持迭代对象。 start\u0026ndash;下标起始位置。\n返回值： 返回enumerate(枚举)对象。为迭代器对象。\na=[1,2,3] print(list(enumerate(a))) print(list(enumerate(a,2))) # [(0,1),(1,2),(2,3)] # [(2,1),(3,2),(4,3)]  pow() 语法： pow(x,y[,z]) 函数是计算x的y次方，如果z在存在，则再对结果进行取模，其结果等效于pow(x,y)%z\n注意：pow()通过内置的方法直接调用，内置方法会把参数作为整型，而math模块则会把参数转换为float。\n参数： x\u0026ndash;数值表达式。 y\u0026ndash;数值表达式。 z\u0026ndash;数值表达式。\n返回值： 返回xy（x的y次方）的值。\nprint(pow(4,2)) print(pow(4,2,3)) # 16 # 1  max() , min() max和min函数的使用介绍： 对可迭代对象元素进行比较，找到最大值 max(iterable,*[,default=obj,key=func]) 对传入的多个参数进行比较，找到最小值 max(arg1,arg2,args,[,key=func])\n简单使用\nlist_a=[1,2,3,5,8] print(max(list_a))  带默认参数\n#如果设定default，当传入的对象为空时则返回default值而不会引发错误 list_a=[] print(max(list_a,default=1))  带key key应当传入一个可调用对象，一般传入的是函数。指定key之后，max函数就会根据key处理后的元素进行比较\n# 需求1：比如下面的情况，每种水果的价格信息都是以字典的形式存放的列表中，要找到最贵的水果和最便宜的水果 fruit_shop=[ {'name':'apple','price':5.68}, {'name':'orange','price':4.22}, {'name':'banana','price':2.65}, {'name':'berries','price':10.75}] cheap=min(fruit_shop,key=lambdas:s['price']) expensive=max(fruit_shop,key=lambdas:s['price']) print('最便宜的水果是:',cheap) print('最贵的水果是:',expensive) # 最便宜的水果是:{'name':'banana','price':2.65} # 最贵的水果是:{'name':'berries','price':10.75} # 需求2：找到列表中出现次数最多的元素。 lt=[2,2,3,1,2,2,1,8,5] max(set(lt),key=lt.count)  sorted sorted(iterable,*,key=None,reverse=False) 必需参数为一个可迭代对象 key指定带有单个参数的函数，用于从iterable的每个元素中提取用于比较的键(例如key=str.lower)。默认值为None(直接比较元素)。\nreverse为一个布尔值。如果设为True，则每个列表元素将按反向顺序比较进行排序。\n返回为一个列表新对象，不会改变原来的对象。\n排序保证是稳定的。这意味着当多个记录具有相同的键值时，将保留其原始顺序\nlist_num=[2,4,8,1,10] print(sorted(list_num)) # [1,2,4,8,10] print(sorted(list_num,reverse=True)) # [10,8,4,2,1] dict_one={2:12,1:23} print(sorted(dict_one)) # [1,2]字典的排序默认为为键进行排序 # 可先转为元组，即可对键或值排序 print(sorted(dict_one.items(),key=lambdax:x[0]))#对键进行排序 # [(1,23),(2,12)] print(sorted(dict_one.items(),key=lambdax:x[1]))#对值进行排序 # [(2,12),(1,23)] ## 多重排序  考虑如下情况 [(2,12),(1,23)，(2,13),(1,33)] 先按照0号位的排序，有相同的再按照1号位的排序 test=[(2,12),(1,23),(2,13),(1,33)] print(sorted(test,key=lambdax:(x[0],x[1])))\n[(1,23),(1,33),(2,12),(2,13)] 有多重的话也是类似的，lambda函数后的元组指定多个即可  ## 使用Operator模块函数  例子可简化为 from operator import itemgetter print(sorted(dict_one.items(),key=itemgetter(1)))#对值进行排序\n[(2,12),(1,23)] print(sorted(test,key=itemgetter(0,1)))\n[(1,23),(1,33),(2,12),(2,13)]  ## next() `next(iterator[,default])` iterator--可迭代对象 default--可选,如果不设置，又没有下一个元素则会触发StopIteration异常,设置后则会抛出默认值。 ## iter() iter函数有两种用法，一种是传一个参数，一种是传两个参数。返回的结果都是返回一个iterator对象。 `i1=iter(itr,'c')`,这个意思是说，返回itr的iterator，而且在之后的迭代之中，迭代到'c'就立马停止（并不会输出c）。对这个itr有什么要求呢？这个itr在这里必须是callable的，即要实现**call**函数 for循环的话就会输出c之前的数据，使用next()的话到c就会抛出StopIteration异常。 ## print() print()可以输出多种类型的数据，并且搭配特殊字符可以实现格式化输出。 seq 参数 print(1,2,3,4,5,6)#123456 print(1,2,3,4,5,6,sep=\u0026quot;:\u0026quot;)#1:2:3:4:5:6 end 参数 print(1,2,3,4,5,6,end=\u0026quot;;\u0026quot;)#123456; print(1,2,3,4,5,6,sep=\u0026quot;:\u0026quot;,end=\u0026quot;;\u0026quot;)#1:2:3:4:5:6; ## isinstance() isinstance 可以接收一个元组,判断是否是多个类型中的一个就非常方便  isinstance(1.3, (float, int)) isinstance(\u0026ldquo;1.3\u0026rdquo;, (float, int, str))\n ","date":"2022-10-25","permalink":"/post/built-in-functions/","tags":["Python"],"title":"built-in-functions"},{"content":"\nCookie 和 Session 的区别\n安全性： Session 比 Cookie 安全，Session 是存储在服务器端的，Cookie 是存储在客户端的。 存取值的类型不同：Cookie 只支持存字符串数据，想要设置其他类型的数据，需要将其转换成字符串，Session 可以存任意数据类型。 有效期不同： Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭（默认情况下）或者 Session 超时都会失效。 存储大小不同： 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie，但是当访问量过多，会占用过多的服务器资源。\n每一次请求都需要携带 token，需要把 token 放到 HTTP 的 Header 里 基于 token 的用户认证是一种服务端无状态的认证方式，服务端不用存放 token 数据。用解析 token 的计算时间换取 session 的存储空间，从而减轻服务器的压力，减少频繁的查询数据库 token 完全由应用管理，所以它可以避开同源策略\nToken 和 Session 的区别\nSession 是一种记录服务器和客户端会话状态的机制，使服务端有状态化，可以记录会话信息。而 Token 是令牌，访问资源接口（API）时所需要的资源凭证。Token 使服务端无状态化，不会存储会话信息。 Session 和 Token 并不矛盾，作为身份认证 Token 安全性比 Session 好，因为每一个请求都有签名还能防止监听以及重放攻击，而 Session 就必须依赖链路层来保障通讯安全了。如果你需要实现有状态的会话，仍然可以增加 Session 来在服务器端保存一些状态。 所谓 Session 认证只是简单的把 User 信息存储到 Session 里，因为 SessionID 的不可预测性，暂且认为是安全的。而 Token ，如果指的是 OAuth Token 或类似的机制的话，提供的是 认证 和 授权 ，认证是针对用户，授权是针对 App 。其目的是让某 App 有权利访问某用户的信息。这里的 Token 是唯一的。不可以转移到其它 App上，也不可以转到其它用户上。Session 只提供一种简单的认证，即只要有此 SessionID ，即认为有此 User 的全部权利。是需要严格保密的，这个数据应该只保存在站方，不应该共享给其它网站或者第三方 App。所以简单来说：如果你的用户数据可能需要和第三方共享，或者允许第三方调用 API 接口，用 Token 。如果永远只是自己的网站，自己的 App，用什么就无所谓了。\nToken 和 JWT 的区别 相同：\n都是访问资源的令牌 都可以记录用户的信息 都是使服务端无状态化 都是只有验证成功后，客户端才能访问服务端上受保护的资源\n区别：\nToken：服务端验证客户端发送过来的 Token 时，还需要查询数据库获取用户信息，然后验证 Token 是否有效。 JWT： 将 Token 和 Payload 加密后存储于客户端，服务端只需要使用密钥解密进行校验（校验也是 JWT 自己实现的）即可，不需要查询或者减少查询数据库，因为 JWT 自包含了用户信息和加密的数据。\n使用 cookie 时需要考虑的问题\n因为存储在客户端，容易被客户端篡改，使用前需要验证合法性 不要存储敏感数据，比如用户密码，账户余额 使用 httpOnly 在一定程度上提高安全性 尽量减少 cookie 的体积，能存储的数据量不能超过 4kb 设置正确的 domain 和 path，减少数据传输 cookie 无法跨域 一个浏览器针对一个网站最多存 20 个Cookie，浏览器一般只允许存放 300 个Cookie 移动端对 cookie 的支持不是很好，而 session 需要基于 cookie 实现，所以移动端常用的是 token\n使用 session 时需要考虑的问题\n将 session 存储在服务器里面，当用户同时在线量比较多时，这些 session 会占据较多的内存，需要在服务端定期的去清理过期的 session 当网站采用集群部署的时候，会遇到多台 web 服务器之间如何做 session 共享的问题。因为 session 是由单个服务器创建的，但是处理用户请求的服务器不一定是那个创建 session 的服务器，那么该服务器就无法拿到之前已经放入到 session 中的登录凭证之类的信息了。 当多个应用要共享 session 时，除了以上问题，还会遇到跨域问题，因为不同的应用可能部署的主机不一样，需要在各个应用做好 cookie 跨域的处理。 sessionId 是存储在 cookie 中的，假如浏览器禁止 cookie 或不支持 cookie 怎么办？ 一般会把 sessionId 跟在 url 参数后面即重写 url，所以 session 不一定非得需要靠 cookie 实现 移动端对 cookie 的支持不是很好，而 session 需要基于 cookie 实现，所以移动端常用的是 token\n使用 token 时需要考虑的问题\n如果你认为用数据库来存储 token 会导致查询时间太长，可以选择放在内存当中。比如 redis 很适合你对 token 查询的需求。 token 完全由应用管理，所以它可以避开同源策略 token 可以避免 CSRF 攻击(因为不需要 cookie 了) 移动端对 cookie 的支持不是很好，而 session 需要基于 cookie 实现，所以移动端常用的是 token\n使用 JWT 时需要考虑的问题\n因为 JWT 并不依赖 Cookie 的，所以你可以使用任何域名提供你的 API 服务而不需要担心跨域资源共享问题（CORS） JWT 默认是不加密，但也是可以加密的。生成原始 Token 以后，可以用密钥再加密一次。 JWT 不加密的情况下，不能将秘密数据写入 JWT。 JWT 不仅可以用于认证，也可以用于交换信息。有效使用 JWT，可以降低服务器查询数据库的次数。 JWT 最大的优势是服务器不再需要存储 Session，使得服务器认证鉴权业务可以方便扩展。但这也是 JWT 最大的缺点：由于服务器不需要存储 Session 状态，因此使用过程中无法废弃某个 Token 或者更改 Token 的权限。也就是说一旦 JWT 签发了，到期之前就会始终有效，除非服务器部署额外的逻辑。 JWT 本身包含了认证信息，一旦泄露，任何人都可以获得该令牌的所有权限。为了减少盗用，JWT的有效期应该设置得比较短。对于一些比较重要的权限，使用时应该再次对用户进行认证。 JWT 适合一次性的命令认证，颁发一个有效期极短的 JWT，即使暴露了危险也很小，由于每次操作都会生成新的 JWT，因此也没必要保存 JWT，真正实现无状态。 为了减少盗用，JWT 不应该使用 HTTP 协议明码传输，要使用 HTTPS 协议传输。\n使用加密算法时需要考虑的问题\n绝不要以明文存储密码 永远使用 哈希算法 来处理密码，绝不要使用 Base64 或其他编码方式来存储密码，这和以明文存储密码是一样的，使用哈希，而不要使用编码。编码以及加密，都是双向的过程，而密码是保密的，应该只被它的所有者知道， 这个过程必须是单向的。哈希正是用于做这个的，从来没有解哈希这种说法， 但是编码就存在解码，加密就存在解密。 绝不要使用弱哈希或已被破解的哈希算法，像 MD5 或 SHA1 ，只使用强密码哈希算法。 绝不要以明文形式显示或发送密码，即使是对密码的所有者也应该这样。如果你需要 “忘记密码” 的功能，可以随机生成一个新的 一次性的（这点很重要）密码，然后把这个密码发送给用户。\n 禁用 Cookies，如何使用 Session ？ 如果禁用了 Cookies，服务器仍会将 sessionId 以 cookie 的方式发送给浏览器，但是，浏览器不再保存这个cookie (即sessionId) 了。 如果想要继续使用 session，需要采用 URL 重写 的方式来实现，可以参考 https://www.cnblogs.com/Renyi-Fan/p/11012086.html\n","date":"2022-10-25","permalink":"/post/cookie_session_token/","tags":["FrontEnd"],"title":"cookie_session_token"},{"content":"1. 阻塞，非阻塞 首先，阻塞这个词来自操作系统的线程/进程的状态模型中，如下图： 进程状态： 一个线程/进程经历的5个状态，创建，就绪，运行，阻塞，终止。各个状态的转换条件如上图，其中有个阻塞状态，就是说当线程中调用某个函数，需要IO请求，或者暂时得不到竞争资源的，操作系统会把该线程阻塞起来，避免浪费CPU资源，等到得到了资源，再变成就绪状态，等待CPU调度运行。\n定义： 阻塞调用是指调用结果返回之前，调用者会进入阻塞状态等待。只有在得到结果之后才会返回。 非阻塞调用是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。\n阻塞调用： 比如 socket 的 recv()，调用这个函数的线程如果没有数据返回，它会一直阻塞着，也就是 recv() 后面的代码都不会执行了，程序就停在 recv() 这里等待，所以一般把 recv() 放在单独的线程里调用。\n非阻塞调用： 比如非阻塞socket 的 send()，调用这个函数，它只是把待发送的数据复制到TCP输出缓冲区中，就立刻返回了，线程并不会阻塞，数据有没有发出去 send() 是不知道的，不会等待它发出去才返回的。\n阻塞和挂起： 阻塞是被动的，比如抢不到资源。挂起是主动的，线程自己调用 suspend() 把自己退出运行态了，某些时候调用 resume() 又恢复运行。\n阻塞和非阻塞 阻塞和非阻塞这两个概念与程序（线程）等待消息通知(无所谓同步或者异步)时的状态有关。也就是说阻塞与非阻塞主要是程序（线程）等待消息通知时的状态角度来说的。\n阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态.\n阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。\n非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。\n同步（Sync）和异步（Async） 同步： 所谓同步，就是发出一个功能调用时，在没有得到结果之前，该调用就不返回或继续执行后续操作。\n简单来说，同步就是必须一件一件事做，等前一件做完了才能做下一件事。\n例如：B/S模式中的表单提交，具体过程是：客户端提交请求-\u0026gt;等待服务器处理-\u0026gt;处理完毕返回，在这个过程中客户端（浏览器）不能做其他事。\n异步： 异步与同步相对，当一个异步过程调用发出后，调用者在没有得到结果之前，就可以继续执行后续操作。当这个调用完成后，一般通过状态、通知和回调来通知调用者。对于异步调用，调用的返回并不受调用者控制。\n对于通知调用者的三种方式，具体如下：\n状态 即监听被调用者的状态（轮询），调用者需要每隔一定时间检查一次，效率会很低。\n通知 当被调用者执行完成后，发出通知告知调用者，无需消耗太多性能。\n回调 与通知类似，当被调用者执行完成后，会调用调用者提供的回调函数。\n例如：B/S模式中的ajax请求，具体过程是：客户端发出ajax请求-\u0026gt;服务端处理-\u0026gt;处理完毕执行客户端回调，在客户端（浏览器）发出请求后，仍然可以做其他的事。\n同步的定义看起来跟阻塞很像，但是同步跟阻塞是两个概念，同步调用的时候，线程不一定阻塞，调用虽然没返回，但它还是在运行状态中的，CPU很可能还在执行这段代码，而阻塞的话，它就肯定不在CPU中跑这个代码了。这就是同步和阻塞的区别。同步是肯定可以在，阻塞是肯定不在。\n异步和非阻塞的定义比较像，两者的区别是异步是说调用的时候结果不会马上返回，线程可能被阻塞起来，也可能不阻塞，两者没关系。非阻塞是说调用的时候，线程肯定不会进入阻塞状态。\n上面两组概念，就有4种组合。 同步阻塞调用： 得不到结果不返回，线程进入阻塞态等待。 同步非阻塞调用： 得不到结果不返回，线程不阻塞一直在CPU运行。 异步阻塞调用： 去到别的线程，让别的线程阻塞起来等待结果，自己不阻塞。 异步非阻塞调用： 去到别的线程，别的线程一直在运行，直到得出结果。\n例子： import time def a(): time.sleep(5) return 2 def b(): b_num = a() return b_num + 2 if __name__ == '__main__': c = b() print(c)  例子中的 a() 函数是阻塞的，在运行到 time.sleep() 后需要等待5秒才能继续下一步（返回值），而对于 b() 函数来说是同步的，b() 函数要执行需要等待 a() 函数完成传递过来值才能进行相加并且返回值。\n3. 并发，并行 并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。\n并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。\n两者区别如下图： 并行是多个程序在多个CPU上同时运行，任意一个时刻可以有很多个程序同时运行，互不干扰。\n并发是多个程序在一个CPU上运行，CPU在多个程序之间快速切换，微观上不是同时运行，任意一个时刻只有一个程序在运行，但宏观上看起来就像多个程序同时运行一样，因为CPU切换速度非常快，时间片是64ms（每64ms切换一次，不同的操作系统有不同的时间），人类的反应速度是100ms，你还没反应过来，CPU已经切换了好几个程序了。\n切换耗时：线程用完了时间片，释放CPU控制权，引发系统中断，调度程序根据相关策略选取下一个线程来运行，这里需要一点耗时。\n时间片大小的选取 时间片取的小，假设是20ms，切换耗时假设是 10ms。 那么用户感觉不到多个程序之间会卡，响应很快，因为切换太快了，但是CPU的利用率就低了，20 / (20 + 10) = 66% 只有这么多，33%都浪费了。\n时间片取的大，假设是200ms，切换耗时是 10ms 那么用户会觉得程序卡，响应慢，因为要200ms后才轮到我的程序运行，但是CPU利用率就高了，200 / (200 + 10) = 95% 有这么多被利用的。\n所以时间片取太大或者太小都不好，一般在 10 - 100 ms 之间。\nCPU调度策略 在并发运行中，CPU需要在多个程序之间来回切换，那么如何切换就有一些策略\n3.1 先来先服务 - 时间片轮转调度 这个很简单，就是谁先来，就给谁分配时间片运行，缺点是有些紧急的任务要很久才能得到运行。\n3.2 优先级调度 每个线程有一个优先级，CPU每次去拿优先级高的运行，优先级低的等等，为了避免优先级低的等太久，每等一定时间，就给优先级低的线程提高一个级别\n3.3 最短作业优先 把线程任务量排序，每次拿处理时间短的线程运行，就像我去银行办业务一样，我的事情很快就处理完了，所以让我插队先办完，后面时间长的人先等等，时间长的人就很难得到响应了。\n3.4 最高响应比优先 用线程的等待时间除以服务时间，得到响应比，响应比小的优先运行。这样不会造成某些任务一直得不到响应。\n3.5 多级反馈队列调度 有多个优先级不同的队列，每个队列里面有多个等待线程。 CPU每次从优先级高的遍历到低的，取队首的线程运行，运行完了放回队尾，优先级越高，时间片越短，即响应越快，时间片大小就不是固定的了。 每个队列的内部还是用先来先服务的策略。\n并发级别  低级并发在这种并发级别中，显式使用了原子操作。 我们不能在构建应用程序时使用这种并发性，因为它非常容易出错并且很难调试。 即使Python不支持这种并发性。 中级并发在这种并发中，没有使用显式的原子操作。 它使用显式锁。 Python和其他编程语言支持这种并发性。 大多数应用程序员使用这种并发性。 高级并发在这种并发中，不使用显式原子操作也不使用显式锁。 Python有concurrent.futures模块来支持这种并发。  并发系统的性质 要使程序或并发系统正确，一些属性必须由它来满足。 与终止系统相关的属性如下 -\n正确性属性： 正确性属性意味着程序或系统必须提供所需的正确答案。 为了简单起见，可以说系统必须正确地将启动程序状态映射到最终状态。\n安全属性 安全属性意味着程序或系统必须保持“良好”或“安全”状态，并且从不做任何“坏”的事情。\n活跃度属性 这个属性意味着一个程序或系统必须“取得进展”，并且会达到一个理想的状态。\n并发系统的行为者 这是并发系统的一个常见属性，其中可以有多个进程和线程，它们同时运行以在他们自己的任务上取得进展。 这些进程和线程称为并发系统的角色。\n并发系统的资源 行为者必须利用内存，磁盘，打印机等资源来执行任务。\n某些规则集 每个并发系统必须拥有一套规则来定义执行者要执行的任务类型和每个任务的执行时间。 任务可能是获取锁，共享内存，修改状态等。\n并发系统的障碍 在实现并发系统时，程序员必须考虑以下两个重要问题，这可能是并发系统的障碍 -\n共享数据 实现并发系统时的一个重要问题是在多个线程或进程间共享数据。 实际上，程序员必须确保锁保护共享数据，以便所有对它的访问都被序列化，并且一次只有一个线程或进程可以访问共享数据。 如果多个线程或进程都试图访问相同的共享数据，那么除了其中至少一个以外，其他所有进程都将被阻塞并保持空闲状态。 换句话说，在锁定生效时，我们只能使用一个进程或线程。 可以有一些简单的解决方案来消除上述障碍 -\n数据共享限制 最简单的解决方案是不共享任何可变数据。 在这种情况下，我们不需要使用显式锁定，并且可以解决由于相互数据而导致的并发障碍。\n数据结构协助 很多时候并发进程需要同时访问相同的数据。 与使用显式锁相比，另一种解决方案是使用支持并发访问的数据结构。 例如，可以使用提供线程安全队列的队列模块。 也可以使用multiprocessing.JoinableQueue类来实现基于多处理的并发。\n不可变的数据传输 有时，我们使用的数据结构(比如说并发队列)不适合，那么可以传递不可变数据而不锁定它。\n可变数据传输 继续上面的解决方案，假设如果它只需要传递可变数据而不是不可变数据，那么可以传递只读的可变数据。\n共享I/O资源 实现并发系统的另一个重要问题是线程或进程使用I/O资源。 当一个线程或进程使用I/O很长时间而另一线程或进程闲置时会出现问题。 在处理I/O大量应用程序时，我们可以看到这种障碍。 可以通过一个例子来理解，从Web浏览器请求页面。 这是一个沉重的应用程序。 在这里，如果数据请求的速率比它消耗的速率慢，那么在并发系统中就会有I/O障碍。\n并行的必要性 我们可以通过在单CPU的不同内核之间或网络内连接的多台计算机之间分配子任务来实现并行。 考虑以下要点来理解为什么有必要实现并行性 -\n有效的代码执行 借助并行性，我们可以高效地运行代码。 它将节省时间，因为部分中的相同代码并行运行。\n比顺序计算更快速 顺序计算受到物理和实际因素的限制，因此无法获得更快的计算结果。 另一方面，这个问题可以通过并行计算来解决，并且比顺序计算提供更快的计算结果。\n执行时间更短 并行处理减少了程序代码的执行时间。 如果要谈论真实生活中并行性的例子，我们计算机的图形卡就是一个例子，它强调了并行处理的真正能力，因为它拥有数百个独立工作的独立处理内核，并且可以同时执行。 由于这个原因，我们也能够运行高端应用程序和游戏。\n异步和多线程区别？（原理篇） 1 异步和多线程有什么区别？ 其实，异步是目的，而多线程是实现这个目的的方法。异步是说，A发起一个操作后（一般都是比较耗时的操作，如果不耗时的操作就没有必要异步了），可以继续自顾自的处理它自己的事儿，不用干等着这个耗时操作返回。\n2 多线程和异步操作的异同 多线程和异步操作两者都可以达到避免调用线程阻塞的目的，从而提高软件的可响应性。甚至有些时候我们就认为多线程和异步操作是等同的概念。但是，多线程和异步操作还是有一些区别的。而这些区别造成了使用多线程和异步操作的时机的区别。\n3 异步操作的本质 所有的程序最终都会由计算机硬件来执行，所以为了更好的理解异步操作的本质，我们有必要了解一下它的硬件基础。 熟悉电脑硬件的朋友肯定对DMA这个词不陌生，硬盘、光驱的技术规格中都有明确DMA的模式指标，其实网卡、声卡、显卡也是有DMA功能的。DMA就是直 接内存访问的意思，也就是说，拥有DMA功能的硬件在和内存进行数据交换的时候可以不消耗CPU资源。只要CPU在发起数据传输时发送一个指令，硬件就开 始自己和内存交换数据，在传输完成之后硬件会触发一个中断来通知操作完成。这些无须消耗CPU时间的I/O操作正是异步操作的硬件基础。所以即使在DOS 这样的单进程（而且无线程概念）系统中也同样可以发起异步的DMA操作。\n4 线程的本质 线程不是一个计算机硬件的功能，而是操作系统提供的一种逻辑功能，线程本质上是进程中一段并发运行的代码，所以线程需要操作系统投入CPU资源来运行和调度。\n5 异步操作的优缺点 因为异步操作无须额外的线程负担，并且使用回调的方式进行处理，在设计良好的情况下，处理函数可以不必使用共享变量（即使无法完全不用，最起码可以减少 共享变量的数量），减少了死锁的可能。当然异步操作也并非完美无暇。编写异步操作的复杂程度较高，程序主要使用回调方式进行处理，与普通人的思维方式有些 初入，而且难以调试。\n6 多线程的优缺点 多线程的优点很明显，线程中的处理程序依然是顺序执行，符合普通人的思维习惯，所以编程简单。但是多线程的缺点也同样明显，线程的使用（滥用）会给系统带来上下文切换的额外负担。并且线程间的共享变量可能造成死锁的出现。\n异步与多线程,从辩证关系上来看,异步和多线程并不时一个同等关系,异步是目的,多线程只是我们实现异步的一个手段.什么是异步:异步是当一个调用请求发送给被调用者,而调用者不用等待其结果的返回.实现异步可以采用多线程技术或则交给另外的进程来处理\n深入理解并发/并行，阻塞/非阻塞，同步/异步 并发vs并行 15分钟读懂进程线程、同步异步、阻塞非阻塞、并发并行\n","date":"2022-10-25","permalink":"/post/cp_1_%E9%98%BB%E5%A1%9E%E9%9D%9E%E9%98%BB%E5%A1%9E%E5%BC%82%E6%AD%A5%E5%90%8C%E6%AD%A5%E5%B9%B6%E8%A1%8C%E5%B9%B6%E5%8F%91/","tags":["Python"],"title":"CP_1_阻塞非阻塞、异步同步、并行并发"},{"content":"概念 进程就是操作系统中执行的一个程序，操作系统以进程为单位分配存储空间，每个进程都有自己的地址空间、数据栈以及其他用于跟踪进程执行的辅助数据，操作系统管理所有进程的执行，为它们合理的分配资源。进程可以通过fork或spawn的方式来创建新的进程来执行其他的任务，不过新的进程也有自己独立的内存空间，因此必须通过进程间通信机制（IPC，Inter-Process Communication）来实现数据共享，具体的方式包括管道、信号、套接字、共享内存区等。\n进程被定义为一个实体，它代表了系统中要实施的基本工作单元。 简而言之，我们将计算机程序编写成文本文件，当我们执行这个程序时，它就成为执行程序中提到的所有任务的过程。 在进程生命周期中，它经历了不同的阶段 - 开始，准备，运行，等待和终止。//原文出自【易百教程】，商业转载请联系作者获得授权，非商业请保留原文链接：https://www.yiibai.com/concurrency_in_python/concurrency_in_python_introduction.html\n创建进程 1、以指定函数作为 target，创建 Process 对象即可创建新进程。 import multiprocessing import os # 定义一个普通的action函数，该函数准备作为进程执行体 def action(max): for i in range(max): print(\u0026quot;(%s)子进程（父进程:(%s)）：%d\u0026quot; % (os.getpid(), os.getppid(), i)) if __name__ == '__main__': # 下面是主程序（也就是主进程） for i in range(100): print(\u0026quot;(%s)主进程: %d\u0026quot; % (os.getpid(), i)) if i == 20: # 创建并启动第一个进程 mp1 = multiprocessing.Process(target=action,args=(100,)) mp1.start() # 创建并启动第一个进程 mp2 = multiprocessing.Process(target=action,args=(100,)) mp2.start() mp2.join() print('主进程执行完成!')  2、继承Process类创建子进程 继承 Process 类创建子进程的步骤如下： 定义继承 Process 的子类，重写其 run() 方法准备作为进程执行体。 创建 Process 子类的实例。 调用 Process 子类的实例的 start() 方法来启动进程。 下面程序通过继承 Process 类来创建子进程：\nimport multiprocessing import os class MyProcess(multiprocessing.Process): def __init__(self, max): self.max = max super().__init__() # 重写run()方法作为进程执行体 def run(self): for i in range(self.max): print(\u0026quot;(%s)子进程（父进程:(%s)）：%d\u0026quot; % (os.getpid(), os.getppid(), i)) if __name__ == '__main__': # 下面是主程序（也就是主进程） for i in range(100): print(\u0026quot;(%s)主进程: %d\u0026quot; % (os.getpid(), i)) if i == 20: # 创建并启动第一个进程 mp1 = MyProcess(100) mp1.start() # 创建并启动第一个进程 mp2 = MyProcess(100) mp2.start() mp2.join() print('主进程执行完成!')  进程启动的三种方式 根据不同的平台， multiprocessing 支持三种启动进程的方法。这些 启动方法 有\n1）spawn 父进程会启动一个全新的 python 解释器进程。 子进程将只继承那些运行进程对象的 run() 方法所必需的资源。 特别地，来自父进程的非必需文件描述符和句柄将不会被继承。 使用此方法启动进程相比使用 fork 或 forkserver 要慢上许多。\n可在Unix和Windows上使用。 Windows上的默认设置。\n2）fork 父进程使用 os.fork() 来产生 Python 解释器分叉。子进程在开始时实际上与父进程相同。父进程的所有资源都由子进程继承。请注意，安全分叉多线程进程是棘手的。\n只存在于Unix。Unix中的默认值。\n3）forkserver 程序启动并选择* forkserver * 启动方法时，将启动服务器进程。从那时起，每当需要一个新进程时，父进程就会连接到服务器并请求它分叉一个新进程。分叉服务器进程是单线程的，因此使用 os.fork() 是安全的。没有不必要的资源被继承。\n可在Unix平台上使用，支持通过Unix管道传递文件描述符。\n在 3.8 版更改: 对于 macOS，spawn 启动方式是默认方式。 因为 fork 可能导致subprocess崩溃，被认为是不安全的，查看 bpo-33725 。\n在 3.4 版更改: 在所有unix平台上添加支持了 spawn ，并且为一些unix平台添加了 forkserver 。在Windows上子进程不再继承所有可继承的父进程句柄。\n在 Unix 上通过 spawn 和 forkserver 方式启动多进程会同时启动一个 资源追踪 进程，负责追踪当前程序的进程产生的、并且不再被使用的命名系统资源(如命名信号量以及 SharedMemory 对象)。当所有进程退出后，资源追踪会负责释放这些仍被追踪的的对象。通常情况下是不会有这种对象的，但是假如一个子进程被某个信号杀死，就可能存在这一类资源的“泄露”情况。（泄露的信号量以及共享内存不会被释放，直到下一次系统重启，对于这两类资源来说，这是一个比较大的问题，因为操作系统允许的命名信号量的数量是有限的，而共享内存也会占据主内存的一片空间）\n要选择一个启动方法，你应该在主模块的 if name == \u0026lsquo;main\u0026rsquo; 子句中调用 set_start_method() 。例如：\nimport multiprocessing as mp def foo(q): q.put('hello') if __name__ == '__main__': mp.set_start_method('spawn') q = mp.Queue() p = mp.Process(target=foo, args=(q,)) p.start() print(q.get()) p.join()  在程序中 set_start_method() 不应该被多次调用。\n或者，你可以使用 get_context() 来获取上下文对象。上下文对象与 multiprocessing 模块具有相同的API，并允许在同一程序中使用多种启动方法。:\nimport multiprocessing as mp def foo(q): q.put('hello') if __name__ == '__main__': ctx = mp.get_context('spawn') q = ctx.Queue() p = ctx.Process(target=foo, args=(q,)) p.start() print(q.get()) p.join()  请注意，对象在不同上下文创建的进程间可能并不兼容。 特别是，使用 fork 上下文创建的锁不能传递给使用 spawn 或 forkserver 启动方法启动的进程。\n想要使用特定启动方法的库应该使用 get_context() 以避免干扰库用户的选择\n进程池 Pool 进程池内部维护一个进程序列，当使用时，则去进程池中获取一个进程，如果进程池序列中没有可供使用的进进程，那么程序就会等待，直到进程池中有可用进程为止。就是固定有几个进程可以使用。\n如果在Python应用程序中讨论简单的并行处理任务，那么多处理模块提供了Pool类。 下面的Pool类方法可以用来在主程序中创建多个子进程。 apply()方法该方法与ThreadPoolExecutor的submit()方法类似，直到结果准备就绪。 apply_async()方法当需要并行执行任务时，需要使用apply_async()方法将任务提交给池。 这是一个异步操作，直到执行完所有的子进程之后才会锁定主线程。 map()方法就像apply()方法一样，它也会阻塞直到结果准备就绪。 它相当于内置的map()函数，它将多个块中的可迭代数据分开并作为单独的任务提交给进程池。 map_async()方法它是map()方法的一个变体，apply_async()是apply()方法的变体。 它返回一个结果对象。 当结果准备就绪时，就会应用一个可调用对象。 可调用函数必须立即完成; 否则，处理结果的线程将被阻止。 例子 以下示例实现执行并行执行的进程池。 通过multiprocessing.Pool方法应用square()函数，可以简单计算数字的平方。 然后使用pool.map()提交5，因为输入是从0到4的整数列表。结果将被存储在p_outputs中并被打印输出结果 -\ndef square(n): result = n*n return result if __name__ == '__main__': inputs = list(range(5)) p = multiprocessing.Pool(processes = 4) p_outputs = pool.map(function_square, inputs) p.close() # 关闭进程池，不允许继续添加进程 p.join() # 等待进程池中的所有进程结束 print ('Pool :', p_outputs)  Concurrent.futures Python标准库有一个叫做concurrent.futures的模块。 这个模块是在Python 3.2中添加的，为开发人员提供了启动异步任务的高级接口。 它是Python的线程和多处理模块的顶层的一个抽象层，用于提供使用线程或进程池运行任务的接口。//原文出自【易百教程】，商业转载请联系作者获得授权，非商业请保留原文链接：https://www.yiibai.com/concurrency_in_python/concurrency_in_python_pool_of_processes.html\nProcessPoolExecutor - 一个具体的子类 它是Executor类的具体子类之一。 它使用多重处理，并且我们获得提交任务的过程池。 此池将任务分配给可用的进程并安排它们运行。//原文出自【易百教程】，商业转载请联系作者获得授权，非商业请保留原文链接：https://www.yiibai.com/concurrency_in_python/concurrency_in_python_pool_of_processes.html\nfrom concurrent.futures import ProcessPoolExecutor from time import sleep def task(message): sleep(2) return message def main(): executor = ProcessPoolExecutor(5) future = executor.submit(task, (\u0026quot;Completed\u0026quot;)) print(future.done()) sleep(2) print(future.done()) print(future.result()) if __name__ == '__main__': main() # 原文出自【易百教程】，商业转载请联系作者获得授权，非商业请保留原文链接：https://www.yiibai.com/concurrency_in_python/concurrency_in_python_pool_of_processes.html  实例化ProcessPoolExecutor - 上下文管理器 实例化ProcessPoolExecutor的另一种方法是借助上下文管理器。 它的工作方式与上例中使用的方法类似。 使用上下文管理器的主要优点是它在语法上看起来不错。 实例化可以在下面的代码的帮助下完成 - with ProcessPoolExecutor(max_workers = 5) as executor\nimport concurrent.futures from concurrent.futures import ProcessPoolExecutor import urllib.request URLS = ['http://www.foxnews.com/', 'http://www.cnn.com/', 'http://europe.wsj.com/', 'http://www.bbc.co.uk/', 'http://some-made-up-domain.com/'] def load_url(url, timeout): with urllib.request.urlopen(url, timeout = timeout) as conn: return conn.read() def main(): with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(load_url, url, 60): url for url in URLS} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: data = future.result() except Exception as exc: print('%r generated an exception: %s' % (url, exc)) else: print('%r page is %d bytes' % (url, len(data))) if __name__ == '__main__': main() # 原文出自【易百教程】，商业转载请联系作者获得授权，非商业请保留原文链接：https://www.yiibai.com/concurrency_in_python/concurrency_in_python_pool_of_processes.html  使用Executor.map()函数 Python map()函数广泛用于执行许多任务。 一个这样的任务是对可迭代内的每个元素应用某个函数。 同样，可以将迭代器的所有元素映射到函数，并将这些作为独立作业提交给ProcessPoolExecutor。//原文出自【易百教程】，商业转载请联系作者获得授权，非商业请保留原文链接：https://www.yiibai.com/concurrency_in_python/concurrency_in_python_pool_of_processes.html\nfrom concurrent.futures import ProcessPoolExecutor from concurrent.futures import as_completed values = [2,3,4,5] def square(n): return n * n def main(): with ProcessPoolExecutor(max_workers = 3) as executor: results = executor.map(square, values) for result in results: print(result) if __name__ == '__main__': main() # 原文出自【易百教程】，商业转载请联系作者获得授权，非商业请保留原文链接：https://www.yiibai.com/concurrency_in_python/concurrency_in_python_pool_of_processes.html  进程间通信 管道 Pipe 顾名思义，管道Pipe 有两端，因而 main_conn, child_conn = Pipe() ，管道的两端可以放在主进程或子进程内，我在实验中没发现主管道口main_conn 和子管道口child_conn 的区别。两端可以同时放进去东西，放进去的对象都经过了深拷贝：用 conn.send()在一端放入，用 conn.recv() 另一端取出，管道的两端可以同时给多个进程。conn是 connect的缩写。\n队列 Queue 可以 import queue 调用Python内置的队列，在多线程里也有队列 from multiprocessing import Queue。下面提及的都是多线程的队列。\n队列Queue 的功能与前面的管道Pipe非常相似：无论主进程或子进程，都能访问到队列，放进去的对象都经过了深拷贝。不同的是：管道Pipe只有两个断开，而队列Queue 有基本的队列属性，更加灵活，详细请移步Stack Overflow Multiprocessing - Pipe vs Queue。\n共享内存 Manager 为了在Python里面实现多进程通信，上面提及的 Pipe Queue 把需要通信的信息从内存里深拷贝了一份给其他线程使用（需要分发的线程越多，其占用的内存越多）。而共享内存会由解释器负责维护一块共享内存（而不用深拷贝），这块内存每个进程都能读取到，读写的时候遵守管理（因此不要以为用了共享内存就一定变快）。\nManager可以创建一块共享的内存区域，但是存入其中的数据需要按照特定的格式，Value可以保存数值，Array可以保存数组，如下。这里不推荐认为自己写代码能力弱的人尝试。下面这里例子来自Python官网的Document。\n设计高性能的多进程时，会遵守以下规则： https://docs.python.org/zh-cn/3/library/multiprocessing.html#the-process-class 尽可能少传一点数据 尽可能减少主线程的负担 尽可能不让某个进程傻等着 尽可能减少进程间通信的频率\n在Python中优雅地用多进程 Python Process创建进程（2种方法）详解\n","date":"2022-10-25","permalink":"/post/cp_2_%E8%BF%9B%E7%A8%8B/","tags":["Python"],"title":"CP_2_进程"},{"content":"概念 一个进程还可以拥有多个并发的执行线索，简单的说就是拥有多个可以获得CPU调度的执行单元，这就是所谓的线程。由于线程在同一个进程下，它们可以共享相同的上下文，因此相对于进程而言，线程间的信息共享和通信更加容易。当然在单核CPU系统中，真正的并发是不可能的，因为在某个时刻能够获得CPU的只有唯一的一个线程，多个线程共享了CPU的执行时间。\nGIL 全称是Global Interpreter Lock(全局解释器锁) 来源是python设计之初的考虑，为了数据安全所做的决定。\n在Python多线程下，每个线程的执行方式： 1.获取GIL 2.执行代码直到sleep或者是python虚拟机将其挂起。 3.释放GIL\npython3.x中，GIL不使用ticks计数，改为使用计时器（执行时间达到阈值后，当前线程释放GIL），这样对CPU密集型程序更加友好，但依然没有解决GIL导致的同一时间只能执行一个线程的问题，所以效率依然不尽如人意。 在python2.x里，GIL的释放逻辑是当前线程遇见IO操作或者ticks计数达到100（ticks可以看作是python自身的一个计数器，专门做用于GIL，每次释放后归零，这个计数可以通过 sys.setcheckinterval 来调整），进行释放。 而每次释放GIL锁，线程进行锁竞争、切换线程，会消耗资源。并且由于GIL锁存在，python里一个进程永远只能同时执行一个线程(拿到GIL的线程才能执行)，这就是为什么在多核CPU上，python的多线程效率并不高。\n1、CPU密集型代码(各种循环处理、计数等等)，在这种情况下，ticks计数很快就会达到阈值，然后触发GIL的释放与再竞争（多个线程来回切换当然是需要消耗资源的），所以python下的多线程对CPU密集型代码并不友好。\n2、IO密集型代码(文件处理、网络爬虫等)，多线程能够有效提升效率(单线程下有IO操作会进行IO等待，造成不必要的时间浪费，而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU的资源，从而能提升程序执行效率)。所以python的多线程对IO密集型代码比较友好。\n多核多线程比单核多线程更差，原因是单核下多线程，每次释放GIL，唤醒的那个线程都能获取到GIL锁，所以能够无缝执行，但多核下，CPU0释放GIL后，其他CPU上的线程都会进行竞争，但GIL可能会马上又被CPU0拿到，导致其他几个CPU上被唤醒后的线程会醒着等待到切换时间后又进入待调度状态，这样会造成线程颠簸(thrashing)，导致效率更低\n2、每个CPU在同一时间只能执行一个线程（在单核CPU下的多线程其实都只是并发，不是并行，并发和并行从宏观上来讲都是同时处理多路请求的概念。但并发和并行又有区别，并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔内发生。）\n死锁 谈谈python的GIL、多线程、多进程\n","date":"2022-10-25","permalink":"/post/cp_3_%E7%BA%BF%E7%A8%8B/","tags":["Python"],"title":"CP_3_线程"},{"content":"","date":"2022-10-25","permalink":"/post/cp_4_%E5%8D%8F%E7%A8%8B/","tags":["Python"],"title":"CP_4_协程"},{"content":"Int(整型) 标准的 Python 实现是用 C 语言编写的。这意味着每一个 Python 对象都是一个伪 C 语言结构体，该结构体不仅包含其值，还有其他信息。例如，当我们在 Python 中定义一个整型，例如 x = 10000 时，x 并不是一个“原生”整型，而是一个指针，指向一个 C 语言的复合结构体，结构体里包含了一些值 查看 Python 3.4 的源代码，可以发现整型（长整型）的定义，如下所示（C 语言的宏经过扩展之后）：\nstruct _longobject { long ob_refcnt; PyTypeObject *ob_type; size_t ob_size; long ob_digit[1]; };  Python 3.4 中的一个整型实际上包括 4 个部分。 ob_refcnt 是一个引用计数，它帮助 Python 默默地处理内存的分配和回收。 ob_type 将变量的类型编码。 ob_size 指定接下来的数据成员的大小。 ob_digit 包含我们希望 Python 变量表示的实际整型值。 这意味着与 C 语言这样的编译语言中的整型相比，在 Python 中存储一个整型会有一些开销， 两者的差异在于，C 语言整型本质上是对应某个内存位置的标签，里面存储的字节会编码成整型。而 Python 的整型其实是一个指针，指向包含这个 Python 对象所有信息的某个内存位置，其中包括可以转换成整型的字节。由于 Python 的整型结构体里面还包含了大量额外的信息，所以 Python 可以自由、动态地编码。但是，Python 类型中的这些额外信息也会成为负担，在多个对象组合的结构体中尤其明显。\nStr(字符串) 通过紧凑数组 实现字符串的存储。\n字符串数据在内存中是连续存放的，空间利用率高。\n原因是：每个字符的大小是固定的，因此一个字符串的大小也是固定的，可以分配一个固定大小的空间给字符串。\n同为序列类型，为什么列表采用引用数组，而字符串采用紧凑数据。 虽然同为序列类型，但列表可以存储的元素类型是多种多样的，并且列表是可变的，无法预估内存空间，所以列表不能通过紧凑数组。\nList(列表) 通过 引用数组 实现列表元素的存储 列表中存储的并不是我们看到的元素的值，而是这些元素的地址 列表所谓的连续，是在内存中连续存储元素的地址，而元素的值是在内存中分散存储的 当访问到列表的某个元素时，是按照列表中存储的元素地址去找到元素的值\n直接赋值，是完完全全的没改变原列表的任何内容，就是原来的列表多了一个别名。\n浅拷贝，确实是把列表拷贝了一份，也就是把列表中存储的地址全部拷贝了一份给新列表，新列表拥有一份独立的地址信息。但这些地址指向的元素和原列表是同一份元素。总结，浅拷贝只是把地址重新拷贝了一份，他们指向的内容还是同一份内容。\n列表的切片也属于浅拷贝。\nlist_one = [1,2,3] list_c = list_one[:2] list_b = list_one[:] list_a = list(list_one) # 等同于 list_one.copy() list_d = list_one.copy() print(list_one,id(list_one[0])) print(list_c,id(list_c[0])) print(list_b,id(list_b[0])) print(list_a,id(list_a[0])) print(list_d,id(list_d[0])) # [1, 2, 3] 139937584847152 # [1, 2] 139937584847152 # [1, 2, 3] 139937584847152 # [1, 2, 3] 139937584847152 # [1, 2, 3] 139937584847152  新增元素：\nlist_one = [1,2,3] print(list_one,id(list_one[0])) list_one.append(1) print(list_one,id(list_one[0]),id(list_one[3])) # [1, 2, 3] 139640997710128 # [1, 2, 3, 1] 139640997710128 139640997710128  新增元素就是增加一个元素的地址。\n列表是 Python 中的标准可变多元素容器。可以创建一个整型值列表，一个字符串列表，因为 Python 的动态类型特性，甚至可以创建一个异构的列表。 但是想拥有这种灵活性也是要付出一定代价的：为了获得这些灵活的类型，列表中的每一项必须包含各自的类型信息、引用计数和其他信息；也就是说，每一项都是一个完整的 Python 对象。来看一个特殊的例子，如果列表中的所有变量都是同一类型的，那么很多信息都会显得多余——将数据存储在固定类型的数组中应该会更高效。 在实现层面，数组基本上包含一个指向连续数据块的指针。另一方面，Python 列表包含一个指向指针块的指针，这其中的每一个指针对应一个完整的 Python 对象（如前面看到的 Python 整型）。另外，列表的优势是灵活，因为每个列表元素是一个包含数据和类型信息的完整结构体，而且列表可以用任意类型的数据填充。固定类型的 NumPy 式数组缺乏这种灵活性，但是能更有效地存储和操作数据。\nTuple(元组) tuple 本质上就是顺序表，不可修改不可扩容，只读； tuple 和 list 相似，本质也是一个数组，但是空间大小固定。不同于一般数组，Python 的 tuple 做了许多优化，来提升在程序中的效率。\n举个例子，为了提高效率，避免频繁的调用系统函数 free 和 malloc 向操作系统申请和释放空间，tuple 源文件中定义了一个 free_list： static PyTupleObject *free_list[PyTuple_MAXSAVESIZE]; 所有申请过的，小于一定大小的元组，在释放的时候会被放进这个 free_list 中以供下次使用。也就是说，如果以后需要再去创建同样的 tuple，Python 就可以直接从缓存中载入。\n当内部有缓存后，再次创建 元组时，会优先去缓存中获取，如果缓存中没有，则再开辟空间去存储元组。\n 空元祖 永远都用同一个。 非空元组 优先去缓存free_list中获取。  tuple_one = (1,2,3) tuple_two = (1,2,3) print(tuple_one,id(tuple_one)) print(tuple_two,id(tuple_two)) # (1, 2, 3) 140576284075712 # (1, 2, 3) 140576284075712  Dict(字典) 通过稀疏数组 实现值的存储与访问\n  字典的创建过程 1.创建一个散列表（稀疏数组，N \u0026raquo;n，可以动态扩充） 2.通过hash()计算键的散列值 3.根据计算的散列值确定其在散列表中的位置（个别时候有哈希冲突，解决办法是开放寻址法 或 链接法 ） 4.在该位置上存入值\n  字典的访问过程 1.计算要访问的键的散列值 2.根据计算的散列值，按照一定的规则，确定其在散列表中的位置 3.读取该位置上存储的值（存在则返回该值，不存在则报错 KeyError）\n  字典数据类型，以空间换时间，内存占用大，空间利用率低，但查找速度快（稀疏数组 N \u0026raquo; n，否则会产生很多冲突，另外动态扩充也是）因为键在字典中显示的顺序，与实际计算出来的它在散列表中的存放位置，是两码事，因此字典表现为无序的\nSet(集合) python的内置集合类型有两种：\nset(): 一种可变的、无序的、有限的集合，其元素是唯一的、不可变的（可哈希的）对象。 frozenset(): 一种不可变的、可哈希的、无序的集合，其元素是唯一的，不可变的哈希对象。  CPython 中集合和字典非常相似。事实上，集合被实现为带有空值的字典，只有键才是实际的集合元素。此外，集合还利用这种没有值的映射做了其它的优化。\n由于这一点，可以快速的向集合中添加元素、删除元素、检查元素是否存在。平均时间复杂度为O(1),最坏的事件复杂度是O(n)。\nset 集合和 dict 一样也是基于散列表的，只是他的表元只包含值的引用而没有对键的引用，其他的和 dict 基本上是一致的。\n可变对象与不可变对象 当对象的值发生变化，但内存地址没有改变时，则说明是可变类型。 当对象的值发生变化，内存地址也发生改变时，则说明是不可变类型。\n在引用不可变对象时，会寻找该对象是否被创建过，若该对象已创建，则变量会直接引用该对象，不会再申请新的内存空间。\n以元组为例子： tuple_one = (1,2,3) tuple_two = (1,2,3) print(tuple_one,id(tuple_one)) print(tuple_two,id(tuple_two)) # (1, 2, 3) 140576284075712 # (1, 2, 3) 140576284075712  += 对于不可变对象实际是生成了一个新对象并赋值，而对于可变对象则是相当于在原来的对象上进行修改。\none = [1,2,3] print(one,id(one)) one = one + [4] print(one,id(one)) a = [1,2,3] print(a,id(a)) a += [4] print(a,id(a)) # [1, 2, 3] 140499252778496 # [1, 2, 3, 4] 140499252779136 # [1, 2, 3] 140499252778496 # [1, 2, 3, 4] 140499252778496  直接赋值、深拷贝、浅拷贝 直接赋值：其实就是对象的引用（别名）。\ntuple_one = (1,2,[3]) tuple_two = tuple_one tuple_two[2].append(1) print(tuple_one,id(tuple_one)) print(tuple_two,id(tuple_two)) # (1, 2, [3, 1]) 140151642064320 # (1, 2, [3, 1]) 140151642064320  浅拷贝(copy)：拷贝父对象，不会拷贝对象的内部的子对象。\nimport copy tuple_one = (1,2,[3]) tuple_two = copy.copy(tuple_one) tuple_two[2].append(1) print(tuple_one,id(tuple_one)) print(tuple_two,id(tuple_two)) # (1, 2, [3, 1]) 140459377247872 # (1, 2, [3, 1]) 140459377247872  深拷贝(deepcopy)： copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象。\nimport copy tuple_one = (1,2,[3]) tuple_two = copy.deepcopy(tuple_one) tuple_two[2].append(1) print(tuple_one,id(tuple_one)) print(tuple_two,id(tuple_two)) # (1, 2, [3]) 139789956724224 # (1, 2, [3, 1]) 139789955601472  函数参数传递 值传递 不可变对象 值传递，实际上就是将实际参数值的副本（复制品）传入函数，而参数本身不会受到任何影响。\n引用传递 如果实际参数的数据类型是可变对象（列表、字典），则函数参数的传递方式将采用引用传递方式。需要注意的是，引用传递方式的底层实现，采用的依然还是值传递的方式。\na = [1,2,3] def app(li): li.append(4) print(a) app(a) print(a) # [1, 2, 3] # [1, 2, 3, 4]  两个例子： 一：\nlist_one = [[]]*3 list_one[0].append(1) print(list_one) # [[1], [1], [1]]  原因：\nlist_one = [[]]*3 for i in list_one: print(id(i)) # 140477489690112 # 140477489690112 # 140477489690112  二：\ndef append_to(element, to=[]): to.append(element) return to my_list = append_to(12) print(my_list) my_other_list = append_to(42) print(my_other_list) # [12] # [12, 42]  原因：在定义函数的时候，t默认参数 to 的值就被计算出来了。 其类似于：\na = [1,2,3] def app(li): li.append(4) print(a) app(a) print(a) # [1, 2, 3] # [1, 2, 3, 4]  python基本数据类型底层实现 python的几种常见数据类型底层细节实现 Python列表和元组的底层实现 Python 直接赋值、浅拷贝和深度拷贝解析\n","date":"2022-10-25","permalink":"/post/datatype-datail-and-deep-shallow-copy/","tags":["Python"],"title":"datatype-datail-and-deep-shallow-copy"},{"content":"闭包 1、闭包的定义：在一些语言中，在函数中可以（嵌套）定义另一个函数时，如果内部的函数引用了外部的函数的变量，则可能产生闭包。闭包可以用来在一个函数与一组“私有”变量之间创建关联关系。在给定函数被多次调用的过程中，这些私有变量能够保持其持久性。—— 维基百科\n2、闭包的作用：闭包可以用来在一个函数与一组“私有”变量之间创建关联关系。在给定函数被多次调用的过程中，这些私有变量能够保持其持久性（保存运行环境与变量状态）\n3、闭包的特征： （1）必须要有函数的嵌套。而且外层函数必须返回内层函数，但是内层函数可以不返回至，也可以返回值；外层函数给内层函数提供了一个“包装起来的运行环境”，在这个“包装的”运行环境里面，内层函数可以完全自己做主。这也是为什么称之为闭包的原因了。\n（2）内层函数一定要用到外层函数中定义的变量。如果只满足了特征（1），也不算是闭包，一定要用到外层“包装函数”的变量，这些变量称之为“自由变量”。\n闭包例子：\ndef decorator(c): # 外层函数，产生包装环境——即闭包 d=200 # c d 都是包装环境中的局部变量——即自由变量 def wrapper(a,b): # 内层函数 return (a+b)*c/d return wrapper wrapper=decorator(150) print(wrapper(100,300)  我们可以通过内层函数的一个属性__closure__查看。\nprint(wrapper.__closure__) print(wrapper.__closure__[0].cell_contents) print(wrapper.__closure__[1].cell_contents) 返回如下结果： (, ) # __closure__属性返回一个元组 150 #对应第一个自由变量c 200 #对应第二个自由变量d  总结：内层函数的__closure__属性返回一个元组； 通过wrapper.closure[i].cell_contents 查看第几个自由变量的值 注意：如果闭包函数没有返回wrapper，即外层函数没有返回内层函数，内层函数是没有__closure__属性的。\n装饰器 1、什么是装饰器？ —— 两个层面\n在Python里面有两层定义：\n第一：从设计模式的层面上 装饰器是一个很著名的设计模式，经常被用于有切面需求的场景，较为经典的应用有插入日志、增加计时逻辑来检测性能、加入事务处理等。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。\n第二：从Python的语法层面上（其实第二种本质上也是第一种，只不过在语法上进行了规范化） 简言之，python装饰器就是用于拓展原来函数功能的一种函数，这个函数的特殊之处在于它的返回值也是一个函数，使用python装饰器的好处就是在不用更改原函数的代码前提下给函数增加新的功能。 如此一来，我们要想拓展原来函数代码，就不需要再在函数里面修改源代码了。\n2、装饰器的作用——两方面 （1）抽离雷同代码，加以重用\n（2）为函数添加额外的功能 （“添加额外功能”是一种抽象的表述，不是说一定要添加什么东西，对被装饰的对象（函数、类）进行某种约束、处理、添加、删减等额外操作统称为添加额外功能。）\n3、装饰器的使用场景 （1）缓存装饰器 （2）权限验证装饰器 （3）计时装饰器 （4）日志装饰器 （5）路由装饰器 （6）异常处理装饰器 （7）错误重试装饰器\n函数装饰器 函数装饰函数 def decorator(function): ''' 第一层函数为装饰器名称 function：参数，即需要装饰的函数 return：返回值wrapper，为了保持与原函数参数一致 ''' def wrapper(*arg,**args): ''' 内层函数，这个函数实现“添加额外功能”的任务 *arg,**args：参数保持与需要装饰的函数参数一致，这里用*arg和**args代替 ''' # 这里就是额外功能代码 function() # 执行原函数 # 这里就是额外功能代码 return wrapper @decorator def function(): print(\u0026quot;hello, decorator\u0026quot;)  函数装饰实例方法 def decorator(function): ''' 第一层函数为装饰器名称 function：参数，即需要装饰的函数 return：返回值wrapper，为了保持与原函数参数一致 ''' def wrapper(self, *arg, **args): ''' 内层函数，这个函数实现“添加额外功能”的任务 *arg,**args：参数保持与需要装饰的函数参数一致，这里用*arg和**args代替 ''' print('---') # 这里就是额外功能代码 return function(self) # 执行原函数 # 这里就是额外功能代码 return wrapper class out(): @decorator def inner(self): print('inner') dec = out() dec.inner() # # --- # inner  与函数装饰函数差别不大，但是由于实例方法的特殊参数self，所以需要传递\n函数装饰类 def decorator(function): ''' 第一层函数为装饰器名称 function：参数，即需要装饰的函数 return：返回值wrapper，为了保持与原函数参数一致 ''' def wrapper(*arg, **args): ''' 内层函数，这个函数实现“添加额外功能”的任务 *arg,**args：参数保持与需要装饰的函数参数一致，这里用*arg和**args代替 ''' print('---') # 这里就是额外功能代码 return function() # 执行原函数 # 这里就是额外功能代码 return wrapper @decorator class out(): def __init__(self): print('+++') def inner(self): print('inner') a=out() a.inner() # --- # +++ # inner  和函数装饰函数类似\n带参数的函数装饰器 装饰器还有更大的灵活性，例如带参数的装饰器：在上面的装饰器调用中，比如@use_logging，该装饰器唯一的参数就是执行业务的函数。装饰器的语法允许我们在调用时，提供其它参数，比如@decorator(a)。这样，就为装饰器的编写和使用提供了更大的灵活性。\ndef use_logging(level): def decorator(func): def wrapper(*args, **kwargs): if level == \u0026quot;warn\u0026quot;: logging.warn(\u0026quot;%s is running\u0026quot; % func.__name__) return func(*args) return wrapper return decorator @use_logging(level=\u0026quot;warn\u0026quot;) def foo(name='foo'): print(\u0026quot;i am %s\u0026quot; % name)  带参数的函数装饰器装饰器与不带参数的大同小异\n类装饰器 类的装饰器（不带参数） 基于类装饰器的实现，必须实现 call 和 __init__两个内置函数。 init ：接收被装饰函数 call ：实现装饰逻辑。\nclass Hint(object): def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): print('{} is running'.format(self.func.__name__)) return self.func(*args, **kwargs)  类的装饰器（带参数） 带参数和不带参数的类装饰器有很大的不同。 init ：不再接收被装饰函数，而是接收传入参数。 call ：接收被装饰函数，实现装饰逻辑。\nclass Hint(object): def __init__(self, coder=None): self.coder = coder def __call__(self, func): def wrapper(*args, **kwargs): print('{} is running'.format(func.__name__)) print('Coder: {}'.format(self.coder)) return func(*args, **kwargs) # 正式调用主要处理函数 return wrapper  创建装饰器时保留函数元信息 你写了一个装饰器作用在某个函数上，但是这个函数的重要的元信息比如名字、文档字符串、注解和参数签名都丢失了。\n解决方案:任何时候你定义装饰器的时候，都应该使用 functools 库中的 @wraps 装饰器来注解底层包装函数。\n例如：\nimport time from functools import wraps def timethis(func): ''' Decorator that reports the execution time. ''' @wraps(func) def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(func.__name__, end-start) return result return wrapper # 下面我们使用这个被包装后的函数并检查它的元信息： @timethis def countdown(n): ''' Counts down ''' while n \u0026gt; 0: n -= 1 countdown(100000) # countdown 0.008917808532714844 countdown.__name__ # 'countdown' countdown.__doc__ # '\\n\\tCounts down\\n\\t' countdown.__annotations__ # {'n': \u0026lt;class 'int'\u0026gt;}  使用偏函数与类实现装饰器 import time import functools class DelayFunc: def __init__(self, duration, func): self.duration = duration self.func = func def __call__(self, *args, **kwargs): print('Wait for {duration} seconds...'.format(duration=self.duration)) time.sleep(self.duration) return self.func(*args, **kwargs) def eager_call(self, *args, **kwargs): print('Call without delay') return self.func(*args, **kwargs) def delay(duration): \u0026quot;\u0026quot;\u0026quot; 装饰器：推迟某个函数的执行。 同时提供 .eager_call方法立即执行 \u0026quot;\u0026quot;\u0026quot; # 此处为了避免定义额外函数，直接调用functools.partial帮助构造 return functools.partial(DelayFunc, duration) @delay(duration=2) def add(a, b): return a + b  使用wrapt模块实现装饰器 import random def provide_number(min_num, max_num): \u0026quot;\u0026quot;\u0026quot;装饰器：随机生成一个在 [min_num, max_num] 范围的整数，追加为函数的第一个位置参数 \u0026quot;\u0026quot;\u0026quot; def wrapper(func): def decorated(*args, **kwargs): num = random.randint(min_num, max_num) # 将 num 作为第一个参数追加后调用函数 return func(num, *args, **kwargs) return decorated return wrapper @provide_number(1, 100) def print_random_number(num): print(num) print_random_number()  上面的装饰器可以正常运行，但假如使用在类上不增加self参数会怎样呢\nimport random def provide_number(min_num, max_num): \u0026quot;\u0026quot;\u0026quot;装饰器：随机生成一个在 [min_num, max_num] 范围的整数，追加为函数的第一个位置参数 \u0026quot;\u0026quot;\u0026quot; def wrapper(func): def decorated(*args, **kwargs): num = random.randint(min_num, max_num) # 将 num 作为第一个参数追加后调用函数 return func(num, *args, **kwargs) return decorated return wrapper class Foo: @provide_number(1, 100) def print_random_number(self, num): print(num) a=Foo() a.print_random_number() # \u0026lt;__main__.Foo object at 0x7fc4b005b0a0\u0026gt;  直接输出了类实例 self\n 之所以会出现这个结果，是因为类方法（method）和函数（function）二者在工作机制上有着细微不同。如果要修复这个问题，provider_number 装饰器在修改类方法的位置参数时，必须聪明的跳过藏在 *args 里面的类实例 self 变量，才能正确的将 num 作为第一个参数注入。\n 现在使用wrapt来创建这个装饰器看看\nimport wrapt def provide_number(min_num, max_num): @wrapt.decorator def wrapper(wrapped, instance, args, kwargs): # 参数含义： # # - wrapped：被装饰的函数或类方法 # - instance： # - 如果被装饰者为普通类方法，该值为类实例 # - 如果被装饰者为 classmethod 类方法，该值为类 # - 如果被装饰者为类/函数/静态方法，该值为 None # # - args：调用时的位置参数（注意没有 * 符号） # - kwargs：调用时的关键字参数（注意没有 ** 符号） # num = random.randint(min_num, max_num) # 无需关注 wrapped 是类方法或普通函数，直接在头部追加参数 args = (num,) + args return wrapped(*args, **kwargs) return wrapper class Foo: @provide_number(1, 100) def print_random_number(self, num): print(num) a=Foo() a.print_random_number()  使用 wrapt 模块编写的装饰器，相比原来拥有下面这些优势：\n• 嵌套层级少：使用 @wrapt.decorator 可以将两层嵌套减少为一层\n• 更简单：处理位置与关键字参数时，可以忽略类实例等特殊情况\n• 更灵活：针对 instance 值进行条件判断后，更容易让装饰器变得通用\n装饰器的嵌套 @decorator1 @decorator2 @decorator3 def func(): pass  对于多层装饰器，作用顺序为decorator3、decorator2、decorator1\n装饰器的缺点 装饰器的缺点总结： （1）被函数装饰器所装饰的对象（函数、类）已经不再是它本身了，虽然从形式上看没有变化，本质上是函数装饰器的内部wrapper；\n（2）被类装饰器所装饰的对象（函数、类）也不再是它本身了，虽然从形式上看没有变化，本质上是类装饰器的一个对象。\n闭包和装饰器的比较 装饰器（decorator） 闭包（lexical closure） 相同点 （1）都是函数的嵌套，分为外层函数和内层函数，而且外层函数要返回内层函数\n（2）代码的实现逻辑大同小异\n（3）二者都可以实现增加额外功能的目的——比如上面的“加法加密运算”\n不同点 （1）外层函数不同，装饰器的外层函数称之为decorator，闭包的外层函数称之为闭包函数closure\n（2）外层函数的目的不同，装饰器的外层函数主要是提供函数形参function，闭包的形参主要目的是提供自由变量。\n（3）二者的特征不一样。装饰器的外层函数可以不提供自由变量，但是闭包的的外层函数一定要提供自由变量，因为如果不提供自由变量，闭包的存在就毫无意义了，即内层函数所依赖的变量却在闭包中根本没有，那还要闭包干什么？\n（4）二者的主要目的不同。 装饰器的目的：代码重用+额外功能 闭包的主要目的：保存函数的运行环境 + 保存闭包的局部变量。虽然二者可以有一些交集。\n推荐 里面有很多装饰器，可以取学习学习 https://wiki.python.org/moin/PythonDecorators\nhttps://github.com/piglei/one-python-craftsman/blob/master/zh_CN/8-tips-on-decorators.md https://python3-cookbook.readthedocs.io/zh_CN/latest/c09/p02_preserve_function_metadata_when_write_decorators.html https://so.csdn.net/so/search?q=%E8%A3%85%E9%A5%B0%E5%99%A8\u0026t=blog\u0026u=qq_27825451 https://www.yisu.com/zixun/641780.html\n","date":"2022-10-25","permalink":"/post/decorator/","tags":["Python"],"title":"decorator"},{"content":"    :\n","date":"2022-10-25","permalink":"/post/descriptors/","tags":["Python"],"title":"descriptors"},{"content":"装饰器实现 函数装饰器实现单例模式 def singleton(cls): __instance = {} # 创建一个字典用来保存被装饰类的实例对象 _instance = {} def _singleton(*args, **kwargs): # 判断这个类有没有创建过对象，没有新创建一个，有则返回之前创建的 if not cls in _instance: _instance[cls] = cls(*args, **kwargs) return _instance[cls] return _singleton @singleton class A(object): def __init__(self, a=0): self.a = a a1 = A(1) a2 = A(2) # id()函数可以获取对象的内存地址，同一内存地址即为同一对象 print(id(a1), id(a2)  类装饰器实现单例模式 class Singleton(object): def __init__(self, cls): self._cls = cls self._instance = {} def __call__(self): if self._cls not in self._instance: self._instance[self._cls] = self._cls() return self._instance[self._cls] @Singleton class B(object): def __init__(self): pass b1 = B() b2 = B() print(id(b1), id(b2))  类实现 class Singleton(object): def __init__(self): pass @classmethod def instance(cls, *args, **kwargs): if not hasattr(Singleton, \u0026quot;_instance\u0026quot;): Singleton._instance = Singleton(*args, **kwargs) return Singleton._instance  一般情况，大家以为这样就完成了单例模式，但是这样当使用多线程时会存在问题\nimport time import threading class Singleton(object): _instance_lock = threading.Lock() def __init__(self): time.sleep(1) @classmethod def instance(cls, *args, **kwargs): if not hasattr(Singleton, \u0026quot;_instance\u0026quot;): with Singleton._instance_lock: if not hasattr(Singleton, \u0026quot;_instance\u0026quot;): Singleton._instance = Singleton(*args, **kwargs) return Singleton._instance def task(arg): obj = Singleton.instance() print(obj) for i in range(10): t = threading.Thread(target=task,args=[i,]) t.start() time.sleep(20) obj = Singleton.instance() print(obj)  这种方式实现的单例模式，使用时会有限制，以后实例化必须通过 obj = Singleton.instance(),如果用 obj=Singleton() ,这种方式得到的不是单例\n看到个比较新奇的 def singleton(cls): instance = cls() instance.__call__ = lambda: instance return instance # Sample use @singleton class Highlander: x = 100 # Of course you can have any attributes or methods you like. def __call__(self): pass highlander = Highlander() another_highlander = Highlander() print(id(highlander) == id(another_highlander))  官方wiki的 import functools def singleton(cls): ''' Use class as singleton. ''' cls.__new_original__ = cls.__new__ @functools.wraps(cls.__new__) def singleton_new(cls, *args, **kw): it = cls.__dict__.get('__it__') if it is not None: return it cls.__it__ = it = cls.__new_original__(cls, *args, **kw) it.__init_original__(*args, **kw) return it cls.__new__ = singleton_new cls.__init_original__ = cls.__init__ cls.__init__ = object.__init__ return cls # # Sample use: # @singleton class Foo: def __new__(cls): cls.x = 10 return object.__new__(cls) def __init__(self): assert self.x == 10 self.x = 15 assert Foo().x == 15 Foo().x = 20 assert Foo().x == 20  new() 实现 实现__new__方法，然后将类的一个实例绑定到类变量_instance上；如果cls._instance为None，则说明该类还没有被实例化过，new一个该类的实例，并返回； 如果cls._instance不为None，直接返回_instance\nclass Singleton(object): def __new__(cls, *args, **kwargs): if not hasattr(cls, '_instance'): orig = super(Singleton, cls) cls._instance = orig.__new__(cls) return cls._instance one = Singleton() two = Singleton() # one和two完全相同，可以用id()，==，is检查 print(id(one)) # 2565285375728 print(id(two)) # 2565285375728 print(one == two) # True print(one is two)  class Singleton(object): __instance=None def __init__(self): pass def __new__(cls, *args, **kwargs): if Singleton.__instance is None: Singleton.__instance=object.__new__(cls,*args, **kwargs) return Singleton.__instance one=Singleton() two=Singleton() print(id(one)) # 2488569943376 print(id(two)) # 2488569943376 print(one == two) # True print(one is two) # True  我们知道，当我们实例化一个对象时，是先执行了类的__new__方法（我们没写时，默认调用object.new），实例化对象；然后再执行类的__init__方法，对这个对象进行初始化，所有我们可以基于这个，实现单例模式\nimport threading class Singleton(object): _instance_lock = threading.Lock() def __init__(self): pass def __new__(cls, *args, **kwargs): if not hasattr(Singleton, \u0026quot;_instance\u0026quot;): with Singleton._instance_lock: if not hasattr(Singleton, \u0026quot;_instance\u0026quot;): Singleton._instance = object.__new__(cls) return Singleton._instance obj1 = Singleton() obj2 = Singleton() print(obj1,obj2) def task(arg): obj = Singleton() print(obj) for i in range(10): t = threading.Thread(target=task,args=[i,]) t.start()  采用这种方式的单例模式，以后实例化对象时，和平时实例化对象的方法一样 obj = Singleton()\n元类实现 \u0026quot;\u0026quot;\u0026quot; 1.类由type创建，创建类时，type的__init__方法自动执行，类() 执行type的 __call__方法(类的__new__方法,类的__init__方法) 2.对象由类创建，创建对象时，类的__init__方法自动执行，对象()执行类的 call 方法 \u0026quot;\u0026quot;\u0026quot;\n\u0026quot;\u0026quot;\u0026quot; class Singleton中的__init__在Myclass声明的时候被执行Myclass=Singleton() Myclass()执行时，最先执行父类的__call__方法（object,Singleton都作为Myclass的父类， 根据深度优先算法，会执行Singleton中的__call__()，Singleton中的__call__()写了单例模式） \u0026quot;\u0026quot;\u0026quot; class Singleton(type): def __init__(self, name, bases, dict): super(Singleton,self).__init__(name,bases, dict) self._instance = None def __call__(self, *args, **kwargs): if self._instance is None: self._instance = super(Singleton,self).__call__(*args, **kwargs) return self._instance class MyClass(object,metaclass=Singleton): a = 1 one=MyClass() two=MyClass() print(id(one)) # 1553247294800 print(id(two)) # 1553247294800 print(one == two) # True print(one is two) # True  import threading class SingletonType(type): _instance_lock = threading.Lock() def __call__(cls, *args, **kwargs): if not hasattr(cls, \u0026quot;_instance\u0026quot;): with SingletonType._instance_lock: if not hasattr(cls, \u0026quot;_instance\u0026quot;): cls._instance = super(SingletonType,cls).__call__(*args, **kwargs) return cls._instance class Foo(metaclass=SingletonType): def __init__(self,name): self.name = name obj1 = Foo('name') obj2 = Foo('name') print(obj1,obj2)  继承 通过__new__ 或者元类实现的子类继承即为单例模式\n使用模块 ","date":"2022-10-25","permalink":"/post/design-patterns_singleton/","tags":["Python"],"title":"design-patterns_singleton"},{"content":"operating environment 系统：manjaro-xfce-20.2 IDE：Pycharm\n个人看法： 系统的话当然是Mac最好了，但是价格摆在那。 没有的话在 Linux 和 Win更推崇使用 Linux，能够对电脑有一个更好的了解，熟悉命令之后使用体验是绝佳的，并且哪怕是长时间的待机，以及系统运行了很长的周期也不会像 Win 那样变得卡顿，而不得不去做一些清理甚至是重装系统，哪怕是重装系统，Win的软件如果多的话一个一个的安装会很耗费时间，而 Linux 的话可以在空闲的时候去编写一个脚本，下次只需准备好包、环境、依赖，运行一下脚本即可慢慢等待安装，后期再根据不同的软件稍微调整一下就可以了。\nIDE目前尝试了 Pycharm 和 Spyder ，一开始使用的是 Pycharm ，但是自己的笔记本在运行的时候有些吃力，想要换一个较为轻量级但是又无需过多配置的，就去尝试了 Spyder ，但是使用感受不是很好，打开的时候会花费很长的时间，使用中代码的提示有很长的延迟，运行的时候有些莫名奇妙的非代码运行错误的提示，创建文件夹及文件的时候，位置每一次都需要去进行选择，删除文件、文件夹的时候UI界面不会刷新，但其实已经删除了，只有重启才行，在做爬虫的时候很多时候需要复制其html、js代码进行格式化后详细分析， Spyder 无法做到，对于数据库的可视化不支持，最终，还是回到了 Pycharm。 准备：\nyay -S pkg-config yay -S gcc yay -S make  IDE 、Pycharm 以下是基于版本2019.3 (1)、去官网 ( https://www.jetbrains.com/ ) 下载自己所需的 linux 的 tar,gz 压缩包，命令行移动到自己安装的目录，个人习惯选择 /opt/\n(2)、解压包 解压全部命令参考：\ntar –xvf file.tar 解压 tar tar -xzvf file.tar.gz 解压tar.gz tar -xjvf file.tar.bz2 解压 tar.bz2 tar –xZvf file.tar.Z 解压tar.Z unrar e file.rar 解压rar unzip file.zip 解压zip 可能因为权限不够导致解压失败，建议以超级用户解压  (3)、把 jetbrains-agent.jar 破解包移动到其bin目录下。\n(4)、修改* .vmoptions 和 *64.vmoptions 这两个文件， 在文件最后加入。\n-javaagent:安装目录/jetbrains-agent.jar  (5)、启动 切换到 bin 目录下或写入绝对路径 cd /opt/soft_name/bin sh name.sh。\n(6)、进入正常配置界面，选择 Activation code 找到能用的从code添加注册即可破解。\n(7)、可能会没有快捷方式： 附上快捷方式制作： 创建快捷方式：\nsudo vim /usr/share/applications/Pycharm.desktop  粘贴模板： 第一个是最基本的能够获取到工作目录以及添加图标的，以前在 ubuntu 可以正常使用，但是在 manjaro 的时候缺少一些东西，不能对图标进行编辑，而且用的是 svg 那个图，不能被识别，添加到桌面后图标为灰色，所以用的是第二个，\n[Desktop Entry] Version=1.0 Type=Application Name=Pycharm Icon=/opt/pycharm-2019.3.1/bin/pycharm.svg Exec=\u0026quot;/opt/pycharm-2019.3.1/bin/pycharm.sh\u0026quot; %f Comment=Lightning-smart Python IDE Categories=Development;IDE; Terminal=false StartupWMClass=jetbrains-pycharm  [Desktop Entry] Categories=Development;IDE; Comment[en_US]=The smartest Python IDE Comment=The smartest Python IDE Exec=\u0026quot;/opt/pycharm-anaconda-2019.3.1/bin/pycharm.sh\u0026quot; %f GenericName[en_US]= GenericName= Icon=/opt/pycharm-anaconda-2019.3.1/bin/pycharm.png MimeType= Name[en_US]=Pycharm Name=Pycharm Path= StartupNotify=true StartupWMClass=jetbrains-pycharm Terminal=false TerminalOptions= Type=Application Version=1.0 X-DBUS-ServiceName= X-DBUS-StartupType= X-KDE-RunOnDiscreteGpu=false X-KDE-SubstituteUID=false X-KDE-Username=  我们需要替换掉两个地方：Exec=\"xx\" 和Icon= ,这里要替换掉我们的 pycharm 解压的目录， 然后保存退出之后 打开 搜索 找到图标 pycharm 然后将其拖到需要放置的位置即可\n后期有了更方便的方法，直接使用后将破解插件拖拽进去安装完成重启即可破解。  [ 2 ] 、Anaconda\nyay -S anaconda sudo vim ~/.zshrc # 添加环境变量 source ~/.zshrc # 刷新  安装后需要在 .zshrc 或 .bashrc 里设置路径 export PATH=\u0026quot;/opt/anaconda/bin:$PATH\u0026quot; 如果没用 zsh 就是在 .bashrc里设置，否则使用 conda命令无效\n貌似安装的是 Anaconda 是默认2,3整合的，整体的大小1G多，而官网的才仅仅500多M，不过最开始自己安装的时候也没怎么留意 后面更新的时候一直报错，无法正常更新，虽然更新不是必要的，但是强迫症还是想更新下，索性换个安装方法吧 最开始下载了Anaconda3-2020.07-Linux-ppc64le.sh，安装报错，版本不对无法进行编译 下载了另一个版本Anaconda3-2020.07-Linux-x86_64.sh 切换到下载的目录下执行./Anaconda3-2020.07-Linux-x86_64.sh按照提示编译即可 同样安装之后也需要配置环境变量\nVirtual environment Pyenv + Pyenv-Virtualenv yay -S pyenv # 管理版本 yay -S pyenv-virtualenv # 是一个插件，管理环境  没怎么用过，\nvirtualenv + virtualenvwrapper $ sudo pip install virtualenv $ mkdir my_venv_folder $ cd my_venv_folder # 创建虚拟环境目录:这里起名为 venv_name # 环境创建完成之后, 此目录下回新增 bin, include, lib 几个目录 $ virtualenv venv_name # 激活环境 (通常情况下, 激活环境后Linux提示符会有改变) $ source bin/activate # 查看当前环境下的 python 和 pip 的指向路径 (venv_name)...$ which python (venv_name)...$ which pip # 当前环境安装 requests 包 (venv_name)...$ pip install requests # 脱离当前环境 (venv_name)...$ deactivate # 再次查看python命令执行时, 已经是系统默认的了 $ which python  # 安装 # 使用 pip 进行安装 (记得是系统环境下哦) $ sudo pip install virtualenvwrapper # Mac自带的 six包 版本过低且不能自动更新时 $ sudo pip install virtualenvwrapper --upgrade --ignore-installed six # 环境配置 # 查看 virtualenvwrapper.sh 路径 $ which virtualenvwrapper.sh /usr/local/bin/virtualenvwrapper.sh # 配置环境变量 (以后创建的环境目录都集中放在 $WORKON_HOME 目录下管理了) # 配置文件位置: ~/.bashrc (bash) 或 ~/.zshrc (zsh) export WORKON_HOME=$HOME/.virtualenvs source /usr/local/bin/virtualenvwrapper.sh  # workon 进入|切换 环境 $ workon ENVNAME # mkvirtualenv 创建环境 $ mkvirtualenv [-a project_path] [-i package] [-r requirements_file] [virtualenv options] ENVNAME # lsvirtualenv 展示环境列表 $ lsvirtualenv [-b] [-l] [-h] # rmvirtualenv 删除环境 $ rmvirtualenv ENVNAME # cpvirtualenv 复制环境 $ cpvirtualenv ENVNAME [TARGETENVNAME] # allvirtualenv 所有环境运行命令 (比如安装包) $ allvirtualenv command with arguments $ allvirtualenv pip install -U pip # deactivate 退出当前环境 $ deactivate # mkproject 创建项目 $ mkproject [-f|--force] [-t template] [virtualenv_options] ENVNAME  使用 conda anaconda东西很多，但是目前不太需要，所以选择 miniconda，具体的选择看每个人的需求 但不管是anaconda 还是 miniconda 都能正常的使用 conda\n列出所有的环境 conda info --envs  创建虚拟环境 这次让我们来创建并命名一个新环境，然后安装另一个版本的python以及两个包 Astroid 和 Babel。\nconda create -n vit_name python=3 Astroid Babel  复制虚拟环境。 conda create -n new_vit --clone old_vit  删除虚拟环境 conda remove -n vit_name --all  注销该环境 当你完成了在snowflakes环境中的工作室，注销掉该环境并转换你的路径到先前的状态：\nsource deactivate  Tox 先保留着，万一需要的话\n 1、tox简介 tox是通用的虚拟环境管理和测试命令行工具。tox能够让我们在同一个Host上自定义出多套相互独立且隔离的python环境，每套虚拟环境中可能使用了不同的 Python 拦截器/环境变量设置/第三方依赖包。所以 tox 最典型的应用就是用于测试 Python 程序的兼容性了。tox是openstack社区最基本的测试工具，比如python程序的兼容性、UT等。 它的目标是提供最先进的自动化打包、测试和发布功能。 1）作为持续集成服务器的前端，大大减少测试工作所需时间； 2）检查软件包能否在不同的python版本或解释器下正常安装； 3）在不同的环境中运行测试代码。 一般 openstack 项目中的 tox 的功能包含了: 打源码包(sdist)、单元测试(UT)、测试覆盖率(coverage)、代码格式检查(pep8，flake) 等功能. 关键字解释：打源码包（sdist）、单元测试（UT）、测试覆盖率（coverage）、代码格式检查（pep8，flake）\n 项目地址 ： https://github.com/tox-dev/tox\nDatabase Redis 安装 wget http://download.redis.io/releases/redis-6.0.8.tar.gz tar xzf redis-6.0.8.tar.gz cd redis-6.0.8 sudo make sudo make install  运行完成后Redis会安装到/usr/local/bin目录下\n启动redis redis-server #  连接 redis-cli  MongoDB 安装 下载：https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.9.tgz 解压复制到/opt 添加环境变量 export PATH=\u0026quot;/opt/mongodb-linux-x86_64-4.0.9/bin/:$PATH\u0026quot;\n启动  mkdir -p /home/fiki/Data/mongodb mongod --dbpath /home/fiki/Documents/Data/mongodb --logpath /home/fiki/Documents/Data/mongod.log --fork --dbpath 指定数据目录 --dblog 指定日志位置 --fork 以守护进程的方式运行MongoDB，创建服务器进程  连接 mongo  MySQL 下载解压 https://cdn.mysql.com//Downloads/MySQL-8.0/mysql-8.0.21-linux-glibc2.12-x86_64.tar.xz下载包解压至/opt,解压至哪看个人喜好了\nxz -d mysql-8.0.21-linux-glibc2.12-x86_64.tar.xz tar -xv -f mysql-8.0.21-linux-glibc2.12-x86_64.tar  创建mysql用户 # 不需要创建HOME目录 useradd -r mysql -M # 将MySQL安装目录的拥有者以及用户组修改为mysql sudo chown -R mysql:mysql /opt/mysql-8.0.21-linux-glibc2.12-x86_64/  修改mysql配置文件/etc/my.cnf [client] # 设置 mysql 客户端 socket 文件位置 socket = /opt/mysql-8.0.21-linux-glibc2.12-x86_64/mysql.sock [mysql] # 设置mysql客户端默认字符集 default-character-set = UTF8MB4 socket = /opt/mysql-8.0.21-linux-glibc2.12-x86_64/mysql.sock [mysqld] innodb_buffer_pool_size = 128M # mysql 安装目录 basedir = /opt/mysql-8.0.21-linux-glibc2.12-x86_64/ # mysql 数据存储目录 datadir = /opt/mysql-8.0.21-linux-glibc2.12-x86_64/data # 端口 port = 3306 # 本机序号为1，表示master server_id = 1 # 设置 mysql 服务端 socket 文件位置 socket = /opt/mysql-8.0.21-linux-glibc2.12-x86_64/mysql.sock # 设置mysql最大连接数 max_connections = 20 # 服务端使用的字符集默认为8比特编码的latin1字符集 character-set-server = UTF8MB4 # 创建新表时将使用的默认存储引擎 default-storage-engine = INNODB # 联合查询操作所能使用的缓冲区大小，和sort_buffer_size一样，该参数对应的分配内存也是每连接独享。 join_buffer_size = 128M # MySQL执行排序使用的缓冲大小。如果想要增加ORDER BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。 sort_buffer_size = 8M # MySQL的随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。 read_rnd_buffer_size = 4M # 一个事务，在没有提交的时候，产生的日志，记录到Cache中；等到事务提交需要提交的时候，则把日志持久化到磁盘。 # 默认binlog_cache_size大小32K binlog_cache_size = 1M # 这个值（默认8）表示可以重新利用保存在缓存中线程的数量，当断开连接时如果缓存中还有空间，那么客户端的线程将被放到缓存中， # 如果线程重新被请求，那么请求将从缓存中读取,如果缓存中是空的或者是新的请求，那么这个线程将被重新创建,如果有很多新的线程， # 增加这个值可以改善系统性能.通过比较Connections和Threads_created状态的变量，可以看到这个变量的作用。(–\u0026gt;表示要调整的值) # 根据物理内存设置规则如下： # 1G —\u0026gt; 8 # 2G —\u0026gt; 16 # 3G —\u0026gt; 32 # 大于3G —\u0026gt; 64 thread_cache_size = 8 # MySQL的查询缓冲大小（从4.0.1开始，MySQL提供了查询缓冲机制）使用查询缓冲，MySQL将SELECT语句和查询结果存放在缓冲区中， # 今后对于同样的SELECT语句（区分大小写），将直接从缓冲区中读取结果。根据MySQL用户手册，使用查询缓冲最多可以达到238%的效率。 # 通过检查状态值'Qcache_%'，可以知道query_cache_size设置是否合理：如果Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲不够的情况， # 如果Qcache_hits的值也非常大，则表明查询缓冲使用非常频繁，此时需要增加缓冲大小；如果Qcache_hits的值不大，则表明你的查询重复率很低， # 这种情况下使用查询缓冲反而会影响效率，那么可以考虑不用查询缓冲。此外，在SELECT语句中加入SQL_NO_CACHE可以明确表示不使用查询缓冲 # query_cache_size = 8M # 指定单个查询能够使用的缓冲区大小，默认1M # query_cache_limit = 2M # 指定用于索引的缓冲区大小，增加它可得到更好处理的索引(对所有读和多重写)，到你能负担得起那样多。如果你使它太大， # 系统将开始换页并且真的变慢了。对于内存在4GB左右的服务器该参数可设置为384M或512M。通过检查状态值Key_read_requests和Key_reads， # 可以知道key_buffer_size设置是否合理。比例key_reads/key_read_requests应该尽可能的低， # 至少是1:100，1:1000更好(上述状态值可以使用SHOW STATUS LIKE 'key_read%'获得)。注意：该参数值设置的过大反而会是服务器整体效率降低 key_buffer_size = 4M # 分词词汇最小长度，默认4 ft_min_word_len = 4 # MySQL支持4种事务隔离级别，他们分别是： # READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, SERIALIZABLE. # 如没有指定，MySQL默认采用的是REPEATABLE-READ，ORACLE默认的是READ-COMMITTED transaction_isolation = REPEATABLE-READ # 默认时区 default-time_zone = '+8:00' sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO  添加环境变量 export PATH=/opt/mysql-8.0.21-linux-glibc2.12-x86_64/bin/:$PATH  初始化数据库 此命令将输出root的初始密码 sudo mysqld --initialize --user=mysql --basedir=/opt/mysql-8.0.21-linux-glibc2.12-x86_64/ --datadir=/opt/mysql-8.0.21-linux-glibc2.12-x86_64/data  报错 yay -S numactl后正常\n启动 没有添加环境变量 sudo /opt/mysql-8.0.21-linux-glibc2.12-x86_64/support-files/mysql.server start 关闭 sudo /opt/mysql-8.0.21-linux-glibc2.12-x86_64/support-files/mysql.server stop 添加后 sudo mysql.server start sudo mysql.server stop  连接测试 Manjaro默认有libtinfo.so.6而没有libtinfo.so.5，软件如果需要可执行以下命令安装： sudo pacman -S ncurses5-compat-libs #或 sudo pacman -S libtinfo5#这两条命令会安装一样的包  修改密码 alter user 'root'@'localhost' identified with mysql_native_password by '新密码'; \u0026amp;\u0026amp; 修改密码，密码必改，不改初始密码是随机的很难记，而且后期使用也会提示改密码的，无法使用初始密码  Dbeaver(数据可视化工具) 本身安装不是很困难，不建议使用pacman安装，其他的软件也不建议使用pacman，里面很多的软件都不更新很久了，yay还可以，但是软件本身是收费的，不激活只能使用社区版(功能受限制)\n 多平台数据库可视化工具，目前的redis，mysql，mongodb等等都有 不要使用yay或者pacman安装，安装的是社区版，社区版的功能有限，比如不支持mnongodb，redis DBeaverEE 7.0.0及以下版本（理论上适用于目前所有新老版本）的破解 下载相应的包并安装，这里就不给出地址了，网上应该能找到。 破解方法: (1). 解压DBeaverEE到自己想要安装的位置。 (2). 将下载压缩包解压后得到dbeaver-agent.jar，把它放到你认为合适的文件夹内。为了方便管理，就放在解压的/opt/dbeaver/目录下 (3). 在DBeaverEE安装目录下找到dbeaver.ini文件 (4). 在打开的dbeaver.ini编辑窗口末行添加：\u0026quot;-javaagent:/opt/dbeaver//dbeaver-agent.jar\u0026quot; 一定要自己确认好路径，填错会导致DBeaverEE打不开！！！最好使用绝对路径。 (5). 启动DBeaverEE即可。注意快捷方式里面的默认指定位置是不对的，所以打不开，自己去修改下复制到快捷方式所在的目录即可。 (6). 如果提示错误: \u0026ldquo;Error opening zip file or JAR manifest missing : dbeaver-agent.jar\u0026rdquo; 这种情况请试着填上jar文件的绝对路径.\n 注意！！！！ 软件运行需要java环境，之前在manjaro-kde的时候不知是自带了还是不知不觉安装了运行的时候没有问题，这次换了manjaro-xfce，提示需要java环境，手动安装了jdk14,还是错，再安装jdk8还是错，无法找到环境，最后在网上找了了一个方法，找到dbeaver.ini文件，直接在里面写入安装的环境位置就可以了，留意换行不可缺少,位置的话最好按照现有的来吧，但是插入在最后是无效的\n-vm /jdk_path/bin/  -startup plugins/org.eclipse.equinox.launcher_1.5.700.v20200207-2156.jar --launcher.library plugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.1.1100.v20190907-0426 -vm /opt/jdk1.8.0_221/bin/ -vmargs -XX:+IgnoreUnrecognizedVMOptions --add-modules=ALL-SYSTEM -Xms128m -Xmx2048m -javaagent:/opt/dbeaver/dbeaver-agent.jar  Others_tools JDK 目前主要是为了配置 Dbeaver 所需的环境，记录一下吧 https://www.oracle.com/ 官网下载老版本需要登录，而且不知道不使用代理的情况下速度能怎样，索性找了一个镜像 https://mirrors.tuna.tsinghua.edu.cn/AdoptOpenJDK/\n解压后配置环境变量就可以了，不知道是不是因为使用zsh替换了之前的bash，在/etc/profile里面配置的环境变量无效，在.zshrc里面的有效\nRedisDump Charles 抓包工具 官方网站：https://www.charlespCharlesroxy.com 付费软件，第一次使用提示有免费30天的试用期限，过后据说会没过30分钟会强制关闭一次 破解： 网上提供的生成破解jar文件的网址已经无法生成，指向的github项目也提示侵权删除了，但是根据名字还能够在github找到相应的，下载了jar文件后，先正常打开一次软件，再关闭，替换原本安装文件的charles.jar文件再次打开就是破解激活的了\nPC端： 使用的话由于chrome使用了插件控制代理，占用了一些端口，而且插件、脚本过多，在打开网页的时候那些插件、脚本也会有网络请求，为了数据的干净利于分析，用firefox作为监控的浏览器。浏览器的代理、端口设置与软件一样，这样浏览器打开网页就可以捕获到相应的数据了\n手机端： 1、需要保证PC 端 和 手机端在同一网络下 2、通过 Charles 的主菜单 Help | Local IP Address 或者通过命令行工具输入 ipconfig 查看本机的 IP 地址。 3、长按手机当前连接的wifi网络，这时会有弹窗，选择修改网络，进入网络详情，勾选【高级选项】，【代理】选择手动，【代理服务器主机名】填写第一步中得到的电脑的局域网ip，【代理服务器端口】填写8888。这个时候Charles会有一个弹窗，（没有的话手机断开wifi重连一下），选择ALLOW，这个时候Charles就会对手机的请求进行抓包。[这步根据不同手机会略有差异]\n 对于Https的请求, 安装证书 Charles中选择【Help】-【SSL Proxying】-【Save Charles Root Certificate\u0026hellip;】将一个证书charles-ssl-proxying-certificate.pem下载到本地，然后将charles-ssl-proxying-certificate.pem放到手机SD卡的某一个位置，进入手机，【设置】-【安全】-【从SD卡安装】，然后选择前面放置在SD卡的charles-ssl-proxying-certificate.pem证书，将这个证书安装到手机。这样对应Https的请求就不会提示证书的问题了。\nCharles抓取http和https的请求了，但是有的时候，我们发现还是有些包抓取不到或者显示为unknown，那就可能是APP使用了SSL强验证，也就是说APP对SSL证书进行了绑定，这个时候的解决版本就是使用Xposed+JustTrustMe关闭SSL证书验证，JustTrusMe是将APP中所用于校验的API进行HOOK从而达到绕过强验证的目的。\n mitmproxy 官方网站：https://mitmproxy.org\nappium (appium -desktop) 官方网站：http://appium.io 这个需要的东西比较多,nodejs,npm,sdk,jdk等等 jdk之前有安装了Android Studio,再通过图形化的方式安装sdk sdk安装 nodejs下载相应的包解压添加环境变量就可以了,目前我用的12.18.4,自带了npm\nnpm install -g appium-doctor [[用于检查待会是否配好了appium]]的环境 appium-doctor --android # 检查环境  之前是有人提议使用 npm install -g appium 安装,但是已经提前下好了appium的包,所以检查的时候出了一些问题\n\u0026rsquo; opencv4nodejs cannot be found ' \u0026rsquo; mjpeg-comsumer cannot be found的问题 '\ncmake --version 提示不存在则安装,有跳过 yay -S cmake manjaro下安装cmkae npm -g install opencv4nodejs 但是这样会有些慢,用的国外源,建议使用代理 npm i -g mjpeg-consumer  \u0026rsquo; bundletool.jar cannot be found '\n 在https://github.com/google/bundletool/releases 下载bundletool.jar， 改名成这个bundletool 在android sdk目录下，创建bundle-tool目录，把jar包放入， 在终端切换到当前目录下 并执行chmod +655 bundletool.jar命令给jar包加权限 修改环境变量，path后追加，:$ANDROID_HOME/bundle-tool/，\n 连接手机: 手机连上电脑以后，打开手机开发人员选项。在电脑终端中运行以下命令查看手机的UDID 使用: 主要的四个参数:\n{ \u0026quot;platformName\u0026quot;: \u0026quot;Android\u0026quot;, # 设备平台。填Android或IOS \u0026quot;deviceName\u0026quot;: \u0026quot;360_n7_lite\u0026quot;, # 设备名。按上边adb查出的设备名填写即可 \u0026quot;appPackage\u0026quot;: \u0026quot;com.kmw.chemeizu\u0026quot;, # 要启动的app的包名 \u0026quot;appActivity\u0026quot;: \u0026quot;com.uzmap.pkg.LauncherUI\u0026quot; # 要启动的界面 }  获取 appPackag, appActivity 的方式: 使用sdk带的aapt工具 网上有很多的工具去提取已经安装的软件变成安装包,\n~/Android/Sdk/build-tools/30.0.2/aapt dump badging /path/softpackage_name.apk | grep \u0026quot;package\u0026quot; ~/Android/Sdk/build-tools/30.0.2/aapt dump badging /path/softpackage_name.apk | grep \u0026quot;launchable-activity\u0026quot;  tesseract 这里推荐一个可视化工具 RoboMongo/Robo 3T，它使用简单，功能强大，官方网站为 https://robomongo.org/，三大平台都支持，下载链接为 https://robomongo.org/download。 另外，还有一个简单易用的可视化工具 —— Studio 3T，它同样具有方便的图形化管理界面，官方网站为 https://studio3t.com，同样支持三大平台，下载链接为 https://studio3t.com/download/。\nLibrary requests requests 库是一个阻塞式 HTTP 请求库，当我们发出一个请求后，程序会一直等待服务器响应，直到得到响应后，程序才会进行下一步处理。\nselenium ChromeDriver Selenium 的对接 Chrome 的驱动 ChromeDriver 下载的时候需要看chrom浏览器的版本号选择相应的版本，不清楚的去官方网站看详细介绍，下载后要么移动到/usr/bin中要么添加环境变量，个人习惯为了系统的整洁选择添加环境变量 官方网站：https://sites.google.com/a/chromium.org/chromedriver 下载地址：https://chromedriver.storage.googleapis.com/index.html\nGeckoDriver Selenium 的对接 Firefox 的驱动 GeckoDriver。 下载地址：https://github.com/mozilla/geckodriver/releases\nPhantomJS PhantomJS 是一个无界面的、可脚本编程的 WebKit 浏览器引擎，它原生支持多种 Web 标准：DOM 操作、CSS 选择器、JSON、Canvas 以及 SVG。\n问题 使用会出一些问题： 大概意思是 selenium 经不支持 PhantomJS 了，解决方法有两个 一、使用较老版本的 selenium ，同时也要使用较老版本的 python3 二、使用 Selenium+Headless Firefox 或 Selenium+Headless Chrome\naiohttp aiohttp 是一个提供异步 Web 服务的库，从 Python 3.5 版本开始，Python 中加入了 async/await 关键字，使得回调的写法更加直观和人性化。aiohttp 的异步操作借助于 async/await 关键字的写法变得更加简洁，架构更加清晰。使用异步请求库进行数据抓取时，会大大提高效率。\nlxml lxml 是 Python 的一个解析库，支持 HTML 和 XML 的解析，支持 XPath 解析方式，而且解析效率非常高\nBeautifulSoup Beautiful Soup 是 Python 的一个 HTML 或 XML 的解析库，我们可以用它来方便地从网页中提取数据。\npyquery pyquery 同样是一个强大的网页解析工具，它提供了和 jQuery 类似的语法来解析 HTML 文档，支持 CSS 选择器，使用非常方便。\npymysql pymongo redis-py Flask Flask 是一个轻量级的 Web 服务程序，它简单、易用、灵活，这里主要用来做一些 API 服务。\nTornado Tornado 是一个支持异步的 Web 框架，通过使用非阻塞 I/O 流，它可以支撑成千上万的开放连接，效率非常高。\npyspider pyspider 是国人 binux 编写的强大的网络爬虫框架，它带有强大的 WebUI、脚本编辑器、任务监控器、项目管理器以及结果处理器，同时支持多种数据库后端、多种消息队列，另外还支持 JavaScript 渲染页面的爬取，使用起来非常方便\n额，运行的时候很多问题，弄了很久没弄好，且不如scrapy，干脆放弃了\nScrapy Scrapy 是一个十分强大的爬虫框架，依赖的库比较多，至少需要依赖的库有 Twisted 14.0、lxml 3.4 和 pyOpenSSL 0.14。在不同的平台环境下，它所依赖的库也各不相同，所以在安装之前，最好确保把一些基本库安装好。\ntesserocr 需要先安装tesseract，否则会安装不成功\npillow（PIL） tesserat识别的时候，大多图片是需要处理才能更准确的识别的\n","date":"2022-10-25","permalink":"/post/development_envirnment/","tags":["Python"],"title":"development_envirnment"},{"content":"Docker安装、运行mysql\n从docker hub的仓库中拉去mysql镜像 sudo docker pull mysql 查看镜像： docker images 2.运行一个mysql容器\ndocker run -p 3306:3306 \u0026ndash;name qmm-mysql -v ~/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=669988 -d mysql:5.6 //创建容器时，最后mysql:5.6表示mysql镜像的版本，可以写，表示指定该版本；如果不写也可以，docker会自动在本地检测有没有最新的，如果没有会自动去docker hub上去下载。 上述命令各个参数的含义：\nrun 运行一个docker容器 \u0026ndash;name 后面这个是生成的容器的名字qmm-mysql -p 3306:3306 表示这个容器中使用3306（第二个）映射到本机的端口号也为3306（第一个） -e MYSQL_ROOT_PASSWORD=123456 初始化root用户的密码 -d 表示使用守护进程运行，即服务挂在后台 查看当前docker容器的运行状态：\ndocker ps : 查看运行中的容器 或者docker ps -a ： 查看所有创建的容器 如果想要访问mysql ，需要在本机上装一个mysql-client。 本机装好mysql后，可以使用mysql命令访问本机的mysql服务器，密码就是上面创建容器时设置的密码为123456， 192.168.95.4 为现在我这台机器的ip， 3306为刚才所示的占用本物理机的端口（不是docker内部的端口）\nmysql -h192.168.95.4 -p3306 -uroot -p123456\n进入docker中mysql：\n$ docker exec -it mysql bash $ mysql -u root -p 数据数据库密码就可以进入docker中的mysql 创建容器注意事项：\n端口映射唯一性：一个容器只能映射到本机的唯一一个端口，故如果创建了一个容器，该容器在运行中，该容器映射到本地的端口为3306， 那么就不能再创建一个容器映射在3306端口号上，因为该端口已经被分配给了第一个容器。 容器名字唯一性：创建的容器的名字不能与已经存在 的容器名字重复。 否则创建容器失败。 删除一个容器：\nsudo docker rm 容器名字（如上容器名字就是：qmm-mysql） 重新再创建一个容器second-mysql，占用物理机的3307端口：\nsudo docker run \u0026ndash;name second-mysql -p 3307:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql 现在两个容器（实例）都正常运行了，为了访问第二个容器，我们指定端口号3307登陆到这个mysql的client。\ndocker exec -it mysql bash // 想进入docker中，这里的mysql表示镜像的名字 mysql -h192.168.95.4 -P3307 -uroot -p123456\n还可以在别人的机器上访问我本机的mysql，这个就是-h参数的作用： 比如：\nmysql -h192.168.95.4 -p3306 -uroot -p123456 如果在同一个局域网下，别人是可以通过ip地址来访问我的电脑的，如果在别人的机器上这 里的-h后面写上我本机的ip地址，以及我的mysql的端口号和密码，那么他就可以访问我电脑 上的数据库mysql，前提是在一个局域网下。\nDocker安装、运行mysql\n","date":"2022-10-25","permalink":"/post/docker/","tags":["Tools","Docker"],"title":"Docker"},{"content":"语法错误 基本由编程不规范造成的，如括号、引号的不匹配，缩进的不正确，二值比较符号写成赋值，冒号的缺少等等造成语句的编译异常。\n异常 在语法完全正确的情况下，程序也可能出现不能正常运行的情况。这种情况称之为异常。 异常的类型很多，比如将0作为除数，尝试更改不可变类型数据，函数接收的参数的类型不对，使用了未定义或者不在该定义域中的参数等等。\ntry-except 为 python 提供的最简单的异常捕获、处理的方法。 1、使用例子：\ndef func(): print(1/1) try: func() print('ok') except: print('error')  解释： try 关键字下的语句块是要执行的代码，如果func()正常运行没有引发任何一个异常，就会继续运行后续的todo1,然后跳过 except，继续后续的代码（如果有的话），没有就正常结束整个程序的运行。出现异常就会去执行 except 里的语句然后再之后的。 可以不捕获异常，而只是单纯的略过该异常，去执行下一个语句，从而达到不中断程序的运行。\n2、捕获异常\ndef func(): print(1/0) try: func() print('ok') except ZeroDivisionError: print('error')  和之前的例子类似，不同的是人为的制造了一个异常，并且在捕获的时候指定了该异常类型。\n3、捕获多个异常\ndef func(): print(1/1) try: func() print('ok') except NameError: print('name error') except TypeError: print('type error') except ZeroDivisionError: print('zero error') except: print('未知异常')  except 语句不是唯一的，可以有多个。捕获到什么类型的异常就会执行该异常的处理语句，如果都不是，则执行最后的语句，如果没有最后的语句，则会抛出异常。\n4、捕获多个异常2\ndef func(): print(1/0) try: func() print('ok') except (ValueError, ArithmeticError, ZeroDivisionError): print('error') except: print('未知异常')  一个 except 语句可以接受多个类型的异常，可以看作是异常 in 异常元组里。这样就可以不必为没一个类型单独写处理语句了。\ntry - except .. as（访问异常信息）  如果程序需要在 except 块中访问异常对象的相关信息，则可通过为异常对象声明变量来实现。 当 Python 解释器决定调用某个 except 块来处理该异常对象时，会将异常对象赋值给 except 块后的异常变量，程序即可通过该变量来获得异常对象的相关信息。 所有的异常对象都包含了如下几个常用属性和方法：\nargs：该属性返回异常的错误编号和描述字符串。 errno：该属性返回异常的错误编号。 strerror：该属性返回异常的描述宇符串。 with_traceback()：通过该方法可处理异常的传播轨迹信息。\n 当一个 except 有多个异常类型或者不确定异常类型时，可以通过 as 得到异常，输出更准确一些的异常信息\ndef func(): print(1/0) try: func() print('ok') except (ValueError, ArithmeticError, ZeroDivisionError) as e: print(e) print(type(e)) print(str(e)) # 返回字符串类型，只给出异常信息，不包括异常信息的类型 print(repr(e)) # 给出较全的异常信息，包括异常信息的类型 except: print('未知异常') # division by zero # \u0026lt;class 'ZeroDivisionError'\u0026gt; # division by zero # ZeroDivisionError('division by zero')  def func(): print(1/0) try: func() print('ok') except Exception as e: # 大多数异常都是继承自 Exception 类，因此在不确定异常时可以以 Exception 作为异常类去做初步判断，实际应用中通常是更准确的捕获异常并处理 print(e) print(type(e)) print(str(e)) # 返回字符串类型，只给出异常信息，不包括异常信息的类型 print(repr(e)) # 给出较全的异常信息，包括异常信息的类型 except: print('未知异常') # division by zero # \u0026lt;class 'ZeroDivisionError'\u0026gt; # division by zero # ZeroDivisionError('division by zero')  异常的传递 def func(): print(1/0) def func2(): func() func2() # Traceback (most recent call last): # File \u0026quot;/home/fiki/Documents/PycharmProjects/thread_yy.py\u0026quot;, line 21, in \u0026lt;module\u0026gt; # func2() # File \u0026quot;/home/fiki/Documents/PycharmProjects/thread_yy.py\u0026quot;, line 19, in func2 # func() # File \u0026quot;/home/fiki/Documents/PycharmProjects/thread_yy.py\u0026quot;, line 16, in func # print(1/0) # ZeroDivisionError: division by zero  异常会一层一层的传递。直到主函数才抛出异常。\ntry - except - else def func(): print(1/1) try: func() print('ok') except (ValueError, ArithmeticError, ZeroDivisionError): print('error') except: print('未知异常') else: print('ok') # 1.0 # ok # ok  很明显。else 语句会在 try 正常执行后执行，重点是 else 的侧重点是什么，咋一看完全可以写在 try 里面，单独出来反而会有点混淆的感觉。\ndef func(): return '1'/0 def func2(): a = func() print('2'/a) try: func2() print('ok') except (ValueError, ArithmeticError, ZeroDivisionError): print('error') except: print('未知异常') else: print('ok') # 未知异常  例子有点牵强，大概意思就是，由于异常的传递，当我们使用 try 语句包裹的是有多个层级的话，捕获到的异常往往不够精准，无法确定是那一个层级的异常。\ndef func(): return '1'/0 def func2(): a = func() try: func2() print('ok') except (ValueError, ArithmeticError, ZeroDivisionError): print('error') except: print('未知异常') else: print('2'/a) # 未知异常  这样我们就能知道该异常是 func() 造成的了\ntry - except - else - finally（资源回收） def func(): return '1' try: func() print('ok') except: print('异常') else: print('else') finally: print('finally') # ok # else # finally  finally 的执行，与程序是否有异常无关，有无 else 无关，最终都会执行 finally 里面的语句。\nelse finally 并不是都必须。 二者可选其一，也可都不选择。\nfinally 的使用情况有打开文件后的关闭，连接数据库后的断开等等。\ntry - return 例子1\ndef func(): return '1' def fucn(): try: func() return 'ok' except: return '异常' else: return 'else' # finally: # return 'finally' a = fucn() print(a) # ok  例子2\ndef func(): return '1' def fucn(): try: func() return 'ok' except: return '异常' else: return 'else' finally: return 'finally' a = fucn() print(a) # finally  例子1中符合正常的，遇到 return 则跳出函数 而例子二中却貌似没有执行 try 中的 return 实则不是，可以看作是 finally 的优先级更高，显式覆盖了前一个 return\n抛出异常 自定义异常类型 自定义异常类应该总是继承自内置的 Exception 类， 或者是继承自那些本身就是从 Exception 继承而来的类。 尽管所有类同时也继承自 BaseException ，但你不应该使用这个基类来定义新的异常。 BaseException 是为系统退出异常而保留的，比如 KeyboardInterrupt 或 SystemExit 以及其他那些会给应用发送信号而退出的异常。 因此，捕获这些异常本身没什么意义。 这样的话，假如你继承 BaseException 可能会导致你的自定义异常不会被捕获而直接发送信号退出程序运行。\n在程序中引入自定义异常可以使得你的代码更具可读性，能清晰显示谁应该阅读这个代码。 还有一种设计是将自定义异常通过继承组合起来。在复杂应用程序中， 使用基类来分组各种异常类也是很有用的。它可以让用户捕获一个范围很窄的特定异常，捕获更大范围的异常\n创建新的异常很简单：定义新的类，让它继承自 Exception （或者是任何一个已存在的异常类型）。\n需要注意的是，如果定义的新异常重写了 init() 方法， 确保你使用所有参数调用 Exception.init()\nclass CustomError(Exception): def __init__(self, message, status): super().__init__(message, status) self.message = message self.status = status  raise raise 必需的参数指定了要被抛出的异常。它必须是一个异常的实例或者是异常的类（也就是 Exception 的子类）。大多数的异常的名字都以\u0026quot;Error\u0026quot;结尾，所以实际命名时尽量跟标准的异常命名一样。\nclass LenthExceptin(Exception): def __str__(self): print(\u0026quot;用户名长度不合法\u0026quot;) def function(): name = 'adad3qsdsdsdsd' if len(name)\u0026gt;10 or len(name)\u0026lt;5: raise LenthExceptin() else: print(1) function()  Python raise用法（超级详细，看了无师自通） raise 语句有如下三种常用的用法：\nraise：单独一个 raise。该语句引发当前上下文中捕获的异常（比如在 except 块中），或默认引发 RuntimeError 异常。 raise 异常类：raise 后带一个异常类。该语句引发指定异常类的默认实例。 raise 异常对象：引发指定的异常对象。 上面三种用法最终都是要引发一个异常实例（即使指定的是异常类，实际上也是引发该类的默认实例），raise 语句每次只能引发一个异常实例。\n断言（assert） 在一些情况下可以将断言用做抛出异常 语法格式如下：assert expression 当 expression 的结果为False 时，引发 AssertionError 异常 其等价于：\nif not expression: raise AssertionError  # assert 后面也可以紧跟参数: assert expression [, arguments] # 等价于： if not expression: raise AssertionError(arguments)  异常机制使用细则 Python异常机制使用细则，正确使用Python异常处理机制（入门必读） 成功的异常处理应该实现如下 4 个目标： 使程序代码混乱最小化。 捕获并保留诊断信息。 通知合适的人员。 采用合适的方式结束异常活动。 下面介绍达到这些效果的基本准则。\n不要过度使用异常 不可否认，Python 的异常机制确实方便，但滥用异常机制也会带来一些负面影响。过度使用异常主要表现在两个方面： 把异常和普通错误混淆在一起，不再编写任何错误处理代码，而是以简单地引发异常来代苦所有的错误处理。 使用异常处理来代替流程控制。 熟悉了异常使用方法后，程序员可能不再愿意编写烦琐的错误处理代码，而是简单地引发异常。实际上这样做是不对的，对于完全己知的错误和普通的错误，应该编写处理这种错误的代码，增加程序的健壮性。只有对于外部的、不能确定和预知的运行时错误才使用异常。 对比前面五子棋游戏中，处理用户输入坐标点己有棋子的两种方式。如果用户试图下棋的坐标点己有棋子：\n#如果要下棋的点不为空 if board[int(y_str) - 1) [int(x_str) - 1] !=\u0026quot;╋\u0026quot; : inputStr = input (\u0026quot;您输入的坐标点己有棋子了，请重新输入\\n\u0026quot;) continue  上面这种处理方式检测到用户试图下棋的坐标点己经有棋子，立即打印一条提示语句，并重新开始下一次循环。这种处理方式简洁明了、逻辑清晰，程序的运行效率也很好程序进入 if 块后，即结束了本次循环。 如果将上面的处理机制改为如下方式：\n#如果要下棋的点不为空 if board[int(y_str) - 1) [int(x_str) - 1) != \u0026quot;╋\u0026quot;: #引发默认的RuntimeError 异常 raise  上面这种处理方式没有提供有效的错误处理代码，当程序检测到用户试图下棋的坐标点己经有棋子时，并没有提供相应的处理，而是简单地引发一个异常。这种处理方式虽然简单，但 Python 解释器接收到这个异常后，还需要进入相应的 except 块来捕获该异常，所以运行效率要差一些。而且用户下棋重复这个错误完全是可预料的，所以程序完全可以针对该错误提供相应的处理，而不是引发异常。 必须指出，异常处理机制的初衷是将不可预期异常的处理代码和正常的业务逻辑处理代码分离，因此绝不要使用异常处理来代替正常的业务逻辑判断。 另外，异常机制的效率比正常的流程控制效率差，所以不要使用异常处理来代替正常的程序流程控制。例如，对于如下代码：\n#定义一个字符串列表 my_list =[\u0026quot;Hello\u0026quot;, \u0026quot;Python\u0026quot;, \u0026quot;Spring\u0026quot;] #使用异常处理来遍历arr数组的每个元素 try: i = 0 while True: print (my_list [i]) i += 1 except: pass  运行上面程序确实可以实现遍历 my_list 列表的功能，但这种写法可读性较差，而且运行效率也不高。程序完全有能力避免产生 indexError 异常，程序“故意”制造这种异常，然后使用 except 块去捕获该异常，这是不应该的。将程序改为如下形式肯定要好得多：\ni = 0 while i \u0026lt; len(my_list): print(my_list[i]) i += 1  注意，异常只应该用于处理非正常的情况，不要使用异常处理来代替正常的流程控制。对于一些完全可预知，而且处理方式清楚的错误，程序应该提供相应的错误处理代码，而不是将其笼统地称为异常。\n不要使用过于庞大的 try 块 很多初学异常机制的读者喜欢在 try 块里放置大量的代码，这看上去很“简单”，但这种“简单”只是一种假象，只是在编写程序时看上去比较简单。但因为 try 块里的代码过于庞大，业务过于复杂，就会造成 try 块中出现异常的可能性大大增加，从而导致分析异常原因的难度也大大增加。 而且当时块过于庞大时，就难免在 try 块后紧跟大量的 except 块才可以针对不同的异常提供不同的处理逻辑。在同一个 try 块后紧跟大量的 except 块则需要分析它们之间的逻辑关系，反而增加了编程复杂度。 正确的做法是，把大块的 try 块分割成多个可能出现异常的程序段落，并把它们放在单独的 try 块中，从而分别捕获并处理异常。\n不要忽略捕获到的异常 不要忽略异常！既然己捕获到异常，那么 except 块理应做些有用的事情，及处理并修复异常。except 块整个为空，或者仅仅打印简单的异常信息都是不妥的！ except 块为空就是假装不知道甚至瞒天过海，这是最可怕的事情，程序出了错误，所有人都看不到任何异常，但整个应用可能已经彻底坏了。仅在 except 块里打印异常传播信息稍微好一点，但仅仅比空白多了几行异常信息。通常建议对异常采取适当措施，比如： 处理异常。对异常进行合适的修复，然后绕过异常发生的地方继续运行；或者用别的数据进行计算，以代替期望的方法返回值；或者提示用户重新操作…… 总之，程序应该尽量修复异常，使程序能恢复运行。 重新引发新异常。把在当前运行环境下能做的事情尽量做完，然后进行异常转译，把异常包装成当前层的异常，重新传给上层调用者。 在合适的层处理异常。如果当前层不清楚如何处理异常，就不要在当前层使用 except 语句来捕获该异常，让上层调用者来负责处理该异常。\n使用 traceback 模块 获取异常信息 随着学习的深入，普通的异常捕获已经不能满足于需求了，捕获输出的为错误的值，而如果想要获得更明确的异常，如异常发生的具体位置，异常的传递过程等等。\nimport traceback import sys def func(): return '1'/0 def func2(): a = func() try: func2() print('ok') except Exception as e: tb = sys.exc_info() print(tb) # (\u0026lt;class 'TypeError'\u0026gt;, TypeError(\u0026quot;unsupported operand type(s) for /: 'str' and 'int'\u0026quot;), \u0026lt;traceback object at 0x7fe6c2fa3080\u0026gt;)  sys 模块的exc_info() 方法的返回为一个元组，第一个为异常的类型，第二个为异常的值，第三个则是一个 traceback object\n Python3 assert（断言） 14.8 创建自定义异常\n","date":"2022-10-25","permalink":"/post/errors-and-exceptions/","tags":["Python"],"title":"errors-and-exceptions"},{"content":"  Flask框架的优点 Flask框架是一个短小精悍、可扩展性强的web框架。html\n  Flask框架依赖组件 Flask依赖于werkzurg组件，实现wsgi使用的就是werkzurg。web\n  Flask蓝图的做用 实际项目中，须要进行项目目录结构的划分，蓝图就是用来帮助开发者进行目录结构的划分。redis\n  列举使用过的Flask第三方组件 werkzurg、DBUtils、wtforms、SQLAlchemy、jinjia二、redis……sql\n  简述Flask上下文管理流程\n  请求到来时，将session和request封装到ctx对象中；2.对session作补充；3.将包含了request和session的ctx对象放到一个容器中（每一个请求都会根据线程/协程加一个惟一标识）；4.视图函数使用的时候须要根据当前线程或协程的惟一标识，获取ctx对象，再取ctx对象中取request和session（视图函数使用的时候，须要根据当前线程获取数据。）5.请求结束时，根据当前线程/协程的惟一标记，将这个容器上的数据移除。数据库\n  Flask中的g的做用\n  Flask中上下文管理主要涉及到了哪些相关的类，这些类的主要做用\n  LocalStack它帮助咱们在local中把一个列表维护成一个栈，方便咱们对列表中的数据进行添加和维护，有了LocalStack操做更加便捷。2. Local帮助咱们为每一个线程/协程开辟空间flask\n  为何Flask要把Local对象中的值stack维护成一个列表\n  Flask中多app应用是怎么完成 使用Flask类建立不一样的app对象，而后借助DispatcherMiddleware类来实现。cookie\n  在Flask中实现WebSocket须要什么组件\n  wtforms组件的做用 wtforms组件有两个做用，自动生成html标签和对用户请求数据进行校验。session\n  Flask框架默认session的处理机制 当请求刚进来时，Flask读取cookie中session对应的值，将该值解密并反序列化为字典，放入内存以便视图函数使用。 当请求结束时，Flask会读取内存中字典的值，进行序列化加密，写入到用户的cookie中。多线程\n  解释Flask框架中的Local对象和threading.local对象的区别 Local对象是根据threading.local作的。app\n  Flask中 blinker 是什么\n  SQLAlchemy中的session和scoped_session的区别 使用的scoped_session是基于Threading.Local实现的，而session并非。\n  SQLAlchemy如何执行原生SQL 我只记住了两种SQLAlchemy能够执行原生sql的方式，第一种是经过session(会话)对象执行execute方法，第二种是经过cursor(游标)对象执行execute方法。\n  第一种是： session = scoped_session(SessionFactory) session.execute(\u0026lsquo;insert users(name) values(:value)\u0026rsquo;, params={\u0026lsquo;value\u0026rsquo;: \u0026rsquo;thanlon\u0026rsquo;})\n第二种是： conn = engine.raw_connection() cursor = conn.cursor() cursor.execute(\u0026lsquo;select *from users\u0026rsquo;)\n ORM的实现原理\n  DBUtils模块的做用 DBUtils模块的做用是实现数据库链接池，是为了解决“多线程状况下请求比较多时性能下降”的问题。\n  如下SQLAlchemy的字段是否正确，若是不正确请更正\n  # -*- coding: utf-8 -*- from datetime import datetime from sqlalchemy.ext.declarative import declarative_base from sqlalchemy import Column, Integer, String, DateTime Base = declarative_base() class UserInfo(Base): __tablename__ = 'userinfo' id = Column(Integer, primary_key=True, autoincrement=True) name = Column(String(64), unique=True) ctime = Column(DateTime, default=datetime.now())   SQLAchemy中如何为表设置引擎和字符编码\n  SQLAchemy中如何设置联合惟一索引 首先在中间表(关联表)的类中加入__table_args__ 字段，而后实例化UniqueConstraint类并放入一个元组中，最后把这个元组赋值给__table_args__。用代码表示以下：\n  table_args = ( UniqueConstraint(\u0026lsquo;student_id\u0026rsquo;, \u0026lsquo;course_id\u0026rsquo;, name=\u0026lsquo;uc_student_course\u0026rsquo;), # 联合惟一索引 )\n Flask请求相关的数据和Django的区别 Flask是直接调用request对象来请求相关的数据，而Django是经过参数传递实现的。\n  Flask和Django最大的区别 对于request，falsk是导入进来的，而Django是参数传递的；对于session，flask也导入进来的，而Django是依附request对象传递过来的。\n  Flask若是开了两个进程，有几个local对象\n  ","date":"2022-10-25","permalink":"/post/flask/","tags":["InterviewQuestions","Python","Flask"],"title":"flask"},{"content":"路由 简单路由 @app.route('/hello/') def hello(): return 'Hello'  带参数的路由 （1） @app.route('/hello/\u0026lt;name\u0026gt;') def hello(name): return 'Hello %s' % name  （2） ?key=value形式传参 上面我们接受参数使用的是path形式，下面我们来使用查询字符串的形式，即?key=value\n如果有多个参数，则用\u0026amp;来拼接： ?key1=value1\u0026amp;key2=value2\nfrom flask import Flask, request @app.route('/d/') def d(): wd = request.args.get('wd') return '您传递的参数是: {}'.format(wd)  参数类型限制 @app.route('/user/\u0026lt;int:user_id\u0026gt;') def get_user(user_id): return 'User ID: %d' % user_id  可以对参数限定数据类型,这样当我们传递的参数类型符合要求的时候才能正常访问，其他类型都会返回404\n类型可以设置提下几种：\n　string: 默认的数据类型，接收没有任何斜杠\u0026quot;\\ /\u0026ldquo;的文本\n　int: 整数形\n　float: 浮点型\n　path: 和string类似，但是接受斜杠\n　uuid: 只接受uuid字符串\n　any: 可以指定多种路径，比如以下例子\n@app.route('/\u0026lt;any(article,blog):url_path\u0026gt;/\u0026lt;id\u0026gt;/') def item(url_path, id): if url_path == 'article': return '文章详情：{}'.format(id) else: return '博客详情：{}'.format(id)  自定义参数规则/自定义路由参数转换器 （也叫正则匹配路由参数） 具体实现步骤为：\n 导入转换器基类：在 Flask 中，所有的路由的匹配规则都是使用转换器对象进行记录 自定义转换器：自定义类继承于转换器基类 添加转换器到默认的转换器字典中 使用自定义转换器实现自定义匹配规则  to_python 这个方法的返回值，将会传递到view函数中作为参数\nclass ListConveter(BaseConverter): def to_python(self, value): #这个value就是我们url的参数 return value.split(\u0026quot;+\u0026quot;) #这个返回值,就是我们视图函数接收到的参数 app.url_map.converters['list'] = ListConveter @app.route('/posts/\u0026lt;list:boards\u0026gt;') def posts(boards): #这里的borads已经被list处理了，变成了列表 return str(borads)  to_url 这个方法的返回值，将会在调用url_for函数的时候生成符合要求的url形式\nclass ListConveter(BaseConverter): def to_python(self, value): return value.split(\u0026quot;+\u0026quot;) def to_url(self, value): return '+'.join(value) app.url_map.converters['list'] = ListConveter @app.route('/') def hello_world(): return url_for('posts', boards=['a', 'b']) @app.route('/posts/\u0026lt;list:boards\u0026gt;') def posts(boards): return str(boards)  多url的路由 @app.route('/') @app.route('/hello') @app.route('/hello/\u0026lt;name\u0026gt;') def hello(name=None): if name is None: name = 'World' return 'Hello %s' % name  HTTP请求方法设置 编号\t方法\t描述 1\tGET\t将数据以未加密的形式发送到服务器，这最常用的方法。 2\tHEAD\t与GET相同，但没有响应主体 3\tPOST\t用于将HTML表单数据发送到服务器。通过POST方法接收的数据不会被服务器缓存。 4\tPUT\t用上传的内容替换目标资源的所有当前表示。 5\tDELETE\t删除由URL给出的所有目标资源的所有表示默认情况下，Flask路由响应GET请求。 但是，可以通过为route()装饰器提供方法参数来更改此首选项。\n@app.route('/login',methods = ['POST', 'GET']) def login(): if request.method == 'POST': user = request.form['name'] return redirect(url_for('success',name = user)) else: user = request.args.get('name') return redirect(url_for('success',name = user))  url_for url_for()函数对于动态构建特定函数的URL非常有用。 该函数接受函数的名称作为第一个参数，并接受一个或多个关键字参数，每个参数对应于URL的变量部分。\n我们之前是通过url来找到对应的视图函数\n　/ =\u0026gt; hello_world\n那么url_for则是通过视图函数找到url\n　hello world =\u0026gt; /\nfrom flask import url_for @app.route('/') def hello_world(): return url_for('my_list') # /list/ @app.route('/list/') def my_list(): return 'list page'  from flask import url_for @app.route('/') def hello_world(): return url_for('my_list', page_id=1) # /list/1/ @app.route('/list/\u0026lt;page_id\u0026gt;/') def my_list(page_id): return 'list page'  # 如果url只配置了一个参数，而url_for设置了多个 from flask import url_for @app.route('/') def hello_world(): return url_for('my_list', page_id=1, count=2) # /list/1/?count=2 @app.route('/list/\u0026lt;page_id\u0026gt;/') def my_list(page_id): return 'list page'  为什么要使用url_for呢？ 当我们视图函数确定了，以后我们的url不管怎么变，只要通过url_for都能够正确地得到url，因为改变url的频率一般大于视图函数。\nurl_for会自动处理特殊的字符\n页面跳转和重定向 永久重定向301 http的状态码是301，多用于旧网址被废弃了要转到一个新的网址确保用户的访问，比如京东的网站，你输入www.jingdong.com的时候，会被重定向到www.jd.com，因为jingdong.com这个网址已经被废弃了，被改成了jd.com，所以这种情况下应该使用永久重定向\n临时重定向302 http的状态码是302，表示页面的临时性跳转。比如访问一个需要权限的网址，如果用户没有登录，应该重定向到登录页面，这种情况下，应该用临时重定向\nlocation表示需要重定向到的URL,应该配合url_for()函数来使用\ncode表示采用哪种重定向，默认是302（临时重定向），也可以改成301来实现永久重定向\n重定向到非本模块的路由地址 @app.route(\u0026quot;/\u0026quot;) def index() retrun redirect(“www.baidu.om”)  重定向到自身模块的路由地址(被跳转的视图函数不携带参数) @app.route(\u0026quot;/\u0026quot;) def index() retrun redirect(“www.baidu.om”) @app.route(\u0026quot;/temp\u0026quot;) def index() retrun redirect(url_for(“index”))  重定向到自身模块的路由地址(被跳转的视图函数携带参数) @app.route(\u0026quot;/index/int:user_id\u0026quot;) def.index(user_id) retrun “用户的id是%s”%user_id @app.route(\u0026quot;/temp\u0026quot;) def.index(user_id) return redirect(url_for(“index”,user_id=999))  模板 使用模板 from flask import Flask from flask import render_template app = Flask(__name__) @app.route('/hello') @app.route('/hello/\u0026lt;name\u0026gt;') def hello(name=None): return render_template('hello.html', name=name) if __name__ == '__main__': app.run(host='0.0.0.0', debug=True)  调用了render_template()方法来渲染模板。方法的第一个参数hello.html指向你想渲染的模板名称，第二个参数name是你要传到模板去的变量，变量可以传多个。\n模板继承 一般我们的网站虽然页面多，但是很多部分是重用的，比如页首，页脚，导航栏之类的。对于每个页面，都要写这些代码，很麻烦。Flask的Jinja2模板支持模板继承功能，省去了这些重复代码。让我们基于上面的例子，在”templates”目录下，创建一个名为”layout.html”的模板：\n\u0026lt;!doctype html\u0026gt;\nHello Sample {% block body %} {% endblock %}  再修改之前的”hello.html”，把原来的代码定义在{% block body %}中，并在代码一开始”继承”上面的”layout.html”： {% extends \u0026ldquo;layout.html\u0026rdquo; %} {% block body %} {% if name %} Hello {{ name }}! {% else %} Hello World! {% endif %} {% endblock %}\n请求、响应及会话 错误处理及消息闪现 数据库集成 ","date":"2022-10-25","permalink":"/post/flask/","tags":["Python"],"title":"flask"},{"content":"安装Git yay -S git  一、新建代码库\n在当前目录新建一个Git代码库 $ git init\n新建一个目录，将其初始化为Git代码库 $ git init [project-name]\n下载一个项目和它的整个代码历史 $ git clone [url] 二、配置 Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。\n显示当前的Git配置 $ git config \u0026ndash;list\n编辑Git配置文件 $ git config -e [\u0026ndash;global]\n设置提交代码时的用户信息 $ git config [\u0026ndash;global] user.name \u0026ldquo;[name]\u0026rdquo; $ git config [\u0026ndash;global] user.email \u0026ldquo;[email address]\u0026rdquo; 三、增加/删除文件\n添加指定文件到暂存区 $ git add [file1] [file2] \u0026hellip;\n添加指定目录到暂存区，包括子目录 $ git add [dir]\n添加当前目录的所有文件到暂存区 $ git add .\n添加每个变化前，都会要求确认 对于同一个文件的多处变化，可以实现分次提交 $ git add -p\n删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] \u0026hellip;\n停止追踪指定文件，但该文件会保留在工作区 $ git rm \u0026ndash;cached [file]\n改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed] 四、代码提交\n提交暂存区到仓库区 $ git commit -m [message]\n提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] \u0026hellip; -m [message]\n提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a\n提交时显示所有diff信息 $ git commit -v\n使用一次新的commit，替代上一次提交 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit \u0026ndash;amend -m [message]\n重做上一次commit，并包括指定文件的新变化 $ git commit \u0026ndash;amend [file1] [file2] \u0026hellip; 五、分支\n列出所有本地分支 $ git branch\n列出所有远程分支 $ git branch -r\n列出所有本地分支和远程分支 $ git branch -a\n新建一个分支，但依然停留在当前分支 $ git branch [branch-name]\n新建一个分支，并切换到该分支 $ git checkout -b [branch]\n新建一个分支，指向指定commit $ git branch [branch] [commit]\n新建一个分支，与指定的远程分支建立追踪关系 $ git branch \u0026ndash;track [branch] [remote-branch]\n切换到指定分支，并更新工作区 $ git checkout [branch-name]\n切换到上一个分支 $ git checkout -\n建立追踪关系，在现有分支与指定的远程分支之间 $ git branch \u0026ndash;set-upstream [branch] [remote-branch]\n合并指定分支到当前分支 $ git merge [branch]\n选择一个commit，合并进当前分支 $ git cherry-pick [commit]\n删除分支 $ git branch -d [branch-name]\n删除远程分支 $ git push origin \u0026ndash;delete [branch-name] $ git branch -dr [remote/branch] 六、标签\n列出所有tag $ git tag\n新建一个tag在当前commit $ git tag [tag]\n新建一个tag在指定commit $ git tag [tag] [commit]\n删除本地tag $ git tag -d [tag]\n删除远程tag $ git push origin :refs/tags/[tagName]\n查看tag信息 $ git show [tag]\n提交指定tag $ git push [remote] [tag]\n提交所有tag $ git push [remote] \u0026ndash;tags\n新建一个分支，指向某个tag $ git checkout -b [branch] [tag] 七、查看信息\n显示有变更的文件 $ git status\n显示当前分支的版本历史 $ git log\n显示commit历史，以及每次commit发生变更的文件 $ git log \u0026ndash;stat\n搜索提交历史，根据关键词 $ git log -S [keyword]\n显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD \u0026ndash;pretty=format:%s\n显示某个commit之后的所有变动，其\u0026quot;提交说明\u0026quot;必须符合搜索条件 $ git log [tag] HEAD \u0026ndash;grep feature\n显示某个文件的版本历史，包括文件改名 $ git log \u0026ndash;follow [file] $ git whatchanged [file]\n显示指定文件相关的每一次diff $ git log -p [file]\n显示过去5次提交 $ git log -5 \u0026ndash;pretty \u0026ndash;oneline\n显示所有提交过的用户，按提交次数排序 $ git shortlog -sn\n显示指定文件是什么人在什么时间修改过 $ git blame [file]\n显示暂存区和工作区的差异 $ git diff\n显示暂存区和上一个commit的差异 $ git diff \u0026ndash;cached [file]\n显示工作区与当前分支最新commit之间的差异 $ git diff HEAD\n显示两次提交之间的差异 $ git diff [first-branch]\u0026hellip;[second-branch]\n显示今天你写了多少行代码 $ git diff \u0026ndash;shortstat \u0026ldquo;@{0 day ago}\u0026rdquo;\n显示某次提交的元数据和内容变化 $ git show [commit]\n显示某次提交发生变化的文件 $ git show \u0026ndash;name-only [commit]\n显示某次提交时，某个文件的内容 $ git show [commit]:[filename]\n显示当前分支的最近几次提交 $ git reflog 八、远程同步\n下载远程仓库的所有变动 $ git fetch [remote]\n显示所有远程仓库 $ git remote -v\n显示某个远程仓库的信息 $ git remote show [remote]\n增加一个新的远程仓库，并命名 $ git remote add [shortname] [url]\n取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch]\n上传本地指定分支到远程仓库 $ git push [remote] [branch]\n强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] \u0026ndash;force\n推送所有分支到远程仓库 $ git push [remote] \u0026ndash;all 九、撤销\n恢复暂存区的指定文件到工作区 $ git checkout [file]\n恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file]\n恢复暂存区的所有文件到工作区 $ git checkout .\n重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file]\n重置暂存区与工作区，与上一次commit保持一致 $ git reset \u0026ndash;hard\n重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit]\n重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset \u0026ndash;hard [commit]\n重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset \u0026ndash;keep [commit]\n新建一个commit，用来撤销指定commit 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit]\n暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop 十、其他\n生成一个可供发布的压缩包 $ git archive\ngit 记住帐号密码 方法一： 这个方法是全局的，该设备上的所有项目都会生效 ~/.gitconfig 配置文件。用文档编辑工具打开该文件 添加：\n[user] name = xxxx //你的用户名 email = xxxx@xxxx //你的git邮箱账号 [credential] helper = store  然后保存就可以了。 当git push的时候输入一次用户名和密码就会被记录 这样保存的密码是明文的，保存在用户目录 ~/.git-credentials 文件中\n方法二：\n直接在git bash 中执行命令：git config --global credential.helper store\n再输入一次账号密码就可以保存了\n方法三：只能单个项目 编辑项目下的 .git/config 文件 ,增加\n[credential] helper = store  使用一次用户名密码后即可保存。\n","date":"2022-10-25","permalink":"/post/git/","tags":["Tools","Git"],"title":"Git"},{"content":"HTTP协议 HTTP的特性 HTTP构建于TCP/IP协议之上，默认端口号是80 HTTP是无连接无状态的 HTTP报文 请求报文 HTTP 协议是以 ASCII 码传输，建立在 TCP/IP 协议之上的应用层规范。规范把 HTTP 请求分为三个部分：状态行、请求头、消息主体。类似于下面这样：\n   \n HTTP定义了与服务器交互的不同方法，最基本的方法有4种，分别是GET，POST，PUT，DELETE。URL全称是资源描述符，我们可以这样认为：一个URL地址，它用于描述一个网络上的资源，而 HTTP 中的GET，POST，PUT，DELETE就对应着对这个资源的查，增，改，删4个操作（CRUD）。 GET用于信息获取，GET请求报文示例： GET /books/?sex=man\u0026amp;name=Professional HTTP/1.1 Host: www.example.com User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.7.6) Gecko/20050225 Firefox/1.0.1 Connection: Keep-Alive POST表示可能修改变服务器上的资源的请求。 POST / HTTP/1.1 Host: www.example.com User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.7.6) Gecko/20050225 Firefox/1.0.1 Content-Type: application/x-www-form-urlencoded Content-Length: 40 Connection: Keep-Alive\nsex=man\u0026amp;name=Professional  举例，Chrome随便打开一个网页，就会看到HTTP报文，我们看到的被浏览器简化的及外国，我们点击view source即可查看原始报文：\n响应报文 HTTP 响应与 HTTP 请求相似，HTTP响应也由3个部分构成，分别是：\n状态行 响应头(Response Header) 响应正文 状态行由协议版本、数字形式的状态代码、及相应的状态描述，各元素之间以空格分隔。\n常见的状态码有如下几种：\n200 OK 客户端请求成功 301 Moved Permanently 请求永久重定向 302 Moved Temporarily 请求临时重定向 304 Not Modified 文件未修改，可以直接使用缓存的文件。 400 Bad Request 由于客户端请求有语法错误，不能被服务器所理解。 401 Unauthorized 请求未经授权。这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务。服务器通常会在响应正文中给出不提供服务的原因 404 Not Found 请求的资源不存在，例如，输入了错误的URL 500 Internal Server Error 服务器发生不可预期的错误，导致无法完成客户端的请求。 503 Service Unavailable 服务器当前不能够处理客户端的请求，在一段时间之后，服务器可能会恢复正常。 下面是一个HTTP响应的例子：\nHTTP/1.1 200 OK\nServer:Apache Tomcat/5.0.12 Date:Mon,6Oct2003 13:23:42 GMT Content-Length:112\n... 同样的可以在Chrome中查看原始报文和简化过的。 会话跟踪 什么是会话？客户端打开与服务器的连接发出请求到服务器响应客户端请求的全过程称之为会话。\n什么是会话跟踪？。会话跟踪指的是对同一个用户对服务器的连续的请求和接受响应的监视。\n为什么需要会话跟踪？浏览器与服务器之间的通信是通过HTTP协议进行通信的，而HTTP协议是”无状态”的协议，它不能保存客户的信息，即一次响应完成之后连接就断开了，下一次的请求需要重新连接，这样就需要判断是否是同一个用户，所以才有会话跟踪技术来实现这种要求。\n会话跟踪常用的方法:\nURL重写。URL(统一资源定位符)是Web上特定页面的地址，URL重写的技术就是在URL结尾添加一个附加数据以标识该会话,把会话ID通过URL的信息传递过去，以便在服务器端进行识别不同的用户。 隐藏表单域。将会话ID添加到HTML表单元素中提交到服务器，此表单元素并不在客户端显示 Cookie。Cookie是Web服务器发送给客户端的一小段信息，客户端请求时可以读取该信息发送到服务器端，进而进行用户的识别。对于客户端的每次请求，服务器都会将Cookie发送到客户端,在客户端可以进行保存,以便下次使用。客户端可以采用两种方式来保存这个Cookie对象，一种方式是保存在客户端内存中，称为临时Cookie，浏览器关闭后这个Cookie对象将消失。另外一种方式是保存在客户机的磁盘上，称为永久Cookie。以后客户端只要访问该网站，就会将这个Cookie再次发送到服务器上，前提是这个Cookie在有效期内，这样就实现了对客户的跟踪。Cookie是可以被禁止的。 Session。每一个用户都有一个不同的session，各个用户之间是不能共享的，是每个用户所独享的，在session中可以存放信息。在服务器端会创建一个session对象，产生一个sessionID来标识这个session对象，然后将这个sessionID放入到Cookie中发送到客户端，下一次访问时，sessionID会发送到服务器，在服务器端进行识别不同的用户。Session的实现依赖于Cookie，如果Cookie被禁用，那么session也将失效。（如果你熟悉requests，那么你肯定用到requests.session，和这里讲到的Session一致，常常用于需要登录才能采集的网站） 跨站攻击 CSRF（Cross-site request forgery，跨站请求伪造）\nCSRF(XSRF) 顾名思义，是伪造请求，冒充用户在站内的正常操作。\n例如，一论坛网站的发贴是通过 GET 请求访问，点击发贴之后 JS 把发贴内容拼接成目标 URL 并访问：\nhttp://example.com/bbs/create_post.php?title=标题\u0026amp;content=内容 那么，我们只需要在论坛中发一帖，包含一链接：\nhttp://example.com/bbs/create_post.php?title=我是脑残\u0026amp;content=哈哈 只要有用户点击了这个链接，那么他们的帐户就会在不知情的情况下发布了这一帖子。可能这只是个恶作剧，但是既然发贴的请求可以伪造，那么删帖、转帐、改密码、发邮件全都可以伪造。\n如何防范 CSRF 攻击？可以注意以下几点：\n关键操作只接受POST请求\n验证码。CSRF攻击的过程，往往是在用户不知情的情况下构造网络请求。所以如果使用验证码，那么每次操作都需要用户进行互动，从而简单有效的防御了CSRF攻击。但是如果你在一个网站作出任何举动都要输入验证码会严重影响用户体验，所以验证码一般只出现在特殊操作里面，或者在注册时候使用。\n检测 Referer。常见的互联网页面与页面之间是存在联系的，比如你在www.baidu.com应该是找不到通往www.google.com的链接的，再比如你在论坛留言，那么不管你留言后重定向到哪里去了，之前的那个网址一定会包含留言的输入框，这个之前的网址就会保留在新页面头文件的 Referer 中。通过检查Referer的值，我们就可以判断这个请求是合法的还是非法的，但是问题出在服务器不是任何时候都能接受到Referer的值，所以 Referer Check 一般用于监控 CSRF 攻击的发生，而不用来抵御攻击。\nToken。目前主流的做法是使用 Token 抵御 CSRF 攻击。Token 使用原则：Token 要足够随机————只有这样才算不可预测；Token 是一次性的，即每次请求成功后要更新Token————这样可以增加攻击难度，增加预测难度；Token 要注意保密性————敏感操作使用 post，防止 Token 出现在 URL 中。注意：过滤用户输入的内容不能阻挡 csrf，我们需要做的是过滤请求的来源。\nXSS（Cross Site Scripting，跨站脚本攻击）XSS 全称“跨站脚本”，是注入攻击的一种。其特点是不对服务器端造成任何伤害，而是通过一些正常的站内交互途径，例如发布评论，提交含有 JavaScript 的内容文本。这时服务器端如果没有过滤或转义掉这些脚本，作为内容发布到了页面上，其他用户访问这个页面的时候就会运行这些脚本。运行预期之外的脚本带来的后果有很多中，可能只是简单的恶作剧——一个关不掉的窗口：\nwhile (true) { alert(\u0026ldquo;你关不掉我~\u0026rdquo;); } 看到上面说的这些，你是不是非常熟悉，这和我们每天做的反爬虫很相似嘛。验证码、Referer、Token，每一点都需要注意，更细的我们以后再说。\n实践 打开知乎首页，下拉，会出现一个叫session_token的参数，去探索，这个token带边什么意思，有什么作用，爬虫会需要它吗？\nHTTP协议\nHTTP有哪些请求方法?# GET POST HEAD PUT DELETE PATCH OPTIONS HTTP常见的状态码有哪些?# 200 302 301 400 403 404 500 502 503\nGET和POST的区别?# GET用于获取数据 GET一般不发请求数据,参数通过URL Query GET参数直接暴露在地址栏中 GET可以被保存,收藏 GET响应可以被缓存 GET是幂等的,POST是非幂等的 GET要注意URL长度限制 URL长度限制为4K/主要验证数据的准确性,包括分页 POST注意点 业务破坏性 是否进行清理 重复提交 服务端验证\nCookie和Session的区别?# Cookie在客户端,可以被篡改 Session在服务段,不能被篡改 Token和Session的区别?# Token是算法计算出来的,验证时通过算法计算进行验证 session实时维护在服务端 HTTP和HTTPS的区别?# HTTPS= HTTP+SSL\n安全 是否需要握手 是否免费 性能\nPython自动化测试面试题-接口篇\n","date":"2022-10-25","permalink":"/post/http%E5%8D%8F%E8%AE%AE/","tags":["FrontEnd"],"title":"HTTP协议"},{"content":"魔法方法介绍  object.iter(self) 此方法在需要为容器创建迭代器时被调用。此方法应该返回一个新的迭代器对象，它能够逐个迭代容器中的所有对象。对于映射，它应该逐个迭代容器中的键。  迭代器对象也需要实现此方法；它们需要返回对象自身。\n object.reversed(self) 此方法（如果存在）会被 reversed内置函数调用以实现逆向迭代。它应当返回一个新的以逆序逐个迭代容器内所有对象的迭代器对象。\n  object.next() 从容器中返回下一项。 如果已经没有项可返回，则会引发 StopIteration 异常。\n  可迭代(Iterable) 如果一个对象具备有 iter() 或者 getitem() 其中任何一个魔术方法的话，这个对象就可以称为是可迭代的。 其中，iter() 的作用是可以让 for 循环遍历，而 getitem() 方法可以让实例对象通过 [index] 索引的方式去访问实例中的元素。所以，List、Tuple、Dictionary、String 等数据类型都是可迭代的。\n迭代器(Iterator) 如果一个对象同时有__iter__()和__next__()魔术方法的话，这个对象就可以称为是迭代器。 iter() 的作用前面我们也提到过，是可以让for循环遍历。而 next()方法是让对象可以通过 next(实例对象) 的方式访问下一个元素。 List、Tuple、Dictionary、String 等数据类型虽然是可迭代的，但都不是迭代器，因为他们都没有 next( )方法。\n一个迭代器(Iterator)对象不仅可以在 for 循环中使用，还可以通过内置函数 next() 函数进行调用。\n如何判断可迭代(iterable) \u0026amp; 迭代器(iterator) 借助Python中的isinstance(object, classinfo)函数来判断一个对象是否是一个已知类型。\nfrom collections import Iterable from collections import Iterator print(f\u0026quot;List is 'Iterable': {isinstance([], Iterable)}\u0026quot;) print(f\u0026quot;Tuple is 'Iterable': {isinstance((), Iterable)}\u0026quot;) print(f\u0026quot;Dict is 'Iterable': {isinstance({}, Iterable)}\u0026quot;) print(f\u0026quot;String is 'Iterable': {isinstance('', Iterable)}\u0026quot;) print(\u0026quot;=\u0026quot;*25) print(f\u0026quot;List is 'Iterator': {isinstance([], Iterator)}\u0026quot;) print(f\u0026quot;Tuple is 'Iterator': {isinstance((), Iterator)}\u0026quot;) print(f\u0026quot;Dict is 'Iterator': {isinstance({}, Iterator)}\u0026quot;) print(f\u0026quot;String is 'Iterator': {isinstance('', Iterator)}\u0026quot;) # 输出如下： # List is 'Iterable': True # Tuple is 'Iterable': True # Dict is 'Iterable': True # String is 'Iterable': True # ========================= # List is 'Iterator': False # Tuple is 'Iterator': False # Dict is 'Iterator': False # String is 'Iterator': False  通过对定义的分析和比较我们得知：迭代器都是可迭代的（因为迭代器都包含__iter__()函数），但可迭代的不一定是迭代器（因为未必每个可迭代就包含__next__()方法）。\n创建一个迭代器 所以只要定义了一个类并在类中实现 iter() 和 next() 方法，那么这个类就可以当做是一个迭代器了。\nfrom collections import Iterator class Data: def __init__(self, x): self.x = x def __iter__(self): return self def __next__(self): if self.x \u0026gt;= 10: raise StopIteration else: self.x += 1 return self.x data = Data(0) print(f\u0026quot;data is 'Iterator': {isinstance(data, Iterator)}\u0026quot;) # 输出如下： # data is 'Iterator': True  for 循环本质 for循环称为迭代器循环，in 后必须是可迭代的对象。\nfor i in 'hello': print(i)  可以看作是：\ns = 'hello' iter_s = s.__iter__() while True: try: print(iter_s.__next__()) except StopIteration: break  更好的例子：\nfrom collections import Iterator class Data: def __init__(self, x): self.x = x def __iter__(self): return self def __next__(self): if self.x \u0026gt;= 10: raise StopIteration else: self.x += 1 return self.x data = Data(0) for d in data: print(d) # 输出如下： # 1 # 2 # 3 # 4 # 5 # 6 # 7 # 8 # 9 # 10  但是在实际的 for 循环中不会引发异常，for 循环会将异常作为一个中断命令，从而停止循环。\n无限迭代器 迭代器对象中的项目不必都是可耗尽的，可以是无限迭代器(永远不会结束)。 处理这样的迭代器时一定要小心。\nfrom collections import Iterator class Data: def __init__(self, x): self.x = x def __iter__(self): return self def __next__(self): self.x += 1 return self.x data = Data(0) for d in data: print(d)  当迭代这些类型的无限迭代器时，请注意指定终止条件。\n逆向迭代器 反向迭代仅仅当对象的大小可预先确定或者对象实现了 reversed() 的特殊方法时才能生效。\nclass MyNumbers: def __init__(self): self.flag=True pass def __iter__(self): self.a=0 return self def __reversed__(self): self.a=0 self.flag=not self.flag return self def __next__(self): if self.flag: x=self.a+1 if self.a\u0026gt;=5: raise StopIteration else: x=5-self.a if self.a\u0026gt;=5: raise StopIteration self.a+=1 return x myclass = MyNumbers() myiter = iter(myclass) print('iter-next') for x in myiter: print(x) print('reversed-next') myrv=reversed(myclass) print(next(myrv)) print(next(myrv)) print(next(myrv)) print(next(myrv)) print(next(myrv)) print('reversed-reversed') myrv=reversed(myclass) for i in myrv: print(i)  本例中直接实现了iter和reversed 两种方法。主要设置一个标志位，通过__reversed__改变标志位来设置是正序输出还是反序输出，通过多次调用reversed可以进行反转。\n可迭代对象转迭代器对象 List、Tuple、Dictionary、String 等数据类型虽然是可迭代的，但都不是迭代器。 通过 iter() 方法可以将其转为迭代器对象。\nfrom collections.abc import Iterable from collections.abc import Iterator list_one = [1,2,3] print(isinstance(list_one,Iterable)) print(isinstance(list_one,Iterator)) it_list = iter(list_one) itr_list = list_one.__iter__() print(isinstance(it_list,Iterable)) print(isinstance(it_list,Iterator)) print(isinstance(itr_list,Iterable)) print(isinstance(itr_list,Iterator)) print(next(it_list)) print(it_list.__next__()) print(next(itr_list)) print(itr_list.__next__()) # True # False # True # True # True # True # 1 # 2 # 1 # 2  iter(obj) 等同于 obj.iter() next(obje) 等同于 obje.next()\n迭代器优缺点：  优点：迭代器对象表示的是一个数据流，可以在需要时才去调用next来获取一个值；因而本身在内存中始终只保留一个值，对于内存占用小可以存放无限数据流。优于其他容器需要一次将所有元素都存放进内存，如：列表、集合、字典\u0026hellip;等 缺点：1.无法获取存放的元素长度，除非取完计数。2.只能向后取值，next()永远返回的是下一个值。取值不灵活，无法取出指定值(无法像字典的key,或列表的下标)，而且迭代器的生命周期是一次性的元素被迭代完则生命周期结束  生成对象为迭代器的函数 enumerate() 、zip()、map()、filter() 生成的对象都为迭代器\nfrom collections.abc import Iterator def add(x,y,z): return x+1 list1 = [1,2,3] list2 = [1,2,3] en = enumerate(list1) print(isinstance(en,Iterator)) zi = zip(list1,list2) print(isinstance(zi,Iterator)) ma = map(add,list1) print(isinstance(ma,Iterator)) fi = filter(lambda x: x\u0026gt;=2,list1) print(isinstance(fi,Iterator)) # Ture # Ture # Ture # Ture  zip() ： 同时迭代多个序列 同时迭代多个序列，每次分别从一个序列中取一个元素。 zip(a, b) 会生成一个可返回元组 (x, y) 的迭代器，其中x来自a，y来自b。 一旦其中某个序列到底结尾，迭代宣告结束。 因此迭代长度跟参数中最短序列长度一致。\na = [1, 2, 3] b = ['w', 'x', 'y', 'z'] for i in zip(a,b): ... print(i) ... # (1, 'w') # (2, 'x') # (3, 'y')  拓展：如果希望得到的是最长序列，即没有值可以自动添加默认值的可以考虑使用 itertools.zip_longest() 函数来代替。\nfor i in zip_longest(a, b, fillvalue=0): print(i) ... # (1, 'w') # (2, 'x') # (3, 'y') # (0, 'z')  fillvalue 的值没有会默认为 None。\nenumerate() ：序列上索引值迭代 迭代一个序列的同时跟踪正在被处理的元素索引。 为了按传统行号输出(行号从1开始)，你可以传递一个开始参数（默认从0开始）：\nmy_list = ['a', 'b', 'c'] for idx, val in enumerate(my_list, 1): print(idx, val) ... # 1 a # 2 b # 3 c  enumerate() 函数返回的是一个 enumerate 对象实例， 它是一个迭代器，返回连续的包含一个计数和一个值的元组， 元组中的值通过在传入序列上调用 next() 返回。\nPython中的迭代器与可迭代 Python迭代器和反转迭代器的一点思考 Python迭代器 python3-cookbook\n","date":"2022-10-25","permalink":"/post/iterator/","tags":["Python"],"title":"iterator"},{"content":"11.1. 简介 本指南归纳于我的几个月的博客，主题是 魔法方法 。\n什么是魔法方法呢？它们在面向对象的Python的处处皆是。它们是一些可以让你对类添加“魔法”的特殊方法。 它们经常是两个下划线包围来命名的（比如 init ， lt ）。但是现在没有很好的文档来解释它们。 所有的魔法方法都会在Python的官方文档中找到，但是它们组织松散。而且很少会有示例（有的是无聊的语法描述， 语言参考）。\n所以，为了修复我感知的Python文档的缺陷，我开始提供更为通俗的，有示例支持的Python魔法方法指南。我一开始 写了一些博文，现在我把这些博文总起来成为一篇指南。\n希望你喜欢这篇指南，一篇友好，通俗易懂的Python魔法方法指南！\n11.2. 构造方法 我们最为熟知的基本的魔法方法就是 init ，我们可以用它来指明一个对象初始化的行为。然而，当我们调用 x = SomeClass() 的时候， init 并不是第一个被调用的方法。事实上，第一个被调用的是 new ，这个 方法才真正地创建了实例。当这个对象的生命周期结束的时候， del 会被调用。让我们近一步理解这三个方法：\nnew(cls,[\u0026hellip;) new 是对象实例化时第一个调用的方法，它只取下 cls 参数，并把其他参数传给 init 。 new 很少使用，但是也有它适合的场景，尤其是当类继承自一个像元组或者字符串这样不经常改变的类型的时候。我不打算深入讨论 new ，因为它并不是很有用， Python文档 中 有详细的说明。\ninit(self,[\u0026hellip;]) 类的初始化方法。它获取任何传给构造器的参数（比如我们调用 x = SomeClass(10, ‘foo’) ， init 就会接到参数 10 和 ‘foo’ 。 init 在Python的类定义中用的最多。\ndel(self) new 和 init 是对象的构造器， del 是对象的销毁器。它并非实现了语句 del x (因此该语句不等同于 x.del())。而是定义了当对象被垃圾回收时的行为。 当对象需要在销毁时做一些处理的时候这个方法很有用，比如 socket 对象、文件对象。但是需要注意的是，当Python解释器退出但对象仍然存活的时候， del 并不会 执行。 所以养成一个手工清理的好习惯是很重要的，比如及时关闭连接。\n这里有个 init 和 del 的例子:\nfrom os.path import join\nclass FileObject: \u0026lsquo;\u0026lsquo;\u0026lsquo;文件对象的装饰类，用来保证文件被删除时能够正确关闭。\u0026rsquo;\u0026rsquo;\u0026rsquo;\ndef __init__(self, filepath='~', filename='sample.txt'): # 使用读写模式打开filepath中的filename文件 self.file = open(join(filepath, filename), 'r+') def __del__(self): self.file.close() del self.file  11.3. 操作符 使用Python魔法方法的一个巨大优势就是可以构建一个拥有Python内置类型行为的对象。这意味着你可以避免使用非标准的、丑陋的方式来表达简单的操作。 在一些语言中，这样做很常见:\nif instance.equals(other_instance): # do something 你当然可以在Python也这么做，但是这样做让代码变得冗长而混乱。不同的类库可能对同一种比较操作采用不同的方法名称，这让使用者需要做很多没有必要的工作。运用魔法方法的魔力，我们可以定义方法 eq\nif instance == other_instance: # do something 这是魔法力量的一部分，这样我们就可以创建一个像内建类型那样的对象了！\n11.3.1. 比较操作符 Python包含了一系列的魔法方法，用于实现对象之间直接比较，而不需要采用方法调用。同样也可以重载Python默认的比较方法，改变它们的行为。下面是这些方法的列表：\ncmp(self, other) cmp 是所有比较魔法方法中最基础的一个，它实际上定义了所有比较操作符的行为（\u0026lt;,==,!=,等等），但是它可能不能按照你需要的方式工作（例如，判断一个实例和另一个实例是否相等采用一套标准，而与判断一个实例是否大于另一实例采用另一套）。 cmp 应该在 self \u0026lt; other 时返回一个负整数，在 self == other 时返回0，在 self \u0026gt; other 时返回正整数。最好只定义你所需要的比较形式，而不是一次定义全部。 如果你需要实现所有的比较形式，而且它们的判断标准类似，那么 cmp 是一个很好的方法，可以减少代码重复，让代码更简洁。\neq`(self, other) 定义等于操作符(==)的行为。\nne(self, other) 定义不等于操作符(!=)的行为。\nlt(self, other) 定义小于操作符(\u0026lt;)的行为。\ngt(self, other) 定义大于操作符(\u0026gt;)的行为。\nle(self, other) 定义小于等于操作符(\u0026lt;)的行为。\nge(self, other) 定义大于等于操作符(\u0026gt;)的行为。\n举个例子，假如我们想用一个类来存储单词。我们可能想按照字典序（字母顺序）来比较单词，字符串的默认比较行为就是这样。我们可能也想按照其他规则来比较字符串，像是长度，或者音节的数量。在这个例子中，我们使用长度作为比较标准，下面是一种实现:\nclass Word(str): \u0026lsquo;\u0026lsquo;\u0026lsquo;单词类，按照单词长度来定义比较行为\u0026rsquo;\u0026rsquo;\u0026rsquo;\ndef __new__(cls, word): # 注意，我们只能使用 __new__ ，因为str是不可变类型 # 所以我们必须提前初始化它（在实例创建时） if ' ' in word: print \u0026quot;Value contains spaces. Truncating to first space.\u0026quot; word = word[:word.index(' ')] # Word现在包含第一个空格前的所有字母 return str.__new__(cls, word) def __gt__(self, other): return len(self) \u0026gt; len(other) def __lt__(self, other): return len(self) \u0026lt; len(other) def __ge__(self, other): return len(self) \u0026gt;= len(other) def __le__(self, other): return len(self) \u0026lt;= len(other)  现在我们可以创建两个 Word 对象（ Word(‘foo’) 和 Word(‘bar’))然后根据长度来比较它们。注意我们没有定义 eq 和 ne ，这是因为有时候它们会导致奇怪的结果（很明显， Word(‘foo’) == Word(‘bar’) 得到的结果会是true）。根据长度测试是否相等毫无意义，所以我们使用 str 的实现来比较相等。\n从上面可以看到，不需要实现所有的比较魔法方法，就可以使用丰富的比较操作。标准库还在 functools 模块中提供了一个类装饰器，只要我们定义 eq 和另外一个操作符（ gt, lt 等），它就可以帮我们实现比较方法。这个特性只在 Python 2.7 中可用。当它可用时，它能帮助我们节省大量的时间和精力。要使用它，只需要它 @total_ordering 放在类的定义之上就可以了\n11.3.2. 数值操作符 就像你可以使用比较操作符来比较类的实例，你也可以定义数值操作符的行为。固定好你的安全带，这样的操作符真的有很多。看在组织的份上，我把它们分成了五类：一元操作符，常见算数操作符，反射算数操作符（后面会涉及更多），增强赋值操作符，和类型转换操作符。\n11.3.2.1. 一元操作符 一元操作符只有一个操作符。\npos(self) 实现取正操作，例如 +some_object。\nneg(self) 实现取负操作，例如 -some_object。\nabs(self) 实现内建绝对值函数 abs() 操作。\ninvert(self) 实现取反操作符 ~。\nround(self， n) 实现内建函数 round() ，n 是近似小数点的位数。\nfloor(self) 实现 math.floor() 函数，即向下取整。\nceil(self) 实现 math.ceil() 函数，即向上取整。\ntrunc(self) 实现 math.trunc() 函数，即距离零最近的整数。\n11.3.2.2. 常见算数操作符 现在，我们来看看常见的二元操作符（和一些函数），像+，-，*之类的，它们很容易从字面意思理解。\nadd(self, other) 实现加法操作。\nsub(self, other) 实现减法操作。\nmul(self, other) 实现乘法操作。\nfloordiv(self, other) 实现使用 // 操作符的整数除法。\ndiv(self, other) 实现使用 / 操作符的除法。\ntruediv(self, other) 实现 true 除法，这个函数只有使用 from future import division 时才有作用。\nmod(self, other) 实现 % 取余操作。\ndivmod(self, other) 实现 divmod 内建函数。\npow 实现 ** 操作符。\nlshift(self, other) 实现左移位运算符 \u0026laquo; 。\nrshift(self, other) 实现右移位运算符 \u0026raquo; 。\nand(self, other) 实现按位与运算符 \u0026amp; 。\nor(self, other) 实现按位或运算符 | 。\nxor(self, other) 实现按位异或运算符 ^ 。\n11.3.2.3. 反射算数运算符 还记得刚才我说会谈到反射运算符吗？可能你会觉得它是什么高端霸气上档次的概念，其实这东西挺简单的，下面举个例子:\nsome_object + other 这是“常见”的加法，反射是一样的意思，只不过是运算符交换了一下位置:\nother + some_object 所有反射运算符魔法方法和它们的常见版本做的工作相同，只不过是处理交换连个操作数之后的情况。绝大多数情况下，反射运算和正常顺序产生的结果是相同的，所以很可能你定义 radd 时只是调用一下 add。注意一点，操作符左侧的对象（也就是上面的 other ）一定不要定义（或者产生 NotImplemented 异常） 操作符的非反射版本。例如，在上面的例子中，只有当 other 没有定义 add 时 some_object.radd 才会被调用。\nradd(self, other) 实现反射加法操作。\nrsub(self, other) 实现反射减法操作。\nrmul(self, other) 实现反射乘法操作。\nrfloordiv(self, other) 实现使用 // 操作符的整数反射除法。\nrdiv(self, other) 实现使用 / 操作符的反射除法。\nrtruediv(self, other) 实现 true 反射除法，这个函数只有使用 from future import division 时才有作用。\nrmod(self, other) 实现 % 反射取余操作符。\nrdivmod(self, other) 实现调用 divmod(other, self) 时 divmod 内建函数的操作。\nrpow 实现 ** 反射操作符。\nrlshift(self, other) 实现反射左移位运算符 \u0026laquo; 的作用。\nrshift(self, other) 实现反射右移位运算符 \u0026raquo; 的作用。\nrand(self, other) 实现反射按位与运算符 \u0026amp; 。\nror(self, other) 实现反射按位或运算符 | 。\nrxor(self, other) 实现反射按位异或运算符 ^ 。\n11.3.2.4. 增强赋值运算符 Python同样提供了大量的魔法方法，可以用来自定义增强赋值操作的行为。或许你已经了解增强赋值，它融合了“常见”的操作符和赋值操作，如果你还是没听明白，看下面的例子:\nx = 5 x += 1 # 也就是 x = x + 1 这些方法都应该返回左侧操作数应该被赋予的值（例如， a += b iadd 也许会返回 a + b ，这个结果会被赋给 a ）,下面是方法列表：\niadd(self, other) 实现加法赋值操作。\nisub(self, other) 实现减法赋值操作。\nimul(self, other) 实现乘法赋值操作。\nifloordiv(self, other) 实现使用 //= 操作符的整数除法赋值操作。\nidiv(self, other) 实现使用 /= 操作符的除法赋值操作。\nitruediv(self, other) 实现 true 除法赋值操作，这个函数只有使用 from future import division 时才有作用。\nimod(self, other) 实现 %= 取余赋值操作。\nipow 实现 **= 操作。\nilshift(self, other) 实现左移位赋值运算符 \u0026laquo;= 。\nirshift(self, other) 实现右移位赋值运算符 \u0026raquo;= 。\niand(self, other) 实现按位与运算符 \u0026amp;= 。\nior(self, other) 实现按位或赋值运算符 | 。\nixor(self, other) 实现按位异或赋值运算符 ^= 。\n11.3.2.5. 类型转换操作符 Python也有一系列的魔法方法用于实现类似 float() 的内建类型转换函数的操作。它们是这些：\nint(self) 实现到int的类型转换。\nlong(self) 实现到long的类型转换。\nfloat(self) 实现到float的类型转换。\ncomplex(self) 实现到complex的类型转换。\noct(self) 实现到八进制数的类型转换。\nhex(self) 实现到十六进制数的类型转换。\nindex(self) 实现当对象用于切片表达式时到一个整数的类型转换。如果你定义了一个可能会用于切片操作的数值类型，你应该定义 index。\ntrunc(self) 当调用 math.trunc(self) 时调用该方法， trunc 应该返回 self 截取到一个整数类型（通常是long类型）的值。\ncoerce(self) 该方法用于实现混合模式算数运算，如果不能进行类型转换， coerce 应该返回 None 。反之，它应该返回一个二元组 self 和 other ，这两者均已被转换成相同的类型。\n11.4. 类的表示 使用字符串来表示类是一个相当有用的特性。在Python中有一些内建方法可以返回类的表示，相对应的，也有一系列魔法方法可以用来自定义在使用这些内建函数时类的行为。\nstr(self) 定义对类的实例调用 str() 时的行为。\nrepr(self) 定义对类的实例调用 repr() 时的行为。 str() 和 repr() 最主要的差别在于“目标用户”。 repr() 的作用是产生机器可读的输出（大部分情况下，其输出可以作为有效的Python代码），而 str() 则产生人类可读的输出。\nunicode(self) 定义对类的实例调用 unicode() 时的行为。 unicode() 和 str() 很像，只是它返回unicode字符串。注意，如果调用者试图调用 str() 而你的类只实现了 unicode() ，那么类将不能正常工作。所有你应该总是定义 str() ，以防有些人没有闲情雅致来使用unicode。\nformat(self) 定义当类的实例用于新式字符串格式化时的行为，例如， “Hello, 0:abc!”.format(a) 会导致调用 a.format(“abc”) 。当定义你自己的数值类型或字符串类型时，你可能想提供某些特殊的格式化选项，这种情况下这个魔法方法会非常有用。\nhash(self) 定义对类的实例调用 hash() 时的行为。它必须返回一个整数，其结果会被用于字典中键的快速比较。同时注意一点，实现这个魔法方法通常也需要实现 eq ，并且遵守如下的规则： a == b 意味着 hash(a) == hash(b)。\nnonzero(self) 定义对类的实例调用 bool() 时的行为，根据你自己对类的设计，针对不同的实例，这个魔法方法应该相应地返回True或False。\ndir(self) 定义对类的实例调用 dir() 时的行为，这个方法应该向调用者返回一个属性列表。一般来说，没必要自己实现 dir 。但是如果你重定义了 getattr 或者 getattribute （下个部分会介绍），乃至使用动态生成的属性，以实现类的交互式使用，那么这个魔法方法是必不可少的。\n到这里，我们基本上已经结束了魔法方法指南中无聊并且例子匮乏的部分。既然我们已经介绍了较为基础的魔法方法，是时候涉及更高级的内容了。\n11.5. 访问控制 很多从其他语言转向Python的人都抱怨Python的类缺少真正意义上的封装（即没办法定义私有属性然后使用公有的getter和setter）。然而事实并非如此。实际上Python不是通过显式定义的字段和方法修改器，而是通过魔法方法实现了一系列的封装。\ngetattr(self, name) 当用户试图访问一个根本不存在（或者暂时不存在）的属性时，你可以通过这个魔法方法来定义类的行为。这个可以用于捕捉错误的拼写并且给出指引，使用废弃属性时给出警告（如果你愿意，仍然可以计算并且返回该属性），以及灵活地处理AttributeError。只有当试图访问不存在的属性时它才会被调用，所以这不能算是一个真正的封装的办法。\nsetattr(self, name, value) 和 getattr 不同， setattr 可以用于真正意义上的封装。它允许你自定义某个属性的赋值行为，不管这个属性存在与否，也就是说你可以对任意属性的任何变化都定义自己的规则。然后，一定要小心使用 setattr ，这个列表最后的例子中会有所展示。\ndelattr(self, name) 这个魔法方法和 setattr 几乎相同，只不过它是用于处理删除属性时的行为。和 setattr_ 一样，使用它时也需要多加小心，防止产生无限递归（在 delattr 的实现中调用 del self.name 会导致无限递归）。\ngetattribute(self, name)  __getattribute__ 看起来和上面那些方法很合得来，但是最好不要使用它。 getattribute 只能用于新式类。在最新版的Python中所有的类都是新式类，在老版Python中你可以通过继承 object 来创建新式类。 getattribute 允许你自定义属性被访问时的行为，它也同样可能遇到无限递归问题（通过调用基类的 getattribute 来避免）。 getattribute 基本上可以替代 getattr 。只有当它被实现，并且显式地被调用，或者产生 AttributeError 时它才被使用。 这个魔法方法可以被使用（毕竟，选择权在你自己），我不推荐你使用它，因为它的使用范围相对有限（通常我们想要在赋值时进行特殊操作，而不是取值时），而且实现这个方法很容易出现Bug。\n自定义这些控制属性访问的魔法方法很容易导致问题，考虑下面这个例子:\ndef setattr(self, name. value): self.name = value # 因为每次属性幅值都要调用 setattr()，所以这里的实现会导致递归 # 这里的调用实际上是 self.__setattr(\u0026rsquo;name\u0026rsquo;, value)。因为这个方法一直 # 在调用自己，因此递归将持续进行，直到程序崩溃\ndef setattr(self, name, value): self.dict[name] = value # 使用 dict 进行赋值 # 定义自定义行为 再次重申，Python的魔法方法十分强大，能力越强责任越大，了解如何正确的使用魔法方法更加重要。\n到这里，我们对Python中自定义属性存取控制有了什么样的印象？它并不适合轻度的使用。实际上，它有些过分强大，而且违反直觉。然而它之所以存在，是因为一个更大的原则：Python不指望让杜绝坏事发生，而是想办法让做坏事变得困难。自由是至高无上的权利，你真的可以随心所欲。下面的例子展示了实际应用中某些特殊的属性访问方法（注意我们之所以使用 super 是因为不是所有的类都有 dict 属性）:\nclass AccessCounter(object): \u0026rsquo;\u0026rsquo;\u0026rsquo; 一个包含了一个值并且实现了访问计数器的类 每次值的变化都会导致计数器自增\u0026rsquo;\u0026rsquo;\u0026rsquo;\ndef __init__(self, val): super(AccessCounter, self).__setattr__('counter', 0) super(AccessCounter, self).__setattr__('value', val) def __setattr__(self, name, value): if name == 'value': super(AccessCounter, self).__setattr_('counter', self.counter + 1) # 使计数器自增变成不可避免 # 如果你想阻止其他属性的赋值行为 # 产生 AttributeError(name) 就可以了 super(AccessCounter, self).__setattr__(name, value) def __delattr__(self, name): if name == 'value': super(AccessCounter, self).__setattr('counter', self.counter + 1) super(AccessCounter, self).__delattr(name)  11.6. 自定义序列 有许多办法可以让你的Python类表现得像是内建序列类型（字典，元组，列表，字符串等）。这些魔法方式是目前为止我最喜欢的。它们给了你难以置信的控制能力，可以让你的类与一系列的全局函数完美结合。在了解激动人心的内容之前，首先你需要掌握一些预备知识。\n11.6.1. 预备知识 既然讲到创建自己的序列类型，就不得不说一说协议了。协议类似某些语言中的接口，里面包含的是一些必须实现的方法。在Python中，协议完全是非正式的，也不需要显式的声明，事实上，它们更像是一种参考标准。\n为什么我们要讲协议？因为在Python中实现自定义容器类型需要用到一些协议。首先，不可变容器类型有如下协议：想实现一个不可变容器，你需要定义 len 和 getitem (后面会具体说明）。可变容器的协议除了上面提到的两个方法之外，还需要定义 setitem 和 delitem 。最后，如果你想让你的对象可以迭代，你需要定义 iter ，这个方法返回一个迭代器。迭代器必须遵守迭代器协议，需要定义 iter （返回它自己）和 next 方法。\n11.6.2. 容器背后的魔法方法 len(self) 返回容器的长度，可变和不可变类型都需要实现。\ngetitem(self, key) 定义对容器中某一项使用 self[key] 的方式进行读取操作时的行为。这也是可变和不可变容器类型都需要实现的一个方法。它应该在键的类型错误式产生 TypeError 异常，同时在没有与键值相匹配的内容时产生 KeyError 异常。\nsetitem(self, key) 定义对容器中某一项使用 self[key] 的方式进行赋值操作时的行为。它是可变容器类型必须实现的一个方法，同样应该在合适的时候产生 KeyError 和 TypeError 异常。\niter(self, key) 它应该返回当前容器的一个迭代器。迭代器以一连串内容的形式返回，最常见的是使用 iter() 函数调用，以及在类似 for x in container: 的循环中被调用。迭代器是他们自己的对象，需要定义 iter 方法并在其中返回自己。\nreversed(self) 定义了对容器使用 reversed() 内建函数时的行为。它应该返回一个反转之后的序列。当你的序列类是有序时，类似列表和元组，再实现这个方法，\ncontains(self, item) contains 定义了使用 in 和 not in 进行成员测试时类的行为。你可能好奇为什么这个方法不是序列协议的一部分，原因是，如果 contains 没有定义，Python就会迭代整个序列，如果找到了需要的一项就返回 True 。\nmissing(self ,key) missing 在字典的子类中使用，它定义了当试图访问一个字典中不存在的键时的行为（目前为止是指字典的实例，例如我有一个字典 d ， “george” 不是字典中的一个键，当试图访问 d[“george’] 时就会调用 d.missing(“george”) ）。\n11.6.3. 一个例子 让我们来看一个实现了一些函数式结构的列表，可能在其他语言中这种结构更常见（例如Haskell）:\nclass FunctionalList: \u0026lsquo;\u0026lsquo;\u0026lsquo;一个列表的封装类，实现了一些额外的函数式 方法，例如head, tail, init, last, drop和take。\u0026rsquo;\u0026rsquo;\u0026rsquo;\ndef __init__(self, values=None): if values is None: self.values = [] else: self.values = values def __len__(self): return len(self.values) def __getitem__(self, key): # 如果键的类型或值不合法，列表会返回异常 return self.values[key] def __setitem__(self, key, value): self.values[key] = value def __delitem__(self, key): del self.values[key] def __iter__(self): return iter(self.values) def __reversed__(self): return reversed(self.values) def append(self, value): self.values.append(value) def head(self): # 取得第一个元素 return self.values[0] def tail(self): # 取得除第一个元素外的所有元素 return self.valuse[1:] def init(self): # 取得除最后一个元素外的所有元素 return self.values[:-1] def last(self): # 取得最后一个元素 return self.values[-1] def drop(self, n): # 取得除前n个元素外的所有元素 return self.values[n:] def take(self, n): # 取得前n个元素 return self.values[:n]  就是这些，一个（微不足道的）有用的例子，向你展示了如何实现自己的序列。当然啦，自定义序列有更大的用处，而且绝大部分都在标准库中实现了（Python是自带电池的，记得吗？），像 Counter , OrderedDict 和 NamedTuple 。\n11.7. 反射 你可以通过定义魔法方法来控制用于反射的内建函数 isinstance 和 issubclass 的行为。下面是对应的魔法方法：\ninstancecheck(self, instance) 检查一个实例是否是你定义的类的一个实例（例如 isinstance(instance, class) ）。\nsubclasscheck(self, subclass) 检查一个类是否是你定义的类的子类（例如 issubclass(subclass, class) ）。\n这几个魔法方法的适用范围看起来有些窄，事实也正是如此。我不会在反射魔法方法上花费太多时间，因为相比其他魔法方法它们显得不是很重要。但是它们展示了在Python中进行面向对象编程（或者总体上使用Python进行编程）时很重要的一点：不管做什么事情，都会有一个简单方法，不管它常用不常用。这些魔法方法可能看起来没那么有用，但是当你真正需要用到它们的时候，你会感到很幸运，因为它们还在那儿（也因为你阅读了这本指南！）\n11.8. 抽象基类 请参考 http://docs.python.org/2/library/abc.html.\n11.9. 可调用的对象 你可能已经知道了，在Python中，函数是一等的对象。这意味着它们可以像其他任何对象一样被传递到函数和方法中，这是一个十分强大的特性。\nPython中一个特殊的魔法方法允许你自己类的对象表现得像是函数，然后你就可以“调用”它们，把它们传递到使用函数做参数的函数中，等等等等。这是另一个强大而且方便的特性，让使用Python编程变得更加幸福。\ncall(self, [args\u0026hellip;]) 允许类的一个实例像函数那样被调用。本质上这代表了 x() 和 x.call() 是相同的。注意 call 可以有多个参数，这代表你可以像定义其他任何函数一样，定义 call ，喜欢用多少参数就用多少。\ncall 在某些需要经常改变状态的类的实例中显得特别有用。“调用”这个实例来改变它的状态，是一种更加符合直觉，也更加优雅的方法。一个表示平面上实体的类是一个不错的例子:\nclass Entity: \u0026lsquo;\u0026lsquo;\u0026lsquo;表示一个实体的类，调用它的实例 可以更新实体的位置\u0026rsquo;\u0026rsquo;\u0026rsquo;\n def __init__(self, size, x, y): self.x, self.y = x, y self.size = size def __call__(self, x, y): '''改变实体的位置''' self.x, self.y = x, y  11.10. 上下文管理器 在Python 2.5中引入了一个全新的关键词，随之而来的是一种新的代码复用方法—— with 声明。上下文管理的概念在Python中并不是全新引入的（之前它作为标准库的一部分实现），直到PEP 343被接受，它才成为一种一级的语言结构。可能你已经见过这种写法了:\nwith open(\u0026lsquo;foo.txt\u0026rsquo;) as bar: # 使用bar进行某些操作 当对象使用 with 声明创建时，上下文管理器允许类做一些设置和清理工作。上下文管理器的行为由下面两个魔法方法所定义：\nenter(self) 定义使用 with 声明创建的语句块最开始上下文管理器应该做些什么。注意 enter 的返回值会赋给 with 声明的目标，也就是 as 之后的东西。\nexit(self, exception_type, exception_value, traceback) 定义当 with 声明语句块执行完毕（或终止）时上下文管理器的行为。它可以用来处理异常，进行清理，或者做其他应该在语句块结束之后立刻执行的工作。如果语句块顺利执行， exception_type , exception_value 和 traceback 会是 None 。否则，你可以选择处理这个异常或者让用户来处理。如果你想处理异常，确保 exit 在完成工作之后返回 True 。如果你不想处理异常，那就让它发生吧。\n对一些具有良好定义的且通用的设置和清理行为的类，enter 和 exit 会显得特别有用。你也可以使用这几个方法来创建通用的上下文管理器，用来包装其他对象。下面是一个例子:\nclass Closer: \u0026lsquo;\u0026lsquo;\u0026lsquo;一个上下文管理器，可以在with语句中 使用close()自动关闭对象\u0026rsquo;\u0026rsquo;\u0026rsquo;\ndef __init__(self, obj): self.obj = obj def __enter__(self, obj): return self.obj # 绑定到目标 def __exit__(self, exception_type, exception_value, traceback): try: self.obj.close() except AttributeError: # obj不是可关闭的 print 'Not closable.' return True # 成功地处理了异常  这是一个 Closer 在实际使用中的例子，使用一个FTP连接来演示（一个可关闭的socket):\n   from magicmethods import Closer from ftplib import FTP with Closer(FTP(\u0026lsquo;ftp.somesite.com\u0026rsquo;)) as conn: \u0026hellip; conn.dir() \u0026hellip;\n   为了简单，省略了某些输出    conn.dir()\n   很长的 AttributeError 信息，不能使用一个已关闭的连接    with Closer(int(5)) as i: \u0026hellip; i += 1 \u0026hellip; Not closable. i 6 看到我们的包装器是如何同时优雅地处理正确和不正确的调用了吗？这就是上下文管理器和魔法方法的力量。Python标准库包含一个 contextlib 模块，里面有一个上下文管理器 contextlib.closing() 基本上和我们的包装器完成的是同样的事情（但是没有包含任何当对象没有close()方法时的处理）。\n   11.11. 创建描述符对象 描述符是一个类，当使用取值，赋值和删除 时它可以改变其他对象。描述符不是用来单独使用的，它们需要被一个拥有者类所包含。描述符可以用来创建面向对象数据库，以及创建某些属性之间互相依赖的类。描述符在表现具有不同单位的属性，或者需要计算的属性时显得特别有用（例如表现一个坐标系中的点的类，其中的距离原点的距离这种属性）。\n要想成为一个描述符，一个类必须具有实现 get , set 和 delete 三个方法中至少一个。\n让我们一起来看一看这些魔法方法：\nget(self, instance, owner) 定义当试图取出描述符的值时的行为。 instance 是拥有者类的实例， owner 是拥有者类本身。\nset(self, instance, owner) 定义当描述符的值改变时的行为。 instance 是拥有者类的实例， value 是要赋给描述符的值。\ndelete(self, instance, owner) 定义当描述符的值被删除时的行为。 instance 是拥有者类的实例\n现在，来看一个描述符的有效应用：单位转换:\nclass Meter(object): \u0026lsquo;\u0026lsquo;\u0026lsquo;米的描述符。\u0026rsquo;\u0026rsquo;\u0026rsquo;\ndef __init__(self, value=0.0): self.value = float(value) def __get__(self, instance, owner): return self.value def __set__(self, instance, owner): self.value = float(value)  class Foot(object): \u0026lsquo;\u0026lsquo;\u0026lsquo;英尺的描述符。\u0026rsquo;\u0026rsquo;\u0026rsquo;\ndef __get(self, instance, owner): return instance.meter * 3.2808 def __set(self, instance, value): instance.meter = float(value) / 3.2808  class Distance(object): \u0026lsquo;\u0026lsquo;\u0026lsquo;用于描述距离的类，包含英尺和米两个描述符。\u0026rsquo;\u0026rsquo;\u0026rsquo; meter = Meter() foot = Foot() 11.12. 拷贝 有些时候，特别是处理可变对象时，你可能想拷贝一个对象，改变这个对象而不影响原有的对象。这时就需要用到Python的 copy 模块了。然而（幸运的是），Python模块并不具有感知能力， 因此我们不用担心某天基于Linux的机器人崛起。但是我们的确需要告诉Python如何有效率地拷贝对象。\ncopy(self) 定义对类的实例使用 copy.copy() 时的行为。 copy.copy() 返回一个对象的浅拷贝，这意味着拷贝出的实例是全新的，然而里面的数据全都是引用的。也就是说，对象本身是拷贝的，但是它的数据还是引用的（所以浅拷贝中的数据更改会影响原对象）。\ndeepcopy(self, memodict=) 定义对类的实例使用 copy.deepcopy() 时的行为。 copy.deepcopy() 返回一个对象的深拷贝，这个对象和它的数据全都被拷贝了一份。 memodict 是一个先前拷贝对象的缓存，它优化了拷贝过程，而且可以防止拷贝递归数据结构时产生无限递归。当你想深拷贝一个单独的属性时，在那个属性上调用 copy.deepcopy() ，使用 memodict 作为第一个参数。\n这些魔法方法有什么用武之地呢？像往常一样，当你需要比默认行为更加精确的控制时。例如，如果你想拷贝一个对象，其中存储了一个字典作为缓存（可能会很大），拷贝缓存可能是没有意义的。如果这个缓存可以在内存中被不同实例共享，那么它就应该被共享。\n11.13. Pickling 如果你和其他的Python爱好者共事过，很可能你已经听说过Pickling了。Pickling是Python数据结构的序列化过程，当你想存储一个对象稍后再取出读取时，Pickling会显得十分有用。然而它同样也是担忧和混淆的主要来源。\nPickling是如此的重要，以至于它不仅仅有自己的模块（ pickle ），还有自己的协议和魔法方法。首先，我们先来简要的介绍一下如何pickle已存在的对象类型（如果你已经知道了，大可跳过这部分内容）。\n11.13.1. Pickling : 小试牛刀 我们一起来pickle吧。假设你有一个字典，你想存储它，稍后再取出来。你可以把它的内容写入一个文件，小心翼翼地确保使用了正确地格式，要把它读取出来，你可以使用 exec() 或处理文件输入。但是这种方法并不可靠：如果你使用纯文本来存储重要数据，数据很容易以多种方式被破坏或者修改，导致你的程序崩溃，更糟糕的情况下，还可能在你的计算机上运行恶意代码。因此，我们要pickle它:\nimport pickle\ndata = {\u0026lsquo;foo\u0026rsquo;: [1,2,3], \u0026lsquo;bar\u0026rsquo;: (\u0026lsquo;Hello\u0026rsquo;, \u0026lsquo;world!\u0026rsquo;), \u0026lsquo;baz\u0026rsquo;: True} jar = open(\u0026lsquo;data.pkl\u0026rsquo;, \u0026lsquo;wb\u0026rsquo;) pickle.dump(data, jar) # 将pickle后的数据写入jar文件 jar.close() 过了几个小时，我们想把它取出来，我们只需要反pickle它:\nimport pickle\npkl_file = open(\u0026lsquo;data.pkl\u0026rsquo;, \u0026lsquo;rb\u0026rsquo;) # 与pickle后的数据连接 data = pickle.load(pkl_file) # 把它加载进一个变量 print data pkl_file.close() 将会发生什么？正如你期待的，它就是我们之前的 data 。\n现在，还需要谨慎地说一句： pickle并不完美。Pickle文件很容易因为事故或被故意的破坏掉。Pickling或许比纯文本文件安全一些，但是依然有可能被用来运行恶意代码。而且它还不支持跨Python版本，所以不要指望分发pickle对象之后所有人都能正确地读取。然而不管怎么样，它依然是一个强有力的工具，可以用于缓存和其他类型的持久化工作。\n11.13.2. Pickle你的对象 Pickle不仅仅可以用于内建类型，任何遵守pickle协议的类都可以被pickle。Pickle协议有四个可选方法，可以让类自定义它们的行为（这和C语言扩展略有不同，那不在我们的讨论范围之内）。\ngetinitargs(self) 如果你想让你的类在反pickle时调用 init ，你可以定义 getinitargs(self) ，它会返回一个参数元组，这个元组会传递给 init 。注意，这个方法只能用于旧式类。\ngetnewargs(self) 对新式类来说，你可以通过这个方法改变类在反pickle时传递给 new 的参数。这个方法应该返回一个参数元组。\ngetstate(self) 你可以自定义对象被pickle时被存储的状态，而不使用对象的 dict 属性。 这个状态在对象被反pickle时会被 setstate 使用。\nsetstate(self) 当一个对象被反pickle时，如果定义了 setstate ，对象的状态会传递给这个魔法方法，而不是直接应用到对象的 dict 属性。这个魔法方法和 getstate 相互依存：当这两个方法都被定义时，你可以在Pickle时使用任何方法保存对象的任何状态。\nreduce(self) 当定义扩展类型时（也就是使用Python的C语言API实现的类型），如果你想pickle它们，你必须告诉Python如何pickle它们。 reduce 被定义之后，当对象被Pickle时就会被调用。它要么返回一个代表全局名称的字符串，Pyhton会查找它并pickle，要么返回一个元组。这个元组包含2到5个元素，其中包括：一个可调用的对象，用于重建对象时调用；一个参数元素，供那个可调用对象使用；被传递给 setstate 的状态（可选）；一个产生被pickle的列表元素的迭代器（可选）；一个产生被pickle的字典元素的迭代器（可选）；\nreduce_ex(self) reduce_ex 的存在是为了兼容性。如果它被定义，在pickle时 reduce_ex 会代替 reduce 被调用。 reduce 也可以被定义，用于不支持 reduce_ex 的旧版pickle的API调用。\n11.13.3. 一个例子 我们的例子是 Slate ，它会记住它的值曾经是什么，以及那些值是什么时候赋给它的。然而 每次被pickle时它都会变成空白，因为当前的值不会被存储:\nimport time\nclass Slate: \u0026lsquo;\u0026lsquo;\u0026lsquo;存储一个字符串和一个变更日志的类 每次被pickle都会忘记它当前的值\u0026rsquo;\u0026rsquo;\u0026rsquo;\n def __init__(self, value): self.value = value self.last_change = time.asctime() self.history = {} def change(self, new_value): # 改变当前值，将上一个值记录到历史 self.history[self.last_change] = self.value self.value = new_value) self.last_change = time.asctime() def print_change(self): print 'Changelog for Slate object:' for k,v in self.history.items(): print '%s\\t %s' % (k,v) def __getstate__(self): # 故意不返回self.value或self.last_change # 我们想在反pickle时得到一个空白的slate return self.history def __setstate__(self): # 使self.history = slate，last_change # 和value为未定义 self.history = state self.value, self.last_change = None, None  11.14. 总结 这本指南的目标是使所有阅读它的人都能有所收获，无论他们有没有使用Python或者进行面向对象编程的经验。如果你刚刚开始学习Python，你会得到宝贵的基础知识，了解如何写出具有丰富特性的，优雅而且易用的类。如果你是中级的Python程序员，你或许能掌握一些新的概念和技巧，以及一些可以减少代码行数的好办法。如果你是专家级别的Python爱好者，你又重新复习了一遍某些可能已经忘掉的知识，也可能顺便了解了一些新技巧。无论你的水平怎样，我希望这趟遨游Python特殊方法的旅行，真的对你产生了魔法般的效果（实在忍不住不说最后这个双关）。\n11.15. 附录1：如何调用魔法方法 一些魔法方法直接和内建函数对应，这种情况下，如何调用它们是显而易见的。然而，另外的情况下，调用魔法方法的途径并不是那么明显。这个附录旨在展示那些不那么明显的调用魔法方法的语法。\n魔法方法\t什么时候被调用\t解释 new(cls [,\u0026hellip;])\tinstance = MyClass(arg1, arg2)\t__new__在实例创建时调用 init(self [,\u0026hellip;])\tinstance = MyClass(arg1,arg2)\t__init__在实例创建时调用 cmp(self)\tself == other, self \u0026gt; other 等\t进行比较时调用 pos(self)\t+self\t一元加法符号 neg(self)\t-self\t一元减法符号 invert(self)\t~self\t按位取反 index(self)\tx[self]\t当对象用于索引时 nonzero(self)\tbool(self)\t对象的布尔值 getattr(self, name)\tself.name #name不存在\t访问不存在的属性 setattr(self, name)\tself.name = val\t给属性赋值 _delattr(self, name)\tdel self.name\t删除属性 getattribute(self,name)\tself.name\t访问任意属性 getitem(self, key)\tself[key]\t使用索引访问某个元素 setitem(self, key)\tself[key] = val\t使用索引给某个元素赋值 delitem(self, key)\tdel self[key]\t使用索引删除某个对象 iter(self)\tfor x in self\t迭代 contains(self, value)\tvalue in self, value not in self\t使用in进行成员测试 call(self [,\u0026hellip;])\tself(args)\t“调用”一个实例 enter(self)\twith self as x:\twith声明的上下文管理器 exit(self, exc, val, trace)\twith self as x:\twith声明的上下文管理器 getstate(self)\tpickle.dump(pkl_file, self)\tPickling setstate(self)\tdata = pickle.load(pkl_file)\tPickling 11.16. 附录2：Python 3中的变化 在这里，我们记录了几个在对象模型方面 Python 3 和 Python 2.x 之间的主要区别。\nPython 3中string和unicode的区别不复存在，因此 unicode 被取消了， bytes 加入进来（与Python 2.7 中的 str 和 unicode 行为类似），用于新的创建字节数组的内建方法。 Python 3中默认除法变成了 true 除法，因此 div 被取消了。 coerce 被取消了，因为和其他魔法方法有功能上的重复，以及本身行为令人迷惑。 cmp 被取消了，因为和其他魔法方法有功能上的重复。 nonzero 被重命名成 bool 。\n\n","date":"2022-10-25","permalink":"/post/magic_mothods/","tags":["Python"],"title":"magic_mothods"},{"content":"","date":"2022-10-25","permalink":"/post/model_functools/","tags":["Python"],"title":"model_functools"},{"content":"基本使用 1、读取文本解析节点 from lxml import etree text=''' \u0026lt;div\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li class=\u0026quot;item-0\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link1.html\u0026quot;\u0026gt;第一个\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;item-1\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link2.html\u0026quot;\u0026gt;second item\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;item-0\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link5.html\u0026quot;\u0026gt;a属性\u0026lt;/a\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; ''' html=etree.HTML(text) # 初始化生成一个XPath解析对象 result=etree.tostring(html,encoding='utf-8') # 解析对象输出代码 data = html.xpath('//li[@class=\u0026quot;item-0\u0026quot;]') # 根据xpaath查找节点 print(data) print(etree.tostring(data[0])) # 将找到的对象转为字符串  2、解析 使用 from lxml import etree text=''' \u0026lt;div\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li class=\u0026quot;item-0\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link1.html\u0026quot;\u0026gt;第一个\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;item-1\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link2.html\u0026quot;\u0026gt;second item\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;item-0\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link5.html\u0026quot;\u0026gt;a属性\u0026lt;/a\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; ''' html=etree.HTML(text) # 初始化生成一个XPath解析对象 data = html.xpath('//li[@class=\u0026quot;item-0\u0026quot;]') # 根据xpaath查找节点，熟练使用xpath，可提取，子节点、子孙节点、兄弟节点、父节点等 print(data)  属性多值匹配 如果某个属性的值有多个时，我们可以使用contains()函数来获取\nfrom lxml import etree text1=''' \u0026lt;div\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li class=\u0026quot;aaa item-0\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link1.html\u0026quot;\u0026gt;第一个\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;bbb item-1\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link2.html\u0026quot;\u0026gt;second item\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[@class=\u0026quot;aaa\u0026quot;]/a/text()') result1=html.xpath('//li[contains(@class,\u0026quot;aaa\u0026quot;)]/a/text()') print(result) print(result1) 通过第一种方法没有取到值，通过contains（）就能精确匹配到节点了  多属性匹配 另外我们还可能遇到一种情况，那就是根据多个属性确定一个节点，这时就需要同时匹配多个属性，此时可用运用and运算符来连接使用：\nfrom lxml import etree text1=''' \u0026lt;div\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li class=\u0026quot;aaa\u0026quot; name=\u0026quot;item\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link1.html\u0026quot;\u0026gt;第一个\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;aaa\u0026quot; name=\u0026quot;fore\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;link2.html\u0026quot;\u0026gt;second item\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[@class=\u0026quot;aaa\u0026quot; and @name=\u0026quot;fore\u0026quot;]/a/text()') result1=html.xpath('//li[contains(@class,\u0026quot;aaa\u0026quot;) and @name=\u0026quot;fore\u0026quot;]/a/text()') print(result) print(result1)  ","date":"2022-10-25","permalink":"/post/model_lxml/","tags":["Python"],"title":"model_lxml"},{"content":"Pillow 调整大小 按宽度缩放 这是一个使用 Pillow 模块来调整图片大小的基本脚本：\nfrom PIL import Image basewidth = 300 img = Image.open('fullsized_image.jpg') wpercent = (basewidth / float(img.size[0])) hsize = int((float(img.size[1]) * float(wpercent))) img = img.resize((basewidth, hsize), Image.ANTIALIAS) img.save('resized_image.jpg')  这几行 Python 代码使用 Pillow 将一张图片 （fullsized_image.jpg） 调整为 300 像素的宽度，宽度在变量 basewidth 中设置，高度则与新的宽度成比例。比例高度的计算方法是：确定 300 像素占原宽度 （img.size[0]） 的百分比，然后将原高度（img.size[1]） 乘以该百分比。所得的高度值保存在变量 hsize 中。\n如果你需要不同的图片宽度，你可以将 basewidth 改为任何其他数字。另外，请注意，因为我想保留全尺寸的图片 （fullsized_image.jpg），因此我将调整后的图片以一个不同的名称 resized_image.jpg 保存。当然，你不必这么做。如果你想，你可以使用相同的文件名将调整后的图片覆盖全尺寸的图片。\n按高度缩放 如果高度是固定的，而宽度是按比例变化的，那也基本差不多，你只需要把东西换一下：\nfrom PIL import Image baseheight = 560 img = Image.open('fullsized_image.jpg') hpercent = (baseheight / float(img.size[1])) wsize = int((float(img.size[0]) * float(hpercent))) img = img.resize((wsize, baseheight), Image.ANTIALIAS) img.save('resized_image.jpg')  注意 basewidth 现在换成了 baseheight，因为高度是固定的。在第三行中，我们在计算高度百分比，所以我们需要 img.size[1] 而不是 img.size[0]。size 属性是一个元组，包含宽度和高度，单位是像素，size[0] 指的是第一个元组元素，也就是宽度，size[1] 是第二个元素，也就是高度。第 4 行也有这样的切换，size[0] 代表宽度，size[1] 代表高度。\n灰度化 from PIL import Image Image.open(imgfile).convert(\u0026lsquo;L\u0026rsquo;)\n二值化 def get_bin_table(threshold=115): # threshold 为设定的阀值，根据实际情况调节 ''' 获取灰度转二值的映射table 0表示黑色,1表示白色 ''' table = [] for i in range(256): if i \u0026lt; threshold: table.append(0) else: table.append(1) return table def main(): image = Image.open('RandomPicture.png') imgry = image.convert('L') table = get_bin_table() binary = imgry.point(table, '1') binary.save('binary.png')  去噪 ","date":"2022-10-25","permalink":"/post/model_pillow/","tags":["Python"],"title":"model_pillow"},{"content":"scrapy 简介 下图显示了Scrapy架构及其组件的概述，以及系统内部发生的数据流的概要（以红色箭头显示）。下面提供了这些组件的简要说明以及有关它们的详细信息的链接。数据流也描述如下。 Scrapy中的数据流由执行引擎控制，如下所示：\n这个 Engine 获取要从 Spider .\n这个 Engine 在中安排请求 Scheduler 并请求下一个要爬行的请求。\n这个 Scheduler 将下一个请求返回到 Engine .\n这个 Engine 将请求发送到 Downloader ，通过 Downloader Middlewares （见 process_request() ）\n一旦页面完成下载， Downloader 生成响应（使用该页）并将其发送到引擎，并通过 Downloader Middlewares （见 process_response() ）\n这个 Engine 接收来自的响应 Downloader 并发送到 Spider 用于处理，通过 Spider Middleware （见 process_spider_input() ）\n这个 Spider 处理响应并向 Engine ，通过 Spider Middleware （见 process_spider_output() ）\n这个 Engine 将已处理的项目发送到 Item Pipelines ，然后将已处理的请求发送到 Scheduler 并请求可能的下一个爬行请求。\n该过程重复（从步骤1开始），直到不再有来自 Scheduler .\n各个组件介绍 爬虫引擎(ENGINE) 爬虫引擎负责控制各个组件之间的数据流，当某些操作触发事件后都是通过engine来处理。\n调度器(SCHEDULER) 调度接收来engine的请求并将请求放入队列中，并通过事件返回给engine。\n下载器(DOWNLOADER) 通过engine请求下载网络数据并将结果响应给engine。\nSpider Spider发出请求，并处理engine返回给它下载器响应数据，以items和规则内的数据请求(urls)返回给engine。 spider是Scrapy用户编写的自定义类，用于解析响应和提取 items 从他们或其他要求跟随。\n管道项目(item pipeline) 负责处理engine返回spider解析后的数据，并且将数据持久化，例如将数据存入数据库或者文件。 项目管道负责处理被蜘蛛提取（或 爬取 ）的项目。典型的任务包括清理、验证和持久性（如将项目存储在数据库中）\n下载中间件 下载器中间件是位于引擎和下载器之间的特定钩子，当它们从引擎传递到下载器时处理请求，以及从下载器传递到引擎的响应。\n如果需要执行以下操作之一，请使用下载器中间件：\n在将请求发送给下载者之前处理该请求（即在Scrapy将请求发送到网站之前）；\n变更在传递给spider之前收到响应；\n发送新的请求，而不是将收到的响应传递给spider；\n在不获取网页的情况下将响应传递给蜘蛛；\n悄悄地放弃一些请求。\nspider 中间件 蜘蛛中间件是位于引擎和蜘蛛之间的特定钩子，能够处理蜘蛛的输入（响应）和输出（项目和请求）。\n如果需要，使用蜘蛛中间件\nspider回调的后处理输出-更改/添加/删除请求或项；\n后处理启动请求；\n处理spider异常；\n对一些基于响应内容的请求调用errback，而不是回调。\n事件驱动的网络¶ Scrapy是用 Twisted 是一个流行的事件驱动的python网络框架。因此，它使用非阻塞（即异步）代码实现并发性。\n简单使用(scrapy.Spider¶) scrapy startproject test1 # 创建一个项目 cd test1 # 切换到项目目录下 scrapy genspider shSpider hshfy.sh.cn # 创建一个爬虫，并指定项目的域名  items.py 负责数据模型的建立，类似于实体类。 middlewares.py 自己定义的中间件。 pipelines.py 负责对spider返回数据的处理。 settings.py 负责对整个爬虫的配置。 spiders目录 负责存放继承自scrapy的爬虫类。 scrapy.cfg scrapy基础配置\n例子：\nimport scrapy class MySpider(scrapy.Spider): name = 'example.com' allowed_domains = ['example.com'] start_urls = [ 'http://www.example.com/1.html', 'http://www.example.com/2.html', 'http://www.example.com/3.html', ] def parse(self, response): for h3 in response.xpath('//h3').getall(): yield {\u0026quot;title\u0026quot;: h3} for href in response.xpath('//a/@href').getall(): yield scrapy.Request(response.urljoin(href), self.parse)  start_urls 支持多个链接，scrapy 会自动读取然后利用自己定义的 start_requests() 方法发起请求然后再返回给 parse() 函数。 如果需要，可以在请求之前做一些处理，如对url的一些处理等等，可以重写父类的start_requests()方法 如下：\nimport scrapy from myproject.items import MyItem class MySpider(scrapy.Spider): name = 'example.com' allowed_domains = ['example.com'] def start_requests(self): yield scrapy.Request('http://www.example.com/1.html', self.parse) yield scrapy.Request('http://www.example.com/2.html', self.parse) yield scrapy.Request('http://www.example.com/3.html', self.parse) def parse(self, response): for h3 in response.xpath('//h3').getall(): yield MyItem(title=h3) for href in response.xpath('//a/@href').getall(): yield scrapy.Request(response.urljoin(href), self.parse)  CrawlSpider scrapy genspider -t crawl shSpider hshfy.sh.cn # 创建一个模板爬虫，并指定项目的域名  import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor class MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( # Extract links matching 'category.php' (but not matching 'subsection.php') # and follow links from them (since no callback means follow=True by default). Rule(LinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))), # Extract links matching 'item.php' and parse them with the spider's method parse_item Rule(LinkExtractor(allow=('item\\.php', )), callback='parse_item'), ) def parse_item(self, response): self.logger.info('Hi, this is an item page! %s', response.url) item = scrapy.Item() item['id'] = response.xpath('//td[@id=\u0026quot;item_id\u0026quot;]/text()').re(r'ID: (\\d+)') item['name'] = response.xpath('//td[@id=\u0026quot;item_name\u0026quot;]/text()').get() item['description'] = response.xpath('//td[@id=\u0026quot;item_description\u0026quot;]/text()').get() item['link_text'] = response.meta['link_text'] url = response.xpath('//td[@id=\u0026quot;additional_data\u0026quot;]/@href').get() return response.follow(url, self.parse_additional_page, cb_kwargs=dict(item=item)) def parse_additional_page(self, response, item): item['additional_data'] = response.xpath('//p[@id=\u0026quot;additional_data\u0026quot;]/text()').get() return item  rules 中的 Rule() 可接收的参数解析\nlink_extractor 是一种 Link Extractor 对象，该对象定义如何从每个爬网页面提取链接。每个生成的链接将用于生成 Request 对象，该对象将在其 meta 词典(在 link_text 密钥)。如果省略，将使用不带参数创建的默认链接提取器，从而提取所有链接。 callback 是要为使用指定链接提取器提取的每个链接调用的可调用或字符串(在这种情况下，将使用来自具有该名称的爬行器对象的方法)。此回调接收一个 Response 作为其第一个参数，并且必须返回单个实例或 item objects 和/或 Request 对象(或其任何子类)。如上所述，收到的 Response 对象将包含生成 Request 在ITS中 meta 词典(在 link_text 密钥) cb_kwargs 是包含要传递给回调函数的关键字参数的dict。 follow 是一个布尔值，用于指定是否从使用此规则提取的每个响应中遵循链接。如果 callback 没有 follow 默认为 True ，否则默认为 False . process_links 是一个可调用的，或一个字符串（在这种情况下，将使用具有该名称的蜘蛛对象中的方法），对于使用指定的 link_extractor . 这主要用于过滤目的。 process_request 是一个可调用的(或字符串，在这种情况下，将使用来自具有该名称的爬行器对象的方法)，它将在 Request 按此规则提取。此可调用对象应将上述请求作为第一个参数，并且 Response 作为第二个参数从其发出请求。它必须返回一个 Request 对象或 None (用过滤发出请求)。 errback 在处理规则生成的请求时引发任何异常时要调用的可调用或字符串（在这种情况下，将使用来自spider对象的具有该名称的方法）。它收到一个 Twisted Failure 实例作为第一个参数。  Scrapy框架的命令行详解 全局的命令有： startproject genspider settings runspider shell fetch view version\n项目命令有： crawl check list edit parse bench\nstartproject 这个命令没什么过多的用法，就是在创建爬虫项目的时候用\ngenspider 用于生成爬虫，这里scrapy提供给我们不同的几种模板生成 spider,默认用的是 basic,我们可以通过命令查看所有的模板\n当我们创建的时候可以指定模板，不指定默认用的basic,如果想要指定模板则通过 scrapy genspider -t 模板名字\ncrawl 这个是用去启动spider爬虫格式为： scrapy crawl 爬虫名字 这里需要注意这里的爬虫名字和通过scrapy genspider 生成爬虫的名字是一致的\ncheck 用于检查代码是否有错误，scrapy check\nlist scrapy list列出所有可用的爬虫\nfetch scrapy fetch url地址 该命令会通过scrapy downloader 讲网页的源代码下载下来并显示出来\n这里有一些参数： \u0026ndash;nolog 不打印日志 \u0026ndash;headers 打印响应头信息 \u0026ndash;no-redirect 不做跳转\nview scrapy view url地址 该命令会讲网页document内容下载下来，并且在浏览器显示出来 因为现在很多网站的数据都是通过ajax请求来加载的，这个时候直接通过requests请求是无法获取我们想要的数据，所以这个view命令可以帮助我们很好的判断\nshell 这是一个命令行交互模式 通过scrapy shell url地址进入交互模式 这里我么可以通过css选择器以及xpath选择器获取我们想要的内容（xpath以及css选择的用法会在下个文章中详细说明）,例如我们通过scrapy shell http://www.baidu.com\nsettings 获取当前的配置信息 通过scrapy settings -h可以获取这个命令的所有帮助信息\nrunspider 这个和通过crawl启动爬虫不同，这里是scrapy runspider 爬虫文件名称 所有的爬虫文件都是在项目目录下的spiders文件夹中\nversion 查看版本信息，并查看依赖库的信息\n下载中间件 请求前 useragent 的配置，代理的配置 或者对响应的初步过滤等等 发生异常的处理等等\nprocess_request(request,spider) 当每个request通过下载中间件时，该方法被调用，这里有一个要求，该方法必须返回以下三种中的任意一种：None,返回一个Response对象，返回一个Request对象或raise IgnoreRequest。三种返回值的作用是不同的。\nNone:Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用,该request被执行(其response被下载)。\nResponse对象：Scrapy将不会调用任何其他的process_request()或process_exception() 方法，或相应地下载函数；其将返回该response。 已安装的中间件的 process_response() 方法则会在每个response返回时被调用。\nRequest对象：Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。\nraise一个IgnoreRequest异常：则安装的下载中间件的 process_exception() 方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录。\nprocess_response(request, response, spider) process_response的返回值也是有三种：response对象，request对象，或者raise一个IgnoreRequest异常\n如果其返回一个Response(可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理。\n如果其返回一个 Request 对象，则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样。\n如果其抛出一个 IgnoreRequest 异常，则调用request的errback(Request.errback)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。\nprocess_exception(request, exception, spider) 当下载处理器(download handler)或 process_request() (下载中间件)抛出异常(包括 IgnoreRequest 异常)时，Scrapy调用 process_exception()。\nprocess_exception() 也是返回三者中的一个: 返回 None 、 一个 Response 对象、或者一个 Request 对象。\n如果其返回 None ，Scrapy将会继续处理该异常，接着调用已安装的其他中间件的 process_exception() 方法，直到所有中间件都被调用完毕，则调用默认的异常处理。\n如果其返回一个 Response 对象，则已安装的中间件链的 process_response() 方法被调用。Scrapy将不会调用任何其他中间件的 process_exception() 方法。\n如果其返回一个 Request 对象， 则返回的request将会被重新调用下载。这将停止中间件的 process_exception() 方法执行，就如返回一个response的那样。 这个是非常有用的，就相当于如果我们失败了可以在这里进行一次失败的重试，\n实现中间件的流程 在 middlewares.py 创建中间件类 实现所需要拦截的函数 在 settings.py 中配置开启中间件 在配置中数字越小越优先执行 下载中间件 from_crawler 类方法,当创建爬虫时回调，仅调用一次 spider_opened 爬虫打开时回调，仅调用一次 process_request\n引擎 -\u0026gt; 下载中间件 -\u0026gt; 下载器 :param request: 请求对象 :param spider: 请求来自的爬虫 :return:\nreturn None 继续处理这个请求 return Response 直接把响应提交给引擎 -\u0026gt; 爬虫 return Request 直接返回引擎 raise IgnoreRequest 触发 process_exception 回调函数 process_response 下载器 -\u0026gt; 下载中间件 -\u0026gt; 引擎 :param request: :param response: :param spider: :return: raise IgnoreRequest 把这请求忽略 process_exception\n当下载中间件异常异常时回调 :param request: :param exception: :param spider: :return:\nreturn None 继续处理异常，向下一个中间件传递异常\nreturn a Response 停止异常链，把响应返回给引擎 return a Request 停止异常链，把请求返回给引擎\n爬虫中间件 from_crawler 类方法,当创建爬虫时回调，仅调用一次 spider_opened 爬虫打开时回调，仅调用一次 process_spider_input 引擎 -\u0026gt; 爬虫中间件 -\u0026gt; 爬虫 参数 response 响应对象 spider 爬虫对象 process_spider_output 当爬虫提交数据或者请求给引擎时触发 process_spider_exception 当 process_spider_input 异常异常时触发 process_start_requests 当引擎向爬虫所要 start_requests 时触发\n管道 item pipeline的主要作用：\n清理html数据 验证爬取的数据 去重并丢弃 讲爬取的结果保存到数据库中或文件中\nprocess_item(self,item,spider)\n每个item piple组件是一个独立的pyhton类，必须实现以process_item(self,item,spider)方法 每个item pipeline组件都需要调用该方法，这个方法必须返回一个具有数据的dict,或者item对象，或者抛出DropItem异常，被丢弃的item将不会被之后的pipeline组件所处理\n下面的方法也可以选择实现 open_spider(self,spider) 表示当spider被开启的时候调用这个方法\nclose_spider(self,spider) 当spider挂去年比时候这个方法被调用\nfrom_crawler(cls,crawler) 这个和我们在前面说spider的时候的用法是一样的，可以用于获取settings配置文件中的信息，需要注意的这个是一个类方法\nscrapy 处理Cookies 一、直接通过已登录实现 Cookies 传递 在请求中携带Cookies\nclass LoginSpider(scrapy.Spider): name = \u0026rsquo;login' allowed_domains = [\u0026lsquo;github.com\u0026rsquo;] def start_requests(self): # cookies_string = \u0026ldquo;_ga=GA1.2.1855430798.1461857641; _octo=GH1.1.783519559.1525492869; tz=Asia%2FShanghai; has_recent_activity=1; gat=1; user_session=6kLvUi9EfjMAdsxTocq1QtHfoyWUFZqs6SIbukKGGj8huCQ; _Host-user_session_same_site=6kLvUi9EfjMAdsxTocq1QtHfoyWUFZqs6SIbukKGGj8huCQ; logged_in=yes; dotcom_user=czwspider; _gh_sess=cDhXV0JnMDBDR0lDajc2V05iY1hHUCt0OW81RDIveVhXR1dOWEgwRjhuNE00S3BHK3NVTzhzR3J3b2lZWXJ1VDhFK2o0VUFremtwek4xZXRMdHlzMUVWTkU1Yk1oYlFOL2JTUXVoWTBXM2dNUFc3VzV1TFg3alNvbEo5ZUxMOEJuUmNiNVoyTVZpbk1lT2tUNVJjTjF6d0xpWjlqNmg3Z2VDbFhRcHIzdGlKdjhXVkx6WGZEcnptTUxhaThZY2xmODBFUllVVnhWRytndjdmaDFKeW52QT09LS1uenNWZjJ5ZUh5bkJqbmk2a0huNlh3PT0%3D\u0026ndash;b74a83a0de2e3caf8142b812428fd805897b5832\u0026rdquo; # cookies = dict([cookie_str.split(\u0026rsquo;=\u0026rsquo;) for cookie_str in cookies_string.split(\u0026rsquo;; \u0026lsquo;)]) # 在发送请求时携带cookies yield scrapy.Request( url=\u0026ldquo;https://github.com/settings/profile\", # cookies=cookies )\ndef parse(self, response): with open(\u0026rsquo;login1.html\u0026rsquo;,\u0026lsquo;w\u0026rsquo;,encoding=\u0026lsquo;utf-8\u0026rsquo;) as f: f.write(response.text) pass 通过中间件设置 Cookies from scrapy import signals import requests class CookiesDownloadMiddleware(object): def process_request(self, request, spider): # cookies_string = requests.get(\u0026ldquo;url\u0026rdquo;) cookies_string = \u0026ldquo;_ga=GA1.2.1855430798.1461857641; _octo=GH1.1.783519559.1525492869; tz=Asia%2FShanghai; has_recent_activity=1; gat=1; user_session=6kLvUi9EfjMAdsxTocq1QtHfoyWUFZqs6SIbukKGGj8huCQ; _Host-user_session_same_site=6kLvUi9EfjMAdsxTocq1QtHfoyWUFZqs6SIbukKGGj8huCQ; logged_in=yes; dotcom_user=czwspider; _gh_sess=cDhXV0JnMDBDR0lDajc2V05iY1hHUCt0OW81RDIveVhXR1dOWEgwRjhuNE00S3BHK3NVTzhzR3J3b2lZWXJ1VDhFK2o0VUFremtwek4xZXRMdHlzMUVWTkU1Yk1oYlFOL2JTUXVoWTBXM2dNUFc3VzV1TFg3alNvbEo5ZUxMOEJuUmNiNVoyTVZpbk1lT2tUNVJjTjF6d0xpWjlqNmg3Z2VDbFhRcHIzdGlKdjhXVkx6WGZEcnptTUxhaThZY2xmODBFUllVVnhWRytndjdmaDFKeW52QT09LS1uenNWZjJ5ZUh5bkJqbmk2a0huNlh3PT0%3D\u0026ndash;b74a83a0de2e3caf8142b812428fd805897b5832\u0026rdquo; cookies = dict([cookie_str.split(\u0026rsquo;=\u0026rsquo;) for cookie_str in cookies_string.split(\u0026rsquo;; \u0026lsquo;)]) request.cookies = cookies return None 二、模拟登录爬取 通过 scrapy.FormRequest 登录爬取数据\n-- coding: utf-8 -- import scrapy class Login1Spider(scrapy.Spider): name = \u0026rsquo;login2\u0026rsquo; allowed_domains = [\u0026lsquo;github.com\u0026rsquo;] start_urls = [\u0026lsquo;https://github.com/login\u0026rsquo;] def parse(self, response): data = { \u0026ldquo;login\u0026rdquo;:\u0026ldquo;xxx\u0026rdquo;, \u0026ldquo;password\u0026rdquo;:\u0026ldquo;qqq\u0026rdquo;, \u0026ldquo;commit\u0026rdquo;:response.xpath(\u0026rsquo;//input[@name=\u0026ldquo;commit\u0026rdquo;]/@value\u0026rsquo;).extract_first(), \u0026ldquo;utf8\u0026rdquo;:response.xpath(\u0026rsquo;//input[@name=\u0026ldquo;utf8\u0026rdquo;]/@value\u0026rsquo;).extract_first(), \u0026ldquo;authenticity_token\u0026rdquo;:response.xpath(\u0026rsquo;//input[@name=\u0026ldquo;authenticity_token\u0026rdquo;]/@value\u0026rsquo;).extract_first() } # 发送post请求进行登录 yield scrapy.FormRequest( url=\u0026ldquo;https://github.com/session\", formdata=data, callback=self.parse_login_success )\ndef parse_login_success(self,response): yield scrapy.Request( url=\u0026ldquo;https://github.com/settings/profile\", callback=self.parse_success ) pass\ndef parse_success(self, response): with open(\u0026rsquo;login2.html\u0026rsquo;,\u0026lsquo;w\u0026rsquo;,encoding=\u0026lsquo;utf-8\u0026rsquo;) as f: f.write(response.text) pass 通过scrapy.FormRequest.from_response 构建请求\n-- coding: utf-8 -- import scrapy class Login1Spider(scrapy.Spider): name = \u0026rsquo;login3\u0026rsquo; allowed_domains = [\u0026lsquo;github.com\u0026rsquo;] start_urls = [\u0026lsquo;https://github.com/login\u0026rsquo;] def parse(self, response): data = { \u0026ldquo;login\u0026rdquo;:\u0026ldquo;xxx\u0026rdquo;, \u0026ldquo;password\u0026rdquo;:\u0026ldquo;qqq\u0026rdquo; }\n ''' 用户定义请求参数和页面中隐藏表单字段合并 ''' yield scrapy.FormRequest.from_response( url=\u0026quot;https://github.com/session\u0026quot;, response=response, formxpath='//form[@action=\u0026quot;/session\u0026quot;]', callback = self.parse_login_success, formdata=data )  def parse_login_success(self,response): yield scrapy.Request( url=\u0026ldquo;https://github.com/settings/profile\", callback=self.parse_success ) pass\ndef parse_success(self, response): with open(\u0026rsquo;login3.html\u0026rsquo;,\u0026lsquo;w\u0026rsquo;,encoding=\u0026lsquo;utf-8\u0026rsquo;) as f: f.write(response.text) pass\n","date":"2022-10-25","permalink":"/post/model_scrapy/","tags":["Python"],"title":"model_scrapy"},{"content":"支持多种类型浏览器，以chrome为例\n基本使用\nfrom selenium import webdriver driver = webdriver.Chrome() driver.get('http://www.baidu.com') print(driver.page_source) driver.close() # 关闭当前窗口 driver.quit() # 关闭浏览器  查找元素\n# 单个元素,有多个结果会取第一个 from selenium import webdriver browser = webdriver.Chrome() browser.get('https://www.taobao.com') input_first = browser.find_element_by_id('q') input_second = browser.find_element_by_css_selector('#q') input_third = browser.find_element_by_xpath('//*[@id=\u0026quot;q\u0026quot;]') print(input_first,input_second,input_third) browser.close() # 常用的查找方法 find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector # 也可以使用通用的方法 from selenium import webdriver from selenium.webdriver.common.by import By browser = webdriver.Chrome() browser.get('https://www.taobao.com') input_first = browser.find_element(BY.ID,'q')#第一个参数传入名称，第二个传入具体的参数 print(input_first) browser.close() # 多个元素，elements多个s input_first = browser.find_elements_by_id('q')  四、元素交互操作-搜索框传入关键词进行自动搜索\nfrom selenium import webdriver import time browser = webdriver.Chrome() browser.get('https://www.taobao.com') input = browser.find_element_by_id('q')#找到搜索框 input.send_keys('iPhone')#传送入关键词 time.sleep(5) input.clear()#清空搜索框 input.send_keys('男士内裤') button = browser.find_element_by_class_name('btn-search')#找到搜索按钮 button.click()  五、交互动作，驱动浏览器进行动作，模拟拖拽动作，将动作附加到动作链中串行执行 from selenium import webdriver from selenium.webdriver import ActionChains#引入动作链\nbrowser = webdriver.Chrome() url = \u0026lsquo;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable' browser.get(url) browser.switch_to.frame(\u0026lsquo;iframeResult\u0026rsquo;)#切换到iframeResult框架 source = browser.find_element_by_css_selector(\u0026rsquo;#draggable\u0026rsquo;)#找到被拖拽对象 target = browser.find_element_by_css_selector(\u0026rsquo;#droppable\u0026rsquo;)#找到目标 actions = ActionChains(browser)#声明actions对象 actions.drag_and_drop(source, target) actions.perform()#执行动作\n更多操作: http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains Python 复制 详细文档\n执行JavaScript 有些动作可能没有提供api，比如进度条下拉，这时，我们可以通过代码执行JavaScript\nfrom selenium import webdriver browser = webdriver.Chrome() browser.get('https://www.zhihu.com/explore') browser.execute_script('window.scrollTo(0, document.body.scrollHeight)') browser.execute_script('alert(\u0026quot;To Bottom\u0026quot;)')  获取元素信息 获取属性\nfrom selenium import webdriver from selenium.webdriver import ActionChains browser = webdriver.Chrome() url = 'https://www.zhihu.com/explore' browser.get(url) logo = browser.find_element_by_id('zh-top-link-logo')#获取网站logo print(logo) print(logo.get_attribute('class')) browser.close() Python 复制 获取文本值 from selenium import webdriver browser = webdriver.Chrome() url = 'https://www.zhihu.com/explore' browser.get(url) input = browser.find_element_by_class_name('zu-top-add-question') print(input.text)#input.text文本值 browser.close()  获取id，位置，标签名，大小\nfrom selenium import webdriver browser = webdriver.Chrome() url = 'https://www.zhihu.com/explore' browser.get(url) input = browser.find_element_by_class_name('zu-top-add-question') print(input.id)#获取id print(input.location)#获取位置 print(input.tag_name)#获取标签名 print(input.size)#获取大小 browser.close()  Frame操作 frame相当于独立的网页，如果在父类网frame查找子类的，则必须切换到子类的frame，子类如果查找父类也需要先切换\nfrom selenium import webdriver from selenium.common.exceptions import NoSuchElementException browser = webdriver.Chrome() url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable' browser.get(url) browser.switch_to.frame('iframeResult') source = browser.find_element_by_css_selector('#draggable') print(source) try: logo = browser.find_element_by_class_name('logo') except NoSuchElementException: print('NO LOGO') browser.switch_to.parent_frame() logo = browser.find_element_by_class_name('logo') print(logo) print(logo.text)  等待 隐式等待 当使用了隐式等待执行测试的时候，如果 WebDriver没有在 DOM中找到元素，将继续等待，超出设定时间后则抛出找不到元素的异常, 换句话说，当查找元素或元素并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是0\nfrom selenium import webdriver browser = webdriver.Chrome() browser.implicitly_wait(10)#等待十秒加载不出来就会抛出异常，10秒内加载出来正常返回 browser.get('https://www.zhihu.com/explore') input = browser.find_element_by_class_name('zu-top-add-question') print(input)  显式等待 指定一个等待条件，和一个最长等待时间，程序会判断在等待时间内条件是否满足，如果满足则返回，如果不满足会继续等待，超过时间就会抛出异常\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC browser = webdriver.Chrome() browser.get('https://www.taobao.com/') wait = WebDriverWait(browser, 10) input = wait.until(EC.presence_of_element_located((By.ID, 'q'))) button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search'))) print(input, button)  expected_conditions title_is 标题是某内容 title_contains 标题包含某内容 presence_of_element_located 元素加载出，传入定位元组，如(By.ID, 'p') visibility_of_element_located 元素可见，传入定位元组 visibility_of 可见，传入元素对象 presence_of_all_elements_located 所有元素加载出 text_to_be_present_in_element 某个元素文本包含某文字 text_to_be_present_in_element_value 某个元素值包含某文字 frame_to_be_available_and_switch_to_it frame加载并切换 invisibility_of_element_located 元素不可见 element_to_be_clickable 元素可点击 staleness_of 判断一个元素是否仍在DOM，可判断页面是否已经刷新 element_to_be_selected 元素可选择，传元素对象 element_located_to_be_selected 元素可选择，传入定位元组 element_selection_state_to_be 传入元素对象以及状态，相等返回True，否则返回False element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False alert_is_present 是否出现Alert  前进后退-实现浏览器的前进后退以浏览不同的网页\nimport time from selenium import webdriver browser = webdriver.Chrome() browser.get('https://www.baidu.com/') browser.get('https://www.taobao.com/') browser.get('https://www.python.org/') browser.back() time.sleep(1) browser.forward() browser.close()  Cookies\nfrom selenium import webdriver browser = webdriver.Chrome() browser.get('https://www.zhihu.com/explore') print(browser.get_cookies()) browser.add_cookie({'name': 'name', 'domain': 'www.zhihu.com', 'value': 'germey'}) print(browser.get_cookies()) browser.delete_all_cookies() print(browser.get_cookies())  选项卡管理 增加浏览器窗口\nimport time from selenium import webdriver browser = webdriver.Chrome() browser.get('https://www.baidu.com') browser.execute_script('window.open()') print(browser.window_handles) browser.switch_to_window(browser.window_handles[1]) browser.get('https://www.taobao.com') time.sleep(1) browser.switch_to_window(browser.window_handles[0]) browser.get('http://www.fishc.com')  十四、异常处理\nfrom selenium import webdriver browser = webdriver.Chrome() browser.get('https://www.baidu.com') browser.find_element_by_id('hello') from selenium import webdriver from selenium.common.exceptions import TimeoutException, NoSuchElementException browser = webdriver.Chrome() try: browser.get('https://www.baidu.com') except TimeoutException: print('Time Out') try: browser.find_element_by_id('hello') except NoSuchElementException: print('No Element') finally:  ","date":"2022-10-25","permalink":"/post/model_selenium/","tags":["Python"],"title":"model_selenium"},{"content":"python lambda 递归 阶乘 f = lambda n: f(n-1) * n if n\u0026gt;=2 else 1\n序列乘积 a=[2,3,4] f = lambda n: f(n-1) * a[n] if n\u0026gt;=0 else 1 print(f(len(a)))\n求和 f = lambda n: f(n-1) * n if n\u0026gt;=2 else 0\n斐波那契数列 lambdafunc = lambda n : 1 if n == 1 or n == 2 else lambdafunc(n -1 ) + lambdafunc(n - 2)\n","date":"2022-10-25","permalink":"/post/my/","tags":["InterviewQuestions","Python"],"title":"MY"},{"content":"MySQL优化插入数据速度\n在 MySQL 中，向数据表插入数据时，索引、唯一性检查、数据大小是影响插入速度的主要因素。本节将介绍优化插入数据速度的几种方法。\n根据不同情况，可以分别进行优化。\n对于 MyISAM 引擎的表，常见的优化方法如下：\n 禁用索引 对非空表插入数据时，MySQL 会根据表的索引对插入的记录进行排序。插入大量数据时，这些排序会降低插入数据的速度。为了解决这种情况，可以在插入数据之前先禁用索引，等到数据都插入完毕后在开启索引。  禁用索引的语句为： ALTER TABLE table_name DISABLE KEYS;\n重新开启索引的语句为： ALTER TABLE table_name ENABLE KEYS;\n对于新创建的表，可以先不创建索引，等到数据都导入以后再创建索引，这样可以提高导入数据的速度。 2. 禁用唯一性检查 插入数据时，MySQL 会对插入的数据进行唯一性检查。这种唯一性检验会降低插入数据的速度。为了降低这种情况对查询速度的影响，可以在插入数据前禁用唯一性检查，等到插入数据完毕后在开启。\n禁用唯一性检查的语句为： SET UNIQUE_CHECKS=0;\n开启唯一性检查的语句为： SET UNIQUE_CHECKS=1;\n使用批量插入 在 MySQL 中，插入多条数据有 2 种方式。第一种是使用一个 INSERT 语句插入多条数据。INSERT 语句的情形如下： INSERT INTO items(name,city,price,number,picture) VALUES (\u0026lsquo;耐克运动鞋\u0026rsquo;,\u0026lsquo;广州\u0026rsquo;,500,1000,\u0026lsquo;001.jpg\u0026rsquo;),(\u0026lsquo;耐克运动鞋2\u0026rsquo;,\u0026lsquo;广州2\u0026rsquo;,500,1000,\u0026lsquo;002.jpg\u0026rsquo;);  第二种是一个 INSERT 语句只插入一条数据，执行多个 INSERT 语句来插入多条数据。INSERT 语句的情形如下： INSERT INTO items(name,city,price,number,picture) VALUES(\u0026lsquo;耐克运动鞋\u0026rsquo;,\u0026lsquo;广州\u0026rsquo;,500,1000,\u0026lsquo;001.jpg\u0026rsquo;); INSERT INTO items(name,city,price,number,picture) VALUES(\u0026lsquo;耐克运动鞋2\u0026rsquo;,\u0026lsquo;广州\u0026rsquo;,500,1000,\u0026lsquo;002.jpg\u0026rsquo;);\n一次性插入多条数据和多次插入数据所耗费的时间是不一样的。第一种方式减少了与数据库之间的连接等操作，其速度比第二种方式要快一些。所以插入大量数据时，建议使用第一种方法。\n注意：如果能用 LOAD DATA INFILE 语句，就尽量用 LOAD DATA INFILE 语句。因为 LOAD DATA INFILE 语句导入数据的速度比 INSERT 语句的速度快。\n对于 InnoDB 引擎的表，常见的优化方法如下：\n 禁用唯一性检查 同 MyISAM 引擎相同，插入数据之前先禁用索引，等到数据都插入完毕后在开启索引。 禁用外键检查 使用外键时，在子表中插入一条数据，首先会检查主表中是否有相应的主键值，然后锁定主表的记录，在插入值。相比较，使用外键多了2步操作，速度会慢一些。  所以我们可以在插入数据之前禁止对外键的检查，数据插入完成之后再恢复对外键的检查。不多对于数据完整性要求较高的系统不建议使用。\n禁用外键检查语句为： SET FOREIGN_KEY_CHECKS=0;\n恢复对外键的检查语句为： SET FOREIGN_KEY_CHECKS=1;\n禁止自动提交 在《MySQL设置事务自动提交》一节我们提到 MySQL 的事务自动提交模式默认是开启的，其对 MySQL 的性能也有一定得影响。也就是说如果你插入了 1000 条数据，MySQL 就会提交 1000 次，这大大影响了插入数据的速度。而如果我们把自动提交关掉，通过程序来控制，只要一次提交就可以了。  所以插入数据之前可以先禁止事务的自动提交，待数据导入完成之后，再恢复自动提交操作。\n禁止自动提交语句为： SET AUTOCOMMIT=0;\n恢复自动提交语句为： SET AUTOCOMMIT=1;\n索引到底对查询速度有什么影响？ 索引是数据库优化中最常用也是最重要的手段之一，通过索引可以帮助用户解决大多数的 SQL 性能问题。\n多数情况下，查询速度很慢时，加上索引便能解决问题。但也并非总是如此，因为优化不是件简单的事情。但是如果你不使用索引，在许多情况下，尝试通过其它途径来提高性能都纯粹是在浪费时间。应该首先使用索引来最大程度的改善性能，然后再看看是否还有其它有用的技术。\n索引提供了高效访问数据的方法，能够快速的定位表中的某条记录，加快数据库查询的速度，从而提高数据库的性能。\n如果查询时不使用索引，那么查询语句将查询表中的所有字段。这样查询的速度会很慢。使用索引进行查询，查询语句不必读完表中的所有记录，而只查询索引字段。这样可以减少查询的记录数，达到提高查询速度的目的。\n下面通过对比使用索引和不使用索引来分析索引对查询速度的影响。 例 1 为了便于读者更好的理解，分析之前，我们先查询一下 tb_students_info 数据表中的记录，SQL 语句和运行结果如下： mysql\u0026gt; SELECT * FROM tb_students_info; +\u0026mdash;-+\u0026mdash;\u0026mdash;+ | id | name | +\u0026mdash;-+\u0026mdash;\u0026mdash;+ | 1 | 张三 | | 2 | 李四 | | 3 | 王五 | | 4 | 赵六 | | 5 | 周七 | | 6 | 吴八 | | 7 | 朱九 | | 8 | 苏十 | +\u0026mdash;-+\u0026mdash;\u0026mdash;+ 8 rows in set (0.02 sec) 使用 EXPLAIN 分析未使用索引时的查询情况，SQL 语句和运行结果如下： mysql\u0026gt; EXPLAIN SELECT * FROM tb_students_info WHERE name=\u0026lsquo;张三\u0026rsquo; \\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_students_info partitions: NULL type: ALL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 8 filtered: 12.50 Extra: Using where 1 row in set, 1 warning (0.00 sec) 由结果可以看到，rows 列的值是 8，说明查询语句扫描了表中的 8 条记录。 没有索引的表就相当于一组无序的行，如果我们想找到某条记录就必须检查表的每一行，看看它是否与那个期望值相匹配。这是一个全表扫描操作，其效率很低，如果表很大，而且仅有少数几条记录与搜索条件相匹配，那么整个扫描过程的效率将会超级低。\n在 tb_students_info 表的 name 字段添加索引，SQL 语句和运行结果如下： mysql\u0026gt; CREATE INDEX index_name ON tb_students_info(name); Query OK, 8 rows affected (0.14 sec) 使用 EXPLAIN 再次执行上面的查询语句，SQL 语句和运行结果如下： mysql\u0026gt; EXPLAIN SELECT * FROM tb_students_info WHERE name=\u0026lsquo;张三\u0026rsquo; \\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_students_info partitions: NULL type: ref possible_keys: index_name key: index_name key_len: 63 ref: const rows: 1 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) 结果显示，rows 列的值为 1，表示这个查询语句只扫描了表中的 1 条记录。创建索引后访问的行由 8 行减少到 1 行，其查询速度自然比扫描 8 条记录快。而且 possible_keys 和 key 的值都是 index_name，这说明查询时使用了 index_name 索引。所以，在查询操作中，使用索引不仅能自动优化查询效率，还会降低服务器的开销。\n注意：由于 tb_students_info 表中记录较少，所以在这没有分析运行时间。表中记录多时，运行时间的差异也会体现出索引对查询速度的影响。\n","date":"2022-10-25","permalink":"/post/mysql%E4%BC%98%E5%8C%96%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE%E9%80%9F%E5%BA%A6/","tags":["DataBase"],"title":"MySQL优化插入数据速度"},{"content":"#!/usr/bin/env python3\nAuthor: aiyoyo🍉 1、可变类型陷阱\nl = [] a = [] for i in range(10): a.append(i) l.append(a) print(l)  l = [] a = {'num': 0} for i in range(10): a['num'] = i l.append(a) print(l) print(a)  都是类似的，每次新增的都是同一个对象，并在其后进行了修改，导致之前的也被修改（覆盖）。\n2、lambda 陷阱\na = [lambda x:x+n for n in range(9)] print(b(0) for b in a)  lambda 中的变量为自由变量，而不是像普通函数那样有自己的空间， 所以上面的变量x是一直在变化的，直至被最后一个覆盖\n","date":"2022-10-25","permalink":"/post/questions/","tags":["Python"],"title":"questions"},{"content":"Python中栈、队列和优先级队列的实现\n栈(Stack) 栈是一种LIFO(后进先出)的数据结构，有入栈(push)、出栈(pop)两种操作，且只能操作栈顶元素。\n1、list list是Python内置的列表数据结构，它支持栈的特性，有入栈和出栈操作。只不过用list实现栈性能不是特别好。 因为list内部是通过一个动态扩容的数组来实现的。当增减元素时就有可能会触发扩容操作。如果在list的头部增减元素，也会移动整个列表。\n如要使用list来实现一个栈的话，可以使用list的append()(入栈)、pop()(出栈)方法。\nstack_list = [] stack_list.append('one') stack_list.append('two') stack_list.append('three') stack_list.pop() stack_list.pop() stack_list.pop() stack_list.pop()  2、collections.deque deque类是一种双端队列。在Python中它就是一个双向列表，可以以常用时间在两端执行添加和删除元素的操作，非常高效，所以它既可以实现栈也可以实现队列。\n如果要在Python实现一个栈，那么应该优先选择deque，而不是list。\ndeque的入栈和出栈方法也分别是append()和pop()。\nfrom collections import deque stack_deque = deque() stack_deque.append('eat') stack_deque.append('sleep') stack_deque.append('code') stack_deque.pop() stack_deque.pop() stack_deque.pop() stack_deque.pop()  3、queue.LifoQueue 顾名思义，这个就是一个栈。不过它是线程安全的，如果要在并发的环境下使用，那么就可以选择使用LifoQueue。 它入栈和出栈操作是使用put()和get()，其中get()在LifoQueue为空时会阻塞。\nfrom queue import LifoQueue stack_queue = LifoQueue() stack_queue.put('eat') stack_queue.put('sleep') stack_queue.put('code') stack_queue.get() stack_queue.get() stack_queue.get() stack_queue.get()\t# 阻塞并一直等待直到栈不为空  队列(Queue) 队列是一种FIFO(先进先出)的数据结构。它有入队(enqueue)、出队(dequeue)两种操作，而且也是常数时间的操作。\n在Python中可以使用哪些数据结构来实现一个队列呢？\n1、list list可以实现一个队列，但它的入队、出队操作就不是非常高效了。因为list是一个动态列表，在队列的头部执行出队操作时，会发生整个元素的移动。\n使用list来实现一个队列时，用append()执行入队操作，使用pop(0)方法在队列头部执行出队操作。由于在list的第一个元素进行操作，所以后续的元素都会向前移动一位。因此用list来实现队列是不推荐的。\nqueue_list = [] queue_list.append('1') queue_list.append('2') queue_list.append('3') queue_list.pop(0) queue_list.pop(0) queue_list.pop(0) queue_list.pop(0)  2、collections.deque 从上文我们已经知道deque是一个双向列表，它可以在列表两端以常数时间进行添加删除操作。所以用deque来实现一个队列是非常高效的。 deque入队操作使用append()方法，出队操作使用popleft()方法。\nfrom collections import deque queue_deque = deque() queue_deque.append('eat') queue_deque.append('sleep') queue_deque.append('code') # 使用popleft出队 queue_deque.popleft() queue_deque.popleft() queue_deque.popleft() queue_deque.popleft()  3、queue.Queue 同样地，如果要在并发环境下使用队列，那么选择线程安全的queue.Queue。 与LifoQueue类似，入队和出队操作分别是put()和get()方法，get()在队列为空时会一直阻塞直到有元素入队。\nfrom queue import Queue queue_queue = Queue() queue_queue.put('eat') queue_queue.put('sleep') queue_queue.put('code') queue_queue.get() queue_queue.get() queue_queue.get() queue_queue.get_nowait() # 队列为空不要执行等待  4、multiprocessing.Queue 多进程版本的队列。如果要在多进程环境下使用队列，那么应该选择multiprocessing.Queue。\n同样地，它的入队出队操作分别是put()和get()。get()方法在队列为空，会一直阻塞直到队列不为空。\nfrom multiprocessing import Queue queue_mq = Queue() queue_mq.put('eat') queue_mq.put('sleep') queue_mq.put('code') queue_mq.get() queue_mq.get() queue_mq.get() queue_mq.get_nowait() # 也会一直阻塞直到队列不为空  优先级队列(PriorityQueue) 一个近乎排序的序列里可以使用优先级队列这种数据结构，它能高效获取最大或最小的元素。\n在调度问题的场景中经常会用到优先级队列。它主要有获取最大值或最小值的操作和入队操作。\n1、list 使用list可以实现一个优先级队列，但它并不高效。因为当要获取最值时需要排序，然后再获取最值。一旦有新的元素加入，再次获取最值时，又要重新排序。所以并推荐使用。\n2、heapq 一般来说，优先级队列都是使用堆这种数据结构来实现。而heapq就是Python标准库中堆的实现。heapq默认情况下实现的是最小堆。\n入队操作使用heappush()，出队操作使用heappop()。\nimport heapq pq_heapq = [] heapq.heappush(pq_heapq, (2, 'code')) heapq.heappush(pq_heapq, (1, 'eat')) heapq.heappush(pq_heapq, (3, 'sleep')) while q: next_item = heapq.heappop(pq_heapq) print(next_item)  3、queue.PriorityQueue queue.PriorityQueue内部封装了heapq，不同的是它是线程安全的。在并发环境下应该选择使用PriorityQueue。\nfrom queue import PriorityQueue pq_qp = PriorityQueue() pq_qp.put((2, 'code')) pq_qp.put((1, 'eat')) pq_qp.put((3, 'sleep')) while not pq_qp.empty(): next_item = pq_qp.get() print(next_item)  collections.deque 是一种双向链表，在单线程的情况下，它可以用来实现 Stack 和 Queue。而 heapq 模块可以帮我们实现高效的优先级队列。 如果要在多并发的情况下使用 Stack、Queue 和 PriorityQueue 的话，那么应该选用queue模块下类： 实现 Stack的queue.LifoQueue 实现 Queue的queue.Queue 或 multiprocessing.Queue 实现 PriorityQueue的queue.PriorityQueue 以上这些类都有 put() 和 get() 方法，且 get() 会在栈/队列为空时阻塞。\n","date":"2022-10-25","permalink":"/post/queues-stacks/","tags":["Python"],"title":"queues-stacks"},{"content":"","date":"2022-10-25","permalink":"/post/read/","tags":["Phone","images"],"title":"read"},{"content":"递归 def normal_recursion(n): if n == 1: return 1 else: return n + normal_recursion(n-1) normal_recursion(5) 5 + normal_recursion(4) 5 + 4 + normal_recursion(3) 5 + 4 + 3 + normal_recursion(2) 5 + 4 + 3 + 2 + normal_recursion(1) 5 + 4 + 3 + 3 5 + 4 + 6 5 + 10 15  线性递归，每深入一层，就会产生新的局部变量，必须创建新的调用栈, 随着递归深度的增加, 创建的栈越来越多, 造成爆栈\n尾递归 尾递归基于函数的尾调用, 每一级调用直接返回递归函数更新调用栈, 没有新局部变量的产生, 类似迭代的实现:\ndef tail_recursion(n, total=0): if n == 0: return total else: return tail_recursion(n-1, total+n) 执行：\ntail_recursion(5, 0) tail_recursion(4, 5) tail_recursion(3, 9) tail_recursion(2, 12) tail_recursion(1, 14) tail_recursion(0, 15)\n可以看到, 尾递归每一级递归函数的调用变成\u0026quot;线性\u0026quot;的形式. 这时, 我们可以思考, 虽然尾递归调用也会创建新的栈, 但是我们可以优化使得尾递归的每一级调用共用一个栈!, 如此便可解决爆栈和递归深度限制的问题!\n但在python中由于对解释器的保护设定了深度，当递归深度较大时尾递归依旧会爆栈。\n递归效率低问题  递归的效率问题及递归与循环比较 1.所谓的递归慢到底是什么原因呢？\n大家都知道递归的实现是通过调用函数本身，函数调用的时候，每次调用时要做地址保存，参数传递等，这是通过一个递归工作栈实现的。具体是每次调用函数本身要保存的内容包括：局部变量、形参、调用函数地址、返回值。那么，如果递归调用N次，就要分配N局部变量、N形参、N调用函数地址、N返回值。这势必是影响效率的。\n2.用循环效率会比递归效率高吗？\n递归与循环是两种不同的解决问题的典型思路。当然也并不是说循环效率就一定比递归高，递归和循环是两码事，递归带有栈操作，循环则不一定，两个概念不是一个层次，不同场景做不同的尝试。\n2.1递归算法：\n优点：代码简洁、清晰，并且容易验证正确性。（如果你真的理解了算法的话，否则你更晕）\n缺点：它的运行需要较多次数的函数调用，如果调用层数比较深，需要增加额外的堆栈处理（还有可能出现堆栈溢出的情况），比如参数传递需要压栈等操作，会对执行效率有一定影响。但是，对于某些问题，如果不使用递归，那将是极端难看的代码。\n2.2循环算法：\n优点：速度快，结构简单。\n缺点：并不能解决所有的问题。有的问题适合使用递归而不是循环。如果使用循环并不困难的话，最好使用循环。\n2.3递归算法和循环算法总结：\n  一般递归调用可以处理的算法，也通过循环去解决常需要额外的低效处理。\n  现在的编译器在优化后，对于多次调用的函数处理会有非常好的效率优化，效率未必低于循环。\n  3.递归和循环两者完全可以互换。如果用到递归的地方可以很方便使用循环替换，而不影响程序的阅读，那么替换成递归往往是好的。（例如：求阶乘的递归实现与循环实现。）\n3.那么递归使用的栈是什么样的一个栈呢？\n首先，看一下系统栈和用户栈的用途。\n3.1系统栈（也叫核心栈、内核栈）是内存中属于操作系统空间的一块区域，其主要用途为： (1)保存中断现场，对于嵌套中断，被中断程序的现场信息依次压入系统栈，中断返回时逆序弹出； (2)保存操作系统子程序间相互调用的参数、返回值、返回点以及子程序(函数)的局部变量。\n3.2用户栈是用户进程空间中的一块区域，用于保存用户进程的子程序间相互调用的参数、返回值、返回点以及子程序(函数)的局部变量。\n我们编写的递归程序属于用户程序，因此使用的是用户栈。\n尾递归优化  cpython本身不支持尾递归优化, 但是一个牛人想出的解决办法：实现一个 tail_call_optimized 装饰器\n#!/usr/bin/env python2.4 # This program shows off a python decorator( # which implements tail call optimization. It # does this by throwing an exception if it is # it's own grandparent, and catching such # exceptions to recall the stack. import sys class TailRecurseException: def __init__(self, args, kwargs): self.args = args self.kwargs = kwargs def tail_call_optimized(g): \u0026quot;\u0026quot;\u0026quot; This function decorates a function with tail call optimization. It does this by throwing an exception if it is it's own grandparent, and catching such exceptions to fake the tail call optimization. This function fails if the decorated function recurses in a non-tail context. \u0026quot;\u0026quot;\u0026quot; def func(*args, **kwargs): f = sys._getframe() if f.f_back and f.f_back.f_back \\ and f.f_back.f_back.f_code == f.f_code: # 抛出异常 raise TailRecurseException(args, kwargs) else: while 1: try: return g(*args, **kwargs) except TailRecurseException, e: args = e.args kwargs = e.kwargs func.__doc__ = g.__doc__ return func @tail_call_optimized def factorial(n, acc=1): \u0026quot;calculate a factorial\u0026quot; if n == 0: return acc return factorial(n-1, n*acc) print factorial(10000)  这里解释一下sys._getframe()函数:\nsys._getframe([depth]): Return a frame object from the call stack. If optional integer depth is given, return the frame object that many calls below the top of the stack. If that is deeper than the call stack, ValueEfror is raised. The default for depth is zero, returning the frame at the top of the call stack. 即返回depth深度调用的栈帧对象.\nimport sys\ndef get_cur_info(): print sys._getframe().f_code.co_filename # 当前文件名 print sys._getframe().f_code.co_name # 当前函数名 print sys._getframe().f_lineno # 当前行号 print sys._getframe().f_back # 调用者的帧 更多关于sys._getframe的使用请看Frame Hacks 说一下tail_call_optimized实现尾递归优化的原理: 当递归函数被该装饰器修饰后, 递归调用在装饰器while循环内部进行, 每当产生新的递归调用栈帧时: f.f_back.f_back.f_code == f.f_code:, 就捕获当前尾调用函数的参数, 并抛出异常, 从而销毁递归栈并使用捕获的参数手动调用递归函数. 所以递归的过程中始终只存在一个栈帧对象, 达到优化的目的.\n","date":"2022-10-25","permalink":"/post/recurrence/","tags":["Python"],"title":"recurrence"},{"content":"网络数据采集是指通过网络爬虫或网站公开 API 等方式从网站上获取数据信息。该方法可以将非结构化数据从网页中抽取出来，将其存储为统一的本地数据文件，并以结构化的方式存储。它支持图片、音频、视频等文件或附件的采集，附件与正文可以自动关联。\n在互联网时代，网络爬虫主要是为搜索引擎提供最全面和最新的数据。\n在大数据时代，网络爬虫更是从互联网上采集数据的有利工具。目前已经知道的各种网络爬虫工具已经有上百个，网络爬虫工具基本可以分为 3 类。 分布式网络爬虫工具，如 Nutch。 Java 网络爬虫工具，如 Crawler4j、WebMagic、WebCollector。 非 Java 网络爬虫工具，如 Scrapy(基于 Python 语言开发)。\n本节首先对网络爬虫的原理和工作流程进行简单介绍，然后对网络爬虫抓取策略进行讨论，最后对典型的网络工具进行描述。 网络爬虫原理 网络爬虫是一种按照一定的规则，自动地抓取 Web 信息的程序或者脚本。\nWeb 网络爬虫可以自动采集所有其能够访问到的页面内容，为搜索引擎和大数据分析提供数据来源。从功能上来讲，爬虫一般有数据采集、处理和存储 3 部分功能，如图 1 所示。 网络爬虫示意 网页中除了包含供用户阅读的文字信息外，还包含一些超链接信息。\n网络爬虫系统正是通过网页中的超链接信息不断获得网络上的其他网页的。网络爬虫从一个或若干初始网页的 URL 开始，获得初始网页上的 URL，在抓取网页的过程中，不断从当前页面上抽取新的 URL 放入队列，直到满足系统的一定停止条件。\n网络爬虫系统一般会选择一些比较重要的、出度（网页中链出的超链接数）较大的网站的 URL 作为种子 URL 集合。\n网络爬虫系统以这些种子集合作为初始 URL，开始数据的抓取。因为网页中含有链接信息，通过已有网页的 URL 会得到一些新的 URL。\n可以把网页之间的指向结构视为一个森林，每个种子 URL 对应的网页是森林中的一棵树的根结点，这样网络爬虫系统就可以根据广度优先搜索算法或者深度优先搜索算法遍历所有的网页。\n由于深度优先搜索算法可能会使爬虫系统陷入一个网站内部，不利于搜索比较靠近网站首页的网页信息，因此一般采用广度优先搜索算法采集网页。\n网络爬虫系统首先将种子 URL 放入下载队列，并简单地从队首取出一个 URL 下载其对应的网页，得到网页的内容并将其存储后，经过解析网页中的链接信息可以得到一些新的 URL。\n其次，根据一定的网页分析算法过滤掉与主题无关的链接，保留有用的链接并将其放入等待抓取的 URL 队列。\n最后，取出一个 URL，对其对应的网页进行下载，然后再解析，如此反复进行，直到遍历了整个网络或者满足某种条件后才会停止下来。 网络爬虫工作流程 如图 2 所示，网络爬虫的基本工作流程如下。\n1）首先选取一部分种子 URL。\n2）将这些 URL 放入待抓取 URL 队列。\n3）从待抓取 URL 队列中取出待抓取 URL，解析 DNS，得到主机的 IP 地址，并将 URL 对应的网页下载下来，存储到已下载网页库中。此外，将这些 URL 放进已抓取 URL 队列。\n4）分析已抓取 URL 队列中的 URL，分析其中的其他 URL，并且将这些 URL 放入待抓取 URL 队列，从而进入下一个循环。 网络爬虫的基本工作流程 网络爬虫抓取策略 Google 和百度等通用搜索引擎抓取的网页数量通常都是以亿为单位计算的。那么，面对如此众多的网页，通过何种方式才能使网络爬虫尽可能地遍历所有网页，从而尽可能地扩大网页信息的抓取覆盖面，这是网络爬虫系统面对的一个很关键的问题。在网络爬虫系统中，抓取策略决定了抓取网页的顺序。\n本节首先对网络爬虫抓取策略用到的基本概念做简单介绍。 1）网页间关系模型 从互联网的结构来看，网页之间通过数量不等的超链接相互连接，形成一个彼此关联、庞大复杂的有向图。\n如图 3 所示，如果将网页看成是图中的某一个结点，而将网页中指向其他网页的链接看成是这个结点指向其他结点的边，那么我们很容易将整个互联网上的网页建模成一个有向图。\n理论上讲，通过遍历算法遍历该图，可以访问到互联网上几乎所有的网页。 网页关系模型图 2）网页分类 从爬虫的角度对互联网进行划分，可以将互联网的所有页面分为 5 个部分：已下载未过期网页、已下载已过期网页、待下载网页、可知网页和不可知网页，如图 4 所示。\n抓取到本地的网页实际上是互联网内容的一个镜像与备份。互联网是动态变化的，当一部分互联网上的内容发生变化后，抓取到本地的网页就过期了。所以，已下载的网页分为已下载未过期网页和已下载已过期网页两类。 网页分类 待下载网页是指待抓取 URL 队列中的那些页面。\n可知网页是指还没有抓取下来，也没有在待抓取 URL 队列中，但是可以通过对已抓取页面或者待抓取 URL 对应页面进行分析，从而获取到的网页。\n还有一部分网页，网络爬虫是无法直接抓取下载的，称为不可知网页。\n下面重点介绍几种常见的抓取策略。\n 通用网络爬虫 通用网络爬虫又称全网爬虫，爬行对象从一些种子 URL 扩展到整个 Web，主要为门户站点搜索引擎和大型 Web 服务提供商采集数据。  为提高工作效率，通用网络爬虫会采取一定的爬行策略。常用的爬行策略有深度优先策略和广度优先策略。\n1）深度优先策略\n深度优先策略是指网络爬虫会从起始页开始，一个链接一个链接地跟踪下去，直到不能再深入为止。\n网络爬虫在完成一个爬行分支后返回到上一链接结点进一步搜索其他链接。当所有链接遍历完后，爬行任务结束。\n这种策略比较适合垂直搜索或站内搜索，但爬行页面内容层次较深的站点时会造成资源的巨大浪费。\n以图 3 为例，遍历的路径为 1→2→5→6→3→7→4→8。\n在深度优先策略中，当搜索到某一个结点的时候，这个结点的子结点及该子结点的后继结点全部优先于该结点的兄弟结点，深度优先策略在搜索空间的时候会尽量地往深处去，只有找不到某结点的后继结点时才考虑它的兄弟结点。\n这样的策略就决定了深度优先策略不一定能找到最优解，并且由于深度的限制甚至找不到解。\n如果不加限制，就会沿着一条路径无限制地扩展下去，这样就会“陷入”到巨大的数据量中。一般情况下，使用深度优先策略都会选择一个合适的深度，然后反复地搜索，直到找到解，这样搜索的效率就降低了。所以深度优先策略一般在搜索数据量比较小的时候才使用。\n2）广度优先策略\n广度优先策略按照网页内容目录层次深浅来爬行页面，处于较浅目录层次的页面首先被爬行。当同一层次中的页面爬行完毕后，爬虫再深入下一层继续爬行。\n仍然以图 3 为例，遍历的路径为 1→2→3→4→5→6→7→8\n由于广度优先策略是对第 N 层的结点扩展完成后才进入第 N+1 层的，所以可以保证以最短路径找到解。\n这种策略能够有效控制页面的爬行深度，避免遇到一个无穷深层分支时无法结束爬行的问题，实现方便，无须存储大量中间结点，不足之处在于需较长时间才能爬行到目录层次较深的页面。\n如果搜索时分支过多，也就是结点的后继结点太多，就会使算法耗尽资源，在可以利用的空间內找不到解。 2. 聚焦网络爬虫 聚焦网络爬虫又称主题网络爬虫，是指选择性地爬行那些与预先定义好的主题相关的页面的网络爬虫。\n1）基于内容评价的爬行策略\nDeBra 将文本相似度的计算方法引入到网络爬虫中，提出了 Fish Search 算法。\n该算法将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关的页面，其局限性在于无法评价页面与主题相关度的大小。\nHerseovic 对 Fish Search 算法进行了改进，提出了 Shark Search 算法，即利用空间向量模型计算页面与主题的相关度大小。\n采用基于连续值计算链接价值的方法，不但可以计算出哪些抓取的链接和主题相关，还可以得到相关度的量化大小。\n2）基于链接结构评价的爬行策略\n网页不同于一般文本，它是一种半结构化的文档，包含了许多结构化的信息。\n网页不是单独存在的，页面中的链接指示了页面之间的相互关系，基于链接结构的搜索策略模式利用这些结构特征来评价页面和链接的重要性，以此决定搜索的顺序。其中，PageRank 算法是这类搜索策略模式的代表。\nPageRank 算法的基本原理是，如果一个网页多次被引用，则可能是很重要的网页，如果一个网页没有被多次引用，但是被重要的网页引用，也有可能是重要的网页。一个网页的重要性被平均地传递到它所引用的网页上。\n将某个页面的 PageRank 除以存在于这个页面的正向链接，并将得到的值分别和正向链接所指的页面的 PageRank 相加，即得到了被链接的页面的 PageRank。\n如图 5 所示，PageRank 值为 100 的网页把它的重要性平均传递给了它所引用的两个页面，每个页面获得了 50，同样 PageRank 值为 9 的网页给它所引用的 3 个页面的每个页面传递的值为 3。\nPageRank 值为 53 的页面的值来源于两个引用了它的页面传递过来的值。 PageRank算法示例、 3）基于增强学习的爬行策略\nRennie 和 McCallum 将增强学习引入聚焦爬虫，利用贝叶斯分类器，根据整个网页文本和链接文本对超链接进行分类，为每个链接计算出重要性，从而决定链接的访问顺序。\n4）基于语境图的爬行策略\nDiligenti 等人提出了一种通过建立语境图学习网页之间的相关度的爬行策略，该策略可训练一个机器学习系统，通过该系统可计算当前页面到相关 Web 页面的距离，距离近的页面中的链接优先访问。 3. 增量式网络爬虫 增量式网络爬虫是指对已下载网页采取增量式更新并且只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。\n增量式网络爬虫有两个目标： 保持本地页面集中存储的页面为最新页面。 提高本地页面集中页面的质量。\n为实现第一个目标，增量式网络爬虫需要通过重新访问网页来更新本地页面集中页面的内容。常用的方法有统一更新法、个体更新法和基于分类的更新法。 在统一更新法中，网络爬虫以相同的频率访问所有网页，而不考虑网页的改变频率。 在个体更新法中，网络爬虫根据个体网页的改变频率来重新访问各页面。 在基于分类的更新法中，网络爬虫根据网页改变频率将其分为更新较快网页子集和更新较慢网页子集两类，然后以不同的频率访问这两类网页。\n为实现第二个目标，增量式网络爬虫需要对网页的重要性排序，常用的策略有广度优先策略、PageRank 优先策略等。 4. 深层网络爬虫 网页按存在方式可以分为表层网页和深层网页。 表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主。 深层网页是那些大部分内容不能通过静态链接获取的，隐藏在搜索表单后的，只有用户提交一些关键词才能获得的网页。\n深层网络爬虫体系结构包含 6 个基本功能模块（爬行控制器、解析器、表单分析器、表单处理器、响应分析器、LVS 控制器）和两个爬虫内部数据结构（URL 列表和 LVS 表）。\n通过网络爬虫采集大数据\n","date":"2022-10-25","permalink":"/post/spider-introduce/","tags":["Python"],"title":"spider-introduce"},{"content":"第七章 测试 1. 禅道和 qc 的区别？  都是缺陷管理工具。\n A.QC  作为缺陷管理工具，QC 在缺陷管理方面，做的 相对完善。 提 bug 页面：填写内容可以根据测试需求，不断修改添加新的字段；以我上一家公司为例，在提 bug 过程中，有一下几个 必填项：\n Bug 状态（new、fixed、closed 等） 发现人员 缺陷发现阶段 (测试阶段、上现阶段等) 缺陷来源（测试人员给出的 bug 定位） Bug 分类（功能、性能等问题） 测试阶段（单元测试、集成测试、系统测试等） 归属需求 缺陷回归次数 优先级 分配给 # 这些必填项再加上 bug 标题和操作描述、上传附件，使很多疑问都变得清晰。   缺陷查看页面：可以根据自己需要选择要呈现的字段，相对人性化可操作，每个显示的字段都可以进行筛选，使研发人员很快能定位到属于自己的 bug，再根据 bug 状态、优先级进行筛选，使未完结的 bug 能有序并无遗漏地完成修改；页面还有注释功能，研发人员能写出针对本问题的各种感想，方便完善而又人性化。\n B. 禅道（开源版）  禅道 涉及面非常广，但是在缺陷管理这方面，与老牌的 QC 还是略逊一筹。 提 bug 页面：页面是非常清晰整洁的 web 页面，但是需要填写的字段，并没有完全覆盖开发和测试人员的全部需求。 页面字段：\n 产品模块（对应 QC 中的项目） 所属项目（对应 QC 中的需求） 影响版本（bug 所属版本？） 当前指派（修改 bug 的人员） bug 标题 重现步骤 相关需求（页面标注了这个字段，但是什么也没有显示，并且没有可填写的位置） 相关任务 类型 / 严重。  2. 编写测试计划的目的是 1、使测试工作顺利进行 2、使项目参与人员沟通更舒畅 3、使测试工作更加系统化\n3. 测试人员在软件开发过程中的任务是什么？ 1、寻找 Bug； 2、避免软件开发过程中的缺陷； 3、衡量软件的品质； 4、关注用户的需求。 5、总的目标是：确保软件的质量。\n4. 您以往的工作中，一条软件缺陷（或者叫 Bug）记录都包含了哪些内容？如何提交高质量的软件缺陷（Bug）记录？ 一条 Bug 记录最基本应包含：\n编号 Bug 所属模块 Bug 描述 Bug 级别 发现日期 发现人 修改日期 修改人 修改方法 回归结果等等；  要有效的发现 Bug 需参考需求以及详细设计等前期文档设计出高效的测试用例，然后严格执行测试用例，对发现的问题要充分确认肯定，然后再向外发布如此才能提高提交 Bug 的质量。\n5. 简述黑盒测试和白盒测试的优缺点 ※ 黑盒测试的优点有： 1） 比较简单，不需要了解程序内部的代码及实现； 2） 与软件的内部实现无关； 3） 从用户角度出发，能很容易的知道用户会用到哪些功能，会遇到哪些问题； 4） 基于软件开发文档，所以也能知道软件实现了文档中的哪些功能； 5） 在做软件自动化测试时较为方便。\n※ 黑盒测试的缺点有： 1） 不可能覆盖所有的代码，覆盖率较低，大概只能达到总代码量的 30%； 2） 自动化测试的复用性较低。\n※ 白盒测试的优点有： 1） 帮助软件测试人员增大代码的覆盖率，提高代码的质量，发现代码中隐藏的问题。\n※ 白盒测试的缺点有： 1） 程序运行会有很多不同的路径，不可能测试所有的运行路径；测试基于代码，只能测试开发人员做的对不对，而不能知道设计的正确与否，可能会漏掉一些功能需求；系统庞大时，测试开销会非常大。\n6. 简述常用的 Bug 管理或者用例管理工具，并且描述其中一个工作流程。  常用：testlink，QC，mantis，禅道，TAPD，JIRA 。 TAPD：产品创建 (需求，计划，模块)–\u0026gt; 项目创建（PM 排期、任务分解）–\u0026gt; 研发 (编码、单元测试等)–\u0026gt; 测试 (测试计划，用例，执行，bug，报告等)。\n 7. 请列出你所知道的软件测试种类，至少 5 项。 单元测试，集成测试，系统测试，验收测试。\n系统测试包含： 功能测试 性能测试 压力测试 兼容性测试 健壮性测试 冒烟测试 文档测试  8. Alpha 测试与 Beta 测试的区别是什么？  Alpha 主要是模拟用户的操作和用户的环境。 Beta 主要验证测试，准备进入发布阶段，Beta 测试是一种验收测试。\n 9. 举例说明什么是 Bug？一个 bug report 应包含什么关键字？  比如聊天中，点击发送按钮后，无法发送消息。 标题，模块，严重程度，bug 类型，版本号，可否重现，描述，附件，日志等等。\n ","date":"2022-10-25","permalink":"/post/test/","tags":["InterviewQuestions","Python"],"title":"test"},{"content":"一、TCP/IP网络模型 计算机与网络设备要相互通信，双方就必须基于相同的方法。比如，如何探测到通信目标、由哪一边先发起通信、使用哪种语言进行通信、怎样结束通信等规则都需要事先确定。不同的硬件、操作系统之间的通信，所有的这一切都需要一种规则。而我们就把这种规则称为协议（protocol）。\nTCP/IP 是互联网相关的各类协议族的总称，比如：TCP，UDP，IP，FTP，HTTP，ICMP，SMTP 等都属于 TCP/IP 族内的协议。\nTCP/IP模型是互联网的基础，它是一系列网络协议的总称。这些协议可以划分为四层，分别为链路层、网络层、传输层和应用层。\n链路层：负责封装和解封装IP报文，发送和接受ARP/RARP报文等。 网络层：负责路由以及把分组报文发送给目标网络或主机。 传输层：负责对报文进行分组和重组，并以TCP或UDP协议格式封装报文。 应用层：负责向用户提供应用程序，比如HTTP、FTP、Telnet、DNS、SMTP等。 在网络体系结构中网络通信的建立必须是在通信双方的对等层进行，不能交错。 在整个数据传输过程中，数据在发送端时经过各层时都要附加上相应层的协议头和协议尾（仅数据链路层需要封装协议尾）部分，也就是要对数据进行协议封装，以标识对应层所用的通信协议。接下去介绍TCP/IP 中有两个具有代表性的传输层协议\u0026mdash;-TCP 和 UDP。\n二、UDP UDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。在OSI模型中，在第四层——传输层，处于IP协议的上一层。UDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。\n它有以下几个特点：\n 面向无连接 首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。  具体来说就是：\n在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了 在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作 2. 有单播，多播，广播的功能 UDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。\n UDP是面向报文的 发送方的UDP对应用程序交下来的报文，在添加首部后就向下交付IP层。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。因此，应用程序必须选择合适大小的报文\n  不可靠性 首先不可靠性体现在无连接上，通信都不需要建立连接，想发就发，这样的情况肯定不可靠。\n  并且收到什么数据就传递什么数据，并且也不会备份数据，发送数据也不会关心对方是否已经正确接收到数据了。\n再者网络环境时好时坏，但是 UDP 因为没有拥塞控制，一直会以恒定的速度发送数据。即使网络条件不好，也不会对发送速率进行调整。这样实现的弊端就是在网络条件不好的情况下可能会导致丢包，但是优点也很明显，在某些实时性要求高的场景（比如电话会议）就需要使用 UDP 而不是 TCP。\n从上面的动态图可以得知，UDP只会把想发的数据报文一股脑的丢给对方，并不在意数据有无安全完整到达。\n头部开销小，传输数据报文时是很高效的。  UDP 头部包含了以下几个数据：\n两个十六位的端口号，分别为源端口（可选字段）和目标端口 整个数据报文的长度 整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误 因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的\n三、TCP 当一台计算机想要与另一台计算机通讯时，两台计算机之间的通信需要畅通且可靠，这样才能保证正确收发数据。例如，当你想查看网页或查看电子邮件时，希望完整且按顺序查看网页，而不丢失任何内容。当你下载文件时，希望获得的是完整的文件，而不仅仅是文件的一部分，因为如果数据丢失或乱序，都不是你希望得到的结果，于是就用到了TCP。\nTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构，你可以把它想象成排水管中的水流。\n TCP连接过程 如下图所示，可以看到建立一个TCP连接的过程为（三次握手的过程）:  第一次握手\n客户端向服务端发送连接请求报文段。该报文段中包含自身的数据通讯初始序号。请求发送后，客户端便进入 SYN-SENT 状态。\n第二次握手\n服务端收到连接请求报文段后，如果同意连接，则会发送一个应答，该应答中也会包含自身的数据通讯初始序号，发送完成后便进入 SYN-RECEIVED 状态。\n第三次握手\n当客户端收到连接同意的应答后，还要向服务端发送一个确认报文。客户端发完这个报文段后便进入 ESTABLISHED 状态，服务端收到这个应答后也进入 ESTABLISHED 状态，此时连接建立成功。\n这里可能大家会有个疑惑：为什么 TCP 建立连接需要三次握手，而不是两次？这是因为这是为了防止出现失效的连接请求报文段被服务端接收的情况，从而产生错误。\nTCP断开链接   TCP 是全双工的，在断开连接时两端都需要发送 FIN 和 ACK。\n第一次握手\n若客户端 A 认为数据发送完成，则它需要向服务端 B 发送连接释放请求。\n第二次握手\nB 收到连接释放请求后，会告诉应用层要释放 TCP 链接。然后会发送 ACK 包，并进入 CLOSE_WAIT 状态，此时表明 A 到 B 的连接已经释放，不再接收 A 发的数据了。但是因为 TCP 连接是双向的，所以 B 仍旧可以发送数据给 A。\n第三次握手\nB 如果此时还有没发完的数据会继续发送，完毕后会向 A 发送连接释放请求，然后 B 便进入 LAST-ACK 状态。\n第四次握手\nA 收到释放请求后，向 B 发送确认应答，此时 A 进入 TIME-WAIT 状态。该状态会持续 2MSL（最大段生存期，指报文段在网络中生存的时间，超时会被抛弃） 时间，若该时间段内没有 B 的重发请求的话，就进入 CLOSED 状态。当 B 收到确认应答后，也便进入 CLOSED 状态。\nTCP协议的特点 面向连接  面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。\n仅支持单播传输\n每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。\n面向字节流 TCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。\n可靠传输\n对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。\n提供拥塞控制\n当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞\nTCP提供全双工通信 TCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）\n四、TCP和UDP的比较\n  对比 |\t|UDP\t|\tTCP| |是否连接\t|无连接\t|面向连接| |是否可靠\t|不可靠传输，不使用流量控制和拥塞控制\t|可靠传输，使用流量控制和拥塞控制| |连接对象个数\t|支持一对一，一对多，多对一和多对多交互通信\t|只能是一对一通信| |传输方式\t|面向报文\t|面向字节流| |首部开销\t|首部开销小，仅8字节\t|\t首部最小20字节，最大60字节| |适用场景\t|适用于实时应用（IP电话、视频会议、直播等）\t|适用于要求可靠传输的应用，例如文件传输|\n  总结 TCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。 虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为 对数据准确性要求高，速度可以相对较慢的，可以选用TCP\n  一文搞懂TCP与UDP的区别 Python自动化测试面试题-接口篇\nTCP,UDP的区别?# 面向连接和无连接 资源耗费 UDP程序结构简单 流模式与数据报模式 确保数据正确性 TCP如何确保可靠传输?# 确认和重传 数据校验 合理分片和排序 流量控制 拥塞控制 三次握手与四次挥手?# 三次握手 请求连接 -\u0026gt; 服务端确认 -\u0026gt; 客户端确认 四次挥手 请求断开 -\u0026gt; 服务端同意 -\u0026gt; 服务端释放连接 -\u0026gt; 客户端确认\n","date":"2022-10-25","permalink":"/post/udptcp/","tags":["FrontEnd"],"title":"udp\u0026tcp"},{"content":"第四章 WEB 框架  一、Flask 1. Flask 中正则 URL 的实现？  @app.route(\u0026lt;URL\u0026gt;) 中 URL 显式支持 string、int、float、path 4 种类型，隐式支持正则。\n  第一步：写正则类，继承 BaseConverter，将匹配到的值设置为 regex 的值。\n class RegexUrl(BaseConverter): def init(self， url_map， *args): super(RegexUrl， self). init(url_map) self.regex = args[0]   第二步：把正则类赋值给我们定义的正则规则。\n app.url_map.converters['re'] = RegexUrl   第三步：在 URL 中使用正则。\n @app.route('/regex/\u0026lt;re(\u0026quot;[a-z]{3}\u0026quot;):id\u0026gt;') def regex111(id): return 'id:% s'% id  2. Flask 中请求上下文和应用上下文的区别和作用？  current_app、g 是应用上下文。 request、session 是请求上下文。\n 手动创建上下文的两种方法： with app.app_context() app = current_app._get_current_object()  两者区别：  请求上下文：保存了客户端和服务器交互的数据。 应用上下文：flask 应用程序运行过程中，保存的一些配置信息，比如程序名、数据库连接、应用信息等。\n 两者作用：  请求上下文(request context)： Flask 从客户端收到请求时，要让视图函数能访问一些对象，这样才能处理请求。请求对象是一个很好的例子，它封装了客户端发送的 HTTP 请求。 要想让视图函数能够访问请求对象，一个显而易见的方式是将其作为参数传入视图函数，不过这会导致程序中的每个视图函数都增加一个参数，除了访问请求对象，如果视图函数在处理请求时还要访问其他对象，情况会变得更糟。为了避免大量可有可无的参数把视图函数弄得一团糟，Flask 使用上下文临时把某些对象变为全局可访问。\n  应用上下文(application context)： 它的字面意思是应用上下文，但它不是一直存在的，它只是 request context 中的一个对 app 的代理(人)，所谓 local proxy。它的作用主要是帮助 request 获取当前的应用，它是伴 request 而生，随 request 而灭的。\n 3. Flask 中数据库设置？ app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://root:mysql@11:3306/test' # 动态追踪修改设置，如未设置只会提示警告 app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = True # 查询时会显示原始 SQL 语句 app.config['SQLALCHEMY_ECHO'] =True   补充 ：app.config[SQLALCHEMY_COMMIT_ON_TEARDOWN]：可以配置请求执行完逻辑之后自动提交，而不用我们每次都手动调用 session.commit()；\n  监听数据库中的数据，当发生改变，就会显示一些内容：app.config['SQLALCHEMY_TRACK_MODIFICATIONS']=True；\n  显示打印的数据以及 sql 语句，建议不设置，默认为 False：app.config['SQLALCHEMY_ECHO'] = True。\n 4. 常用的 SQLAlchemy 查询过滤器？ 5. 对 Flask 蓝图(Blueprint) 的理解？ 蓝图的定义\n 蓝图 / Blueprint 是 Flask 应用程序组件化的方法，可以在一个应用内或跨越多个项目共用蓝图。 使用蓝图可以极大地简化大型应用的开发难度，也为 Flask 扩展提供了一种在应用中注册服务的 集中式机制。\n 蓝图的应用场景\n 把一个应用分解为一个蓝图的集合。这对大型应用是理想的。一个项目可以实例化一个应用对象，初始化几个扩展，并注册一集合的蓝图。 以 URL 前缀和 / 或子域名，在应用上注册一个蓝图。URL 前缀 / 子域名中的参数即成为这个蓝图下的所有视图函数的共同的视图参数（默认情况下）。 在一个应用中用不同的 URL 规则多次注册一个蓝图。 通过蓝图提供模板过滤器、静态文件、模板和其它功能。一个蓝图不一定要实现应用或者视图函数。 初始化一个 Flask 扩展时，在这些情况中注册一个蓝图。  3) 蓝图的缺点 不能在应用创建后撤销注册一个蓝图而不销毁整个应用对象。\n4) 使用蓝图的三个步骤 # 1、创建一个蓝图对象 blue = Blueprint(\u0026quot;blue\u0026quot;， name) # 2、在这个蓝图对象上进行操作，例如注册路由、指定静态文件夹、注册模板过滤器... @blue.route('/') def blue_index(): return 'Welcome to my blueprint' # 3、在应用对象上注册这个蓝图对象 app.register_blueprint(blue，url_prefix='/blue')  6. Flask 中 WTF 表单数据验证？ 在 Flask 中，为了处理 web 表单，我们一般使用 Flask-WTF 扩展，它封装了 WTForms，并且它有验证表单数据的功能。\nWTForms 支持的 HTML 标准字段：    字段 对象说明     StringField 文本字段   TextAreaField 多行文本字段   PasswordField 密码文本字段   HiddenField 隐藏文件字段   DateField 文本字段，值为 datetime.date 文本格式   DateTimeField 文本字段，值为 datetime.datetime 文本格式   IntegerField 文本字段，值为整数   DecimalField 文本字段，值为 decimal.Decimal   FloatField 文本字段，值为浮点数   BooleanField 复选框，值为 True 和 False   RadioField 一组单选框   SelectField 下拉列表   SelectMutipleField 下拉列表，可选择多个值   FileField 文件上传字段   SubmitField 表单提交按钮   FormField 把表单作为字段嵌入另一个表单   FieldList 一组指定类型的字段    WTForms 常用验证函数    验证函数 对象说明     InputRequired 确保字段中有数据   DataRequired 确保字段中有数据并且数据为真   EqualTo 比较两个字段的值，常用于比较两次密码输入   Length 验证输入的字符串长度   NumberRange 验证输入的值在数字范围内   URL 验证 URL   AnyOf 验证输入值在可选列表中   NoneOf 验证输入值不在可选列表中     使用 Flask-WTF 需要配置参数 SECRET_KEY。 CSRF_ENABLED 是为了 CSRF（跨站请求伪造）保护。SECRET_KEY 用来生成加密令牌，当 CSRF 激活的时候，该设置会根据设置的密匙生成加密令牌。\n 7. Flask 项目中如何实现 session 信息的写入？  Flask 中有三个 session：\n  第一个：数据库中的 session，例如:db.session.add() 第二个：在 flask_session 扩展中的 session，使用：from flask_session importSession，使用第三方扩展的 session 可以把信息存储在服务器中，客户端浏览器中只存储 sessionid。 第三个：flask 自带的 session，是一个请求上下文， 使用：from flask import session。自带的 session 把信息加密后都存储在客户端的浏览器 cookie 中。\n 8. 项目接口实现后路由访问不到怎么办？ 可以通过 postman 测试工具测试，或者看 log 日志信息找到错误信息的大概位置。\n9. Flask 中 url_for 函数？ 1.URL 反转：根据视图函数名称得到当前所指向的 url。 2.url_for() 函数最简单的用法是以视图函数名作为参数，返回对应的 url，还可以用作加载静态文件。\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot;href=\u0026quot;{{url_for('static',filename='css/index.css')}}\u0026quot;\u0026gt;该条语句就是在模版中加载 css 静态文件。\n3.url_for 和 redirect 区别 url_for 是用来拼接 URL 的，可以使用程序 URL 映射中保存的信息生成 URL。url_for() 函数最简单的用法是以视图函数名作为参数， 返回对应的 URL。例如，在示例程序中 hello.py 中调用 url_for(index’) 得到的结果是 /。 redirect 是重定向函数，输入一个 URL 后，自动跳转到另一个 URL 所在的地址，例如，你在函数中写\nreturn redirect(https://www.baidu.com)页面就会跳转向百度页面。\nfrom flask import Flask,redirect,url_for app = Flask(name) @app.route('/') def index(): login_url = url_for('login') return redirect(login_url) return u' 这是首页 ' @app.route('/login/') def login(): return u' 这是登陆页面 ' @app.route('/question/\u0026lt;is_login\u0026gt;/') def question(is_login): if is_login == '1': return u' 这是发布问答的页面 ' else: return redirect(url_for('login')) if name == ' main ': app.run(debug=True)  10.Flask 中请求钩子的理解和应用？  请求钩子是通过 装饰器 的形式实现的，支持以下四种： 1，before_first_request 在处理第一个请求前运行 2，before_request: 在每次请求前运行 3，after_request: 如果没有未处理的异常抛出，在每次请求后运行 4，teardown_request: 即使有未处理的异常抛出，在每次请求后运行\n 应用：\n# 请求钩子 @api.after_request def after_request(response): \u0026quot;\u0026quot;\u0026quot;设置默认的响应报文格式为 application/json\u0026quot;\u0026quot;\u0026quot; # 如果响应报文 response 的 Content-Type 是以 text 开头，则将其改为 # 默认的 json 类型 if response.headers.get(\u0026quot;Content-Type\u0026quot;).startswith(\u0026quot;text\u0026quot;): response.headers[\u0026quot;Content-Type\u0026quot;] = \u0026quot;application/json\u0026quot; return respon  11. 一个变量后写多个过滤器是如何执行的？ {{expression | filter1 | filter2 | ...}} 即表达式(expression) 使用 filter1 过滤后再使用 filter2 过滤。  12. 如何把整个数据库导出来，再导入指定数据库中？ 导出：mysqldump[-h 主机] -u 用户名 - p 数据库名 \u0026gt; 导出的数据库名.sql\n导入指定的数据库中: 第一种方法：mysqldump[-h 主机] -u 用户名 - p 数据库名 \u0026lt;导出的数据库名.sql\n第二种方法： 先创建好数据库，因为导出的文件里没有创建数据库的语句，如果数据库已经建好，则不用再创建。create database example charset=utf8;（数据库名可以不一样） 切换数据库：use example; 导入指定 sql 文件：mysql\u0026gt;source /path/example.sql;\n13.Flask 和 Django 路由映射的区别？ 在 django 中，路由是浏览器访问服务器时，先访问的项目中的 url，再由项目中的 url 找到应用中url，这些 url 是放在一个列表里，遵从从前往后匹配的规则。 在 flask 中，路由是通过装饰器给每个视 vcb图函数提供的，而且根据请求方式的不同可以一个 url 用于不同的作用。\n14. 跨站请求伪造和跨站请求保护的实现？  (https://img.alicdn.com/imgextra/i3/296192965/O1CN01LKxyUG1Xm0yEFEkhU_!!296192965.png \u0026ldquo;null\u0026rdquo;)\n 图中 Browse 是浏览器，WebServerA 是受信任网站 / 被攻击网站 A，WebServerB 是恶意网站 / 点击网站 B。 （1） 一开始用户打开浏览器，访问受信任网站 A，输入用户名和密码登陆请求登陆网站 A。 （2） 网站 A 验证用户信息，用户信息通过验证后，网站 A 产生 Cookie 信息并返回给浏览器。 （3） 用户登陆网站 A 成功后，可以正常请求网站 A。 （4） 用户未退出网站 A 之前，在同一浏览器中，打开一个 TAB 访问网站 B。 （5） 网站 B 看到有人方式后，他会返回一些攻击性代码。 （6） 浏览器在接受到这些攻击性代码后，促使用户不知情的情况下浏览器携带 Cookie（包括 sessionId）信息，请求网站 A。这种请求有可能更新密码，添加用户什么的操作。\n从上面 CSRF 攻击原理可以看出，要完成一次 CSRF 攻击，需要被攻击者完成两个步骤：\n 登陆受信任网站 A，并在本地生成 COOKIE。 在不登出 A 的情况下，访问危险网站 B。  如果不满足以上两个条件中的一个，就不会受到 CSRF 的攻击，以下情况可能会导致 CSRF：\n 登录了一个网站后，打开一个 tab 页面并访问另外的网站。 关闭浏览器了后，本地的 Cookie 尚未过期，你上次的会话还没有已经结束。（事实上，关闭浏览器不能结束一个会话，但大多数人都会错误的认为关闭浏览器就等于退出登录 / 结束会话了……）  解决办法：就是在表单中添加 from.csrf_token。\n15.Flask(name) 中的 name 可以传入哪些值？ 可以传入的参数： 1，字符串：hello’, 但是 abc’, 不行，因为 abc 是 python 内置的模块 2， name ，约定俗成\n不可以插入的参数 1，python 内置的模块，re,urllib,abc 等 2，数字\n二、Django 1. Django ORM 查询中 select_related 和 prefetch_related 的区别？？ ** 参考博客:** https://www.cnblogs.com/Dominic-Ji/p/9213887.html\n2. Django ORM 详解？ ** 参考博客:** https://www.cnblogs.com/Dominic-Ji/p/9209341.html\n1. Django 创建项目的命令？ django-admin startproject 项目名称 python manage.py startapp 应用 app 名  2. Django 创建项目后，项目文件夹下的组成部分（对 mvt 的理解）？  项目文件夹下的组成部分： manage.py 是项目运行的入口，指定配置文件路径。 与项目同名的目录，包含项目的配置文件。 __init__.py 是一个空文件，作用是这个目录可以被当作包使用。\nsettings.py 是项目的整体配置文件。\nurls.py 是项目的 URL 配置文件。\nwsgi.py 是项目与 WSGI 兼容的 Web 服务器。\n 3. 对 MVC,MVT 解读的理解？  M：Model，模型，和数据库进行交互\nV：View，视图，负责产生 Html 页面\nC：Controller，控制器，接收请求，进行处理，与 M 和 V 进行交互，返回应答。\n  (https://img.alicdn.com/imgextra/i3/296192965/O1CN01F6qPRL1Xm0yBz09zW_!!296192965.png \u0026ldquo;null\u0026rdquo;)\n  1、用户点击注按钮，将要注册的信息发送给网站服务器。\n2、Controller 控制器接收到用户的注册信息，Controller 会告诉 Model 层将用户的注册信息保存到数据库\n3、Model 层将用户的注册信息保存到数据库\n4、数据保存之后将保存的结果返回给 Model 模型，\n5、Model 层将保存的结果返回给 Controller 控制器。\n6、Controller 控制器收到保存的结果之后，或告诉 View 视图，view 视图产生一个 html 页面。\n7、View 将产生的 Html 页面的内容给了 Controller 控制器。\n8、Controller 将 Html 页面的内容返回给浏览器。\n9、浏览器接受到服务器 Controller 返回的 Html 页面进行解析展示。\nM：Model，模型，和 MVC 中的 M 功能相同，和数据库进行交互。\nV：view，视图，和 MVC 中的 C 功能相同，接收请求，进行处理，与 M 和 T 进行交互，返回应答。\nT：Template，模板，和 MVC 中的 V 功能相同，产生 Html 页面\n(https://img.alicdn.com/imgextra/i4/296192965/O1CN01Tt6wBa1Xm0yDyZc9u_!!296192965.png \u0026ldquo;null\u0026rdquo;)\n  1、用户点击注册按钮，将要注册的内容发送给网站的服务器。\n2、View 视图，接收到用户发来的注册数据，View 告诉 Model 将用户的注册信息保存进数据库。\n3、Model 层将用户的注册信息保存到数据库中。\n4、数据库将保存的结果返回给 Model\n5、Model 将保存的结果给 View 视图。\n6、View 视图告诉 Template 模板去产生一个 Html 页面。\n7、Template 生成 html 内容返回给 View 视图。\n8、View 将 html 页面内容返回给浏览器。\n9、浏览器拿到 view 返回的 html 页面内容进行解析，展示。\n 4. Django 中 models 利用 ORM 对 Mysql 进行查表的语句（多个语句）？  字段查询 all(): 返回模型类对应表格中的所有数据。\u0026gt; get(): 返回表格中满足条件的一条数据，如果查到多条数据，则抛异常：MultipleObjectsReturned，\u0026gt; 查询不到数据，则抛异常：DoesNotExist。 filter(): 参数写查询条件，返回满足条件 QuerySet 集合数据。\u0026gt; 条件格式：\n  模型类属性名 条件名 = 值\n注意：此处是模型类属性名，不是表中的字段名\n关于 filter 具体案例如下：\n判等 exact。\n BookInfo.object.filter(id=1) BookInfo.object.filter(id exact=1) 此处的 exact 可以省略   模糊查询 like 例：查询书名包含’传’的图书。contains contains BookInfo.objects.filter(btitle contains=’ 传’)\n  空查询 where 字段名 isnull BookInfo.objects.filter(btitle isnull=False)\n  范围查询 where id in(1，3，5) BookInfo.objects.filter(id in=[1，3，5])\n  比较查询 gt lt(less than) gte(equal) lte BookInfo.objects.filter(id gte=3)\n  日期查询\n BookInfo.objects.filter(bpub_date year = 1980) BookInfo.objects.filter(bpub_date gt = date(1980，1，1)) ```\u0026gt; exclude: 返回不满足条件的数据。``` BookInfo.objects.exclude(id=3)  F 对象  作用：用于类属性之间的比较条件。\n from django.db.models import F 例：where bread \u0026gt; bcomment BookInfo.objects.filter(bread gt =F(‘bcomment’)) 例：BookInfo.objects.filter(bread gt=F(‘bcomment’)*2)  Q 对象  作用：用于查询时的逻辑条件。可以对 Q 对象进行 \u0026amp; | ~ 操作。\n from django.db.models import Q BookInfo.objects.filter(id gt=3， bread gt=30) BooInfo.objects.filter(Q(id gt=3) \u0026amp; Q(bread gt=3)) 例：BookInfo.objects.filter(Q(id gt=3) | Q(bread gt=30)) 例：BookInfo.objects.filter(~Q(id=3))  order_by 返回 QuerySet  作用：对查询结果进行排序。\n 例： BookInfo.objects.all().order_by('id') 例： BookInfo.objects.all().order_by('-id') 例： BookInfo.objects.filter(id gt=3).order_by('-bread')  聚合函数  作用：对查询结果进行聚合操作。 sum count max min avg\n  aggregate：调用这个函数来使用聚合。\n from django.db.models import Sum，Count，Max，Min，Avg 例：BookInfo.objects.aggregate(Count('id'))   {id count’: 5} 注意返回值类型及键名\n 例：BookInfo.objects.aggregate(Sum(‘bread’))   {bread sum’:120} 注意返回值类型及键名\ncount 函数\n作用：统计满足条件数据的数目。\n例：统计所有图书的数目。 BookInfo.objects.all().count()\n  例：统计 id 大于 3 的所有图书的数目。 BookInfo.objects.filter(id gt = 3).count()\n 模型类关系  一对多关系\n例： 图书类 - 英雄类\nmodels.ForeignKey() 定义在多的类中。\n2）多对多关系\n例：新闻类 - 新闻类型类\nmodels.ManyToManyField() 定义在哪个类中都可以。\n3）一对一关系\n例：员工基本信息类 - 员工详细信息类\nmodels.OneToOneField() 定义在哪个类中都可以。\n 5. django 中间件的使用？  Django 在中间件中预置了六个方法，这六个方法的区别在于不同的阶段执行，对输入或输出进行干预，方法如下：\n   初始化：无需任何参数，服务器响应第一个请求的时候调用一次，用于确定是否启用当前中间件。   def init(): pass   处理请求前：在每个请求上调用，返回 None 或 HttpResponse 对象。   def process_request(request): pass   处理视图前：在每个请求上调用，返回 None 或 HttpResponse 对象。   def process_view(request, view_func, view_args, view_kwargs): pass   处理模板响应前：在每个请求上调用，返回实现了 render 方法的响应对象。   def process_template_response(request, response): pass   处理响应后：所有响应返回浏览器之前被调用，在每个请求上调用，返回 HttpResponse 对象。   def process_response(request, response): pass   异常处理：当视图抛出异常时调用，在每个请求上调用，返回一个 HttpResponse 对象。   def process_exception(request,exception): pass  6. 谈一下你对 uWSGI 和 nginx 的理解？ 1.uWSGI 是一个 Web 服务器，它实现了 WSGI 协议、uwsgi、http 等协议。Nginx 中HttpUwsgiModule 的作用是与 uWSGI 服务器进行交换。WSGI 是一种 Web 服务器网关接口。它是一个 Web 服务器（如 nginx，uWSGI 等服务器）与 web 应用（如用 Flask 框架写的程序）通信的一种规范。 要注意 WSGI /uwsgi/uWSGI 这三个概念的区分。\nWSGI 是一种通信协议。\nuwsgi 是一种线路协议而不是通信协议，在此常用于在 uWSGI 服务器与其他网络服务器的数据通信。\nuWSGI 是实现了 uwsgi 和 WSGI 两种协议的 Web 服务器。\nnginx 是一个开源的高性能的 HTTP 服务器和反向代理：  作为 web 服务器，它处理静态文件和索引文件效果非常高； 它的设计非常注重效率，最大支持 5 万个并发连接，但只占用很少的内存空间； 稳定性高，配置简洁； 强大的反向代理和负载均衡功能，平衡集群中各个服务器的负载压力应用。    7. 说说 nginx 和 uWISG 服务器之间如何配合工作的？  首先浏览器发起 http 请求到 nginx 服务器，Nginx 根据接收到请求包，进行 url 分析，判断访问的资源类型，如果是静态资源，直接读取静态资源返回给浏览器，如果请求的是动态资源就转交给 uwsgi 服务器，uwsgi 服务器根据自身的 uwsgi 和 WSGI 协议，找到对应的 Django 框架，Django 框架下的应用进行逻辑处理后，将返回值发送到 uwsgi 服务器，然后 uwsgi 服务器再返回给 nginx，最后 nginx 将返回值返回给浏览器进行渲染显示给用户。\n 如果可以，画图讲解效果更佳，可以将下面的图画给面试官。\n(https://img.alicdn.com/imgextra/i3/296192965/O1CN019iRedm1Xm0yBPGvMe_!!296192965.png \u0026ldquo;null\u0026rdquo;)\n8. django 开发中数据库做过什么优化？   设计表时，尽量少使用外键，因为外键约束会影响插入和删除性能；   使用缓存，减少对数据库的访问； 在 orm 框架下设置表时，能用 varchar 确定字段长度时，就别用 text； 可以给搜索频率高的字段属性，在定义时创建索引；\n5.Django orm 框架下的 Querysets 本来就有缓存的； 如果一个页面需要多次连接数据库，最好一次性取出所有需要的数据，减少对数据库的查询次数； 若页面只需要数据库里某一个两个字段时，可以用 QuerySet.values()； 在模板标签里使用 with 标签可以缓存 Qset 的查询结果。  9. 验证码过期时间怎么设置？ 将验证码保存到数据库或 session，设置过期时间为 1 分钟，然后页面设置一个倒计时(一般是前端 js 实现这个计时) 的展示，一分钟过后再次点击获取新的信息。\n10.Python 中三大框架各自的应用场景？ django：主要是用来搞快速开发的，他的亮点就是快速开发，节约成本，正常的并发量不过 10000， 如果要实现高并发的话，就要对 django 进行二次开发，比如把整个笨重的框架给拆掉，自己写 socket 实现http 的通信，底层用纯 c，c++ 写提升效率，ORM 框架给干掉，自己编写封装与数据库交互的框架，因为啥呢，ORM 虽然面向对象来操作数据库，但是它的效率很低，使用外键来联系表与表之间的查询；\nflask：轻量级，主要是用来写接口的一个框架，实现前后端分离，提升开发效率，Flask 本身相当于一个内核，其他几乎所有的功能都要用到扩展（邮件扩展 Flask-Mail，用户认证 Flask-Login），都需要用第三方的扩展来实现。比如可以用 Flask-extension 加入 ORM、窗体验证工具，文件上传、身份验证等。 Flask 没有默认使用的数据库，你可以选择 MySQL，也可以用 NoSQL。\n其 WSGI 工具箱采用 Werkzeug（路由模块），模板引擎则使用 Jinja2。这两个也是 Flask 框架的核心。Python 最出名的框架要数 Django，此外还有 Flask、Tornado 等框架。\n虽然 Flask 不是最出名的框架，但是 Flask 应该算是最灵活的框架之一，这也是 Flask 受到广大开发者喜爱的原因。\nTornado： Tornado 是一种 Web 服务器软件的开源版本。Tornado 和现在的主流 Web 服务器框架（包括大多数 Python 的框架）有着明显的区别：它是非阻塞式服务器，而且速度相当快。得利于其非阻塞的方式和对 epoll 的运用，Tornado 每秒可以处理数以千计的连接，因此 Tornado是实时 Web 服务的一个理想框架。\n11.django 如何提升性能（高并发）？ 对一个后端开发程序员来说，提升性能指标主要有两个一个是并发数，另一个是响应时间网站性能的优化一般包括 web 前端性能优化，应用服务器性能优化，存储服务器优化。\n对前端的优化主要有：\n 减少 http 请求，减少数据库的访问量，比如使用雪碧图。 使用浏览器缓存，将一些常用的 css，js，logo 图标，这些静态资源缓存到本地浏览器，通过设置 http 头中的 cache-control 和 expires 的属性，可设定浏览器缓存，缓存时间可以自定义。\n3 对 html，css，javascript 文件进行压缩，减少网络的通信量。  对我个人而言，我做的优化主要是以下三个方面：\n 合理的使用缓存技术，对一些常用到的动态数据，比如首页做一个缓存，或者某些常用的数据做个缓存，设置一定得过期时间，这样减少了对数据库的压力，提升网站性能。 使用 celery 消息队列，将耗时的操作扔到队列里，让 worker 去监听队列里的任务，实现异步操作，比如发邮件，发短信。 就是代码上的一些优化，补充：nginx 部署项目也是项目优化，可以配置合适的配置参数，提升效率，增加并发量。 如果太多考虑安全因素，服务器磁盘用固态硬盘读写，远远大于机械硬盘，这个技术现在没有普及，主要是固态硬盘技术上还不是完全成熟， 相信以后会大量普及。 另外还可以搭建服务器集群，将并发访问请求，分散到多台服务器上处理。 最后就是运维工作人员的一些性能优化技术了。  12. 什么是 restful api，谈谈你的理解？\u0026gt; REST:Representational State Transfer 的缩写，翻译：具象状态传输。一般解释为表现层状态转换。 REST 是设计风格而不是标准。是指客户端和服务器的交互形式。我们需要关注的重点是如何设计\nREST 风格的网络接口。\nREST 的特点：\n 具象的。一般指表现层，要表现的对象就是资源。比如，客户端访问服务器，获取的数据就是资源。比如文字、图片、音视频等。 表现：资源的表现形式。txt 格式、html 格式、json 格式、jpg 格式等。浏览器通过 URL 确定资源的位置，但是需要在 HTTP 请求头中，用 Accept 和 Content-Type 字段指定，这两个字段是对资源表现的描述。 状态转换：客户端和服务器交互的过程。在这个过程中，一定会有数据和状态的转化，这种转化叫做状态转换。其中，GET 表示获取资源，POST 表示新建资源，PUT 表示更新资源，DELETE 表示删除资源。HTTP 协议中最常用的就是这四种操作方式。  RESTful 架构：\n 每个 URL 代表一种资源； 客户端和服务器之间，传递这种资源的某种表现层； 客户端通过四个 http 动词，对服务器资源进行操作，实现表现层状态转换。  12.1 如何设计符合 RESTful 风格的 API\n一、域名：\n将 api 部署在专用域名下：\nhttp://api.example.com\n或者将 api 放在主域名下：\nhttp://www.example.com/api/\n二、版本：\n将 API 的版本号放在 url 中。\nhttp://www.example.com/app/1.0/info\nhttp://www.example.com/app/1.2/info\n三、路径：\n路径表示 API 的具体网址。每个网址代表一种资源。资源作为网址，网址中不能有动词只能有名词，一般名词要与数据库的表名对应。而且名词要使用复数。\n错误示例：\nhttp://www.example.com/getGoods\nhttp://www.example.com/listOrders\n正确示例：\nhttp://www.example.com/app/goods/1 # 获取单个商品 http://www.example.com/app/goods # 获取所有商品  四、使用标准的 HTTP 方法：\n对于资源的具体操作类型，由 HTTP 动词表示。常用的 HTTP 动词有四个。\nGET SELECT ：从服务器获取资源。\nPOST CREATE ：在服务器新建资源。PUT\nUPDATE ：在服务器更新资源。DELETE\nDELETE ：从服务器删除资源。\n示例：\nGET http://www.example.com/goods/ID # 获取指定商品的信息 POST http://www.example.com/goods # 新建商品的信息 PUT http://www.example.com/goods/ID #更新指定商品的信息 DELETE http://www.example.com/goods/ID # 删除指定商品的信息  五、过滤信息：\n如果资源数据较多，服务器不能将所有数据一次全部返回给客户端。API 应该提供参数，过滤返回结果。\n实例：\nhttp://www.example.com/goods?limit=10 # 指定返回数据的数量 http://www.example.com/goods?offset=10 # 指定返回数据的开始位置 http://www.example.com/goods?page=2\u0026amp;per_page=20 # 指定第几页，以及每页数据的数量  六、状态码：\n服务器向用户返回的状态码和提示信息，常用的有：\n200 OK ：服务器成功返回用户请求的数据\n201 CREATED ：用户新建或修改数据成功。\n202 Accepted：表示请求已进入后台排队。\n400 INVALID REQUEST ：用户发出的请求有错误。\n401 Unauthorized ：用户没有权限。\n403 Forbidden ：访问被禁止。\n404 NOT FOUND ：请求针对的是不存在的记录。\n406 Not Acceptable ：用户请求的的格式不正确。\n500 INTERNAL SERVER ERROR ：服务器发生错误。\n七、错误信息：一般来说，服务器返回的错误信息，以键值对的形式返回。\n{error: 'Invalid API KEY'}  八、响应结果：针对不同结果，服务器向客户端返回的结果应符合以下规范。\nGET http://www.example.com/goods # 返回商品列表 GET http://www.example.com/goods/cup # 返回单个商品 POST http://www.example.com/goods # 返回新生成的商品 DELETE http://www.example.com/goods # 返回一个空文档  九、使用链接关联相关的资源： 在返回响应结果时提供链接其他 API 的方法，使客户端很方便的获取相关联的信息。\n十、其他： 服务器返回的数据格式，应该尽量使用 JSON，避免使用 XML。\n13. 什么 csrf 攻击原理？如何解决？  (https://img.alicdn.com/imgextra/i1/296192965/O1CN01x5vuEX1Xm0yFqvPuh_!!296192965.png \u0026ldquo;null\u0026rdquo;)\n  简单来说就是：你访问了信任网站 A，然后 A 会用保存你的个人信息并返回给你的浏览器一个cookie，然后呢，在 cookie 的过期时间之内，你去访问了恶意网站 B，它给你返回一些恶意请求代码，要求你去访问网站 A，而你的浏览器在收到这个恶意请求之后，在你不知情的情况下，会带上保存在本地浏览器的 cookie 信息去访问网站 A，然后网站 A 误以为是用户本身的操作，导致来自恶意网站 C 的攻击代码会被执：发邮件，发消息，修改你的密码，购物，转账，偷窥你的个人信息，导致私人信息泄漏和账 户财产安全收到威胁\n 14. 启动 Django 服务的方法？  runserver 方法是调试 Django 时经常用到的运行方式，它使用 Django 自带的 WSGI Server 运行，主要在测试和开发中使用，并且 runserver 开启的方式也是单进程。\n 15. 怎样测试 django 框架中的代码？  在单元测试方面，Django 继承 python 的 unittest.TestCase 实现了自己的django.test.TestCase，编写测试用例通常从这里开始。测试代码通常位于 app 的 tests.py 文件中(也可以在 models.py 中编写，一般不建议）。在 Django 生成的 depotapp 中，已经包含了这个文件，并且其中包含了一个测试用例的样例：\n python manage.py test：执行所有的测试用例 python manage.py test app_name， 执行该 app 的所有测试用例 python manage.py test app_name.case_name: 执行指定的测试用例   一些测试工具：unittest 或者 pytest\n  有过部署经验？用的什么技术？可以满足多少压力？ 有部署经验，在阿里云服务器上部署的 技术有：nginx + uwsgi 的方式来部署 Django 项目 无标准答案（例：压力测试一两千）  16.Django 中哪里用到了线程？哪里用到了协程？哪里用到了进程？  1.Django 中耗时的任务用一个进程或者线程来执行，比如发邮件，使用 celery。\n 部署 django 项目的时候，配置文件中设置了进程和协程的相关配置。  17.django 关闭浏览器，怎样清除 cookies 和 session？  设置 Cookie\n def cookie_set(request): response = HttpResponse(\u0026quot;\u0026lt;h1\u0026gt; 设置 Cookie，请查看响应报文头 \u0026lt;/h1\u0026gt;\u0026quot;) response.set_cookie('h1', 'hello django') return response   读取 Cookie\n def cookie_get(request): response = HttpResponse(\u0026quot;读取 Cookie，数据如下：\u0026lt;br\u0026gt;\u0026quot;) if request.COOKIES.has_key('h1'): response.write('\u0026lt;h1\u0026gt;' + request.COOKIES['h1'] + '\u0026lt;/h1\u0026gt;') return response   以键值对的格式写会话\n request.session[' 键 ']= 值   根据键读取值。\n request.session.get(' 键 ', 默认值)   清除所有会话，在存储中删除值部分。\n request.session.clear()   清除会话数据，在存储中删除会话的整条数据。\n request.session.flush()   删除会话中的指定键及值，在存储中只删除某个键及对应的值。\n del request.session[' 键 ']   设置会话的超时时间，如果没有指定过期时间则两个星期后过期。\n如果 value 是一个整数，会话将在 value 秒没有活动后过期。\n如果 value 为 0，那么用户会话的 Cookie 将在用户的浏览器关闭时过期。\n  如果 value 为 None，那么会话永不过期。 request.session.set_expiry(value)\n  Session 依赖于 Cookie，如果浏览器不能保存 cookie 那么 session 就失效了。因为它需要浏览器的 cookie 值去 session 里做对比。session 就是用来在服务器端保存用户的会话状态。\ncookie 可以有过期时间，这样浏览器就知道什么时候可以删除 cookie 了。如果 cookie 没有设置过期时间，当用户关闭浏览器的时候，cookie 就自动过期了。你可以改变SESSION_EXPIRE_AT_BROWSER_CLOSE 的设置来控制 session 框架的这一行为。 缺省情况下，SESSION_EXPIRE_AT_BROWSER_CLOSE 设置为 False ，这样，会话 cookie 可以在用户浏览器中保持有效达 SESSION_COOKIE_AGE 秒（缺省设置是两周，即 1，209，600 秒）如果你不想用户每次打开浏览器都必须重新登陆的话，用这个参数来帮你。如果 SESSION_EXPIRE_AT_BROWSER_CLOSE设置为 True，当浏览器关闭时，Django 会使 cookie 失效。SESSION_COOKIE_AGE：设置 cookie 在浏览器中存活的时间。\n 18. 代码优化从哪些方面考虑？有什么想法？ 1、优化算法时间\n算法的时间复杂度对程序的执行效率影响最大，在 Python 中可以通过选择合适的数据结构来优化时间复杂度，如 list 和 set 查找某一个元素的时间复杂度分别是 O(n) 和 O(1)。不同的场景有不同的优化方式，总得来说，一般有分治，分支界限，贪心，动态规划等思想。\n2、循环优化\n每种编程语言都会强调需要优化循环。当使用 Python 的时候，你可以依靠大量的技巧使得循环运行得更快。然而，开发者经常漏掉的一个方法是：避免在一个循环中使用点操作。每一次你调用方法 str.upper，Python 都会求该方法的值。然而,如果你用一个变量代替求得的值，值就变成了已知的，Python 就可以更快地执行任务。优化循环的关键，是要减少 Python 在循环内部执行的工作量，因为 Python 原生的解释器在那种情况下，真的会减缓执行的速度。（注意：优化循环的方法有很多，这只是其中的一个。例如，许多程序员都会说，列表推导是在循环中提高执行速度的最好方式。这里的关键是，优化循环是程序取得更高的执行速度的更好方式之一。）\n3、函数选择\n在循环的时候使用 xrange 而不是 range；使用 xrange 可以节省大量的系统内存，因为 xrange() 在序列中每次调用只产生一个整数元素。而 range() 將直接返回完整的元素列表，用于循环时会有不必要的开销。在 python3 中 xrange 不再存在，里面 range 提供一个可以遍历任意长度的范围的iterator。\n4、并行编程\n因为 GIL 的存在，Python 很难充分利用多核 CPU 的优势。但是，可以通过内置的模块 multiprocessing 实现下面几种并行模式：\n多进程：对于 CPU 密集型的程序，可以使用 multiprocessing 的 Process，Pool 等封装好的类，通过多进程的方式实现并行计算。但是因为进程中的通信成本比较大，对于进程之间需要大量数据交互的程序效率未必有大的提高。\n多线程：对于 IO 密集型的程序，multiprocessing.dummy 模块使用 multiprocessing 的接口封装 threading，使得多线程编程也变得非常轻松(比如可以使用 Pool 的 map 接口，简洁高效)。\n分布式：multiprocessing 中的 Managers 类提供了可以在不同进程之共享数据的方式，可以在此基础上开发出分布式的程序。\n不同的业务场景可以选择其中的一种或几种的组合实现程序性能的优化。\n5、使用性能分析工具\n除了上面在 ipython 使用到的 timeit 模块，还有 cProfile。cProfile 的使用方式也非常简单：python-mcProfilefilename.py，filename.py 是要运行程序的文件名，可以在标准输出中看到每一个函数被调用的次数和运行的时间，从而找到程序的性能瓶颈，然后可以有针对性地优化。\n6、set 的用法\nset 的 union，intersection，difference 操作要比 list 的迭代要快。因此如果涉及到求 list 交集，并集或者差的问题可以转换为 set 来操作。\n7、PyPy\nPyPy 是用 RPython(CPython 的子集) 实现的 Python，根据官网的基准测试数据，它比 CPython 实现的 Python 要快 6 倍以上。快的原因是使用了 Just-in-Time(JIT) 编译器，即动态编译器，与静态编译器(如 gcc，javac 等) 不同，它是利用程序运行的过程的数据进行优化。由于历史原因，目前 pypy 中还保留着 GIL，不过正在进行的 STM 项目试图将 PyPy 变成没有 GIL 的 Python。如果 python程序中含有 C 扩展(非 cffi 的方式)，JIT 的优化效果会大打折扣，甚至比 CPython 慢（比 Numpy）。所以在 PyPy 中最好用纯 Python 或使用 cffi 扩展。\n19.Django 中间件是如何使用的？  中间件不用继承自任何类（可以继承 object），下面一个中间件大概的样子：\n class CommonMiddleware(object): def process_request(self, request): return None def process_response(self, request, response): return response   还有 process_view， process_exception 和 process_template_response 函数。\n 1） 初始化：无需任何参数，服务器响应第一个请求的时候调用一次，用于确定是否启用当前中间件。\ndef init(self): pass   2） 处理请求前：在每个请求上，request 对象产生之后，url 匹配之前调用，返回 None 或 HttpResponse 对象。\n def process_request(self， request): pass   3） 处理视图前：在每个请求上，url 匹配之后，视图函数调用之前调用，返回 None 或 HttpResponse 对象。\n def process_view(self， request， view_func， *view_args，**view_kwargs): pass   4） 处理响应后：视图函数调用之后，所有响应返回浏览器之前被调用，在每个请求上调用，返回 HttpResponse 对象。\n def process_response(self， request， response): pass   5） 异常处理：当视图抛出异常时调用，在每个请求上调用，返回一个 HttpResponse 对象。\n def process_exception(self， request，exception): pass  20. 有用过 Django REST framework 吗？  Django REST framework 是一个强大而灵活的 Web API 工具。使用 RESTframework 的理由有：\nWeb browsable API 对开发者有极大的好处\n包括 OAuth1a 和 OAuth2 的认证策略\n支持 ORM 和非 ORM 数据资源的序列化\n全程自定义开发 —— 如果不想使用更加强大的功能，可仅仅使用常规的 function-based views 额外的文档和强大的社区支持\n 21.Celery 分布式任务队列？  情景：用户发起 request，并等待 response 返回。在本些 views 中，可能需要执行一段耗时的程\n序，那么用户就会等待很长时间，造成不好的用户体验，比如发送邮件、手机验证码等。\n使用 celery 后，情况就不一样了。解决：将耗时的程序放到 celery 中执行。\n将多个耗时的任务添加到队列 queue 中，也就是用 redis 实现 broker 中间人，然后用多个 worker 去监听队列\n里的任务去执行。\n  (https://img.alicdn.com/imgextra/i4/296192965/O1CN01GdfgAg1Xm0yBQJZZX_!!296192965.png \u0026ldquo;null\u0026rdquo;)\n 任务 task：就是一个 Python 函数。 队列 queue：将需要执行的任务加入到队列中。 工人 worker：在一个新进程中，负责执行队列中的任务。 代理人 broker：负责调度，在布置环境中使用 redis。\n23.Jieba 分词  Jieba 分词支持三种分词模式：\n  精确模式：试图将句子最精确地切开，适合文本分析；\n全模式：把句子中所有的可以成词的词语都扫描出来， 速度非常快，但是不能解决歧义；\n搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词\n  功能：\n分词，添加自定义词典，关键词提取，词性标注，并行分词，Tokenize：返回词语在原文的起始位置，ChineseAnalyzer for Whoosh 搜索引擎。\n 24.ngnix 的正向代理与反向代理？  web 开发中，部署方式大致类似。简单来说，使用 Nginx 主要是为了实现分流、转发、负载均衡，以及分担服务器的压力。\n  Nginx 部署简单，内存消耗少，成本低。\n  Nginx 既可以做正向代理，也可以做反向代理。\n  正向代理：请求经过代理服务器从局域网发出，然后到达互联网上的服务器。\n特点：服务端并不知道真正的客户端是谁。\n反向代理：请求从互联网发出，先进入代理服务器，再转发给局域网内的服务器。\n特点：客户端并不知道真正的服务端是谁。\n区别：正向代理的对象是客户端。反向代理的对象是服务端。\n 25. 简述 Django 下的（内建的）缓存机制？  一个动态网站的基本权衡点就是，它是动态的。每次用户请求页面，服务器会重新计算。从开销处理的角度来看，这比你读取一个现成的标准文件的代价要昂贵的多。\n这就是需要缓存的地方。\nDjango 自带了一个健壮的缓存系统来保存动态页面这样避免对于每次请求都重新计算。方便起见，Django 提供了不同级别的缓存粒度：可以缓存特定视图的输出、可以仅仅缓存那些很难生产出来的部分、或者可以缓存整个网站 Django 也能很好的配合那些下游缓存， 比如 Squid 和基于浏览器的缓存。这里有一些缓存不必要直接去控制但是可以提供线索， (via HTTPheaders) 关于网站哪些部分需要缓存和如何缓存。\n设置缓存：\n缓存系统需要一些设置才能使用。也就是说，你必须告诉他你要把数据缓存在哪里 - 是数据库中，文件系统或者直接在内存中。这个决定很重要，因为它会影响你的缓存性能，是的，一些缓存类型要比其他的缓存类型更快速。\n你的缓存配置是通过 setting 文件的 CACHES 配置来实现的。这里有 CACHES 所有可配置的变量值。\n  参考文档：https://yiyibooks.cn/xx/django_182/topics/cache.html\n 26. 请简述浏览器是如何获取一枚网页的？  在用户输入目的 URL 后，浏览器先向 DNS 服务器发起域名解析请求； 在获取了对应的 IP 后向服务器发送请求数据包； 服务器接收到请求数据后查询服务器上对应的页面，并将找到的页面代码回复给客户端； 客户端接收到页面源代码后，检查页面代码中引用的其他资源，并再次向服务器请求该资源； 在资源接收完成后，客户端浏览器按照页面代码将页面渲染输出显示在显示器上；  27. 对 cookie 与 session 的了解？他们能单独用吗？  Session 采用的是在服务器端保持状态的方案，而 Cookie 采用的是在客户端保持状态的方案。但是禁用 Cookie 就不能得到 Session。因为 Session 是用 Session ID 来确定当前对话所对应的服务器Session，而 Session ID 是通过 Cookie 来传递的，禁用 Cookie 相当于失去了 SessionID，也就得不到 Session。\n 28.Django HTTP 请求的处理流程？Django 和其他 Web 框架的 HTTP 处理的流程大致相同，Django 处理一个 Request 的过程 是首先通过中间件，然后再通过默认的 URL 方式进行的。我们可以在 Middleware 这个地方把所有 Request 拦截住，用我们自己的方式完成处理以后直接返回 Response。\n  加载配置\nDjango 的配置都在 “Project/settings.py” 中定义，可以是 Django 的配置，也可以是自定义的配置，并且都通过 django.conf.settings 访问，非常方便。\n  启动\n最核心动作的是通过 django.core.management.commands.runfcgi 的 Command 来启动，它运行 django.core.servers.fastcgi 中的 runfastcgi，runfastcgi 使用了 flup 的 WSGIServer 来启动 fastcgi 。而 WSGIServer 中携带了 django.core.handlers.wsgi 的 WSGIHandler 类的一个实例，通过 WSGIHandler 来处理由 Web 服务器（比如 Apache，Lighttpd 等）传过来的请求，此时才是真正进入 Django 的世界。\n  处理 Request\n当有 HTTP 请求来时，WSGIHandler 就开始工作了，它从 BaseHandler 继承而来。WSGIHandler 为每个请求创建一个 WSGIRequest 实例，而 WSGIRequest 是从http.HttpRequest 继承而来。接下来就开始创建 Response 了。\n  创建 Response\nBaseHandler 的 get_response 方法就是根据 request 创建 response，而具体生成 response 的动作就是执行 urls.py 中对应的 view 函数了，这也是 Django 可以处理 “友好 URL” 的关键步骤，每个这样的函数都要返回一个 Response 实例。此时一般的做法是通过 loader 加载template 并生成页面内容，其中重要的就是通过 ORM 技术从数据库中取出数据，并渲染到Template 中，从而生成具体的页面了。\n  处理 Response\nDjango 返回 Response 给 flup，flup 就取出 Response 的内容返回给 Web 服务器，由后者返回给浏览器。总之，Django 在 fastcgi 中主要做了两件事：处理 Request 和创建 Response，而它们对应的核心就是 “urls 分析”、“模板技术” 和 “ORM 技术”。\n   (https://img.alicdn.com/imgextra/i2/296192965/O1CN01M8T4ev1Xm0yC0qYUi_!!296192965.png \u0026ldquo;null\u0026rdquo;)\n  如图所示，一个 HTTP 请求，首先被转化成一个 HttpRequest 对象，然后该对象被传递给Request 中间件处理，如果该中间件返回了 Response，则直接传递给 Response 中间件做收尾处理。否则的话 Request 中间件将访问 URL 配置，确定哪个 view 来处理，在确定了哪个 view 要执行，但是还没有执行该 view 的时候，系统会把 request 传递给 view 中间件处理器进行处理，如果该中间件返回了 Response，那么该 Response 直接被传递给 Response 中间件进行后续处理，否则将执行确定的 view 函数处理并返回 Response，在这个过程中如果引发了异常并抛出，会被 Exception 中间件处理器进行处理。\n 29.Django 里 QuerySet 的 get 和 filter 方法的区别？   输入参数\nget 的参数只能是 model 中定义的那些字段，只支持严格匹配。\nfilter 的参数可以是字段，也可以是扩展的 where 查询关键字，如 in，like 等。\n  返回值\nget 返回值是一个定义的 model 对象。\nfilter 返回值是一个新的 QuerySet 对象，然后可以对 QuerySet 在进行查询返回新的 QuerySet 对象，支持链式操作，QuerySet 一个集合对象，可使用迭代或者遍历，切片等，但是不等于 list 类型(使用一定要注意)。\n  异常\nget 只有一条记录返回的时候才正常，也就说明 get 的查询字段必须是主键或者唯一约束的字段。当返回多条记录或者是没有找到记录的时候都会抛出异常\nfilter 有没有匹配的记录都可以\n  30.django 中当一个用户登录 A 应用服务器（进入登录状态），然后下次请求被 nginx 代理到 B 应用服务器会出现什么影响？  如果用户在 A 应用服务器登陆的 session 数据没有共享到 B 应用服务器，那么之前的登录状态就没有了。\n 31. 跨域请求问题 django 怎么解决的（原理） 启用中间件 post 请求 验证码 表单中添加 csrf_token 标签\n32.Django 对数据查询结果排序怎么做，降序怎么做，查询大于某个字段怎么做？* \u0026gt; 排序使用 order_by() 降序需要在排序字段名前加 - 查询字段大于某个值：使用 filter(字段名_gt = 值)\n33.Django 重定向你是如何实现的？用的什么状态码？ 使用 HttpResponseRedirect redirect 和 reverse 状态码：302,301\n34. 生成迁移文件和执行迁移文件的命令是什么？ python manage.py makemigrations python manage.py migrate  35. 关系型数据库的关系包括哪些类型？ ForeignKey：一对多，将字段定义在多的一端中。 ManyToManyField：多对对：将字段定义在两端中。 OneToOneField：一对一，将字段定义在任意一端中。\n36. 查询集返回列表的过滤器有哪些？ all() ：返回所有的数据 filter()：返回满足条件的数据 exclude()：返回满足条件之外的数据，相当于 sql 语句中 where 部分的 not 关键字 order_by()：排序\n37. 判断查询集正是否有数据？  exists()：判断查询集中否有数据，如果有则返回 True，没有则返回 False。\n 38.Django 本身提供了 runserver，为什么不能用来部署？  runserver 方法是调试 Django 时经常用到的运行方式，它使用 Django 自带的 WSGI Server 运行，主要在测试和开发中使用，并且 runserver 开启的方式也是单进程。\n  uWSGI 是一个 Web 服务器，它实现了 WSGI 协议、uwsgi、http 等协议。注意 uwsgi 是一种通信协议，而 uWSGI 是实现 uwsgi 协议和 WSGI 协议的 Web 服务器。uWSGI 具有超快的性能、低内存占用和多 app 管理等优点，并且搭配着 Nginx 就是一个生产环境了，能够将用户访问请求与应用 app 隔离开，实现真正的部署。相比来讲，支持的并发量更高，方便管理多进程，发挥多核的优势， 提升性能。\n 39.apache 和 nginx 的区别？  Nginx 相对 Apache 的优点：\n轻量级，同样起 web 服务，比 apache 占用更少的内存及资源；\n抗并发，nginx 处理请求是异步非阻塞的，支持更多的并发连接，而 apache 则是阻塞型的，在高并发下 nginx 能保持低资源低消耗高性能；\n配置简洁；\n高度模块化的设计，编写模块相对简单；\n社区活跃。\nApache 相对 Nginx 的优点：\nrewrite ，比 nginx 的 rewrite 强大；\n模块超多，基本想到的都可以找到；\n少 bug ，nginx 的 bug 相对较多；\n超稳定。\n 40.varchar 与 char 的区别？  char 长度是固定的，不管你存储的数据是多少他都会都固定的长度。而 varchar 则处可变长度但他要在总长度上加 1 字符，这个用来存储位置。所以在处理速度上 char 要比 varchar 快速很多，但是对费存储空间，所以对存储不大，但在速度上有要求的可以使用 char 类型，反之可以用 varchar 类型。\n 41. 查询集两大特性？惰性执行？  惰性执行、缓存。\n创建查询集不会访问数据库，直到调用数据时，才会访问数据库，调用数据的情况包括迭代、序列化、与 if 合用\n 42.git 常用命令？  git clone 克隆指定仓库\ngit status 查看当前仓库状态\ngit diff 比较版本的区别\ngit log 查看 git 操作日志\ngit reset 回溯历史版本\ngit add 将文件添加到暂存区\ngit commit 将文件提交到服务器\ngit checkout 切换到指定分支 git rm 删除指定文件\n 43. 电商网站库存问题 一般团购，秒杀，特价之类的活动，这样会使访问量激增，很多人抢购一个商品，作为活动商品，库存肯定是很有限的。控制库存问题，数据库的事务功能是控制库存超卖的有效方式。\n 在秒杀的情况下，肯定不能如此频率的去读写数据库，严重影响性能问题，必须使用缓存，将需要秒杀的商品放入缓存中，并使用锁来处理并发情况，先将商品数量增减（加锁、解析）后在进行其他方面的处理，处理失败再将数据递增（加锁、解析）, 否则表示交易成功。 这个肯定不能直接操作数据库的，会挂的。直接读库写库对数据库压力太大了，要用到缓存。 首先，多用户并发修改同一条记录时，肯定是后提交的用户将覆盖掉前者提交的结果了。这个直接可以使用加乐观锁的机制去解决高并发的问题。  44.HttpRequest 和 HttpResponse 是什么？干嘛用的？  HttpRequest 是 django 接受用户发送多来的请求报文后，将报文封装到 HttpRequest 对象中去。\nHttpResponse 返回的是一个应答的数据报文。render 内部已经封装好了 HttpResponse 类。 视图的第一个参数必须是 HttpRequest 对象，两点原因：表面上说，他是处理 web 请求的，所以必须是请求对象，根本上说，他是基于请求的一种 web 框架，所以，必须是请求对象。\n因为 view 处理的是一个 request 对象，请求的所有属性我们都可以根据对象属性的查看方法来获取具体的信息：格式：request. 属性\nrequest.path 请求页面的路径，不包含域名\nrequest.get_full_path 获取带参数的路径\nrequest.method 页面的请求方式\nrequest.GET GET 请求方式的数据\nrequest.POST POST 请求方式的数据\nrequest.COOKIES 获取 cookie\nrequest.session 获取 session\nrequest.FILES 上传图片（请求页面有 enctype=multipart/form-data 属性时 FILES 才有数据。\n？a=10 的键和值时怎么产生的，键是开发人员在编写代码时确定下来的，值时根据数据生成或者用户填写的，总之是不确定的。\n403 错误：表示资源不可用，服务器理解客户的请求，但是拒绝处理它，通常由于服务器上文件和目录的\n权限设置导致的 web 访问错误。如何解决：1、把中间件注释。2、在表单内部添加 {% scrf_token %}\nrequest.GET.get() 取值时如果一键多值情况，get 是覆盖的方式获取的。getlist（）可以获取多个值。\n在一个有键无值的情况下，该键名 c 的值返回空。有键无值：c: getlist 返回的是列表，空列表\n在无键无值也没有默认值的情况下，返回的是 None 无键无值：e:None\nHttpResponse 常见属性：\ncontent： 表示返回的内容\ncharset: 表示 response 采用的编码字符集，默认是 utf-8\nstatus_code: 返回的 HTTP 响应状态码 3XX 是对请求继续进一步处理，常见的是重定向。\n常见方法：\ninit: 创建 httpResponse 对象完成返回内容的初始化\nset_cookie：设置 Cookie 信息：格式：set_cookies(key’,’value’,max_age=None,expires=None)\nmax_age 是一个整数，表示指定秒数后过期，expires 指定过期时间，默认两个星期后过期。\nwrite 向响应体中写数据\n应答对象：\n方式一：render(request,index.html) 返回一个模板\nrender(request,index.html, context) 返回一个携带动态数据的页面\n方式二：render_to_response(index.html) 返回一个模板页面\n方式三：redirect(/) 重定向\n方式四：HttpResponseRdeirect(/) 实现页面跳转功能\n方式五：HttpResponse（itcast1.0) 在返回到额页面中添加字符串内容\n方式六：HttpResponseJson() 返回的页面中添加字符串内容。\nJsonResponse 创建对象时候接收字典作为参数，返回的对象是一个 json 对象。\n能接收 Json 格式数据的场景，都需要使用 view 的 JsonResponse 对象返回一个 json 格式数据\najax 的使用场景，页面局部刷新功能。ajax 接收 Json 格式的数据。\n在返回的应答报文中，可以看到 JsonResponse 应答的 content-Type 内容是 application/json\najax 实现网页局部刷新功能：ajax 的 get() 方法获取请求数据 ajax 的 each() 方法遍历输出这些数据\n 45. 什么是反向解析  使用场景：模板中的超链接，视图中的重定向\n使用：在定义 url 时为 include 定义 namespace 属性，为 url 定义 name 属性\n  在模板中使用 url 标签：{% url namespace_value:name_value’%}\n在视图中使用 reverse 函数：redirect(reverse(namespce_value:name_value’))\n根据正则表达式动态生成地址，减轻后期维护成本。\n注意反向解析传参数，主要是在我们的反向解析的规则后面天界了两个参数，两个参数之间使用空格隔开：位置参数\n 46.Django 日志管理：  配置好之后：\n import logging logger=logging.getLogger(name) # 为 loggers 中定义的名称 logger.info(\u0026quot;some info ...)   可用函数有：logger.debug() logger.info() logger.warning() logger.error()\nDjango 文件管理：对于 jdango 老说，项目中的 css，js, 图片都属于静态文件，我们一般会将静态文件放到一个单独的目录中，以方便管理，在 html 页面调用时，也需要指定静态文件的路径。静态文件可以放在项目根目录下，也可以放在应用的目录下，由于这些静态文件在项目中是通用的，所以推荐放在项目的根目录下。\n在生产中，只要和静态文件相关的，所有访问，基本上没有 django 什么事，一般都是由 nignx 软件代劳了，为什么？因为 nginx 就是干这个的。\n .uWSGI 与 uwsgi 区别  uWSGI 是一个 Web 服务器，它实现了 WSGI 协议、uwsgi、http 等协议。注意 uwsgi 是一种通信协议，而 uWSGI 是实现 uwsgi 协议和 WSGI 协议的 Web 服务器。uWSGI 具有超快的性能、低内存占用和多 app 管理等优点，并且搭配着 Nginx 就是一个生产环境了，能够将用户访问请求与应用 app 隔离开，实现真正的部署。相比来讲，支持的并发量更高，方便管理多进程，发挥多核的优势，提升性能。\n . 什么是 gitlab,github 和 gitlab 的区别？  ** 参考博客:**https://blog.csdn.net/zhang_oracle/article/details/77317717\n . git 中 .gitignore 文件的作用？  ** 参考博客:**https://www.cnblogs.com/kevingrace/p/5690241.html\n 三、Tornado 1. Tornado 的核是什么？ Tornado 的核心是 ioloop 和 iostream 这两个模块，前者提供了一个高效的 I/O 事件循环，后者则封装了一个无阻塞的 socket 。 通过向 ioloop 中添加网络 I/O 事件，利用无阻塞的 socket，再搭配相应的回调函数，便可达到梦寐以求的高效异步执行。\n","date":"2022-10-25","permalink":"/post/web_framework/","tags":["InterviewQuestions","Python"],"title":"web_framework"},{"content":"跨域 当一个请求url的协议、域名、端口三者之间的任意一个与当前页面url不同即为跨域。\n跨域产生原因？ 出于浏览器的同源策略限制。\n同源策略（Same Orgin Policy） 是一种约定，它是浏览器核心也最基本的安全功能，它会阻止一个域的js脚本和另外一个域的内容进行交互，如果缺少了同源策略，浏览器很容易受到XSS、CSFR等攻击。\n所谓同源（即在同一个域）就是两个页面具有相同的协议（protocol）、主机（host）和端口号（port）。\n非同源会出现的限制 无法读取非同源网页的cookie、localstorage等 无法接触非同源网页的DOM和js对象 无法向非同源地址发送Ajax请求\n跨域的分类 跨域分为以下3种\n名称\t英文名\t说明 简单请求\tSimple Request\t发起的Http请求符合： 1.无自定义请求头， 2.请求动词为GET、HEAD或POST之一， 3.动词为POST时，Content-Type是application/x-www-form-urlencoded， multipart/form-data或text/plain之一 预检请求\tPreflighted Request\t发起的Http请求符合其中之一： 1.包含了自定义请求头， 2.请求动词不是GET、HEAD或POST， 3.动词是POST时， Content-Type不是application/x-www-form-urlencoded， multipart/form-data或text/plain。 即：简单请求的相反 凭证请求\tRequests with Credential\t发起的Http请求中带有凭证\n跨域解决方案 https://qiufeng.blue/frontend/cors.html\n","date":"2022-10-25","permalink":"/post/%E8%B7%A8%E5%9F%9F/","tags":["FrontEnd"],"title":"跨域"},{"content":"今天是周末，按照惯例小川一般都会推送些相对轻松的内容。但是昨晚小川偶尔发现这部拍摄差不多十年前，反思中国贫困阶层教育问题的纪录片《父亲》时，触动很深，无论如何也想要第一时间分享推荐给大家。 我一直都有个观点，成功绝大多数不可以复制，所以成功学的东西看得越少越好，以免被带歪带偏。而失败的案例要多听多看多想，然后以此为警，时刻提醒自己不要重蹈覆辙。 这部影片故事的发生时间，正好是中国城镇化历史进程全面提速的时间点。而故事中的父子主角，用近乎令人“心痛”的方式，向我们展现了未能跟上中国城镇化脚步，家庭阶层命运继续保持贫穷的案例。 先简要介绍下纪录片的背景和内容：\n一个偶然的机会，导演李军虎遇到了典型的“中国式父亲”，并把他的故事拍成了一部时长47分钟的纪录片《父亲》。纪录片的主人公叫韩培印，为了让儿子上学而来到西安打工，对儿子充满了期望。该片在2009年的第二届香港华语纪录片节上，获得了最佳短片大奖。\n2002年，韩培印的儿子胜利考上了西安一所大学，从农村来到城市。为了让胜利上学，老韩变卖了家里所有值钱的东西，到西安打工，为儿子解决大学的生活费和来年的学费。做了一辈子农民的老韩坚信知识会改变命运，他乐观地盼望着，儿子将来一定会有出息。\n来城里打工的人越来越多，工作也就越来越难找，每月400元收入基本没法保障，可儿子胜利每年需要9000元的学费。老韩住的地方，一块钱一晚，没有枕头，每天枕着砖头入睡。儿子胜利则每天都在想下课之后可以吃什么，同时盯着同学手中的矿泉水瓶，考虑是否可以拿来卖钱。胜利面临毕业，性格内向的他面对强大的就业压力，感觉这个城市越来越远，上过大学的他很可能赚得比父亲还要少。\n一、典型的中国式父亲\n实话实说，我不知道如何评价纪录片中的“父亲”韩培印。\n一方面，为了儿子韩胜利能够念大学，他卖掉了家里最值钱的牛和粮食，还跑去西安做苦力赚钱给儿子读大学。正是这种砸锅卖铁供儿子读书的行为，所以才有了这部纪录片《父亲》的诞生。\n但是另一个方面，父亲韩培印也是彻底毁掉韩胜利一生的根源。他用极其错误的教育方法和人生三观，将儿子韩胜利一步步拉入和他一样几乎一生注定贫穷的万丈深渊。\n而最让人感到心酸和痛苦的是，你还完全无法对他过多进行批评和指摘。因为他所做的一切错误教育，在他的思维和三观中都是“正确”的，这更像是一场“无心之过”。\n这种感觉，一度让我想起近代史中清后期的闭关锁国。相比中国历朝历代的皇帝，清朝的所有皇帝在勤奋这一点上，是毋庸置疑的。但是就是这样一代代勤奋的皇帝，却没有看到世界的变化和时代的趋势，在决定国家与民族命运的关键时间点，封闭了国门，也酿成了中华民族历史上最惨痛和耻辱的百余年历史。\n二、典型的中国穷人教育思维\n整部纪录片中，父亲韩培印始终不离手的，是一个小的笔记本。相比同村出来的其他农名工，韩胜利算是读过书，因为他识字，而且还很喜爱写写对联或是给儿子写信。\n如果不是他读过书，他或许不会意识到读书的重要性。也就没有后来的所谓“砸锅卖铁”供儿子读大学。但很遗憾的是，他的书读的太过局限，他只从书中读到了“拼命刻苦”，但是却没有意识到“天下大势，浩浩荡荡”的时代趋势。\n为什么要提父亲韩培印手中的这本笔记本，因为这个笔记本，可以说是韩培印教育思维的符号。在这本笔记上，韩培印密密麻麻写了很多内容，其中有很多的借款信息。\n由于儿子韩胜利上大学需要定期来拿生活费，而打零工的收入又不固定。所以当儿子来拿生活费而他手中暂时没钱的时候，他就会向身边的工友借。多的一两百，少则二三十，等到还了钱再把笔记本上的信息划掉。\n韩培印给自己制定的生活费标准是每月150元，于是他以此为参考，给儿子韩胜利的生活费标准是200元。而韩胜利依照这个标准，一日三餐的开销是：早餐1块多，午餐和晚餐各2块多，荤菜几天吃一次。\n除吃之外，韩胜利再没有其他任何开支。只有在学校要求购买学习材料时，才会找父亲韩培印。父亲的态度和对待吃饭时“不要吃太差也别吃太好”一样，告诉韩胜利材料“不要不买也不要买太多”。\n三、如何彻底击垮年轻人的心理防线\n我为什么称之纪录片中的父亲是典型的那个时代的“中国式父亲”代表？我们来看看他是教育自己的儿子韩胜利的——\n1.持续性向儿子表示你上学花了家里多少钱。一方面，向子女表示我们“砸锅卖铁”也会供你读书，另一方面会持续高频向子女展示自己为了让你读书我吃了多少苦、受了多少罪。而当子女表示那干脆就放弃读书时，又表示出大义凛然。这种内心渴望从子女上大学事件上获得面子上的荣誉，以及未来有可能从事高收入工作从而改变自身和家庭命运的渴望，被隐藏在“父爱无疆”的幌子下持续，最终形成畸形的家庭和教育观。\n在父母与子女之间，并不存在父母养育子女完全是单向的付出毫无收获，孩子带给父母的陪伴，孩子的成长进步，孩子的无限希望，统统都是父母在养育子女这件事上的持续性回报。双方关系对等，不存在谁付出更多之说。\n2.持续性向儿子表示我们有多穷。哭穷教育，是过去几十年甚至更久时间上，大范围毁掉无数中国年轻人的最具破坏力教育思维与方式。因为我们很穷，所以你就必须怎么做，这显然是彻底打碎一名年轻人脊梁和膝盖的最残忍方式。\n纪录片中，韩胜利所在的寝室六人中，有三台电脑，五部手机还有MP3等工具。而面对镜头的韩胜利时说，在上大学前他从未见过这些东西，宿舍的公用电话几乎是他一人在用。韩胜利的衣服是同寝看他没衣服时送他的，韩胜利学习之外做的最多的一件事，是在学校里捡废品补贴家用。\n你问为什么韩胜利不去兼职赚钱？第一是父亲韩培印不允许，他表示他可以赚钱给韩胜利生活费（每月200元），韩胜利只需要专心读书；第二是韩胜利所在的学校位于西安远郊户县（现鄠邑区），即使现在也依旧勉强算是县城的经济水平，十年前基本可以理解为是“荒郊野岭”，想兼职也找不到；第三是贫困和自卑早已深入韩胜利的骨髓，他连最基础的社交能力都不具备。\n生育权始终都掌握在父母手中，越是处于社会下阶层的家庭，越会高度渴望从子女身上获得更多面子上和金钱上的回报。明明知道依照自身的家境和资源无力培养出“光宗耀祖”的子女，却不断向子女施加压力，将畸形的“出人头地”观念注入他们残缺不整的人生观中。在过去我接触的很多创业失败者，创业根源都并不是来自于看到了巨大的商机或是远大的理想，而是来自于迫切渴望“出人头地”的心理需求。当然，他们基本也全都失败了。\n而这种具有高腐蚀性和破坏性的观念一旦落定，轻者影响一个人的下个十年，重则终其一生都无法从这种永远无法实现家庭厚望的压力中挣脱。原本可以进入一家不错的公司脚踏实地，步步为营，却在错误的道路上越走越远。\n四、除了读书，我还会什么？\n到纪录片的后半部分，韩胜利大学毕业在即，前往招聘会上寻找工作。但是几乎没有社交能力的韩胜利，在招聘会上全程都没有说过一句话。\n作为通讯专业的毕业生，社交的这一弊端先放在一边，韩胜利还有一个巨大的短板，他几乎不会使用电脑和互联网。因为父亲眼中电脑是“洪水猛兽”，是让无数的好孩子走向堕落的魔鬼。\n作为对计算机和互联网高度关联的通讯专业却电脑水平几乎为零，这就基本和学习英语专业但是口语完全不会一个道理。不难想象，韩胜利如果想从事本专业的工作会有多难。\n在纪录片中，韩胜利对于自己职场的迷茫同样令人心碎。他说，600块一月的工作如果找不到的话就找300块的，300块的工作找不到话那就不给钱也跟着干，只要有人要我。或者我就去做服务员、保洁、保安。\n记者问他，你如果大学毕业就从事这些岗位的话，你的父母能接受吗？韩胜利的回答是：他们不能接受也必须得接受，因为这就是社会的现实。\n为什么一名颇有行业前景的通讯专业毕业生，在求职时只能想到的是服务员、保洁、保安这样的岗位？原因不外乎就是，在他过往二十多年的人生当中，他目力所能及的工作岗位就只有这些而已。\n非常多的工作岗位和机会，一定要在相应的城市才会存在。这件事不仅十年前是这样，十年后的今天依旧是这样。比如现在做互联网、做物联网、做人工智能、做大数据、做区块链、做虚拟现实、做金融科技。那么你离开经济最靠前的那些城市，就是最不明智的选择。\n五、时代给予的城镇化机会\n我们一起来看看十年前，主角一家人的收入情况——\n几乎没读过书的父亲韩培印在西安出苦力打零工，每月的收入大约是500~800元之间。\n在西安石油大学通讯专业毕业的儿子韩胜利，先是到青海装宽带每月月薪600元。工作三年后回到西安，找到一份月薪1500元的工作。\n中学毕业即远赴深圳工厂打工的女儿韩明利，月收入已经有3000~4000元。\n从工作的辛苦程度上看，无论是工地出苦力的韩培印，还是在偏远山区装宽带的韩胜利，或是在工厂里做工人的韩明利，其付出应该说是相差无几的。但是为什么反映在收入上，差距就会如此悬殊？尤其是最让人难以接受的，是费尽千辛万苦考上大学的韩胜利，只能找到一份月薪几百或一千多的工作？\n原因是多方面的，但最核心的点，就在于因为没有看到中国城镇化的大势所趋，选择留在青海和陕西工作。小川做个假设，如果通讯专业的韩胜利，能够在当年奔赴深圳寻找工作，如果有幸还能加入当年的华为这样的企业。十年不需要他有多高的天赋，多强的能力，只要踏踏实实做好一份工程师的工作，收入都将是不菲的数字。对于他的出身而言，那决然可以称得上是逆袭命运。\n如果说城镇化进程是浪潮，那重要城市是船，而我们尽是船上之人。你敢想象你凭借一己之力靠游泳，追得上一艘艘装备精良、人才齐备，而且还顺风顺浪的舰艇么？\n而关于韩胜利的婚恋方面，从镜头中我们能够看到的就只有他本人憨憨的傻笑带过。\n最后，小川强烈建议所有的朋友都看看这部纪录片，尤其是家在经济落后的小城市的寒门子弟，即将谋划毕业后发展方向的大学生，还有为人父母的朋友们。\n为什么现在穷人的孩子很难翻身，往往考了大学也没有用还是过着穷人的生活？ 1.成熟晚。他们中的大多数要到30多岁的时候才能明白整个社会的运行逻辑和人性。之前俗话总说，穷人家的孩子早当家，其实，那哪里是早当家啊，只不过在极端环境下掌握了一些基本的生存技巧而已。\n2.无有效的指导。一个几代人穷且老实的家庭，在30岁之前人生最大的阻力来自于自己的家人，家人和人脉认知的局限性会在各种各样的事情上帮倒忙或给出错误的指导。之所以很多穷人家比较叛逆的孩子容易成功，就是因为他们早早就摆脱了家庭的束缚。富裕家庭的孩子，在人生的关键节点上，有人指点那么几下子，很可能就改变了一生。但穷人家的孩子只能自己摸爬滚打，没有人能帮她，因为上一代人没有足够的认识层次和人脉，不能够给下一代人指引和帮助。\n3.没有试错资本。有钱家的孩子，可以试错，不断试不断成长，实践出真知，一般家庭的孩子或许可以试错一两次，但大部分错一次就跌入谷底，然后起起伏伏，直到自己撞得头破血流了，尝尽疾苦才能明白一些规则或积累一点资本，却被社会磨尽了锐气，像可怜的闰土，背负着自己的家庭，跌入轮回的循环。\n4.时间成本。认知层次的领悟和传承，资本的积累与继承，都是需要时间的，老鼠的孩子会打洞，穷人家的孩子所有的一切都只能自己花时间去积累，学习，领悟，去提高一些层次，获得一些资本!然而，很多大好的青春岁月和美好的事物就这样消失了，以及没有时间顾及下一代，孩子潜意识造成的缺陷永远难以弥补。别人的顺利，都是上一代人拒绝安稳、拼搏奋斗、不断试错传承来的。 所以穷人的孩子最需要做的是突破思维的极限，能读书的好好读书，不能读书的去读天地的大到，否则不仅是财富积累的不容易，更大的是认识层次难跨越。\n","date":"2022-04-11","permalink":"/post/%E8%BF%99%E9%83%A8%E7%BA%AA%E5%BD%95%E7%89%87%E5%91%8A%E8%AF%89%E4%BD%A0%E7%A9%B7%E4%BA%BA%E6%98%AF%E5%A6%82%E4%BD%95%E6%8A%8A%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%95%99%E8%82%B2%E6%88%90%E6%8C%81%E7%BB%AD%E8%B4%AB%E7%A9%B7%E7%9A%84/","tags":["Thought"],"title":"这部纪录片告诉你,穷人是如何把下一代教育成持续贫穷的"},{"content":"有很多人反对学英语，理由是，从小到大我们学了那么多年的英语，还是学 得不怎么样，不如不学。这个理由是经不起推敲的。如果英语很重要，却没学好， 那么应该是要想方设法尽量学好，而不是说干脆不学了，那不是因噎废食吗？\n英语学不好原因很多，例如学习的目的和学习的方法都有问题。学英语有不 同的目的，为了当翻译，为了以后搞科研，为了应付考试，为了跟外国人交流， 为了阅读英语的材料增加对信息的掌握……根据不同的学习目的，相应地就有不 同的学习要求，在英语的说、听、写、读这四方面就会有不同的侧重。\n先说“说”。以前中国人学英语被说成是学的是哑巴英语，不会说，后来反 过来了，变得特别地强调要会说。有各种各样的英语培训班，都是以“说纯正的 英语”作为招牌，重金聘请外教，学员重金进这种培训班，为了能够向外教学习 纯正的英国口音、美国口音。这就导致很多人认为，英语发音很标准，说明口语 好；讲英语有口音，就说明英语口语不好，甚至认为英语不行，还怎么去跟外国 人交流？\n其实，“口音”和“口语”并不是一回事。发音只是口语中的一小部分，不 能因为一个人讲英语有口音，就认为这个人英语口语不好，甚至说他英语不好。 就像我说普通话，很多人也都抱怨说我普通话有口音，以前《南方周末》为了抹 黑我干脆说我讲普通话人们听不大懂，但是能说我汉语口语不好吗？甚至说我汉 语不好吗？显然不行。所以，不能把“口音”和“口语”混为一谈。\n中国人之所以特别强调英语的发音，花了很多的精力试图让自己发出很标准 的英语发音，我觉得有一个原因，就是中国人特别歧视那些讲话有口音的人，喜 欢嘲笑他们。在国内歧视那些讲普通话有口音的人，到了国外又去歧视那些讲英 语有口音的人，结果导致好多中国人因为怕暴露出自己说英语有口音被笑话，不 敢讲英语了。这就形成了一个恶性循环，越是不敢讲越是不会讲。\n美国人一般是不会嘲笑别人讲话有口音的，那被认为是很不礼貌的。而且， 美国人认为，外国人讲英语有口音是很自然的事，不会因此就瞧不起你，就去笑 话你。只要不影响交流，别人能够听得懂你在说什么，那么，讲英语有口音不是 一件多么可怕或者可笑的事。\n别人听不懂你讲英语，不是因为你有口音，而是因为别的问题。比如说，发 音是乱的，不同的音分不清，那么就很容易让人听不明白你在说什么。或者把重 音搞错了，别人也很难猜出你究竟是在说哪一个词。更关键的是，词汇量太少， 表达也不行，这样讲出来的是“破碎英语”，别人当然就很难明白你在讲什么了。 如果因为担心有口音，不会讲也不敢讲，那么就会导致越来越不会讲。所以， “说”最关键的就是要经常说。\n但是，如果没有说英语的环境，不在国外生活，或者虽然在国外生活但是是 在中国人的圈子里面混，没有说英语的机会，那么，发音再标准、记的单词再多、 掌握的表达方法再多，真正碰到说的机会的时候就都紧张地用不上。没有说的环 境，是很难提高说的能力的。但是，反过来，如果没有说的环境，说的能力也就 用不上，那么也就没有必要花很大的精力在“说”上面了。\n再说“听”。提高英语的听力要比提高说的能力容易多了，多听就可以了。 多听要比多说更容易有机会，特别是在互联网时代，要找到各种英语听力材料太 容易了。我读书的时候就没这么幸运。我上大学本科的时候，基本上没有什么听 力材料，只能是每天拿着一个短波收音机去收听美国之音，还是被干扰的，听上 去很模糊。后来有磁带录音机，英语磁带很少，只是录一些托福考试的听力资料。\n因为我在出国之前没有接触过多少英语听力材料，初到美国留学面临的一个 最大的问题就是听力的问题。上课的时候听不大懂老师在讲什么，甚至在日常的 交流中有时候也听不明白对方在说什么。我听从了一个老生的建议，拿到第一笔 奖学金马上就去买电视机，然后每天坚持看几个小时的电视，主要是看美国的情 景喜剧，把字幕调出来。这样坚持看了几个月，听基本上就没问题了。\n现在英语听力材料太容易找了，美剧、好莱坞的大片，网上还有各种美国视 频节目、音频节目。看英语节目不要看中文的字幕。翻译有可能是非常糟糕的， 即使翻译得很准确，看中文字幕对听力的提高也没有好处。看字幕要看英语的， 对照英语字幕听，对听力很有帮助。\n再来说“写”。在说、听、读、写中，写是最难的。我看过国内一些著名翻 译家、英语教授写的英文文章，写得也不怎么样。其实这不奇怪，一般的美国人 也写不好英文的文章，就像一般的中国人写不好中文的文章，是一个道理。所以， 对于写大家没必要把标准定得太高。问题是，很多人即使学了一辈子的英语，不 要说写英语文章了，连一个短信、一封简单的电子邮件都写不好，写的时候担心 有拼写错误、用词不当、语法错误……越担心越写不出来。最主要的原因还是因 为写得太少。\n怎么增加写英语的机会呢？我刚到美国留学时，有一段时间经常玩网游。那 个时候的网游只有英文的，而且是文字的网游，所谓“泥巴”（MUD)。大家玩网 游最主要的还不是玩，而是在那儿聊天。聊天有一个好处，大家不会在乎你的拼 写错误或语法错误，讲究的是速度要快。这对英文的表达是一个很好的训练，想 说什么敲出来，马上就发送出去，锻炼反应的能力。这样练一段时间，对提高英 语的“写”很有好处，不是写作水平，而是简单的写，不要担心拼写或者是语法， 只要能写出来，就很好。美国小学对小学生的拼写错误、语法错误也很不在乎， 就是为了鼓励学生写。这跟中国是不一样的，中国小学对错别字看得非常严重， 反而打击了学生写的积极性。现在也可以到网上去跟美国人、英国人聊天，比我 们那个时候平台多多了。\n最后说一说“读”。四种能力当中，我认为“读”是最重要的，对每一个人 来说都是非常重要的，是大部分人学英语的最主要的目的和最主要的用处。很多 人一辈子都没有和外国人交流的机会，说、听、写有可能都用不上。但是，如果 英语阅读能力非常强，那么就能够掌握很多英语信息，这些信息会是比较准确、 全面的信息。\n读的能力是最重要的，而读的能力也是最容易提高、最容易自学的。听、说、 写也许还得去培训培训，读是用不着的，因为只要多读就可以了。每天都读英语 的文章，特别是新闻的报道，就是一种很好的自我训练。我的英语阅读能力就是 通过阅读《纽约时报》得来的。我刚到美国留学的时候，我们系的阅览室订有 《纽约时报》，我每天都去读。我们生化的实验往往要等那么两三个小时，这个 时候我就跑到阅览室看报纸，主要就是看《纽约时报》。而且我是什么都看，一 份《纽约时报》从第一页看到最后一页，甚至包括里面的广告也看，主要就是为 了扩大知识面，增加词汇量。\n刚开始会有很多单词不认识，要经常查字典。查起来很麻烦，因为那个阅览 室里面的字典是一本《韦氏大辞典》，找一个单词翻半天。现在查字典就很容易 了，因为读的都是电子版的，电脑、手机里头都有字典，一点单词就出来解释， 不会让阅读中断。生词很多，读的时候老是中断也是挺烦的。一开始可能生词比 较多，像我那个时候读《纽约时报》一开始也经常要翻字典。随着阅读量的增加， 积累下来，要查字典的次数就越来越少了。到最后，我读《纽约时报》完全就不 用查字典了，甚至能发现它里面的拼写错误、语法错误。\n对一般的人来说，一开始就去阅读《纽约时报》可能门槛太高了一些。如果 有中级的英语水平，首先可以阅读比《纽约时报》更浅显一点的新闻报道，例如 美国电视新闻报道的文字稿，就比报纸的文章要简单多了，也比较短。第二步再 去读《纽约时报》这样的报纸。第三步可以读杂志的文章，像《时代周刊》《纽 约客》《大西洋月刊》《国家地理》。杂志文章篇幅比较长，写法比较文雅，读 起来会困难一些。最后一步，要读英语的著作，小说或者非虚构的作品都可以。 从学英语的角度来说，读非虚构的作品比读小说要好。读历史、文化、传记或是 科普的著作，对提高阅读能力、扩大知识面，要比阅读小说好得多。\n到后来养成习惯了，每天自然而然地都要看英语的新闻报道、英文书。所以， 学英语其实也没有什么窍门，就是要坚持，持之以恒，每天都看，慢慢地积累， 越看越多，最后英语就变成生活的一部分，再也离不开。\n2021.3.11.录制，2021.8.29.整理\n再谈谈怎样学英语\n·方舟子·\n上次我谈了要怎么样学英语，这个话题引起了不少网友的兴趣，问了我一些 问题，我就再来谈一谈这个话题。\n我上次讲的学英语，指的是已经有一定的英语基础，至少有中国中学英语的 基础，要在这基础上怎么保持、提高。有人问我，零基础的要怎么学英语？\n零基础要学英语比较麻烦，因为必须系统地、从头开始学。以前只能是去上 课，现在有比较方便的方法，可以上网课。也有很多学英语的软件，包括可以下 载到手机里的app。像美国有一款免费的软件叫Duolingo，就是帮助学各种各样 外语的，很适合于初学者。我不知道中国有没有类似的软件，我想应该有，有的 可能是要收费的。我不了解中国这方面的市场，大家可以去探听一下，看哪一个 口碑比较好可以去试一试。\n有一些人说，现在已经有谷歌翻译，做得很好，没必要去学英语了，到时候 靠机器翻译不就得了吗？还有人说，可以用谷歌翻译来帮助学英语，写中文让它 翻译成英文，然后再改一改，是很不错的方法。\n现在的机器翻译要比以前强多了，但是也没有到过关的程度，我不赞成依赖 或者借助机器翻译来学英语。有不少不懂中文的人看我的推特，我想他们只能靠 机器翻译。推特现在可以自动翻译，我发了推文后有时会试一下，结果发现，能 够比较准确翻译的还是比较少的，大部分翻译都有问题，甚至会把意思搞反。所 以，你是不能依赖机器翻译的。如果你对英语根本就不懂，它翻译错了，甚至意 思翻反了也不知道，就比较麻烦。机器翻译最多就是一个辅助工具，而且很不可 靠。\n有人说，既然英语这么重要，我要坚持学，每天背10个单词，这样长期坚持 下去就可以认识很多很多的英语单词。\n背单词并不是一种很好的学习英语的方法。第一，背单词很枯燥，很难坚持 下去，下再大的决心，过一阶段可能慢慢地就放弃了。第二，单词靠死记硬背是 记不住的，过一段时间前面记的那些单词慢慢地也就忘了。第三，背单词脱离了 语境，对单词的理解是不准确的。背单词是应付考试的一个办法，要考试了，平 时积累不够，没办法，只好硬背一大堆单词。我以前考GRE也背过单词，有所谓 GRE词汇，平时中级程度的阅读很少遇到，不背不行。应付了GRE之后，我就再也 没有背过单词了。\n认识单词应该靠阅读，读文章、读书时见到不认识的单词，去查字典来了解 它的意思。因为是在阅读的时候遇到的，有上下文关系，容易理解单词的意思， 也更容易把它记住。即使一次记不住，第二次、第三次再碰到，总会把它记住的。 见得多了自然而然地也就记住了。这才是记单词的好方法，而不要去死记硬背。\n还有人介绍他提高英语听力的经验，说是经常听托福听力资料，做听力习题， 对听力的提高很快。\n我也不认为这是学英语的好方法。跟背单词一样，这也是非常枯燥的，很难 坚持。其次，那是应付考试用的，并不是在听活生生的、现实的英语。提高英语 听力的更好办法，是看美剧、网上的视频，或者是听英语音频、广播，一方面让 人觉得有趣味，另一方面听的是现实生活中的英语。\n为了应付考试和为了使用的英语是不一样的。要看学英语的目的是什么，是 为了在英语考试中得高分，还是为了真正能够使用这门语言？如果学习的目的不 一样，学习的方法也就不一样。能够应付考试，不等于掌握了这门语言。我以前 托福和GRE的分数都非常高，GRE还是当年中国科大的最高分。但是刚到美国那段 时间发现，考试考得再好也没用，听不懂，看文章、读报纸一堆的新词。所以， 不要把应付考试当成是学语言的正常方法。\n我再说一说上次提到的发音问题。我并不是说英语的发音不重要，如果能讲 发音很标准的英语当然很好。我只是说，第一，不要花很多的精力在发音上面， 不要过分地强调发音的重要性，因为对大部分人来说，是很难发很标准的英语的。 如果不是从小在英美国家长大的，要说没有口音的英语几乎是不可能的。即使是 那些自以为英语发音很标准、喜欢嘲笑其他中国人讲英语有口音的人，你让他讲 英语其实也是有口音的，说不定口音还很重。\n其次，我建议大家不要把发音作为一种负担，因为害怕有口音被人嘲笑，不 敢去讲英语，或者讲的时候很犹豫，老是怕发音不准确，讲得结结巴巴。讲英语 更重要的是讲得流利，有足够的词汇量，知道怎么样表达，能讲完整的句子，而 不是只会背几个句子，就是说，要有真正的对话能力。能不能讲得很流畅是更重 要的。美国人称赞一个外国人英语讲得好，是称赞他讲得流利，而不是称赞他发 音很完美。发音标准和口语好、讲得流利是两回事。\n我再说一下写。上次我说了，没必要太在乎拼写错误、语法错误。我不是说 拼写和语法不重要，而是说一开始别太在乎，因此搞得不敢去写。这是从鼓励你 写的角度来说的。我举了美国小学生的例子，有人说，那是美国公立学校的老师 偷懒，懒得去改小学生当中的拼写错误、语法错误，私立学校就不会这样。美国 私立学校我没接触过，不知道是怎样的，但美国公立学校也有很不错的，也有很 负责任的老师，像我女儿的小学。而且，一个老师才教20多个学生，怎么可能懒 得去改这些学生的作业呢？这个问题我曾经跟我女儿的老师探讨过，作文里语法 错误、拼写错误很不少，为什么不给她改？要不要注意这方面的问题？老师说， 她以后阅读多了、写得多了，自然而然地也就好了。的确她后来也就好了。所以， 一开始没必要太在乎这个问题。到一定的程度，当然也要注意语法、拼写。但是 那是下一步的，第一步是鼓励写作，没必要因为担心语法问题、拼写问题而不敢 动笔。\n学语言最主要的是要有语言的环境。在中国没有听、说、写英语的环境，也 难以自己营造出这样的环境，这三方面的能力要提高是很难的。听可能还好一点， 经常看美剧什么的不就行了吗？实际上那跟日常生活当中的听还是有区别的。但 是，读就不一样了，你是可以给自己营造一个读的环境的。现在比我学英语时候 好多了，有了互联网，可以接触到大量的英文文献，只要你愿意，随时可以处于 阅读英语的环境当中。所以，在英语的四方面能力当中，读是最容易提高的，最 容易保持的，也是最有用的。\n2021.3.13.录制，2021.8.30.整理 http://xys.org/xys/netters/Fang-Zhouzi/humanities/english4.txt\n我是中国人，更要学英语\n ·方舟子·  北京开“两会”有一个好处，就是能让大家欣赏到人大代表、政协委员各种 各样的奇谈怪论。例如，今年“两会”有一个政协委员建议，取消义务教育阶段 英语作为必修课，把它改成选修课，高考也不要考英语了。\n这个建议其实不新，以前也有人提出过。两年前有一个人大代表也是在开 “两会”期间提出过类似的建议。后来让大家翻出来，那个人大代表自己就在办 双语幼儿园和英语培训班，他提出那样的建议是不是有私心？是不是想让学生在 学校学不到英语，都去校外上类似他办的那种英语培训班呢？\n不只是人大代表、政协委员要求减少甚至取消英语教学，有一些作家、普通 的网民也都提出过类似的主张。其中有一部分是打着“爱国主义”、“民族主义” 旗号的“爱国贼”。他们认为，花那么多时间要求大家都学英语，是崇洋媚外： 我们是中国人，首先要学好语文，学好国学，干嘛去学英语？这种论调其实也不 新，上一次文革时就流传过一个顺口溜：“我是中国人，何必学外文？不学ABC， 照当接班人。”\n反对学英语的人未必都有什么私心或是打民族主义招牌的“爱国贼”，更多 的人可能是觉得，学英语对大数人来说没什么用，不当翻译学英语干什么呢？一 开始我提到的那个政协委员，他反对把英语设为必修课的理由就是，大学毕业生 只有10%的人会用到英语，所以对大多数人来说，让他们从小就学英语是在浪费 时间。\n我不知道他这个数据是从哪来的，姑且同意他这个说法，只有10%的人大学 毕业、工作了会用到英语，那么按他这个理由，我们也不应该要求在中小学学数 理化了，因为工作了以后能够用到数理化的那一部分人肯定比要用到英语的人数 少得多，对大部分人来说，的确是用不到那些从中学学来的数学的，会数数、算 算四则运算就够了，高深点的数学，以及物理、化学都用不到的。崔永元连氯化 钠是食盐都不知道，中学化学没学好也不影响他当著名主持人，化学对他来说一 点用也没有。\n对绝大多数人来说，的确数理化是没啥用的。但是，基础教育属于通才教育， 并不是职业培训。我们并不知道以后中小学生会去干什么，我们只能是尽量地教 给他们我们认为很有必要的、很有用的知识。我们不能从小就把学生的前途给定 得死死的，认为他们长大以后肯定不会用到英语、数理化，所以就不教了，要学 自己去自学吧。等到以后他们长大、工作了，万一需要用到英语、数理化，再重 新学就很难了。古人都知道“书到用时方恨少”，现代的人却不知道，也许需要 用到英语、数理化的时候才“恨少”呢？\n英语是一种国际语言，也可以说是当今世界唯一的国际语言，所以学英语就 显得特别重要。尤其是对于搞科研的人来说，是绝对离不开英语的。世界上所有 上档次的学术期刊，全部都是英语的期刊。不管是哪一个国家办的，不只是英美 这些以英语为母语的国家办的，其他国家办的科技期刊，好一点的都是英文期刊。 以其他语言发表的论文，基本上其他国家的人都不会去看的。所以稍微上一点档 次的论文也都是用英文发表的。如果要搞科研却不懂英语，看不懂英文论文，就 没法及时了解到本领域的最新进展，还怎么做科研呢？也没法把自己的研究结果 写成英文论文发到国际期刊上跟国际同行们交流。\n不懂英语或者英语很不好，没法跟人交流，甚至连参加真正的国际学术会议 都有问题。我一直在批的“赤脚医生院士”李兰娟，几个月前参加了一次肝方面 的国际学术会议。因为是疫情期间，所以这个国际会议只能在网上开。轮到她做 报告，她就嗑嗑巴巴地念英文PPT，因为国际会议只能使用英文。PPT有文字显示 （大概是手下人替她写的），倒是不影响别人理解她的意思，但是她嗑嗑巴巴地 念完了PPT以后，会议主持人问了她一个问题，她听不懂，假装断线跑掉了。这 不仅很尴尬，而且很下作，就像跟人在网上下棋要输了，就假装断线跑掉了一样， 相当于认输了。\n有人也许会说，反正我数理化学得也不好，以后不会去做科研，英语对我来 说是不是就没啥用，也就不用学英语了？也不是的。现在是全球化时代，中外交 流越来越频繁，出国旅游、做外贸生意的人越来越多。因为英语是国际语言，在 出国旅游、做外贸生意的时候，如果自己懂英语，不需要依赖翻译，就很方便。 很多人正是等到自己年纪大了出国旅游，或者突然之间有了一个做外贸的机会， 才很痛惜自己以前没有好好地学英语，英语不行。以前他哪会想到自己以后还有 可能出国旅游、做外贸呢？这时候真的是觉得“书到用时方恨少”。全球化的趋 势是不可阻挡的，我们的下一代跟外国的交流会越来越多，英语对他们的用处也 就会越来越大。\n你即使这辈子根本就没想到要出国、没想到要做外贸去跟外国人打交道、一 辈子就待在中国，那么如果懂英语也是很有好处的。其中的一个好处是，能够让 你的生活变得更丰富多彩、更有趣味。因为英语的国际影响力是任何别的语言都 比不上的，所以就让英语文化变成了最强势的文化。全世界最有才的人都在用英 语创作，最好听的歌、最好看的书、最好看的电影电视基本上都是英语的。\n我小时候觉得很好听的一些中文歌曲，出国留学后才发现，原来那些歌曲的 曲子都是从美国很多年前的流行音乐抄过去的。电影、电视也一样，你是觉得好 莱坞的大片、美剧好看，还是中国在横店拍的那些电视、电影好看？这是不言而 喻的。如果懂英语，那么就能直接地欣赏到原作，而不需要等别人的翻译。更何 况，别人的翻译质量未必是可靠的。实际上，中国的翻译，不管是书的翻译还是 影视的翻译，都存在着很大的问题。如果不懂英语，被这些劣质的翻译误导了， 自己都不知道。更何况，中国现在控制越来越严，包括在翻译这方面也控制得越 来越严，最近“人人影视翻译组”的人都被抓、被重判了。以后想通过看翻译这 条途径就越来越难了。但是，如果能直接看原作，那难度就小多了。\n有一些人可能觉得，那无所谓，不看英文书、不看英文影视、不听英文歌曲 也没有什么大不了的嘛。那你总要上网吧？总要看新闻吧？总要查资料吧？你如 果懂英语，那么你掌握的信息就会更全面、更准确。英语是实际上的国际语言， 世界各国的人都在用，这样就让英语的资料是最丰富、最全面的，没有哪一个语 言能够比得上。你到网上去搜，那些最有价值的资料基本上都是英语的。就说医 学资料吧，在百度搜中文的医学资料，你看到的大部分都是广告，说不定还是莆 田系医院的广告，那你不就受骗上当了吗？即使是中国官方机构的网站提供的医 学资料，也不一定是可靠的。但是，如果懂英语，你能够直接看英文的资料，那 么就能够直接到国际权威机构网站去找到最可靠的资料。\n不仅科学、医学方面的英语资料是最可靠的，关于中国历史、中国时事，英 语方面的资料也是比较可靠的，至少它们让你能够了解另外一种说法。中国国内 关于中国历史、特别是近现代史，以及中国当代时事，这些信息是严加管控的。 所以，你只能听到一个声音。你如果会翻墙，就有机会去听另外一种声音做参考。 但是你如果不懂英文，翻了墙还是只能看到中文资料，而墙外中文资料的可靠程 度未必比墙内的要高，甚至有可能还更差。比如说，你看到的都是法轮功的那些 中文材料，那不是更糟糕吗？刚出了虎口又跑进了狼窝。但是，你如果懂英文， 那么就能够看到至少是相对比较客观、比较可靠的关于中国历史、中国政治、中 国时事的资料。即使你不完全相信它们，至少知道了另外一种声音，能够跟你在 国内听到的声音做比较，才能有一个比较靠谱的结论。\n所以，懂英语不仅能够帮助你了解世界，还能够帮助你了解中国，相当于让 你多了一双眼睛。我想这也是为什么有很多人反对学英语的一个原因，就是怕大 家会多出一双能够有助于明辨是非的慧眼。\n","date":"2022-04-10","permalink":"/post/%E6%88%91%E4%BB%AC%E5%BA%94%E8%AF%A5%E6%80%8E%E6%A0%B7%E5%AD%A6%E8%8B%B1%E8%AF%AD/","tags":["Thought"],"title":"我们应该怎样学英语"}]